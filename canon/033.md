


Skip navigation
Search




9+

Avatar image



0:00 / 9:10:02


ACT & MFPS 17 June AM

ACT & MFPS 2024
5 subscribers

Subscribed


1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9



Share

Downloaded


1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9


1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9
0
1
2
3
4
5
6
7
8
9

 views
Streamed live on Jun 17, 2024
Transcript
Follow along using the transcript.


Show transcript

ACT & MFPS 2024
5 subscribers
Videos
About
0 Comments
mooncop
Add a comment...
Transcript


0:17
oh hi hi are you giving the to now Sor are you giving this to yeah need to already like start
0:26
something up or just not my laptop so I got this long time we can try we can try yeah I
0:35
think no no I'm just that's exactly what I'm doing so it's perf
1:24
yeah do you want to go
1:34
maybe I'll ask you to T the AIO
2:26
you and
2:47
you connect could you connect
2:54
again um well yeah I just press here but then
3:08
okay but it's it's mirrored that's normal isn't it because you see
3:18
yourself pict I see
3:40
oh uh yeah
3:49
sorry uh so should I just say something
3:56
when minutes uh yeah sure uhuh
4:28
yeah e
5:14
Hi Sam can you hear me
5:30
hello
6:01
hello
6:28
test
6:58
e e
7:33
all
8:03
and and that way there's also like the the atum I think there's like where you get
8:09
like registration over
8:28
there e
9:17
hi yeah I think a lot of people are at the registration desk um over there I
9:23
don't know if you looking if you were looking for that
9:30
registration L uh yeah that's over there I don't know if you've already been there or I think lots of people are
9:38
are over there see you
10:18
don't
10:28
know e
11:28
so yeah Jo sorry it say I joined the
11:34
meeting so could you hear me or [Music]
11:44
what how do you make it so that people don't
11:50
have be allowed in to join not
11:58
sure e
12:38
yeah
12:44
so sounds yeah I don't see
12:50
try we want to do the first
12:55
yeah just just want to see one screen
13:13
you I just
13:48
the I was
14:24
so just
14:53
so you
15:28
uh physics but
15:57
more e
16:52
you [Music]
17:08
but here
17:40
oh
18:27
you
18:57
like
19:16
PR
19:39
people can just come in you can
20:05
right all everyone thank you for coming to ATP 2024 uh collocated with mfbs
20:11
later in the week um so I just want to you know there's a there's a lot of things to go around here that make this
20:17
possible I want to especially thank Sam and all the local organizers for for getting this all together um everyone
20:22
for submitting we have a lot of really great uh great talks to see and we're going to have setings later on uh and
20:28
thank you also to everyone who reviewed for us because we had it was a lot of work so um uh yeah I don't have anything
20:35
to say about where the bathrooms are but you know if you look around you'll find them and although it's a big maze don't go downstairs um and yeah so I think
20:42
we're gonna get started uh off with uh Ruben vanell talking about um
20:48
convergence of marals uh enrich to dagger category so take it away thank you
21:00
uh yeah so hello everyone um yeah I'm going to talk about Martin
21:06
Gales this is a joint work with Paulo um and in particular I'm going to talk
21:14
about the convergence of Martin GES we're going to use some category Theory to to do that namely these enriched
21:22
D um so
21:28
the plan is to First say something about Mark GS what are marketing gills what's intuition um and then we're going to do
21:35
the category Theory um so we're going to focus on a category called per and a
21:40
functor LM and then we're going to look at all kinds of different structures and properties that these uh two things have
21:48
and from that then we can get a mark convergence
21:56
results okay so uh for Mar also we needs conditional
22:02
expectation um so I'll first tell you what conditional expectation is um so if
22:08
I have a probability space so I set with a sigma algebra a and a probability measure on the measurable space and if
22:17
you have a subset uh a sub Sigma sub algebra b of a
22:23
uh then we can look at a random variable on X that is a measurable so integral
22:31
random variable uh and then we say that the conditional
22:36
expectation of that random variable F that is a measurable is a b me integral
22:42
map now which we use this notation for so it's again a map from X to R uh that
22:48
is being measurable now such that integrating F or integrating the conditional expectation over subsets in
22:55
our smaller Sigma algebra is the same
23:00
um so yeah and the inter sort of is when this B is is a sub algebra of a then
23:08
your conditional expectation can take less different values so you sort of integrate certain things partially
23:15
integrating certain things to get your conditional expectation
23:23
um so an example of this if we have a
23:28
space yeah just this space but the B measure may be normalized um and we at
23:34
sign function uh that's definitely measurable uh if we then look at this
23:40
sub Sigma algebra which is these four subsets uh then our f is of course this
23:46
Red's function and the conditional expectation is this blue function so you see we sort of yeah we have integrated
23:52
it under here and then normalize and here as well
23:59
uh and we can take yeah less values because we can only we only have four
24:04
different uh measurable substance uh okay so that's conditional
24:11
expectation so we need that to say something marting Gales or well to to
24:16
marting Gales um so what's the marting Gale so we start with a directed procet
24:22
usually natural numbers or maybe the real line positive real line um and
24:28
again we fix probability space and now instead of one Sigma subalgebra we take
24:34
a bunch of them uh uh and that in an increasing way so they they Ed when i1 is smaller than I2
24:42
then B i1 isset of bi2 um and yeah all these data combined
24:50
is called a filter prop space and this is then called the fil um
25:00
so yeah what's the marting Gale then uh so we have a collection of uh
25:08
random variables fi from X to R they're all integrable and fi is um measurable with
25:15
respect to bi uh and then this is the marting G
25:21
condition that if we have an FJ and we take a conditional expectation
25:26
with respect to an earlier Sigma algebra then we get fi
25:37
um uh yeah so and then we can also talk about backwards filter produ backwards
25:43
marting GS if we have a decreasing net of Sigma
25:50
Al um okay so what does that mean so this is an example that Pao made uh
25:56
which I think uh visualizes Mark deals very well so
26:03
uh if every picture is a random variable sort of then this is the conditional
26:08
expectation of so you can sort of interpret that as like every four pixels
26:15
we take the average color and that comes this this bigger pixel
26:21
um so yeah the operation of condition expectation
26:27
in this example means U pixelating the picture
26:33
um and so since the sigma algebra uh increase we always get more information
26:40
and we can get more values or more different values so we get a finer and finer picture um so we can yeah refine
26:47
the picture U more more as the sigma algebra
26:53
increase um so then uh yeah ideally what we would
26:59
like is that there's like an ultimate refinement such that all these pictures in the Mar in the marting Gale are yeah
27:07
pixelated version of these one bit refinements um and that's what's the
27:12
Martin Hill converion theerum says that we can usually do that under the right
27:19
conditions um so what are these conditions so if you have a marking G uh that's Ln bounded and it's bigger
27:27
than two um on some filter probability space then
27:32
there exists an F uh in Ln XA such that all these FIS
27:40
are indeed a refinement of this F and even more the FIS converge in
27:51
LM um and then again there's a similar theorem for backwards mark
27:58
yes um so yeah and the interesting thing here is that we have uh sort of two sort
28:04
of limits going on where we have this refinement is common refinement which is
28:10
yeah reminds us of a categorical limit maybe um that's common refinements but then we also have these topological
28:16
limits uh so we want to yeah have sort of uh
28:22
categorical uh framework that covers these the both these these kind of
28:27
limits um and that's sort of the the goal of our
28:36
paper um okay let's then now talk about the
28:42
category Theory maybe um so yeah we're going to Define
28:50
this category C so we have two probability spaces X
28:56
and Y um and then we see that a mark of Kernel
29:02
is measure preserving if this is satisfyed so um in the case that your K
29:09
is a determined uh deterministic Mark of Kernel then this is really saying the
29:14
push forward of P along your case is Q uh maybe another way of interpreting
29:20
this is if we have a one that points to P in X and points to Y in Q then that
29:27
triangle commutes what meas
29:33
means um and then we also have this almost surely equal definition for Mark
29:40
of kernels and that is well they're almost surely equal for
29:46
every um yeah and then the the sets on which
29:53
actually equal depends and so oops
30:02
um okay then what is this category per so the objects are essentially standard
30:07
barel probability spaces so standard barel space is uh when
30:13
it's uh yeah when it comes from a a poish
30:19
space a separable poish space um and then probability space and then this
30:25
essentially means that it's yeah a probability space that is
30:32
isomorphic to such a standard probability
30:38
um so those are the
30:43
objects then the morphisms are equivalence classes of almost equal measure preserving Mark of kernels so we
30:50
saw the mark of kernels they have to pres the measures with probability spaces and then we take ctions or
30:59
equivalence classes when of almost Sur equal
31:05
hers and then the composition is given by Mark of Bal composition and
31:16
I uh then we're also going to use this category of B spaces um b space and
31:23
one I use for
31:32
uh then yeah I used the full sub category of H spaces notation is is that for
31:43
for um okay then this functor Ln so an N number between yeah n sorry bigger than
31:52
one that possibly be infinite as well um and it goes from current op to
31:59
ban uh and on objects it's defined in the the way you would expect I guess
32:05
from the name of the functor so it sends the probability space to the Ln space um so that's yeah random variables
32:13
on X that are have a finite end moment I guess or or have a finite L
32:25
Norms um okay then what about uh morphisms so if we take a mark of
32:31
Kernel so yeah it's really an equivalence class of measure preserving Mark of kernels but abolutely say
32:37
measure preserv Kels um from X to Y then we want to yeah
32:44
Define uh a linear map from Ln y to Ln X so let's take a random variable in Ln Y
32:53
and then we can define a random variable in Ln X in the following way
32:59
so it sends every X to this integral and yeah this is really
33:07
um Mark of Bal composition really if we interpret this F as a deterministic mark
33:12
of Bal um so okay we have turned our F into
33:20
a random variable on X um so that yeah gives us this
33:25
operation one is linear Maps that's um okay so yeah that defines the
33:36
fun um then yeah remark in the case n is two then or l two space is a hit space
33:42
so then this vectors true Hill and yeah I'm also going to when I write L2 I'm
33:48
just going to assume that the qu is
33:55
Hill um okay then we had all these different structures and properties that
34:01
these categories have and the first one is that it has a dager structure um so yeah I'll just remind
34:09
you what the dager cat dagger category means uh so dagger category is a category C together with a funter from C
34:16
up to C such that it's the identity on objects and if you apply the D twice and you get
34:23
the same
34:30
um okay an example of this category of hbert spaces then you can define a
34:36
dagger structure by taking add joints of your linear
34:47
Maps uh okay then the DI structure on this kernel uh have this current
34:53
category so if we have a me measure preserv Cur uh then there's this thing called the
35:01
basan inverse of G so that is a measure serving kernel in the other
35:07
direction um such that for every a we have this property um so what this really means is
35:14
for example if K is the terministic then every Y is being sent
35:20
to the conditional probability on X
35:25
uh given the pre-image of your element y under
35:34
k um so it's not obvious that such a thing always exists uh but then there's
35:39
this nice theem that if you have standard barel spaces even an essentially standard bar spaces then uh
35:47
such a basan inverse always exists and is almost surely unique so in particular it's uh yeah an actual unique morphism
35:55
in um okay so
36:01
that then gives a dag structure by sending K to this equ
36:08
inverse um and to see that this satisfies this this properties of of a
36:13
dagger structure so you just write down the to find property of
36:20
your basion inverse since next Vols for every a and every B that means a have to
36:26
be almost surely equal and therefore have equal
36:38
in um okay
36:44
uh so yeah we also have this uh L2 functor um that goes from one dager
36:50
category to other dager C so then we might ask ourselves does that preserve
36:56
this dager structure um and so yeah it does so you
37:02
can write as this commutative diagram so first applying L2 and then taking the dagger is the same as taking DG and
37:10
apply um so concretely what does that mean so if we have a a measure
37:16
preserving kernel here then uh yeah this commutative diagram means that uh this
37:23
is equal for every F and every G and this is yeah the inner product of our Ln
37:28
space or L2
37:36
space uh and then a particular example so uh yeah if we again take b as a sub
37:42
algebra of a then this map that is the identity on on the sets is is measurable
37:50
and measure preserving of course we sub algebra of a um and we can call that by
37:57
and if you then look at G an indicator function for some B in B then this uh
38:05
equality exactly means that this holds um and then if you remember the
38:11
first slide I showed you this is exactly the defining property of conditional
38:18
expectation uh so that means that yeah pre-composting uh this pi+ with f is
38:26
exactly the condition expectation of effort respect to um which of course yeah I tell you
38:33
this because that's going to be important for the the mark
38:40
later um okay so what have do we have so far uh we have yeah these these
38:48
categories and these functors uh and they have a de structure for current that's given by Bion inversion in h it's
38:56
ad joints and our fun L2 preserves the D
39:04
structure um okay so let's continue with the other properties or other uh yeah
39:11
properties of the categories uh namely the item P what do they look like and
39:16
what can we say about to um okay so I'm first to define a dagger item
39:24
for in our dagger category C has to be not important and then the deer is uh of
39:31
EAS itself and then if we it importance so
39:37
we can talk about splittings and for dagger ports can talk about dagger splittings um dagger splitting of some
39:45
night important like this is uh a pair uh of two
39:51
maps uh such that yeah this composition is the identity and the other composition is are high important and
39:59
then with the property the additional property that they're each other's
40:06
bagger okay so that's
40:12
definitions um okay let's look at the item importance or the item importance
40:18
in these categories um so if you have a hbert space X and dker item import
40:26
e then then that has to be equal to the orthogonal projection on the image of
40:32
this map
40:37
um and then a d spting is given by the inclusion and the the orog projection so
40:44
we see if we compose these then that's exactly this projection map and if you compose them the other way around then
40:54
that's um and there's there's there they are others attributes as well so
41:01
it's um so yeah this this image of e has
41:08
always uh is always closed because it's an item bance um so that means we have
41:15
this Isis every dager item P we can send that to its image that's going to be a
41:20
close of space and then every close sub space um gives us projection map like
41:26
this talk projection because we're working with HRI spaces so for general b spaces that doesn't work anymore um that
41:34
gives us
41:42
that um okay then for um our other category
41:49
of Mark of kernels um so I'm first going to give you an example of of a deer ion port and
41:55
then I'm going to say a bit more about uh about you yeah if all the item
42:03
points can be written in that way so if we have a standard ra probability space
42:09
and sigma sub algebra
42:14
b um then yeah we can look at this map again that I showed you before so the
42:21
identity setwise but then it's measurable the speed sub algebra um then this is a
42:29
dataimport yeah almost by construction um and
42:35
yeah P plus was a conditional expectation operator really so if you
42:42
precompose uh this with with a random variable then that's taking the condition expectation so we can interpret these EBS as sort of condition
42:51
expectation operator um so yeah like if is again a
42:57
map for every Sigma sub algebra we get a dger it importance namely this
43:03
one um and then and yeah then the the
43:08
proposition here is that this is objective so that's not obvious but every deer item P can be written in such
43:15
a way um however it's not necessarily
43:21
injective so we can make it injective to well just identify the the
43:28
the subalgebras are being sent to the to the same item for so
43:41
relation um and then yeah we can also say a bit more about these equivalence
43:47
classes so every equivalence class has the finest element and that's the infar sigma algebra and we use this notation
43:54
for
44:03
okay and then uh an interesting properties about this these Item B that
44:10
um uh yeah these dagger item is that
44:15
there's an order but on these sets of dagger item so we
44:21
say that for two dagger item orts E1 and E2 we say that E1 is smaller than 2 if
44:28
uh if the E1 E2 is equal to
44:35
e um so and that finds a partial
44:42
order um so okay in the case of Hilbert spaces um if we have two leer item
44:50
poorts hbert Space X then well those correspond to to those sub spaces uh and
44:57
then we see that this property uh yeah first projecting on A2 and then projecting on A1 if that Al
45:04
equal to projecting on A1 then that means A1 has to be a Subspace of
45:09
A2 um so we see that this I important order corresponds to um just inclusion
45:17
of your sub spaces um so yeah that means we have
45:24
nice and morph
45:30
uh then for the other category Cur it's a yeah bit more complicated so if you
45:38
have uh so you need to define something first if you have probability space
45:43
again and we have two Sigma sub algebras then we say that B1 is almost
45:49
surely forcer than B2 if for every B1 there is a B2 uh in to such that uh
45:59
symmetric difference so this symmetric difference has measure zero so that
46:04
means you're measure um yeah I cannot really tell the difference between1 and
46:12
B2 and then we rewrite this um so yeah in
46:18
particular if we want is naturual subalgebra B2 then then this is also the
46:23
so this is sort of had up to n set for version of of the usual
46:34
inion okay then we have the following results
46:39
so this holds it and only if we have this sort of inclusion sorry order and
46:47
that's if and only if these invariant Sigma alas are actually
46:55
um and that means that we get an isomorphism as follows so well this was
47:01
just defined on on Sigma sub albas but because of this results we can also
47:06
Define only classes and that's well defined and then yeah because of this we
47:11
get exactly that
47:19
St um okay so we have four sets of these deer item bems so we can maybe also look
47:25
at the the suprema and the infa that exist and what they look like and in
47:31
particular we're going to focus on directed Supra and infa so first for our H
47:40
spaces uh so we have n space X and we have a CO filtered collection of dagger
47:46
item poons um then that corresponds to a collection
47:53
of close sub spaces or by the just the inclusion and then the supremum is the
47:59
union closed as again a closed uh Subspace of
48:06
X so that means that PR of our D ion p is exactly this
48:14
projection map on this
48:21
C uh and we similar we have something for the the infat uh yeah here because
48:27
the intersection is already closed we don't have to anything else is just
48:36
intersection um but for the yeah direct Supreme in inur it's
48:43
a bit more complicated um so okay yeah fix again probability
48:49
space and we're going to look at a coiled collection of Sigma sub algebras but ordered by the inclusion so that's
48:56
in particular ordered by the I po order but we're just going to look at the ones that
49:02
are that come from an actual inclusion and then we can say that the supremum is
49:07
well the thing that you would hope it would be right so it's it corresponds to the the supremum of your Sigma so aless
49:16
then we need to Union and update the smallest Sigma alra
49:22
that um so and that works for all these uh
49:27
collections uh however for the infa it's a bit more complicated so for a
49:33
decreasing sequence we can still uh have our dual results but for a general
49:40
collection uh we we cannot just simply copy this so then we have to take the union of of these
49:46
finer um Sigma algebra so the finest one in the equ glass and then they infection
49:52
of those and you see that the thing that you might think could be is actually
49:57
smaller
50:04
inside um yeah so another characterization of
50:11
this infa and Supra is uh uh as follows so suppose we have fil
50:19
collection of dager item bons that all have dagger splitting like this with another one e with another dagger
50:26
splitting then we can say that um this e is a
50:31
supremum of all these e if and only if uh yeah
50:37
this diagram is a limited
50:45
diagram um and yeah that's something that we're going to use now um but first
50:51
yeah L2 uh yeah we can ask does that preserve the this this well does preserve the
50:57
order but does it preserve this direct suprema and that's maybe not always the case but in our case that is the case
51:07
um uh and then combining that with the previous result that means if L2
51:12
preserves Supreme that corresponds with saying that L2 preserves the limit of
51:18
these diams um and to the go
51:24
in um okay so we're going to apply that now so if we have an increasing sequence
51:30
of Sigma sub algebras we a join B then yeah we can write that saying this
51:39
diagram has this limits where again these these maps are all always yeah
51:46
iten on on the the sets and they're measurable because they're order to app
51:53
um then by the two previous results combined really we can say that L2 preserves the limit of these of this
52:02
U this diagram um so that means we have that
52:08
this has to be the co limit of all these L2 spaces
52:13
um but we also knew that L2 was um a dagger uh functor um so we can first
52:23
apply the dagger fun uh sorry the D funter and
52:28
then because L2 preserves the dger structure well the ASX and the plus
52:35
switch uh and we can say that this is the limit of this diagram um and remember these were
52:42
always sort of condition expectation operators right um okay so that allows us to say
52:49
something about marting gills um Nam the first part of the marting Gill convergence theorem um that there's this
52:57
sort of U common refinement of of all the
53:03
elements in Mar Gale
53:08
um uh and the proof is in the following way so our L2 bounded Mar hill we can
53:14
represent that as a cone over the diagram here um and here it's important
53:20
that it's L2 bounded because they have to be one Li shets um
53:28
okay so we have a cone over the diagram this is the limit of the diagram so that means there's a unique uh F such that
53:35
these are all refiners um of this F because these are functional
53:43
expectation um okay so that's the first parts of uh our marting
53:50
converence um okay in a similar way we can to allly and then with a backwards
53:57
marting convergence result um but then because then we're working with this infa we have to be a bit more careful if
54:03
it's not a sequence if it's a general um all filter it's
54:09
U uh uh net of of sign of algebras that
54:17
this B then might be this uh the INF in the item
54:22
order so maybe a bit bigger than what
54:27
May with exts um okay so what do we have so far
54:34
we had that yeah we sort of represented it importance in these
54:41
categories in current that was equivalence classes of Sigma Sub algebras in help was this closed sub
54:48
spaces or that's that's known that are these close sub spaces the order was this sort of uh
54:57
inclusion up to n sets kind of order and in the hillbrick
55:03
spaces case it was just the actual inclusion um the direct
55:08
Supra were the ones that you would expect the ones that you would call for the infa for hbert spes was also the one
55:16
that you would expect but then for uh incur uh it was more
55:22
compated and L2 preserved all this Str Serv the order and these kind
55:28
of INF
55:34
and um okay so and that gave us this all this information gave us like a first
55:41
part of the mark uh but now we want to also say something about the top logical liit so
55:47
that's where we going to need these enrichments
55:53
um so we're going to enrich over top spaces so then we probably yeah we want our cat
56:00
of topological spaces to have a mon structure um which uh you can Define in the
56:09
following way if you have two topological spaces you can find a tensor product uh by giving this yeah the
56:16
cartisian product of the sets the following topology the topology generated by all
56:23
these Maps so final topology that makes all these M
56:30
continuous um and then as internal home you can fli at a set of all continuous
56:36
maps from X to Y you can give that to point by
56:43
verions um okay and that forms a monal Clos structure
56:57
uh okay then of course you wonder the the categories that we've been working with to be enriched over top so for a BX
57:05
baces um if you have two BX spaces X and Y we can look at the sets one lip linear
57:12
maps from X to Y and then we can Define topology on that sort of variation of this strong Opera
57:19
topology um and that's defined in the following way so if we have a net one
57:27
uh one Li linear Maps we say that it converges to F if and only if U
57:33
converges Point Y3 so for every X you have that FY X converts to FX in y
57:47
so um and that then that makes B enriched and in a similar way you can
57:54
enrich over top
58:00
okay then our other category Cur um have can
58:07
two probability spaces essentially standard probability spes spaces then we
58:13
Define topology in the following way we say that a net of measure preserving kernels converges to a measure conser k
58:22
k if and only if yeah this integral goes to zero so there's sort of an L1 aspect
58:27
to it and a pointwise aspect so we want for every B that uh this uh K Lambda V
58:36
converges to KB in in L1 um yeah and okay that defines
58:45
topology on on our set of of measure reserving Kels uh and we call it one side of
58:52
topology and and whiches burn over top
59:02
okay so we have this topological space now of maps from from X to
59:08
X we can look at the Subspace of item Po and yeah what topological things we
59:15
can say about that sub so it's flow Subspace and the convergence of these uh
59:23
leer leer item bons that we could see uh represents like this uh converges to e
59:32
if and only if uh forever have we have this kind ofces in L1
59:38
so you might maybe already recognize some like Mar Gale convergence propy
59:48
here um okay so we have that all our countries are enriched over top in a
59:54
certain way um then we want our L sorry our forit L1 also to be enriched
1:00:00
over which is the case but there's even a stronger results
1:00:05
um which is yeah quite important because we're going to use that later so if we
1:00:11
have a net of Mark of Kels that converges to certain Mark of Kel uh that implies that
1:00:20
the yeah applying L Ln on on the Nets converges to Ln of K uh but it's also
1:00:27
the other way around so if lnk Lambda converts to lnk then the original kernels converge
1:00:36
to K so it's an Ean statement which is important
1:00:45
later um okay so back to the overview we had an enrichments for K it
1:00:50
was with this one-sided topology for Hil it was this variation of this strong operator topology really Point wise
1:00:57
topology um for b as well and our L2 Ln funs are also
1:01:08
used uh okay now we're going to Define like certain property that
1:01:15
certain dagger categories can have this import Ley property um and it's really yeah based
1:01:22
on on this or in R if you have an increasing bar SE then that conver
1:01:28
topre so in a enriched dger category in a topologically ened dager category uh
1:01:35
that translates to if you have an increasing net of dger item bance that
1:01:40
has a supremum then it also converts top loic to the Supreme that is the
1:01:48
it and then also the
1:01:54
decreasing um yeah if it converges in order then it converges in in
1:02:00
to that's the property that we
1:02:06
after um and then okay categ H has this property and there's bit of
1:02:13
work to prove that it has this property but it has that property and if you know that this has this item po L property
1:02:20
then for current it becomes uh not too yeah quite easy to prove that it also
1:02:27
has this property um so to prove that let's look at an increase in net of dger item
1:02:34
importance supremum then because our l n funter
1:02:42
sorry L2 funter preserved the order structure and the supremum we can say that
1:02:49
these also an increasing Neta has a supremum then because in h we have this
1:02:56
L property uh that implies that it converges
1:03:02
topologically um and then yeah because of the
1:03:09
sorry this results if we have that these converts to this one then we can go back
1:03:15
so these converts to this um so that means
1:03:22
uh yeah these converge to supremum so
1:03:28
um and then the decreasing one is is
1:03:33
J so okay that means both these cies have this Le property B doesn't have
1:03:41
it yeah we don't tax
1:03:47
so um okay so then the the second part of the Martin
1:03:52
Gill convergence theorem is uh yeah filter probability space and we
1:03:58
have a certain F in Ln
1:04:04
XB U so yeah where B is the sigma generated by the union of the B uh then
1:04:12
we know that this converts to this in LM so the proof for that is we already saw
1:04:19
that the uh supremum of the EB I is EB uh
1:04:26
we have this item P Ley property in current so that means uh the item Port
1:04:33
converge topolog as well and then we our l two
1:04:40
fun or sorry our Ln fun and which these convert to these and yeah because these
1:04:47
are umal expectation operators that means
1:04:53
for uh yeah that this holds for
1:04:59
because yeah this is the pre composition of this one with f andus yeah who defines the
1:05:08
topology uh of these maps to be
1:05:13
exactly um okay again there's a backwards version where again we have to
1:05:18
be careful with what is actually is so this is the inum in the item poent order
1:05:25
and just
1:05:30
intersection okay so then the conclusion we
1:05:37
um yeah have this Martingale convergence theem so we already sets that for L L2
1:05:44
balance Martin Gale um we can find a common refinements such that we can
1:05:50
represent our Mar Gale like this and then by the previous slides we know that
1:05:56
markting bills of this form uh converge uh topologically as well so
1:06:01
that gives us the second Parts here [Music]
1:06:08
um yeah and then in the our paper we also have sort of a generalization to factor value of marting bills so that's
1:06:15
why defining your LM function slightly differ um but you can sort of buy the
1:06:21
same strategy to say something but talk yes uh yeah know
1:06:42
everything right let's take questions
1:07:05
sorry can you see that
1:07:13
again um yeah because otherwise it becomes bit more complicated and in well
1:07:19
the application we only really needed the actual inclusions and not this uh almost sure inclusion
1:07:27
but yeah yeah I guess you can generalize it and then you get maybe even stronger marting convergence result but yeah
1:07:34
things here became a bit complicated so we we yeah just stick with the actual
1:07:44
inions yeah so you you work on this nice categorical structure
1:08:01
so now you have this nice abstraction you have some new setting in that
1:08:11
you um so yeah we have this Factor plan marketing bills um we yeah also sort of
1:08:18
been thinking about maybe Mar values in uh a general uh J algebra albra of
1:08:26
probability moment um so yeah that that could generalize
1:08:32
Mar variables and maybe we could use same strategy to
1:08:38
say something we Haven really works how to
1:08:50
[Music] De um not really no uh yeah again we
1:08:57
focused on on these because these yeah filter probability space filtered or Co
1:09:03
filters version yeah not sure not sure
1:09:10
about the other super
1:09:27
um yeah sure I I'm not sure what if there's a name I don't know if you know
1:09:32
there's a know name for this usually people say
1:09:38
is not but
1:09:46
it's no no this is this
1:09:53
is internal
1:10:12
stand um well I the proof that I know is is quite different
1:10:19
um yeah it's different maybe there's there's for sure there is uh different
1:10:25
proofs than the what what it's a classical proof for me but uh I think
1:10:31
proof that you usually see in textbooks is is
1:10:37
quite this seems similar to something
1:10:45
[Music] involing you know
1:10:52
[Music] but I'm not I haven't seen
1:10:59
Pro I see yeah yeah I'm not completely aware of that's why I I think this
1:11:05
[Music]
1:11:16
might questions all right great let's thank
1:11:22
our speaker again
1:11:33
here we have a n minutes I think
1:12:11
sorry I didn't know either but right now what you do
1:12:18
either if you have okay
1:12:37
have [Music]
1:13:09
Wonder
1:13:51
very nice windows
1:14:54
got for
1:15:48
spe
1:16:14
did
1:16:37
r
1:17:23
conations yeah
1:18:24
yeah yeah
1:19:07
well
1:19:25
um
1:20:23
um
1:20:47
all right everyone
1:21:18
very
1:21:31
okay everyone welcome to tell us about the AL H in cical
1:21:38
probabili um okay well uh thanks to the organizers and also to everyone here um
1:21:46
this uh paper that we are working working on right now is joint work with
1:21:51
Le how Chan to Fritz and so uh to talk here about the Aldo
1:21:59
theorem and categorical probability we will divide the presentation in three first our R idea of the statement
1:22:07
secondly the categorical prob framework so Mark categories and then we will uh
1:22:13
talk about the super theorem in this synthetic perspective so for the rough
1:22:19
idea I will start off with the definitive theorem which is un easier
1:22:25
statement and uh the idea is as follows um we have uh Cooper the guy in the
1:22:33
green t-shirt uh who decides to play individual games uh with his
1:22:39
friends and they he wins he loses whatever but keep in mind that we need
1:22:45
to assume something very important for uh for math which is this is an infinite
1:22:51
sequence so Cooper has a lot of friends and um yeah also let let me tell you
1:22:59
that we are not really interested in the outcomes for say but we are interested in the probabilities of Victories of all
1:23:07
games okay now we we ask that Cooper is um a strong player like uh
1:23:16
psychologically in the sense that if we start to uh move around a finite number
1:23:22
of strengths is going to still perform in the same way and uh with these assumptions the
1:23:30
definitive theorem tell us that if we do not know the outcome of the first two
1:23:35
games and uh but we do know uh the rest of them then we can do independent
1:23:41
predictions on the firsts this is the idea of the defintive theorem for the ALU theorem
1:23:49
Cooper uh has other friends with him and so we have like these two groups we have
1:23:56
an infinite number of Americans an infinite number of Europeans and they all play games with each other and uh
1:24:04
yeah so I put a for when the American win and E for when the European win just
1:24:10
for for our sake but of course we are still interested in the probabilities of Victories of each game single game okay
1:24:19
uh as before we want uh this uh invariance okay so if we move around a
1:24:26
finite number of Americans or we move around a finite number of Europeans then the outcome does not change and now the
1:24:33
aluu theorem tells us that if we know the result of all games but the first
1:24:38
four then we can do independent predictions as before but there are
1:24:43
something more which is like that every uh of these uh four games okay they only
1:24:51
depend on their uh row their column and the tail so well the row is telling us
1:24:57
the skill of the American the column is telling us the skill of the European involved okay well the Gen the tail is
1:25:05
telling us the general difference in skills so to say like if we are playing
1:25:10
Texas Holden maybe the Americans are better if we are playing football maybe
1:25:16
Europeans are better so that's the of the general
1:25:22
differenes okay um well um I hope now you have a little
1:25:30
bit of an idea of what this uh theorem is about and uh we can go on with Market
1:25:36
categories if you have any question about
1:25:41
statement okay yeah was the fact that it was the
1:25:47
first two like 2 by two was to important or could that could that have been any finite number yeah any finite number
1:25:55
yeah yeah it's just for for my own sake didn't want to get involved in the
1:26:00
package too much but yeah an infinite number uh works for both the defin
1:26:05
theorem and Y okay so whatever Mark of categories
1:26:11
well Mark of categories are symmetric mon categories uh so we have a cancel product and a Mon Mon unit which we also
1:26:19
require to be terminal and then we also have this uh addition structure which is
1:26:25
this copy map and the Del map well actually the Del map is not really an
1:26:31
additional structure um so we have the copy and the deletion as you see the copy goes from X
1:26:39
to uh X tensor tensor with itself and the Del map goes from X to I and since I
1:26:45
is terminal this is the only map that we have uh of this form okay um for a mark
1:26:53
of C atory once need to require that these two maps will give you a common
1:26:59
structure on each object and uh also there is um like this structure has to
1:27:06
be well behaved with respect to the tensor product but I don't want to dve into the details the only thing that we
1:27:12
care about for us is that the copy is an idea to copy information without
1:27:17
introducing Randomness okay so this is going to be like a deterministic way of
1:27:23
uh uh having uh the information twice okay uh this is very important in
1:27:30
probability in in the sense that um when you have a probability you want maybe
1:27:37
you want one which is identically distributed and uh with the same information and this is how you get it
1:27:44
using the copy uh also another thing is that for for us we are just going to talk about
1:27:52
uh probabilities which are morphisms from the monoidal units um everything
1:27:59
seems to be like we should be able to generalize it to also uh uh other
1:28:06
morphisms but for uh for the sake of this presentation we'll just see uh
1:28:12
probabilities okay and and now maybe I will tell you about a toy example that
1:28:18
uh allows us to understand uh what is the concept of Mark of categories and
1:28:23
why um these things are called probabilities so if we consider fin sets
1:28:31
um what what can probabalistic morphisms be well uh we do know functions so we
1:28:37
take an element and we we connect it Associated to it an element in the co
1:28:45
domain but if we want to consider something with some Randomness well we want uh we want to
1:28:53
associate to to an element in the domain uh something in the co domain with a
1:29:00
certain problem and this is the idea of Mark of kernels so like for the this
1:29:06
first element we go here 50% of the time and here 50% time so on and so forth uh
1:29:12
we we can have the same typ of product that you may imagine for finance sets IE
1:29:18
ction product um and everything works out fine composition and the of product
1:29:24
okay and and this will give you a mark category so we're very happy but but the
1:29:31
definity theem already is talking about infinite arrays so finite sets are not
1:29:38
as good as one might think and that's the reason why we use standard World spaces which we already saw a little bit
1:29:47
um so we are interested in standard World spaces with Mark of kernels I
1:29:53
don't really want to give give you details of Mark of kernels for measurable spaces because uh but but Eda
1:30:00
is basically the same that we saw for fin sets and standard World spaces what are those well you can use this theorem
1:30:08
which is the kosis theorem and standard Bal spaces isomorphic as a measurable
1:30:13
space to either r with the Boral sets Z with power set or a set with power sets
1:30:22
Okay and this is uh this will be our framework for many
1:30:28
reasons uh which I will explain or also
1:30:34
not just a good amount say um and and this is uh maybe uh the
1:30:43
most important definition I guess uh that I will give um so if we fix a
1:30:48
probability from I so it is a from I to x w y then we say that X is conditional
1:30:56
independent of Y given W and we will write this in symbols but not really too
1:31:04
much um if we have this string diagram okay this is quality of string diagrams
1:31:10
what does this Tri quality means well actually uh you just need to to see like
1:31:17
the inputs of f and g okay like the input of f is only w
1:31:24
but there is no y involved in the input of f so this means that X is not
1:31:30
concerned with what happens with with Y and the same is true also for y like we
1:31:36
have no connection with X and that's the reason why we always conditionally independ okay for some
1:31:44
f uh but now if uh we actually require F
1:31:50
to be equal G which is going to be our cases generally then we can say that X
1:31:56
and Y are identically distributed given W and some of you may argue that if F
1:32:03
isal to G then X is equal to Y right but generally we want to label objects with
1:32:10
different uh namings because um well if I say x is conditionally
1:32:18
independent of X even W you don't even know what I'm what I mean maybe uh but
1:32:24
if we say X1 is conditionally independent of X2 given W then this
1:32:29
makes sense so uh this is actually what we're going to to do uh we are going to
1:32:36
have an object an infinite amount of time and we were going to label it with
1:32:41
a natural number or a couple of natural numbers as in the case for the
1:32:48
the uh this thing can also be uh generalized for um a finite or infinite
1:32:55
amount of XIs appearing uh instead of just X and Y and uh and so we can also
1:33:03
use this new symbol which is a little bit more General
1:33:09
okay and and this concludes the second part so if you have any questions
1:33:18
now okay um for the ALU theorem we we will do
1:33:24
exactly as we did at the beginning okay so we first talk about the definity
1:33:31
theorem and to talk about the definitive theorem and also the AL super theorem we need uh the Tor product of this infinite
1:33:39
array and the infinity Matrix and to do so uh we just introduced the notation XM
1:33:46
okay and basically this is given by uh by a limit procedure because we have
1:33:54
this deltion so uh if we consider like a finite set okay uh we can consider a
1:34:00
finite typ of product Associated to it and then uh using the using the deletion
1:34:06
on on one of the components or more of these components you can get this string
1:34:12
this uh diagram this uh morphisms and now you just take the limit of this Pro
1:34:19
procedure and you guess uh the contable T of product I'm putting something under
1:34:24
the carpet but we don't care and um and of course uh with this idea in mind um I
1:34:33
think you can imagine that if I take a fin pration uh Sigma I can use the swap
1:34:40
a bunch of times and then I will get this uh automorphism X
1:34:48
Sigma and now an exchangeable probability will be a probability which
1:34:53
is in variant of every finite permutation which is exactly what we required at the beginning right we can
1:34:59
move around the finite amount of players and uh so the definitive theorem
1:35:08
tells us that for every natural number n we have that the first n entries are
1:35:14
going to be conditionally independent of each other given the tail
1:35:20
okay uh but uh what is more actually is that they are identically distributed
1:35:26
given the tell so this F these maps are actually the same okay
1:35:33
and uh as I as I said before we we are labeling this objects although they are
1:35:40
still the same object every time
1:35:47
okay um about the assumptions uh so uh the synthetic proof of the the defintive
1:35:54
theorem holds under uh the following assumptions the first one is uh well we
1:36:00
want to State the theorem so uh we need countable tons of
1:36:05
products uh secondly we want conditionals basically because we are
1:36:11
doing probability so uh it's good if we have
1:36:16
conditionals and uh these assumptions uh were also used in the previous paper uh
1:36:23
by by toz thas gond and Paulo perona where they use these two assumptions and
1:36:30
another one which is more about the monadic um nature of the mark of
1:36:37
category involved but here we do not want to go in that direction and we
1:36:42
actually use a new information flow axum which is called the koshish axum I'm
1:36:49
going to talk about this axum a little bit more after
1:36:54
but for the moment I want to tell you and to stress that B stock satisfies
1:37:00
these assumptions so we are very happy um and uh also these assumptions do make
1:37:09
sense uh but uh more importantly so is that under the same hypothesis we can
1:37:15
prove the AL super theorem so these These are are our assumptions and
1:37:23
if we were were to use the monadic uh nature that was studied in the previous
1:37:29
paper then we wouldn't have uh enough to prove the ALU theorem so this is very
1:37:34
important that we have this new information flowx and it is for this reason that I wanted to just let you see
1:37:42
the statement of this axim uh just for people who are interested in this uh for
1:37:48
us is just that um well we we have a way to control this equality and this
1:37:55
equality is very important because in for Mark of categories the the last
1:38:00
equation this equation is actually telling us that f is p almost surely
1:38:06
equals g so basically this axum is a way
1:38:11
to control almost equality and that's how it's used basically in our um in our
1:38:20
proof another very important thing is that
1:38:26
when we try to State defin theorem we have only um one thing going on
1:38:33
basically so um we can state it using string using string diagrams in an easy
1:38:39
way but this is not the case for the AL super theorem because it's painful to
1:38:45
write and uh so we need to use this plate notation and you will see why it
1:38:51
will be painful to write in a second uh but uh yeah so we need this bit
1:38:57
notation to just uh write a bunch of
1:39:02
copies of the same morphisms okay so the thetive theorem for instance will have
1:39:10
this form a little bit uh nicer if you want but also not really interested for
1:39:17
interesting for the defin theorem but um you will see for the AL theorem
1:39:25
uh okay so uh as we said at the beginning uh when we are considering the
1:39:31
AL super theorem we want this row and column exchangeability so if we have a final permutation and we use it to
1:39:38
permute the rows or the columns this is not going to change our probability and this is exactly what we
1:39:45
require and um okay so this is the statement please
1:39:50
don't be scared too much uh
1:39:56
so uh okay um uh so we have our probability okay
1:40:03
and and this is going to uh be this like um yeah decomposed in this way we have
1:40:10
the tail okay so we have this part which is greater than n and greater than n
1:40:15
okay and this is the general difference in scale okay then we have the columns
1:40:20
and the rows okay the scale of the American and the European involved and
1:40:26
we have the outcome of their match and basically here we will have a
1:40:33
copy of G for every column a copy of f for every row and then we have a copy of
1:40:39
H for every entry so as you can imagine if we try to write this string diagram
1:40:44
without display notation it's going to be very messy very
1:40:49
soon um so yeah this is actually the the idea and uh here I'm
1:40:57
using like this double wire to tell you that um here we only have rows and then
1:41:04
we we go back to columns uh here same and here we had only one entry okay
1:41:10
Associated to I and J and then we have
1:41:15
uh yeah then we put on uh we go out of the the column box and that's the reason
1:41:22
why we have a double wire and then a triple wire to uh say that we are at
1:41:28
level so something about the idea of the proof uh graphically so the gray area
1:41:35
means that we are conditioning over that okay as we said at the beginning uh
1:41:41
remember the uh conditional independent given W so uh the the gray area is W
1:41:48
here okay and now we need to prove that all of this uh entries are independent
1:41:54
of each other and this is the first part and then we have the second part which
1:41:59
is we just marginalize everything in the n * n Matrix but one
1:42:07
entry and then we show that this is independent of all other uh rows and all
1:42:13
other columns given its uh its row is is
1:42:18
column well I'm not that good okay and also the
1:42:24
okay uh these two parts only require you to to use the definitive theorem and uh
1:42:31
some properties which are very well known to uh to people who do probability
1:42:36
Theory which are which are the semra properties but the third one is where we
1:42:41
need to use the koshish V axum so here we marginalize the whole n * n Matrix
1:42:50
and we need to show that all rows and all columns are conditionally independent given the tell and which all
1:42:58
rows and columns are conditionally independent I mean every row is condition independent of every other row
1:43:04
and every column okay and of course bys uh with these three uh these three
1:43:13
are basically three Lamas that we prove and then putting them together we end up
1:43:18
with the AL super statement and uh there is actually
1:43:24
another way uh that uh we are working on right now uh using the dis operation
1:43:30
Criterion and so other um order properties about
1:43:36
the the dog structure that is uh under the AL super theorem and uh okay uh
1:43:45
thank you for your attention
1:43:58
questions so at the end in the go back
1:44:04
to is the two here significant as in the two
1:44:09
Dimensions like take like it only goes up to matri or could you have done this for threedimensional ARS uh yeah yeah yeah yeah uh we're
1:44:18
trying to to do something more General at the moment this is what we
1:44:24
what we've working on but uh yeah I I think we can find a way to to State and
1:44:30
prove something way more General than this yeah um you you use plate notation
1:44:39
um I've seen plate notation before and I'm just wondering do you know of like have have
1:44:48
you formalized PL notation at all this is a bit of a a holy grail for a friend of it's
1:44:53
so uh that part uh Andreas is working on it okay and uh I'm letting
1:45:01
Andreas but yeah I I think that's uh the only problem with that okay so this
1:45:08
definition is okay the real problem is trying
1:45:13
to make this understandable okay because that that's going to be a hard task and
1:45:21
um yeah so you're saying the hard part is compositionality yes yes because this
1:45:26
compositionality is like on the side right so you cannot even close the Box
1:45:31
you need to act inside the box that's a little bit
1:45:43
painful know this diagram I
1:45:50
don't but I was wondering if this is kind first
1:46:00
time used with effect uh I think we're the first ones and uh actually maybe
1:46:07
Dario knows something more than us because he the one who suggested us the
1:46:12
play notation I I used it a bit like you can you can formalize it with kind of
1:46:19
you can have the type of Cl for each fter
1:46:24
uh I guess but I don't have this overlap like this overlap is much more complicated than what I
1:46:32
considered Evan used it Evan Patterson used it informally like this I don't know if he had a nesting I don't know if
1:46:39
he had overlap I think he had Nas oh but not overlapping boxes like in his disseration
1:46:55
easy wayct something
1:47:06
welled we call um if you want I can scare you and let you see the statement for for n
1:47:14
equal 2 without the play notation okay I would like to be scar
1:47:30
um are you
1:47:38
happy got the color yeah yeah uh color to a either
1:47:44
side I mean that's just a mess
1:47:51
yeah so I meal like
1:47:57
saying be
1:48:09
something um uh things scking below and above the
1:48:16
BL which would be to in this Cas for example you can have a m like where you
1:48:23
use the the accumul like Mom distribution so then you have copying on
1:48:28
the inside and accumulation on the outside what's really unclear is on
1:48:34
theid yeah uh yeah right good
1:48:42
question I I hope there someone so um we can use an inner
1:48:49
product interpretation uh we om miss the the P under it and now we can write the
1:48:56
antecedent as f f
1:49:03
FG and GG okay good game and and this
1:49:08
implies that f is equal to G right uh now uh the idea is that if we
1:49:16
were able to do some algebraic manipulation here we could write this thing as
1:49:25
FG squ equal to the inner product of f itself and the inner product of G with
1:49:34
itself and uh if you use the koshish ver uh inequality statement for the equality
1:49:41
you get that this is exactly the implication that you generally have so it is a Kish equality a
1:49:54
is that good enough
1:50:07
okay there some easy reason uh yes um so basically when you
1:50:17
try to prove this uh you are asking if I have that F minus G is uh squared is
1:50:24
equal to zero almost surely can I say that f is equal to G almost
1:50:31
surely and this is basically like a positive definite kind of
1:50:42
thing my
1:50:49
Z of uh yeah uh but maybe you're better than me at this
1:50:57
uh where is
1:51:13
oh so huh oh
1:51:20
uh okay can can I use this maybe it's better I don't know well uh I'm not
1:51:28
really sure exactly how to write this but should be like
1:51:35
[Music] f yeah there is like the integral of f
1:51:43
squar then you have that this is equal to the integral F time G and this is
1:51:48
equal to the integral of g s and that's basically the reason why at
1:51:54
the end of the day you get that you you get the integral of f F2 minus FG which
1:52:00
is equal to zero and now uh if you look also on this equality you have the
1:52:07
integral of f g minus G2 uh maybe on the oh
1:52:14
yeah so integral of minus FG + g s and this is equal to zero now you add this
1:52:20
two up and you get exactly uh Fus 2 FG plus G and this is exactly
1:52:29
the integral of Fus G
1:52:45
screen now a copy break and we are going to meet that in here at
1:52:51
11 so sorry copy third of you could come downstairs
1:53:41
[Music]
1:53:51
are
1:54:21
for get
1:54:27
[Laughter]
1:54:48
and
1:54:59
there
1:55:22
where
1:55:54
[Music]
1:56:06
[Music] um
1:56:39
oh yeah this
1:56:46
is I thought this was still looking for to your no
1:57:09
they said that [Applause]
1:57:49
see over there standing in
1:57:54
Fr
1:58:17
yeah yeah hope
1:58:28
start
1:58:51
I e
1:59:40
oh W this is a nice room um I mean last year it
1:59:51
was then started
2:00:20
the
2:00:50
e
2:01:20
e
2:01:50
e
2:02:20
e e
2:03:21
e
2:03:55
enjoy the school I've made some pretty close friends so I remember going to summer school
2:04:03
yeah actually soon and you know you I still know some of those people now that profess same same yeah yeah it's it's so
2:04:11
this is really lovely um yeah
2:04:18
[Music] yeah well I've already seen a bu of
2:04:50
people
2:05:20
e e
2:06:17
is the second room over there
2:06:50
e
2:07:20
e
2:07:50
e
2:08:20
e e
2:09:05
seems
2:09:15
honly oh oh there's a break room back there but
2:09:29
you want to see if anyone anyone's taking the
2:09:38
quiet prob I have no
2:09:49
idea I your the next speaker yes all right I'll your chair okay thanks
2:09:55
everything good with the first
2:10:02
used can use this right this that that and there's
2:10:11
here the only problem is that eil in the other room might not be able to see the
2:10:17
laser pointer which other room oh there's there's an overflow room yeah yeah so maybe use it only if it's
2:10:22
necessary otherwise just things yeah
2:10:28
nice so I emailed you but obviously you're totally busy so that was stupid of me not to think no I was I was about
2:10:35
to answer you like level are you high level yes right now
2:10:43
I'm especially yeah so I mean I've done dat
2:10:55
yes
2:11:03
abely
2:11:10
say Ian I wish you're right I'm to fav of it yeah but I'm not
2:11:18
sure anyway um
2:11:32
yeah but the trouble is like what's coming is to
2:11:38
um they basically haven't put for anything except we're not that right so
2:11:43
they're not doing not anything complex problems trying to as a result it's
2:11:49
entirely possible they fall
2:12:00
the right the
2:12:06
left Scot not just in Scotland but in the
2:12:25
right let me call people in five minutes
2:12:50
right e
2:13:50
go e
2:14:44
oh
2:15:19
look e
2:16:19
e
2:16:49
e
2:17:19
for e
2:17:55
hi
2:18:03
nervous a little
2:18:08
bit so jonesing you're
2:18:18
joning yeah I've left University system so what are you doing now
2:18:25
symbolic so what is that that's a AI startup company
2:18:31
that's I you're have to explain how exact C
2:18:39
well on Thursday boss ising be a general
2:18:46
intruction I think the best thing to say is that at a high level details are
2:18:53
different just DET a high a high level ml tends to avoid
2:19:01
structure in what it does it's if you matter like computations what see low
2:19:10
stuff and C it's entir posible just like type
2:19:16
system we use the structure software artifacts
2:19:25
structure
2:19:34
not what
2:19:41
learning imagine got
2:20:00
yeah yeah well
2:20:47
got the other people yeah principal
2:21:44
well
2:22:15
hi to see you it's nice going
2:22:25
country
2:23:17
I want myy
2:24:20
start
2:24:39
right
2:24:54
all right so welcome to Second session of today as you know at act we try to do
2:25:02
category Theory but also be applied and keep ties as close as possible to the industry so here's the first Speaker
2:25:08
from the industry we are very pleased to have G from symbolica who's going to
2:25:14
talk about approximate Game Theory so so thank thank you very much it's makes me nervous to be in the front of uh
2:25:21
academics you know industry person uh I always wilded by the kind of super
2:25:27
brains you we're just simple people in Industry forgive me if I what I'm doing
2:25:32
doesn't rise to the kind of intellectual Heights that you're usually used to I hope one day to become one of
2:25:41
you so uh yes so how long do I have you have about minutes so I'll stop at 11:30
2:25:50
right so um as I say so I'm this person and I now work at symbolic in London and
2:25:55
I'm going to give a talk on approximate Game Theory so um so game theory is something that people have been studying
2:26:02
for a long time it's about how we make decisions oh this is not
2:26:10
working um and it's been studied within the ACT
2:26:16
Community for a little while because it has sorry applied on the other hand has of categorical structure and has lots of
2:26:22
compositionality involved so so it's familiar to a number of people in this in this room but what I'm trying to do
2:26:27
is add approximation to it so approximation basically you know rather than searching for the best decision
2:26:33
which could take you a long time could be quite hard could search for something that's not too bad approximately good
2:26:39
enough so what I'm going to do is I'm going to talk about a particularly simple model SEL or Game Theory called selection functions and then talk about
2:26:46
approximation for that particular model of Game Theory and and in order to try and
2:26:51
validate what I do here um the approach I'm taking is compositional so what I
2:26:57
want to do is then say well this compositional approach to approximation in Game Theory does it work well with
2:27:03
the other compositional structure that people have identified that's the kind of you know is the mathematics good it's
2:27:08
good of itself hopefully but does it work well with the other mathematics um so that would be
2:27:14
approximation and compositionality and then a lot of people have worked on open games and some people are in the room
2:27:19
here I've seen already and Favio and various people so then I'll take the
2:27:24
simple model of selection functions of Game Theory apply to open games and then there'll be some conclusions and I might
2:27:30
mention some machine learning there right so firstly so why approximation so approximation been coming up quite a lot
2:27:36
recently I know lots of people have been uh thinking about this real valued this
2:27:42
and that where things are not necess equal or predic that actually hold but they hold approximately or are
2:27:50
approximately equal and I think this arises from a number of uh reasons P can I let you do the
2:27:58
admits um so why does approximation occur so it occurs all over the place of course so uh sensor data might be
2:28:05
approximate systems might have inherent stochasticity uh systems might have imperfect knowledge we don't really know
2:28:11
the state of the whole world we only know some of it or Exact Solutions might be too complex to comp compute so I
2:28:17
think it's a really exciting research topic lots of people in the applied category Theory Community have started
2:28:23
latching onto this and working on it it's my a little tiny contribution uh but there's huge amounts of fascinating
2:28:28
work going on here and I'd recommend those of you who are thinking about other things to think about because you know you've kind of solved all the
2:28:34
problems you for mention the huge brains um I think it's a really cool area and
2:28:39
it's just picking off picking up starting off taking offly recommend it so I'm going to take
2:28:45
this General uh uh vision and apply it to Game Theory so game theory is about how do we take decisions so those of you
2:28:52
who like politics and social structures and things like that very interested in that and here's a particularly simple
2:28:58
model of um Game Theory these are called selection functions so a selection function has Type X to V to P of X um
2:29:05
basically the idea is this is a utility function you're trying to think about what x should you do you know what's the
2:29:11
best X to do now each X has a value and so you might and then the selection of
2:29:16
function says if I have one of these things so this is a utility function assigns to every move of value and the
2:29:22
selections function says give me a utility function I'll give you a set of the best moves and the obvious example
2:29:28
arax you know you take a function and say which X's are the best and that's a set but actually Game Theory uh is
2:29:36
really much about much more than arat U because for example if you've got decentralized systems with lots of
2:29:43
components then there is no Global God who can say this is the best thing for everyone because you've got these
2:29:48
autonomous agents and they will do what they think is best for them and therefore the um equilibrium you get
2:29:55
won't be ARS in fact this is what's kicked off game theory in general Nash equilibri is exactly that realization
2:30:02
it's a different equ credic the other thing I want to you to notice is that this model of Game Theory works on all
2:30:08
utility functions that's a Hallmark of this kind of categorical composition game theory that we've been doing for
2:30:14
the last five or six years most Game Theory says here's utility function but I think what we believe and we've
2:30:20
discovered and we've got the mathematics to prove it is that that's too limited that you need to strengthen your induction hypothesis you need more
2:30:26
structure to develop a compositional theory and one of the one of the extra pieces of structure you need is to work
2:30:31
with all utility functions simultaneously not just one okay so I mentioned about
2:30:37
multi-agent systems uh Nash equilibria that arises compositionally out of these things so actually you don't need to go
2:30:44
as far as open games to do compositional Game Theory selection functions give you a little bit of compositionality they
2:30:50
can explain Nash equilibrium but then later on we'll do open games because although you get a
2:30:56
little bit of compositionality you can't model sequential composition of games what open games did was say what further
2:31:03
structure do we need in order to do sequential competition of open games so I'm so we'll do some open games at the
2:31:10
end and uh that was uh Jules and Victor and Philip myself right okay so some key
2:31:19
ideas in definitions so if you've got utility function uh you know what are the best
2:31:26
things you could do what are the best moves so one option is to say an equilibrium for such a utility function
2:31:31
is a set of moves X and what should X satisfy well if you evaluate if you get
2:31:37
the utility for x and you consider the utility for any other possibility this is better so that's our Max um and
2:31:44
that's fine and that's where things kick off and if start in such
2:31:50
so what is so this is all just stuff from the literature in the literature there is a notion of approximate equilibrium they're called Epsilon
2:31:55
equilibrium so Epsilon is a real number small little number usually and what's it say it says that well um a a possible
2:32:03
move is in aylon equilibrium of f not if f ofx is bigger than every other
2:32:09
possible move but you might have to add a little bit on or if you like you might not be
2:32:15
bigger than everything else but you're not much smaller so that's what an observant equilibria is and so if you
2:32:20
like this paper is about saying okay well that's a very concrete definition in a very specific setting how can we
2:32:25
generalize that to the work we've been doing on compositional Game Theory to selection functions and to open
2:32:31
game um and as I've said before while these
2:32:36
two are instances of agmax in general we don't want to just be you know working with these kinds of things because
2:32:42
they're very AR maxy you know in general when we study uh multi-agent systems we're going to be going to things like
2:32:47
Nash equilibria so we've got to leave all this kind of stuff behind very concrete fiddly bits of first order
2:32:53
logic um so so the question is if we have our selection functions as a model
2:33:00
which can express these complicated uh equilibrial predicates then can we Define approximation for selection
2:33:09
functions so this is the key definition and on this key slide this is a key line so if you want to look at anything look
2:33:15
at this line if you like pausing first of logic pause this line well if you don't then I guess you should listen to
2:33:22
me right so um so the idea is we've got a selection function and it has some
2:33:28
equilibrium these are the best moves that's utility now what happens we kind of relax and say okay well that's fine
2:33:34
but what about the approximate equilibrium so I'm going to build you a new selection function which is basically the old one but with a kind of
2:33:41
relaxed approximate equilibrium uh so I'm going to imagine
2:33:46
we've got our Epsilon which is a little number and and we're going to use some metrics because we're talking about
2:33:51
approximation distances so we'll have a metric on the function space and the obvious one is a suit metric but there
2:33:57
are obvious generalizations of this this is just approximation of predicat cetric spaces but I've specializ G Theory so
2:34:04
here's the definition what is this new selection function but notice it has the same type so we're not changing the
2:34:09
moves and we're not changing the values everything's the same but now I'm going to have to Define I'm going to say well
2:34:15
if s is my original selection function here's a new selection function what I it
2:34:20
equilibrium well I'm going have to give you the equilibrium for every utility function I made that point earlier so
2:34:26
there's my utility function f so you might say well you could say x
2:34:32
is in the equilibrium of f um oh soorry X is in the equilibrium
2:34:37
of my selection function s for the utility function f that's what that's
2:34:42
what the selection function s is but now we saying you don't have to be in the equilibrium function sorry you don't
2:34:48
have to be in the equilibria of s for your selection function f you can tweak F you can you know if you imagine
2:34:55
over here so there's my utility function there's my moves here's my values so the
2:35:02
selection function is s there's a Val there's a move x0 and it is in the equilibrium you know the value is higher
2:35:08
than everything else but look at this point this is actually in the equilibria
2:35:14
but it's not in the equilibri of s because it's lower but if I were to somehow give it a little bit more so
2:35:20
change F into a g which kind of goes like this it would be so that's what this definition says it says that X
2:35:26
should be in the equilibrium for S not at F but at some other G and what is g g is not too far apart from F it's
2:35:34
within epsylon so basically you have your utility function you can tweak it add a little bit more take a little bit
2:35:39
away and if you're in the equilibri for that utility function then you're in the
2:35:44
equilibria for this game so uh it took me a while to find this
2:35:50
uh but I found it eventually and it's nice and simple capes an intuition which I hope I've explained and you've got um
2:35:57
it's very general uh I think we'll see that it's a as I said it's just metric
2:36:02
it's just um approximation predicates of a metric space basically uh and it extends the usual definition so so
2:36:09
that's all nice stuff but this is good mathematically so to be good mathematically I want to say does it have good properties in and of
2:36:17
itself and also does it act well all the other compositional operators I say with the other compositional operators
2:36:23
because this is of course compositionality we're taking a game and building a new game so it's another compositional
2:36:30
operator right okay so um so let's talk about the mathematical properties so
2:36:37
we've got selection functions we can consider what maps are of selection functions so I've drawn this diagram
2:36:43
over here to help you so if you've got a selection function over X andv and another selection function X
2:36:49
Prime and V Prime you might say what are the mounts between them and inside all of this game theory and all inside a lot
2:36:54
of machine learning and stuff there are lenses lurking I'm sure many of you know about lenses so what is a map between
2:37:01
this selection function and that one it's a map between the underlying lenses which preserves the predicates it
2:37:08
preserves the predicates contravariant again that's a property of lenses if you've seen container work you'll know
2:37:14
the M containers have a contra variance and this is what it says it just says that you've got to map between the
2:37:20
lenses uh such that whenever you have an equilibrium for the co-domain you get an
2:37:25
equilibrium for the domain and I've used uh I've represented the utility functions from the code main game and
2:37:33
the states of the original game and you can wrap this up in nice form a state of X is mapped from one tox and a utility
2:37:40
function X to b x Prime to B Prime is just a lens map into one so this is a
2:37:46
sort of you know if you know the mathematics it's simp simplifies the notation it's really cool so I can just
2:37:52
write alpha x but this is lens composition and this is lens composition simplifies stuff okay so we've got a a
2:37:59
notion of map between selection functions and so we can observe that uh this operation is functorial basic
2:38:05
trivial stuff that you want and it's not obvious you're going to get these things when you might have switched these orders the way around and said you know
2:38:11
I'm going to do this cently well then it wouldn't be functorial so so it's not
2:38:16
it's not trivial even though it might simple uh uh what else can we ask for well we've got this index atlon
2:38:24
somewhere no oh there it is we can look at the structure of the Epsilon so those of those who studied graded stuff should
2:38:31
could understand ask themselves what about this Epsilon what about the structure of Epsilon well we can do some
2:38:37
obvious things so you know there the kind of ifon zero you don't change
2:38:42
things you know there's a kind of weakening rule that you know if you weaken by if you approximate by this much you certainly got as much as if you
2:38:49
approximate by that much and then there's sort of transitivity of grading where you kind of add up the
2:38:54
approximations and that just follows from the trans from the um triangle law for metric
2:39:00
spaces right 10 more minutes okay compositionality okay so so that's this
2:39:06
operator this compositional operator for approximation this teplon um what about the other
2:39:12
compositional operators so most I've been going on about compositionality but now I've got some motivation although
2:39:17
all of you know this no right large code from scratch you glue together pre-existing things you know in general
2:39:24
for our models of the world we should glue together pre-existing things and our savior is C category Theory because
2:39:30
it's about structure that's the way that's the way I would do these things
2:39:35
so uh compositional game theory that means building big games from small ones and so there's a monoidal product on the
2:39:41
category of lenses it just takes on actual lenses so well on actual pairs of objects it's just the cartisian stuff
2:39:48
but remember lenses have forward and a backward way so when you extend this to uh the maaps you know you're not going
2:39:54
to get anything cartisian in general although there are important special cases with reverse derivatives and things where you uh where you do get
2:40:01
products but we're not going there here so this is just a monal structure and the beautiful thing is that uh if you
2:40:07
take two selection functions and apply this tensor uh you can build a new selection
2:40:12
function and what will be the equilibrium so you'll have to give me a utility function f and its domain will
2:40:19
be a pair of moves and its co-domain will be a pair of values so you can say when is a pair of moves in the
2:40:25
equilibrium for the selection function this compound composed selection function well this is just the
2:40:31
mathematics that arises from here um it says well X is going to be okay for the
2:40:37
first agent if well X is okay for the first agent um but what utility function
2:40:42
well F takes two input we're fixing the second player's move and this will give be a pair of values and I'm the first
2:40:49
agent so I only care about the first agent and similarly X Prime needs to be happy so if if they first agent doesn't
2:40:56
change their move is X happy well they'll be happy with respect to their own utility fun their own selection
2:41:03
function for the similar utility function and the beautiful thing that when I saw this I thought oh my God this
2:41:09
is so beautiful is that Nash equilibrium no longer is a primitive thing that you know was very cleverly discovered in the
2:41:16
50s um and validated by EXP experimental intuition it's actually a math it's
2:41:22
actually a nonprimitive thing it's actually just a product of two Ares so that's that's very beautiful reminds
2:41:30
yeahor what's that less than sign doing there in the
2:41:36
the X between the X and the X Prime oh it's an and ah oh there yes yes
2:41:45
it's yes sorry I have not found out the perfectly
2:41:50
uh academic way to write
2:41:56
animation I guess I could write and with a little silon underneath it and see what
2:42:03
happens this is about sprinkling it syons everywhere but then trying to find out maybe not everywhere what's a nice
2:42:09
place to sprinkle it syons right okay so five more minutes I guess
2:42:15
right so what we've done is we've taken um what we've done is we've
2:42:20
taken uh yeah we've taken a selection function which is essentially uh a function space index
2:42:27
collectional predicates and we've built a ball around it for every utility function f we don't
2:42:33
take um that's F which is a certain set you sort of build a ball around it by
2:42:39
looking at the other G's that are that are close to F so there's no particular
2:42:45
reason that when you do that it's going to resp ECT the compositional structure
2:42:50
that we've got so for example tenses here you know I I take some weird little predicate I sort of you know build a
2:42:56
little ball around it and then I operate on it well you know that's going to destroy that little weirdnesses of what
2:43:02
I started with maybe but actually I was kind of really surprised and that's what made me think oh we're on to something
2:43:09
here because the mathematics works so for example you could ask yourself if I take two selection functions two models
2:43:17
of games and I put them in parall composition I take the Nash product and then I weaken them and I do it
2:43:23
approximately so what's an approximate Nash equilibrium I don't know per se but
2:43:28
it transpires you can prove this so an approximate National equilibrium is consists of it must be an approximate
2:43:35
equilibrium for the first player and an approximate equilibrium for the second player so that gives us some concrete
2:43:40
stuff it means if we want to find these things it tells us to look inside we don't have to start from scratch
2:43:47
that it mean you can compositionally kind of look for these things by look looking for looking for uh the equilibria of the
2:43:55
first game approximate equilibri of the first game and the approximate equilibri of the second
2:44:01
game so start minutes later you okay
2:44:08
33 bad here anyway um hopefully not the worst anyway uh
2:44:16
right where do we get to right yes so application so these kind of mathematical theorems I mean this shows
2:44:21
there some kind of LAX monoidal structure going on here but these are these have concrete uses it says how we can look for the approximate equilibrium
2:44:27
of a n product through the component systems but then oh my God I can do more
2:44:34
so the reverse is true as well providing the these sets of moves have decidable equality and that just because you know
2:44:41
you've got some utility function and you've got these particular moves and you want to kind of tweak the utility at
2:44:47
that particular point so you need to to say you need you need to be able to write an equation saying well you know
2:44:52
if got if I got to move and has this utility give it a little bit more for all the others leave them the same so
2:44:58
that's where the decid equality turns up because it's all about tweaking equal um utility functions and
2:45:05
then you can prove the reverse so this is a really strong theorem and I was totally surprised to get it which is
2:45:10
nice some sense the best theorems are the ones that ought to be false because this can't be true bloody hell it is you
2:45:16
know wow SLE sorry somebody
2:45:23
forgot somebody forgot me such as like it's a problem right right okay so like
2:45:30
I said very strong unexpected uh and so it says the approximate equilibri of a Nash of a
2:45:36
compound system is exactly of those components so that's really good right so I'm going to have five more minutes
2:45:43
so I'm not going to totally tell you about open games those of you who know it will know that it's got a comp at
2:45:49
little gnarly definition which you know once you get the hang of it sort of makes sense but when you first see it
2:45:55
it's it's pretty gnarly and it's been improved upon by a variety of different people um over the years but the best
2:46:02
way is to ignore this gnarly definition and say and again phrase it in terms of lenses I mean lenses are beautiful uh
2:46:08
and you basically say an open game from again a pair of sets to another pair of sets so you've got a set of strategies
2:46:14
these are like the moves now we're separating moves from strategies uh and then for every
2:46:19
um strategy we have a lens so in other words some way of playing things and some way of feeding back utility and
2:46:27
also an equilibrium predicate and since I've put the strategies uh of index equilibrium predicate by you'll notice
2:46:34
oh that looks similar to what we had before it's a bit like a selection function I mean it's not exactly a selection function because that doesn't
2:46:39
say x but basically it's a relation uh predicate
2:46:45
between uh util utilities and moves so we can apply the same thing to what we
2:46:52
had before we can do the same thing we did before and so we basically
2:46:57
say uh if you got an open game you can build another one where you weaken it or you approximate by by saying you for a
2:47:04
particular strategy a move is in the equilibrium for the approximated game at
2:47:09
a particular utility function if there is another utility function that's not too far away from it and then your move
2:47:16
is in the equilibrium predicate for the original game at that strategy for this
2:47:22
proximity utility function so it generalizes smoothly the work we did on selection functions it pays the way for
2:47:28
this more complex open games material right I'm going to now skip the rest oh
2:47:34
no I'm going to say one thing briefly so you can go through the same thing before you can prove functoriality you can show
2:47:40
it's LAX monoidal and you can show it's monoidal uh if the moves are decided same as before but the point about
2:47:47
introducing open games is you also have another sequential operator uh sequential comp sorry
2:47:52
another composition operator sequential composition and many more as well it's not just that one you don't want to tweak your definition every time you
2:47:59
have a new operator um and so we can form this sort sequential composition of open games and here you only get a
2:48:05
one-way inclusion which is all I wanted to say all I want to say if you have
2:48:11
something which is in the equilibrium of an approximated sequential composition
2:48:17
then you know it was in the equilibrium of the composite of the two approximated
2:48:23
games so we don't have backwards and forwards we only have one way that's still good for the first application of
2:48:28
saying if I want to know what are the equilibrium of this approximated compound game will I have some kind of
2:48:35
you know I've limited my search space that's all you can right conclusion um so so we've got
2:48:42
a simple and compositional definition for the approximation of games has two key mathematical properties uh it uh
2:48:50
interacts surprisingly well with the other operators at prior there's no particular reason it should but it does um and it has good graded
2:48:57
structure um future work um I think there's two things that I'd be interested in one is a kind of metric
2:49:03
Game Theory so what's the difference between two games how far apart are they from each other could you put a metric
2:49:08
space on the space of games well one thing is to say well you know U take my
2:49:14
two games how much do I need to approximate one by so that the others
2:49:19
contained in it and vice versa so those of you who know about the hous do metric will recognize where this comes from uh
2:49:26
so that gives you a distance and then you can say oh so if you want to do Quant quantitative Game Theory you might
2:49:32
say I don't just want to put these two games together compositionally I might say well if this is so far from this one
2:49:39
and this is so far from this one when I put these two together how far away they these two so that would be the kind of
2:49:44
quantitative behavioral distances thing you would do with gante and and
2:49:51
this would be kind of part of that I think the other exciting thing particularly for me as a as a kind of
2:49:56
lowly industrial person working in machine learning is that um so I think we know when I say we know I'm not sure
2:50:04
what the semantics of we is or the semantics of no probably approximate of course um but we know weon noon the game
2:50:13
theory and machine learning are much the same thing um so this whole program I've been
2:50:18
done that I've been talking about could form the basis of a kind of explainable ml quantitative ml where you have where
2:50:25
you sprinkle your silons you know you can track the pylons as they move around as they're computed and when you learn
2:50:31
something you I've learned this you actually get to know how much you've learned and then you could also start
2:50:37
and if that is very low you can say oh well let's assume that it actually is this and then proceed there's a lot of
2:50:43
things you could do uh once you um once you start quantifying the learning process and I should say the reason that
2:50:49
machine learning and game much say is it's all about back propagation okay so thank you very much
2:50:56
for listening to me and I'll stop
2:51:08
you right thank you very much for talk there are a few questions
2:51:14
okay than name for the first
2:51:27
what are the applications this
2:51:32
to by right so um on Thursday our boss Dominic
2:51:41
ver is going to give a talk and and he'll tell you in general what we're doing um this stuff is not on our
2:51:47
critical path I mean we're really focusing on machine learning code synthesis if we were to have a broader
2:51:55
you know if we were if we had a research division which forgot about short-term near scale industrial impact I would say
2:52:01
that well it's about kind of um there's a lot of stuff in explainable ml where you have to say you know okay you've learned this are you sure You' learned
2:52:07
it what do you mean you've learned it you know can you give me some kind of evidence so I think that would be to get a quantitative version which would lead
2:52:14
to ml verification all right uh I don't think we're gonna have time for all three
2:52:20
questions so maybe let's start from the far right far right
2:52:27
right or should I call you's sorry
2:52:41
there's the T Epsilon it's a LAX Minal LAX funter endoor of the bat
2:52:49
of gam so you think that's a general setting to work at these like I would
2:52:56
say this is why I feel intimidated by you academics such a broad Vision with you
2:53:02
know so much all I can say is I don't know maybe it looked quite complicated to write it all out and so you know I
2:53:09
chose not to I chose to tell you the simple story but yes there is definitely the the two-dimensional setting there if
2:53:14
you like the types of the games which describes the moves and Val that there's the games themselves and
2:53:21
then there's the maps between the games and you definitely want Maps between the games because you want to character games by Universal properties so you do
2:53:28
want a two-dimensional Approach at a minimum uh and so yes there is definitely work to be done but that's
2:53:33
for that's for academics not for little people put just one more question let's
2:53:39
say for m is industry person smooth brain question so uh if you can go back
2:53:46
to the definition of something yeah um oh
2:53:54
pointer I probably put it
2:54:08
in so basically here you my question is how since you are searching for a G over
2:54:16
a metric space of I guess that that's where you hit the complexity right so
2:54:25
this so basically you're asking me so you're ask me the question of like I the
2:54:31
nice thing about open games and stuff like this is is because you have both utility all utility functions are
2:54:37
considered as well as all moves usually just all moves and one so you got both you could reverse engineer stuff you
2:54:42
could say if I have this what sorry what sorry instead of giving function what
2:54:48
are it's equilibria you could say I want this to be an equilibria what utility functions can I have so this is very
2:54:54
interesting and this is a nice application of open games it's about policy definition how do I set my tax rates to get something hand so I
2:55:02
interpret your question as if I want something to be in an equilibrium maybe an approximate equilibrium how do I find
2:55:08
my G to prove the statement uh as you say I didn't really consider it I mean I've been thinking about you know finite
2:55:14
games uh finite move spaces that situation could start doing search you could start doing some things but it's
2:55:20
not a problem of investigate as you say it could well be described as hidden right uh unfortunately there's
2:55:27
more questions but we don't have time please keep them in mind and ask them offline can I can I just say
2:55:33
unfortunately I could only be here today that you really are ask me a question rather just little bit find me I right
2:55:40
all right so keep them in mind for one hour and ask him at the lunch break for now let's s our speaker again
2:56:14
and I need
2:56:32
yeah stand near theic no the microphones are that right here just make sure speak
2:56:38
up and not right uh let's get started our second speaker today is Toby smice who's
2:56:45
going to talk about copy composition for probabilistic graphical marks please yes
2:56:50
okay great that's what I'm going to talk about thank you right so what do I mean by copy composition well sometimes we
2:56:57
want composition not just to be like this where we you know hide this middle thing but somehow to keep it around you
2:57:04
know this this scares some people but it's actually quite useful sometimes and in particular in probabalistic modeling
2:57:12
often the object we care about is not like the composite Channel which is like the marginal at the end but the whole
2:57:18
whole set of the things in the middle which is often called The Joint distribution or conditional joint
2:57:24
distribution and here this is you know one picture of such a joint distribution you've got some prior which is a state
2:57:30
and you keep that around and you have some Channel or kernel which is your Mar
2:57:36
your conditional here on Y and that's called a likelihood in lots of settings
2:57:42
and the The Joint distribution is like the product of these two things and keeping this wire around corresponds to
2:57:48
keeping that product rather than marginalizing out X Neil just talked
2:57:53
about um approximate Game Theory and last year I talked about a kind of approximation for doing basian inference
2:58:01
which I call statistical games and it had a very similar structure where you have a game which is in this case a pair
2:58:06
of two things one of which does Bas inference and a way to measure how good it was and that's a loss function it's a
2:58:12
bit like a sort of quantitative um selection function in Game Theory but the loss function I was interested in
2:58:19
the weird thing about them was that they were only kind of copy compositional for the for the rule I wanted to obtain to
2:58:25
work I needed to keep around this middle thing all the time and I sort of came up with a by category ad hog it needs to be
2:58:32
two-dimensional because you have this extra thing here um and but I wasn't very satisfied by that I wanted like to
2:58:38
have some kind of structural origin for that by category and I I was started thinking about it and this is what this
2:58:43
talk is really about I've got two cases of copies cop composition to talk about
2:58:49
one is in um what are called directed models and probability Theory often known as basian networks and another one
2:58:55
is in undirected models and they're kind of similar but kind of different um and I hopefully we'll talk about both and
2:59:01
I'll show some spring diagrams and it'll be nice okay so what is a basian network I
2:59:08
don't have time to give a sort of full definition but I can give you an intuition it's a probability
2:59:14
distribution that factorizes over a directed graph and this is why it's Direct um the nodes in your graph are like
2:59:21
random variables and then the edges represent um conditional dependence in your um in your your distribution
2:59:28
structure um Brendan fong in his master thesis showed that any basian network
2:59:33
can be written as a composite of certain morphisms of this form we've got a lot of wires which represent sort of
2:59:40
uninvolved um random variables and you know some things which represent parents
2:59:46
of your random variable in your graph structure and you feed in the sort of parents and you do your you know your
2:59:52
conditional dependence thing and you keep around its output and you keep the whole structure around but the key part
2:59:57
is that you keep the whole structure around right you keep the copies of the inputs and you keep around all the proceeding inputs and so this is
3:00:04
inherently like it's a copy compositional thing but Brendan just just like I did he just constructed this
3:00:10
thing by hand um but actually this thing comes out from a sort of very natural kind of type theoretic kind of
3:00:16
perspective so the first thing to say is that with sets and functions if you have
3:00:21
a function X to Y and you precompose it with a copier like this it gives you what's known as its graph and the thing
3:00:27
to say about the graph is that it's the section of the projection I mean this is kind of obvious because if you throw
3:00:33
away this x you get the function again and that's what it means for this thing to be a copier it has to be this this
3:00:40
choid and so that means it has to behave well with your you know
3:00:45
deleting being a section of this projection means that we could think of it as a sort of type theoretic we think of it term of this product type with X
3:00:53
being the context um but this is all you know this story so far is all deterministic it's
3:00:59
just with set and function but we can tell the same thing with probability kernels and so that's what we'll do um
3:01:05
but before we do that I want to say that in that previous diagram for basing networks I had you know a bunch of wires
3:01:12
that were kind of uninvolved representing the parents the sort of non-parents as it were
3:01:19
and where do they come from well they also come from a sort of standard type theoretic thing which is called context
3:01:26
extension that just means pulling back along another projection and so we've got these two things one is kind of um
3:01:33
pulling back along this projection and one is the fact that these sort of copy composite things look like sections and
3:01:40
sort of the main job really is to sort of instantiate this sort of type theoretic setting in in this kind of
3:01:46
probabilistic World um and the way to do that is to say okay well what I need is a b fibration and the B fibration
3:01:52
structure gives you both of these operations okay so what I need is a b fibration of probability kernels we've
3:01:59
seen some examples of what a probability kernel is um I say probability kernels
3:02:05
but really I mean something a bit more General than probability kernels because a probability kernel is a morphism which
3:02:12
always admits a probability distribution and a probability distribution integrates to one but I'm going to need
3:02:19
them to be a bit more General than that because I want to have um later I want to have kind of um predicates fuzzy
3:02:25
predicates which are going to be morphisms into the monodic units and in the case that the probability colel those are all trivial but so to say a
3:02:33
little bit more about what a a kernel is going to be for me it's a function from some measurable space and a sigma
3:02:40
algebra on another measurable space which to domain into the positive reals and these things satisfying some like
3:02:46
finiteness condition and known as site kernels um and I'm working with these
3:02:52
kernels but you know the story I'm telling is not you know specific to this particular by category I would say in a
3:02:59
minute that you can tell this story for a more general setting um but this is to say that we can do this for the
3:03:04
probabalistic situation that I was interested in um but okay so I've said what a kernel is it's between two
3:03:10
measurable spaces it's one of these functions it's measurable in the first argument and it gives you a measure IE of distribution in the second argument
3:03:18
and I want to build a sort of vibration of of these and the way I'm going to
3:03:23
work is just by analogy with sets and functions and you may know about something that's called the codomain
3:03:28
vibration or the arrow category which has a vibration structure and there you have um your objects being functions
3:03:36
into some base object which I'm calling B and these represent sort of um their
3:03:42
bundles they represent collections of types indexed by some base or collections of sets indexed by some base
3:03:47
and here I'm going to be doing the same thing to for my object so I work with measurable functions into my base and
3:03:54
then the morphisms between them are going to be kernels the the sort of kernels fiberwise so they preserve this
3:04:01
base Point um and that means they satisfy this equation that you know if you write it out it's like a triangle
3:04:06
it's exactly the equation that you satisfy in your sort of slice category but the difference is that we have this
3:04:12
Randomness there in your morphism and the the key thing that allows you to sort of um compose these measurable
3:04:19
functions with these kernels is that every measurable function induces a deterministic kernel and you compose
3:04:25
them by something push forward um okay and so if you have these kernels which
3:04:31
satisfy this kind of equation this triangle condition then you could think of them as fiberwise and to say
3:04:38
precisely what I mean by that um I'm going to use this notation that D FIV uses for pullbacks so if you've got a a
3:04:46
sort of a bundle here from P to B and some morphism into b um little B then
3:04:54
you can you we write this pullback object as P know square brackets speed
3:05:00
and you can really easily show that if you've got a a kernel satisfying this condition it restricts to a kernel sort
3:05:06
of fiberwise that is to say it restricts to a kernel between the sort of fibers over this generalized
3:05:12
element it's I mean this proposition 2. in the paper but it sort of barely deserves that name it's very very easy
3:05:18
to show you just you precompose your kernel function with these projections on E and D Prime and that's your that's
3:05:24
your sort of pull kernel okay so that's like the main sort of technical slide
3:05:29
I'm going to say a little bit more about like the B vibration structure but it's really easy it's just comes exactly from the the B fibration structure of the
3:05:36
codomain vibration of measurable functions so you get you know this pullback thing which gives you a
3:05:41
substitution functor um it acts by restrictions on kernels exactly like this um it it's you know very easy it
3:05:49
gives you this it gives you the vibration structure it's contravariant and then you have a a
3:05:54
covariant um left ad joins to these funs which uh gives you what's known type as
3:06:00
dependent sum and this just acts on the objects by post composition so if you've got you know some p over J and some B
3:06:09
from J to Big B then you just post and post like this it's an object it's really easy and on the kernels it's just
3:06:15
act by identity that's nice and and you get an adjunction which is quite to show I mean in Mees you have that adjunction
3:06:21
so you just put it Forward by this deterministic sort of embedding and then these satisfy this
3:06:28
condition called bet chal which means that if you have a pullback in your base category here it's measurable um bases
3:06:35
and functions then you can sort of go around your pullback Square using substitution dependent sum either way
3:06:42
and it doesn't really matter which way around you go and we're going to need this for associativity of the composition that I'm going to Define
3:06:49
ask have any chance prod uh I thought someone might ask this well no in the
3:06:54
case of Mees you don't even have a cartisian uh close structure I know that with like some versions of some
3:07:02
probability categories you get a carto in structure particularly in qu barel
3:07:07
spaces um I thought for a while that I could make a dependent product come out of that structure and then I got lost in
3:07:14
the weeds and didn't finish trying to convince myself it was true but I sort of thought maybe I could get one but maybe someone else knows better but it's
3:07:21
not very nice and I don't want to say that there is um but maybe there are settings of which you could but it it
3:07:27
wouldn't it wouldn't it wouldn't work like sarcastic you would only get Randomness in of one argument of your
3:07:33
products um because there's no way to introduce Randomness in two arguments we
3:07:39
can talk more about it later if you want okay so that's my B bration
3:07:44
structure and I can use this now to to do these two operations which I introduced earlier which are going to
3:07:50
give me my copy composition so okay well what I really want to do is if I've got a graph of some stochastic function some
3:07:57
kernel and a graph of another one I want to compose them and in this kind of way where I keep all of the objects around
3:08:04
um and the answer to doing this is exactly using these two operations of um
3:08:10
of substitution and dependent sum it's will pull push because first of all you sort of pull back along one projection
3:08:16
and then you push forward along another and to sort of make that really clear I've drawn this out diagram so if I've
3:08:22
Got My Graph so I've got these spans which represent my which are my projections from my two product objects
3:08:29
and I decorated them with sections and what I do is I take my
3:08:34
section on the sort of second um second span and I pull it back along this
3:08:41
projection and then I sort of push it forwards along the first one along the first left leg in order to be in the
3:08:48
sort of the right space or the right fiber and then I can compose these two as kernels and this gives me uh a
3:08:54
section of this composite thing and so I I said decorated and really this is a
3:09:00
sort of decorated span construction and so we can use the Machinery of decorated
3:09:05
decorated spans or decorated Coast band and particular here as I said we before I had a by category we going to have a
3:09:11
sort of two-dimensional structure so we can use a double Ren deconstruction perspective on decorating SP post band
3:09:17
which Evan pison talked about last year um which is really very nice and it's
3:09:23
quite easy and it's quite General as I said if we have a a b fibration like you know this some B fibration and we have a
3:09:30
section of it in the paper right you can obtain exactly a double vibration I call
3:09:36
it you know Blackboard s over spans in the base category in a slightly annoying
3:09:43
restriction that the spans with morphisms of spans those are your cells and your um double category they have to
3:09:49
be they have to be sort of cartisian they have to be pullback squares in your Mor um and that's to of make horizontal
3:09:55
composition of squares work nicely so as we've seen in the previous diagram we decorate these stands by
3:10:01
sections of their left leg that's what the section of your vibration is for horizontal composition is by pull push
3:10:07
it's exactly what I showed you um and then we need this vect condition for the
3:10:12
associativity and this does exactly what I wanted and it's quite nice because it has this sort of type theoretic structure and in particular um we can re
3:10:20
recapitulate this this sort of construction of Brendan's um exactly in the way I said and so that that works
3:10:26
very nicely um the way to do that is to say okay well I've got kernels they
3:10:31
embed into this double category in a sort of LAX horizontal way mapping kernels to their sort of their now
3:10:37
sarcastic graphs um and so what we say is okay well now I take my image of this sort of
3:10:44
uh this factor of my basian network in this it pre it with the copers and then I can sort of pull it back along the
3:10:50
projection that that extends the context by these other objects and that this
3:10:55
gives me another section of this whole thing and then I can compose these in in this sort of copy composite way and this
3:11:01
this give me my Basi and network so that that sort of recovers what Brendan did in a sort of neat kind of typ theoretic
3:11:06
way and kind of tentatively it means you could sort of think about Basia networks where some of your sort of objects your
3:11:12
random variables have like some kind of type dependent structure which I think is quite neat you don't see very much in probab modeling where people have
3:11:18
dependent types around because I think people don't really think much about these sort of SPS but I think it could be useful okay
3:11:26
so that was the uh directed is how much time do I have now uh 10 minutes 10
3:11:32
minutes okay great so then let's talk about the undirected kind of models so these are these are kind of different because
3:11:39
you don't really work with kernels per se you work with some other way of
3:11:44
factorizing your distributions um and one of these ways in the sort of modeling literature is called Factor
3:11:50
graphs and what these are are ways of representing the density functions
3:11:56
Associated to your distributions as factors of other sort of smaller
3:12:02
functions which may or may not be density functions but they are just functions and so sometimes people draw these
3:12:08
things like this in this bipar graph structure where you've got kind of random variables or or sort of nodes
3:12:15
that represent types and then factors which are functions from these
3:12:21
objects or the products of these objects into the reals or the unit interval or something like this and then you form a
3:12:27
graph by just taking products of all these functions um and I haven't seen a
3:12:32
s of presentation of this in a sort of compositional setting and I was quite interested in inference on these things
3:12:38
that's what I talked about last year this is what I'm quite interested in generally approximate inference um but I you know one thing
3:12:44
you can notice is that you have a sort of similar copying situation here because you've got a for instance that
3:12:50
goes into F0 and F1 so you need to copy this a to put it into these and of
3:12:55
course it had some other composition structure which is that you can you know these are functions and they could in
3:13:02
some sense also contain some factorization so it has this kind of sort of operadic kind of nesting
3:13:07
structure where you have like Factor graphs within Factor graphs so how do we like formalize this
3:13:13
well I told you about site kernels they're like kernels which don't have to integrate into one now if you sort of
3:13:19
think back to my definition if you think of what is a kernel into the sort of one
3:13:24
element space if I just skip back a bit maybe I should have written it out then
3:13:30
it sort of has a trivial Sigma algebra in the in this right hand factor and so
3:13:35
you can sort of figet out this part and then you get this function positive real and so the sort of co- States in this
3:13:41
monal category are exactly um these facts and this is quite a common pattern
3:13:49
in in modeling or in these so-called copy discard categories where you think of um the the sort of co-states as
3:13:57
predicates in some way they're sort of like dual to States they of measure you know you compose a state with a
3:14:03
predicate and it gives you a number is it is to say how good this state is in this kind of in this kind of logical
3:14:10
setting something like that uh there are copy discard categories we've seen a few examples of
3:14:16
um there are s of specializations of this notion this is just where you have pones everywhere but specializations
3:14:21
which explicitly give you these kind of non-trivial co- States weekly or partial
3:14:27
Mark categories or just categories of modules or things called effuses in partial form um but the key is that
3:14:33
we've got these Coates we form our tensor and then we pre-compose by copier so again we have this pre-compose by copier situation and we're going to have
3:14:41
another sort of decorated something situation here we got decorated coand um because we're going to decorate like
3:14:47
post spans of objects with factors just to be decorated spans of something like objects before with
3:14:54
sections but previously you know if you look in the paper you'll see that the construction I make doesn't really make
3:15:01
use of the whole double categor Machinery because I I don't require in
3:15:06
the the previous case the the decoration to depend on the whole span but here I'm going to want my decorations to depend
3:15:13
on a whole span the whole span is going to be made up of something like this which I'll tell you about in a minute and you see why I need the whole SP in a
3:15:20
moment um okay so the factors the domains of these factors are like finite
3:15:25
tenses of objects so we take finite sets and these are going to represent like indexes of my objects and I do
3:15:31
decorating I decorate them by sort of type information which the functions into the objects of some category and
3:15:37
then a co- span encodes which objects are exposed for composition so then we've got a double
3:15:43
category of co- spans the vertical morphisms are just functions I decorate these with pronoid homomorphisms in my
3:15:50
category that I'm starting with where the factors live so these are deterministic functions and we need this
3:15:55
homomorphism property so can be copied because I want to do copy composition along these vertical
3:16:01
morphisms then we've got coand so they've got you know two legs and an apex and we decorate the Apes with both
3:16:07
interfaces which are these type decorations and then factors on the tensils of these of these decorations so
3:16:14
then we've got this kind of double decoration thing going on and decorating usually as a sign that you've got a grit deconstruction and
3:16:20
indeed we've got sort of two gr deconstructions one over another and then okay there's a lot of
3:16:25
stuff going on but we we have these now squares but now we've got morphisms in the Apes as well um but we need them to
3:16:32
project out to deterministic functions on the legs and that's why we have this dependence on the whole span structure
3:16:38
because we need to look at the legs to say okay which bits of deterministic and then okay that gives
3:16:44
you a kind of way to say okay well got deterministic morphism exposed bits but not necessarily deterministic morphisms
3:16:51
between the decorated factors between the factor decorations in the middle and this gives me my whole double category
3:16:57
structure and it's in detail in the paper of course and you know to see why you need this common morphism it's
3:17:03
basically as I said we want to compose along these like these determin along these transformations of the
3:17:10
interfaces but you know we need to be able to pull these Transformations through the copia which means that they
3:17:16
have to be common so that's that's easy but the the the nice fact of this double category is
3:17:22
that we can have like non- determinated Transformations on the unexposed interfaces um so you can do things like
3:17:29
marginalization which is something that you often want you don't necessarily want to keep all of the things around and that would be just put a state on
3:17:35
your hidden part and I guess I should finish soon but I've got some nice kind
3:17:40
of ideas about graphical calculus which kind of recapitulates some of the stuff that you see in the literature is only
3:17:46
the appendix in my paper but the idea is that we have these kind of Bubbles and we sort of compose them with these
3:17:53
spiders like you see in the sort of quantum information Theory work and if
3:17:58
we've got a sort of deterministic thing you I I write it with a sort of pointy box you can sort of pull it through a
3:18:04
spider and and you can sort of draw a bubble around like your network and then if you've got a determinist you can sort
3:18:10
of pull it outside of the bubble so this is a kind of a graphical way of representing these things which are often graphical in the literature
3:18:17
um and you know we can recapitulate quite easily the thing we started with and it's quite this form if you look at
3:18:24
this and you you look at some of these Factor graphs you see in the literature written by people they they often have
3:18:30
exactly not quite exactly but almost exactly the same kind of depiction which is obviously nice that you're sort of
3:18:36
recovering the thing that people do um informally I say informally but this
3:18:41
sort of double undirected wire undirected wiring diagram calculus isn't formalized it's not what I was do I was
3:18:47
just kind of thinking about copy composition and modeling um so it'll be interesting to see people work on double
3:18:55
trans double categorical you know wiring diagrams I'm sure people are thinking
3:19:00
about that and so okay that's that's it that's copy composition in these two cases we had directed ones and
3:19:07
undirected ones and they were kind of different but they were both formalized by this kind of gr structure which
3:19:12
Associates models in some sense to like types or interfaces and composes them by this topic composition
3:19:19
method one thing I sort of started thinking about when I was thinking about this problem was that it would be nice
3:19:25
to have amongst the undirected models the directed models in the same way that you get amongst relations
3:19:32
functions um you can sometimes think of kernels as like enriched relations and you know I guess in this enriched
3:19:38
setting you get a similar thing but this isn't the right setting to do this um you would need to work with more like
3:19:44
this in Rich relation set and I guess in that context you may get directed models
3:19:50
or directed things amongst the undirected ones but they wouldn't necessarily be like these models this is really about sort of building up the
3:19:57
models and as I say it' be nice to have this kind of 2D calculus for undirected wiring diagrams um one thing we'll hear
3:20:04
about later in the conference is kind of belief propagation belief propagation is like a thing you do on a factor graph in
3:20:11
some sense to compute like um approximations of the distrib at the
3:20:17
nodes given some constraints and one of the things I was thinking about when constructing this double category is
3:20:23
using this thing for inference for doing this kind of approximate inference by
3:20:29
algorithms like relief propagation and I hope that strates like this might be for that um and that's all thank you
3:20:45
everybody okay question yes
3:20:53
please you go back sleven slide seven yes no that's
3:21:01
fine yeah
3:21:08
yeah it's actually quite subtle yes the reason is that at some point you might
3:21:14
want be taking the direct image yes the bar set on the projection yes that's not
3:21:19
yes considered that potentially a gap it's potentially a gap okay so yes um
3:21:26
there are lots of things which one might want to make a little bit more precise I'm working also in this very General
3:21:31
Session we don't necess always work in that general setting I was also interested in this kind of like product
3:21:37
structure there's lots of questions about like making this like as nice as it could be
3:21:43
yeah so I I I haven't got answers to that but I would like to have yeah okay
3:21:50
all right there's two more questions all right maybe question on everyone mind um
3:21:56
in your Factor graphs yeah and you make them do plate
3:22:02
notation well you give me the I don't know the two two dimensional stuff maybe
3:22:10
I don't know uh yeah I know it's the thing that people of every seen so I
3:22:15
don't know yes all right could you comment on the
3:22:20
relationship between this oh uh yes so when you say this do
3:22:26
you mean this or the this the previous one direct direct yeah uh it's uh it's
3:22:32
pretty much exactly a coop power thing um you can write it yeah you can write
3:22:38
this as a kind of dependent Coop power and it's it gives you exactly the same
3:22:43
double category but another thing wanted to do was come to the conference and say hey Evan's work on double decorated
3:22:51
Coast bands was really useful for this okay and that's not the same one thing I
3:22:56
think is that this story of um double gin deconstructions for deorations blah
3:23:03
blah blah is more General than the co- power story um because of the fact you
3:23:09
can have this dependence on both legs so in this case as I said I have trivial decorations on the legs and makes it
3:23:16
relatively easy to do the decoration the sort of coopar thing but when I was working through doing that I didn't I
3:23:24
didn't see a way to do what I would wanted was the theorem to say oh like these decorated these
3:23:30
double decorations are the same as coopar but there's it's not I can I think I can always show a decorate like
3:23:37
a copara thing of some form gives you a double uh decorated thing of of this
3:23:43
form but I don't have a implication in the other direction yeah but I haven't written that down
3:23:50
yeah as always
3:23:59
cool oh right yeah um so here I'm just saying I want to
3:24:05
consider this as a factor and I or I I've got this factor which I'm considering as this composite Network
3:24:11
and I have like a two cell which allows me to sort of reveal the fact that inside it is a factor and but it is made
3:24:18
up of this composite with this this morphism on this exposed Edge
3:24:26
um it's just a way of saying that the it's kind of way of depicting the naturality of the horizontal
3:24:33
composition is that what's the oh the squares I I drew them because
3:24:40
you the diamond is kind like I thought you s puncture a hole in the bubble so they're the deterministic on so you can
3:24:46
you know like you know like in biology where you have some molecules that can get out of your cell um and these ones they're they're too flat they push up
3:24:53
and you can't get out so these are the nonous yeah but that's not
3:24:58
formal made sure that yeah it's very annoying T to I just used diamonds and
3:25:04
squares and oriented them yeah okay any more
3:25:11
questions yes could you say something uh more about you started with string diagrams for and then you're like using
3:25:18
upad for the back of yeah well you mean stream diagrams
3:25:24
Here Yeah you mentioned uh opad in a positive way
3:25:30
oh I well I I said operad but actually I don't like saying operad I meant
3:25:36
multicategory or what people call colored operad um but yes you know
3:25:41
because you can Nest a diagram instead another diagram but I didn't present it like that I present it like this double
3:25:48
I guess it's a monal double category and it probably corresponds to some double operat algebra
3:25:55
um there's something underlying your question I thought I think that that that is u a
3:26:03
big future Direction yeah it's useful to you I think it might be useful to other
3:26:09
yeah yeah yeah yeah this is an example of I think of one of those yeah again
3:26:14
not not proved in the paper but yeah thanks one last question
3:26:20
anyone okay so last seconder speaker again
3:26:50
oh
3:27:00
[Music]
3:27:44
whoa what's up
3:28:11
are come on come on all right
3:28:17
the last speaker for this morning is Mark shelis from nean who's
3:28:24
going to talk about coms Cazal and contractions in atomic Market cies
3:28:30
please thank you so hello hello everyone and welcome to this presentation titled kum's cality and contractions in atomic
3:28:36
Mark of categories this is Joint work with Darin uh and before I get into what we actually interested in I want to
3:28:42
start with a brief historical overview of what preceded paper because it's I we
3:28:48
consider it as a refinement of several previous developments so the first one is the PHD thesis of hton Larson from
3:28:54
2021 in which he worked in a setting of semi Cartesian monoidal categories assuming Universal dilations which is a
3:29:00
fairly strong property and in that case he defined the notion of a causal Channel and he hinted at the notion of
3:29:06
trace for those caal channels but he deliberately didn't go into that direction so we kind of wanted to fill in that hole the second prerequisite I
3:29:14
want to mention is this paper about uh kums in which they Define different Notions of Kum equivalence and they give some conditions on when different
3:29:21
Notions of Kum equivalence coincide so we will also generalize that result in there and the last paper I want to
3:29:26
mention by fital or like all of this people is U is the one that defines the
3:29:32
atomicity aium for Mark of categories which will be very crucial for our development so for historical accuracy I think I uh wanted to give a shout out so
3:29:40
what are we actually interested in are so-called contraction identities so here is a simple instance of a contraction identity so as you can see on the left
3:29:47
let just look at the morphism with F1 you kind of can see from the structure of the string diagram that the
3:29:52
information flow is organized in a way that uh to compute the output X you don't need to use the input X so ideally
3:29:59
I would want to be able to feed the output of Type X to the input wire of Type X and then kind of uh pull that
3:30:06
wire together that's what you can see on the right and the question that we ask is in which cases do these kind of
3:30:11
identities hold so if I have two different kind of factorization of a morphism which we later call signaling
3:30:16
then if I pull the wire together of equal wires together from equal morphisms will this still be equal and
3:30:22
this is an instance of a contraction identity which later I will formal definition of but uh so first okay
3:30:29
contraction identity is always holding Trace monoidal categories this shouldn't be too surprising so as you can see on the left I have my original morphism but
3:30:36
I took a trace of it now using the trace AUM first I uh tightened it then I applied yanking then I applied the mark
3:30:43
of category abum of being the being commutative I managed to reduce to this
3:30:48
thing so it's easy to see them than the contraction identity on the previous slide holes but usually the mark of categories we are interested in are
3:30:54
definitely not tra monoidal so we want to give some other kind of condition and we can see that contraction identities
3:30:59
fail in certain Mark of categories for example in Bal stock which we have already encountered before the category
3:31:05
of standard Boral spaces and the mark of kernels between them and there's a coroller of this immediately because as
3:31:12
we've seen these contraction identities are always validated in tra idal categories is which Bal stock cannot be
3:31:18
embedded by which I mean Faithfully embedded into any tras monoidal category why do we carry my T like you
3:31:25
never wanted to do that in your life well uh whom and cality so there was this paper by yakob inas in 20121 in
3:31:32
which they use a solve a causal inference problem using string dimatic techniques in in finto and for that they
3:31:39
actually used an embedding to compact Lo category of matrices with positive ENT to be able to use the cup and cap
3:31:44
structure that's available I will later get back to this example when we thought more about but it is upor not clear
3:31:50
how we would do a similar thing for Boral stock uh so this is really the question if you want to do causal
3:31:56
inference in ble stock then uh we don't have this method available so we definitely care about this embedding and now I will show why the
3:32:03
contraction it fails in B stock and I will give you give you the idea of why it fails has be atomicity axum so some
3:32:12
minimal measure Theory going on here so I just take the unit interval and I take the measure on it and I take two
3:32:17
measurable predicates uh one is the falsity predicate one is the equality predicate uh and let's see why the
3:32:23
contraction identity fails here is why it is fails so what happens intuitively if I uh sample from the distribution and
3:32:30
compare it to any element in the unit interval they have a zero probability of being equal so they are almost TR not
3:32:36
equal so as you can see in the reduction then uh the two the two string diagrams
3:32:42
here and here will be equal to each other to thing in the middle uh but if I I pull the wires tight again just
3:32:47
intuitively if I the left will of course be constant zero but on the right I just take a sample from the distribution I
3:32:53
copy that sample and then I compare if the sample is equal to itself which it is so we see that the contraction
3:32:58
identity fails in bble stock uh sad so why does it happen it is not Atomic so I
3:33:04
claim that this is the source of the problem and by using the atomicity aium which I'm I will shortly recall we will
3:33:11
actually solve this problem uh for Atomic Mark of categories okay what happens here next oh a recap on Mark of
3:33:18
categories so we have seen this today so I will be very brief about it a mark of category is a symmetric oid category
3:33:25
where every object is equipped with a co- commutative commo structure by that I mean there's a morphism copy or I my
3:33:31
Delta here which goes from X to X tensor X denoted like this and there's a discard morphism which goes from X to
3:33:37
the tensor unit such that all the co- commutative common equations are satisfied these are these three and that
3:33:44
the discard map is natural so that implies that the tenser unit and Mark of categories is a final object so this is
3:33:50
the setting that we are working in oh it's there um and then we recall the
3:33:55
notion of Almost sure equality in Mark of categories which is um if you don't believe it I can just tell that this is
3:34:00
a generalization of the measur usual measured theoretic notion and so we say that F1 is p almost surely equal to FS2
3:34:06
if this equation of string diagrams holds so I think in earlier versions of the axum we didn't have the extra wire
3:34:12
there I think nowadays this is fashionable to put it there as well so this seems to be a better notion and then we can say that P is absolutely
3:34:19
continuous with respect to Q if Q Almost sure in quality implies P Almost sure
3:34:25
equality okay and now we are ready to finally recap the atomicity
3:34:30
axium uh so first we say that a morphism p in a mark of category is atomic if
3:34:35
copy after P Almost sure equality no if copy after p is absolutely continuous
3:34:41
with respect to p tenser p so p tenser p Almost sure equality imply copy after P
3:34:46
Almost sure equality there's a graphical depiction of it if this seems a bit too complicated so if the we say that P is
3:34:53
atomic if the equation on the right holds then the equation equation on the left holds then the equation on the
3:34:59
right also holds so what is the intuition behind this aium uh first an
3:35:04
example is okay I have to say that the mark of category is atomic if every morphism in the mark of category is
3:35:10
atomic and fin stock is a such an atomic Mark of category intuitively because
3:35:15
copy support of copy after p is contained in the support of P tensor p uh in general the relation between
3:35:22
supports and atomicity is not this simple but in this case this really holds because this support inclusion
3:35:27
holds but then I claim that for example Bal stock is not Atomic and for that we
3:35:32
can see what are the atomic morphisms in Bol stock and the mark of Kernel from a
3:35:38
to X is atomic if and only if it is completely Atomic in the sense of measure Theory so what does this means
3:35:44
the atoms of a measure are the points which have a mass is this written it's written right
3:35:50
so with the points which have a mass and I say that Mark of Kernel is completely Atomic if for all inputs a the the
3:35:56
weight of the weight of the set of atoms is one and the L back measure is not completely Atomic in fact it doesn't
3:36:01
have any atoms uh and then we can recap the previous counter example that we have seen so it the idea is again very
3:36:08
same it's uh interesting I think it's still visible enough so on the left we we say exactly that if we take the
3:36:15
measure Stander with itself then the equality holds again because if I take two independent samples from the measure
3:36:21
and compare them for equality they are have a zero probability of being equal so this will hold but on the bottom
3:36:26
again if I copy the sample and check with it s for equality then we can see that uh it will they will always be
3:36:32
equal but the falsity predicate always give me zero so we see that formally this atomicity axium fails in BOS stock
3:36:38
U okay so let's Rec up some more Notions
3:36:43
before we can finally to see by atomic Mark of categories satisfy the contraction identities so this is the
3:36:50
standard notion of conditionals I say that the mark of category has conditionals if every F can be
3:36:55
factorized in this way so this is really just a categorical generalization of conditional probability I can first uh
3:37:02
take one of the marginals and from the marginals using a conditional probability I can compute the other marginal a mark of category has
3:37:07
conditionals if I can do this for all the morphisms and then there's another annoying technical property that we
3:37:13
require in our developments which is that the mark of category is cancellative if those implications hold
3:37:20
so if I dis if these equations where I discard left input hold then these should also hold our favorite Mark of
3:37:27
categories in general are not necessarily consultive so sets is not cancela f stock is not consultive if we
3:37:33
put the empty set on that wire because if this is the empty set here then this is just the empty function and the two
3:37:39
empty function will always be equal to each other we can usually resolve this problem by excluding this degenerate cas
3:37:45
and just saying that the empty set is not included in our collections of objects or we just assume cancel activity on the Fly for certain buyers
3:37:52
so this is not a real obstacle but it's important side condition of the theorem that will soon
3:37:58
appear so knowing that now we Define non- signaling morphisms and these are
3:38:03
the class of morphisms for which we hope to define a notion of Trace which will lead to the contraction identities so I
3:38:09
say that f is non signaling from the W input to the W output intuitively if to compute the W output I don't need the W
3:38:16
input so how does this string diagram encode this if I discard the left output
3:38:24
then I might as well Define the right input and I can just have a morphism from X to W that directly computes W
3:38:31
from X without using the W input so this exact this exactly mean this the existence of this factorization that I
3:38:37
don't need this uh input to calculate this output and we say a morphism f is non signaling if it admits such a
3:38:43
factorization and then now we will Define a notion of trace for non signal
3:38:49
morphes how do we do that if a mark of category C has conditionals we can
3:38:55
nicely characterize all the non signaling morphisms all the non signaling morphism have a disintegration of this form so
3:39:02
this is the result that appears in our paper and this is an if and only if so if a morphism ad me such a factorization
3:39:08
then it is known to be non signaling and now we can use this uh representation to calculate a trace so how do we do that I
3:39:15
want to Define this Trace operation for f where I assume that f is non signaling so like normally here would be an output
3:39:21
here would be an input and I kind of want to form this feedback loop so I take such a disintegration which I know exists because F was non signaling and
3:39:28
then like you can graphically see how I pull this wire tight I will arrive at this and this will be the definition of
3:39:33
the caal trace okay you might say you can give me a different disintegration I
3:39:38
give you a different caal if we don't get anywhere and normally this would be true but if our Mark of C is atomic this
3:39:47
is independent of the choice of this integration and I would say this is the most technical result in the paper so
3:39:53
this is from what everything follows and the Assumption of the atomicity is very important when we try to prove this kind
3:39:59
of theorem so this is not the case for any any Mark of category conditionals okay so what do we do with
3:40:05
this so this caal better satisfy some properties which resemble the tra monoidal category aums and in fact they
3:40:13
do I will show you one examples of analogy strengthening just to see how these propositions compare to the TR
3:40:18
monoidal category aums so let's just look at the picture for a moment if this was a tra monoidal category the exium
3:40:25
would sound like this these two things are equal period uh we do it a bit
3:40:30
harder so we say that if f is non- signaling so f is traceable then G tensor f is also traceable moreover this
3:40:37
equation holds and the for many of the other aums we also have this kind of one directional implication um so we don't
3:40:44
necessarily have want to say that uh if this is traceable this is also traceable because this will not be true in general
3:40:50
but we always have this one direction of the implication and we get a restricted version of the tra monoidal category
3:40:55
aumes uh just for historical accuracy uh this is weaker than the partially Trace
3:41:01
categories from Harian stock if SC if uh someone would think about those
3:41:09
okay now I will show that Cal traces not only exist in a atomic Mark of
3:41:15
categories with conditional but also in free Mark of categories if you haven't seen free Mark of categories over
3:41:20
generators yet at all you need to know if they exist they are kind of like categories of string diagrams and we can nicely represent them by hyper graphs if
3:41:27
you've seen the hypergraph representation this is for you uh so string diagrams that look weirder so
3:41:32
this string diagram can be represented by this hypergraph uh the idea is that these are the sort of the input ports
3:41:37
where the data can go in these noes correspond to the wires and the box and the hyper edges correspond to the boxes
3:41:44
and the idea I want to I want to take take the trace of this morphism so I kind of want to glue these two points together but first I have to make sure
3:41:51
that this is non signaling and there's a very simple condition to express that this morphism is non signaling I can
3:41:57
just say that a morphism in a free Mark of category is non signaling from a set of output input ports to a set of output
3:42:03
ports if and not if there is no directed path from here to here we can see that there is no directed PA there so we can
3:42:11
just uh glue this points together getting the caal trace so what happened is there were two different points I
3:42:18
glued them together here I removed one one output part and then I got the trace of this morphism and this also satisfies
3:42:26
all the causal Trace properties there's a bit more things going on I didn't put it on the slides but I can draw here
3:42:33
maybe with the black right I black uh so if we were to take a
3:42:41
trace of let's say this morphism where this is an a this is an ax then I can
3:42:48
clearly do this trays but after this I would need to normalize the hyper graph
3:42:53
uh right because uh this should be equal to this this should be equal to this so the
3:42:59
combinator is a bit more complicated than just glowing here but it all works out nicely and I satisfy all the causal Trace
3:43:05
properties okay and now the big thing the big definition we say that a mark of
3:43:11
category C satisfies all contraction identities uh what this is saying before I go into
3:43:17
the formals is that if I have two morphisms in the category which are equal to each other then it doesn't
3:43:22
matter how I factorize these morphisms and then take the trace uh the traces will still be equal so formally what
3:43:29
happens here is uh if I give an interpretation to boxes in the signature Sigma then I get a functor from this
3:43:35
free Mark of category to C and then I say that if I take to any two non- signaling morphisms in the free Markov
3:43:42
category uh and I choose an interpretation of box uh if the interpretation of these uh
3:43:48
morphisms is equal then the interpretation of the syntactic traces will also be equal by by this contol w i
3:43:54
mean the syntactic trace uh when I contract on W and essentially if we
3:43:59
would go back to the beginning the contraction identity I have shown is also of this form and then the main
3:44:05
result uh which is really nice is that if my mark of category is cancela atomic and has conditionals then all of the
3:44:12
contraction identities are satisfied yeah so what is this good for
3:44:19
um oh I wanted to discuss the premises of this theorem for a moment uh so I
3:44:24
both assume Atomic and conditional so you might ask whether they have to do something with each other uh we have seen a category which has conditionals
3:44:31
but is not Atomic B stock we also conjectures that there are categories which are Atomic but do not have all
3:44:38
conditionals and we conjecture that the fre Mark of categories are such we didn't manage to prove this but we are
3:44:44
almost sure of it so speak okay but this is something for the future but we are definitely
3:44:49
interested in the information flow properties of three Market categories okay now I can go to so what our
3:44:56
we they already mentioned uh like there was talk talk of lenses app people here like it uh so extensionally I can
3:45:02
just think of AUM as a non- signaling morphism drawn in a very evocative shape so I kind of want to say that to compute
3:45:09
this B output I don't need a b Prime input so I just need I I should be able to put a morphism in there from more
3:45:15
intentional definition I could say that AUM is a pair of morphisms which have exactly the right
3:45:20
types and again my aim with the is to put morphisms in that hole and how do I do this it's called
3:45:27
Kum insertion and this is actually an instance of a caal trace uh so if you see that I told that this is a non-
3:45:33
signaling morphism so to compute this output I don't need this input so if I
3:45:38
draw c a bit more normal you would see this kind of structure and this is exactly a causal Trace so Kum insertion
3:45:44
if have clal tra really an instance of this and then I can turn back to this caal inference paper where they wanted
3:45:50
to do exactly this so they used For A Cause insance problem in fin stock but they wanted to do this insertion to
3:45:57
be valid defined so they made an Excursion to a compact L category of matrices with positive entries there
3:46:04
they done this kind of stuff which they can do because there's cups there's caps cops and caps uh and then they Prov that
3:46:10
the result again lives in fin stock so they get a stochastic Matrix this whole exercise now we know is unnecessary
3:46:17
because we can just say that this is an instance of a Callo Trace fock has Callo traces I am free to form this kind of
3:46:22
thing so this maybe simplifies the formalism of a causal inference problems in this manner okay and to conclude before we
3:46:31
all have lunch uh I have something to say about Kum equivalence so this is from a paper by he and comfort I think
3:46:38
they will be referred to on the next slide that I have three different ways to say that gos are equivalent to each other the first one is the simplest and
3:46:45
most elegant in my opinion is that their extensions are equal so how do I get the extensions I just insert the swap map so
3:46:51
this is called extensional equivalence I can also have that they are contextually equivalent which means that all whom
3:46:58
insertions are equal and then I also have this optic equivalence which looks a bit scary for people not friendly with
3:47:04
Co like me but it's actually just quoting under this equivalence relation by sliding so I say that if they are
3:47:10
identified by the equival insulation generated by all of these slidings so here this age belongs to the lower part
3:47:16
of the K perod belongs to the upper part of the KB then they are optically equivalent and the general results from
3:47:23
a heaven and comfort is that there's a chain of implications optic equivalence always implies contextual always implies
3:47:30
extensional but they also show that all three Notions of this Kum equivalence coincide in compact CL categories and
3:47:36
cartisian monoidal categories now there's more to it thanks to us we say
3:47:41
that if a category has causal traces then the extensional and contextual [Music] Kum is also coincide and if you also put
3:47:49
throw Universal dilations in the mix if you know what they are then that implies that all Notions of kumal coincide so
3:47:56
specifically in finto the category for finite discrete probability all three of
3:48:01
these Notions coincide so that is pretty nice okay so at this point I think I
3:48:07
will wrap up so what did we achieve uh we refined several previous vers and
3:48:13
defined the notion of a causal Trace uh and show that if they exist in Mark of categories they imply all contraction
3:48:18
identities so this is really a strong addition to the equational theory of Mark of categories we recognize the role
3:48:24
of the atomicity axium in this uh in this theory of causal traces and using the existence of causal traces we gave
3:48:30
new sufficient conditions for equivalence of Kum equivalences maybe like there's a better phrasing uh in
3:48:36
Mark of categories oh these last two points are kind of the same sorry about that uh
3:48:41
then I would like to thank you for your attention and if you have any questions uh feel free
3:48:54
okay thank you very much there is a question Alex
3:49:08
please exactly you say anything about the so if I have if you have
3:49:22
I haven't thought of it so maybe I would refer the question to Dario immediately but I think we haven't thought of this
3:49:27
right future work we'd love to know the the free Mark is one of these
3:49:34
things where um I mean it does it doesn't have a trace but can you can you Ed it where
3:49:42
would you embed it probably into a three so that should work
3:49:50
but wasn't there some problem with that yeah so if we understand that P
3:49:57
then that will show the way [Music]
3:50:03
to right any more
3:50:10
questions seems not so let speaker
3:50:28
we stop the recording or pause it or
3:50:49
yeah yeah
3:51:17
did you know I was his TA in undergrad yeah he in the
3:51:23
course yeah and then and
3:51:30
then I'm out Thurs
3:51:35
morning I'm yes Tuesday or Wednesday night yeah okay great
3:51:56
you're collecting yeah you have you have a
3:52:03
rather bad habit you're back at some stealing them from American
3:52:13
insti okay yeah are
3:52:38
[Music] youim you
3:52:45
yes
3:53:34
I was few years
3:53:47
oh that's
3:54:03
[Music] nice my handle is
3:54:21
how do I send you well I'll
3:54:26
just
3:54:40
yeah apparently Nate all good very in doing mcmc
3:54:46
over of like formal models which
3:54:52
is I didn't know he
3:55:03
wasc have you met yeah yeah
3:55:09
so um he's he's very interested did in a
3:55:14
lot of the stuff that like we doing
3:55:20
essentially I think exciting when yeah he both coming from been doing
3:55:29
anyway yeah well apparent like all of a lot of his stuff is like
3:55:35
doing Petr stuff like
3:55:41
yeah I'm G to try and Gra
3:56:09
I
3:56:23
[Music]
3:56:31
you
3:57:12
EXA
3:57:18
right
3:57:45
itory
3:59:13
e
3:59:43
e
4:00:12
e
4:00:42
e
4:01:12
e
4:01:42
e
4:02:12
e
4:02:42
e
4:03:12
e
4:03:42
e
4:04:12
e
4:04:42
e
4:05:12
e
4:05:42
e
4:06:12
e
4:06:42
e
4:07:12
e
4:07:42
e
4:08:12
e
4:08:42
e
4:09:12
e
4:09:42
e
4:10:12
e
4:10:42
e
4:11:12
e
4:11:42
e
4:12:12
e
4:12:42
e
4:13:12
e
4:13:42
e
4:14:12
e
4:14:42
e
4:15:11
e
4:15:41
e
4:16:11
e
4:16:41
e
4:17:11
e
4:17:41
e
4:18:11
e
4:18:41
e
4:19:11
e
4:19:41
e
4:20:11
e
4:20:41
e
4:21:11
e
4:21:41
e
4:22:11
e
4:22:41
e
4:23:11
e
4:23:41
e
4:24:11
e
4:24:41
e
4:25:11
e
4:25:41
e
4:26:11
e
4:26:41
e
4:27:11
e
4:27:41
e
4:28:11
e
4:28:41
e
4:29:11
e
4:29:41
e
4:30:10
e
4:30:40
e
4:31:10
e
4:31:40
e
4:32:10
e
4:32:40
e
4:33:10
e
4:33:40
e
4:34:10
e
4:34:40
e
4:35:10
e
4:35:40
e
4:36:10
e
4:36:40
e
4:37:10
e
4:37:40
e
4:38:10
e
4:38:40
e
4:39:10
e
4:39:40
e
4:40:10
e
4:40:40
e
4:41:10
e
4:41:40
e
4:42:10
e
4:42:40
e
4:43:10
e
4:43:40
e
4:44:10
e
4:44:40
e
4:45:09
e
4:45:39
e
4:46:09
e
4:46:39
e
4:47:09
e
4:47:39
e
4:48:09
e
4:48:39
e
4:49:09
e
4:49:39
e
4:50:09
e
4:50:39
e
4:51:09
e
4:51:39
e
4:52:09
e
4:52:39
e
4:53:09
e
4:53:39
e
4:54:09
e
4:54:39
e
4:55:09
e
4:55:39
e
4:56:09
e
4:56:39
e
4:57:09
e
4:57:39
e
4:58:09
e
4:58:39
e
4:59:09
e
4:59:39
e
5:00:08
e
5:00:38
e
5:01:08
e
5:01:38
e
5:02:08
e
5:02:38
e
5:03:08
e
5:03:38
e
5:04:08
e
5:04:38
e
5:05:08
e
5:05:38
e
5:06:08
e
5:06:38
e e
5:07:35
Oh by I got
5:07:42
my how I design how I design well very
5:07:49
good so here we can do resume stuff
5:07:55
okay so yeah and hopefully the pile is not too big oh still I thought I had enabled
5:08:02
disabled waiting no there's oh
5:08:17
um in 10 minutes since uh according to the
5:08:23
schedule I don't have any tort duties in minutes I have to uh make my students to
5:08:29
meet but if there's anything I can do first I can tell them to come admin lator oh I see so you're meeting some
5:08:36
I'm meeting a student just oh right but it's if you need me around at least the first 10 minutes I can tell no what
5:08:43
on I mean I should not
5:08:50
yeah no it's right you don't yeah so yeah I think if Zoom is working oh I
5:08:58
just check it's still on downstairs yeah and then if of course I'm still in
5:09:04
the department so [Music]
5:09:13
[Music]
5:09:19
[Music]
5:09:26
here yeah wait for the second session is it
5:09:32
the no act school is at 2: oh okay after act and after act school okay it goes to
5:09:38
parallel session one session in physics and one session here so would be so be good to have some
5:09:46
some res uh because also I have to leave around my son is in
5:09:54
the yeah I mean I guess I could I could be in either room I suppose right then we need to find yeah Mario city is
5:10:01
definitely here so yeah ex but after the copy so copy copy
5:10:10
everything is in physics then just we come here for some people hopefully they come and they don't to stay
5:10:17
other
5:10:33
yeah it's true that's going to do that one maybe it's not oh that's fine oh but then okay then we probably want to
5:10:40
the from here I think they go by okay now they're off lights on in the
5:10:48
previous session
5:10:54
right much more air in this it's definitely air so
5:11:03
this thank very much
5:11:34
yeah me too normally too but I'm wondering if I want to be visible to the speaker and my
5:11:52
here I think I think I'll Che
5:11:58
okay just yeah it's slow enough I'm thinking
5:12:18
it's like you go into one other room
5:13:08
and so
5:14:08
okay I don't know why
5:14:35
so next one is operating systems okay
5:14:42
so it's a mixture of like datab and
5:14:49
like people have been doing stuff like connecting an iPad or
5:14:55
connecting let's see what we can do
5:15:02
here um can we put USB on here just making sure that all get on to
5:15:10
the zoom and everything
5:15:20
so okay can be somebody who can do
5:15:25
this when David asked me I should have said do I get technical assistance
5:16:03
the other so it goes between glas because I know a nice
5:16:12
[Music] yeah I'm try to figure out
5:16:37
[Music] yeah I
5:16:50
[Music] know you can help this
5:17:37
you
5:17:54
I don't know one
5:18:23
call
5:18:41
the microphone so make sure you speak up
5:18:46
okay talk right here if you speak like in
5:18:52
Italian like to me it comes naturally but every people need to be I'm very
5:19:00
quiet oh yeah
5:19:09
TR again click on it
5:19:24
first that's the
5:20:00
know
5:20:13
my speakers know what they're doing so Michael
5:20:21
speaking number three the isn't it
5:20:41
you probably
5:20:55
know very good yes hasn't collapsed
5:21:16
introduce our first speaker for session Michael Lambert be speaking on
5:21:22
joint work with pson on representing knowledge and querying data using double
5:21:27
factorial semantics thank you well thanks everyone
5:21:33
for coming thanks also to the uh organizers for
5:21:39
um putting on our conference um yeah this is a report on some joint work with
5:21:46
my co-author Evan Patterson um we did most of this work a couple years ago in
5:21:51
summer of 2022 uh we got around finally to writing
5:21:57
up the draft preprint and uh that's on the archive um take a look at it um
5:22:05
right so yeah we're going to be talking about representing knowledge and cing data um I believe the Innovation
5:22:12
here's use of double categories so um I'm going to try to illustrate that
5:22:17
throughout um this is at once kind of a
5:22:23
very high level overview or advertisement what that's in the paper
5:22:29
we're also look at some examples um so let's just uh start off
5:22:35
with um a question uh what is representation uh maybe this is a little
5:22:41
bit of a vague question or sort of a vague phrase which I will um answer with
5:22:47
um I don't know maybe sort of an esoteric one um ontology logs or ologs for short um what are those uh schematic
5:22:55
representations basically of let's just say concepts um attributes
5:23:02
and facts about them um what do these look like well so Concepts are very
5:23:09
flexible right think of a concept Arginine for example um we schematically
5:23:17
represent these in boxes um attributes are arrows uh be between Concepts and
5:23:26
facts are equational statements that say that different paths given by attributes
5:23:33
between concepts are the same um if that kind of looks like a
5:23:38
diagramm in category well that's kind of a conceit behind um ologs uh generally
5:23:46
we think of uh or identify objects and Concepts identify attributes and arrows
5:23:52
identify facts commuted diagrams so an olog is kind of like small category in some sense and uh in practice we take
5:24:00
this V and ining of an sort generating some
5:24:06
more C um these are often identified with certain types of database um
5:24:13
schemes um these are small ads right um
5:24:18
so there's kind of two vocabularies here one of
5:24:24
knowledge orentation and one of databases we're going to kind of go back and forth between those um and so in
5:24:32
this vein um in as much as an olog is kind of an abstract framework for relationships between Concepts and
5:24:38
attributes and Sh like that and it's merely abstract we can instance the
5:24:44
abstract knowledge base or scheme with a set valued functor which we can think of
5:24:51
as populating the abstraction of actual thing for actual
5:24:58
entities we typically represent these as tables so here's an example this is one
5:25:05
of these more or less sort of standard um employee Department um examples uh
5:25:13
the main type here is one for employee um every employee is in some Department
5:25:18
every employee has a first and last name and every employee has a
5:25:24
manager um there are no facts here this is relatively simple um and we might
5:25:29
instance this with a table uh where every attribute is a
5:25:37
column in the table um and uh we notice here right that these
5:25:46
attributes are all functional in the sense that every employee has one first name one first last name one manager
5:25:53
Works in one Department Etc um these examples are all very sort of standard
5:25:59
I've actually lifted these um straight out of the references on this material
5:26:07
um these are what I think of as being functional OLS right these are indexed by categories and the attributes are
5:26:15
closly function um but not every relationship between Concepts that
5:26:23
we we might like to formalize mbly function so my
5:26:29
uh co-author Evan Patterson came up with this idea of what I like to think of relational bogs uh there's a lot more
5:26:37
machinery in this definition if you know about such things he basically said well a relational olog is a small by category
5:26:46
of relations in the sense of caronian Walters um and in particular such a thing as a cartisian b uh these
5:26:53
definitions are kind of mouthful they're fairly abstract but um generally the
5:26:59
idea is that the errors are supposed to includ relation genine relations not just uh functions and these are way more
5:27:06
expressive right the relations friend of enemy of hopefully everybody have more
5:27:11
than one friend hopefully most people have no enemies right these are not necessarily
5:27:16
functions um we get the functions back using definition
5:27:23
that caronian wals give of maps which are supposed to be
5:27:28
functions so it's kind of a generalization of the purely functional perspective the the issue is that um
5:27:35
like I said cian by categories are pretty abstract a little hard to work with um
5:27:41
there's a lot of overhead um in kind of getting into the definitions
5:27:47
um the thing that I find kind of frustrating about the B categorical approach two is that um since the
5:27:55
relations are encoded Maps there's a bunch of equations Associated so from an implementation
5:28:02
perspective if you were to try to base data structures upon such a thing and actually code it at all you're going to
5:28:09
have to ask for a bunch of equations and that's computationally not ter terribly efficient so that makes equational
5:28:15
reason hard maybe there's a different way of doing this that actually gives you the
5:28:21
expressiveness of of the relational Viewpoint that makes the functional reasoning easier and that's where the
5:28:27
double categories come in um so we've already heard double categor mentioned
5:28:33
I'm presuming this to be like a ground level uh Exposition so I just want to
5:28:40
remind us what a double category um is this is how I think about them we've got
5:28:45
ordinary objects arrows just a category you also have a second a second class of
5:28:52
what are thought of as arrows namely Pro arrows these go once again between objects I display
5:28:59
them horizontally like this they have a slash on them to distinguish them from
5:29:05
the ordinary arrows which I display vertically like this relating all the structure are these cells right which
5:29:11
have um an internal domain internal C domain and an external source and Target
5:29:16
in the form of ordinary arrows uh there's an external composition for pro arrows and cells that's associated only
5:29:23
up to isomorphism um arrows do compose strictly associatively um less be the a little
5:29:32
bit esoteric seeming there are as for most of us know tons of examples of such things uh sets and functions and spans
5:29:41
uh form double category sets and and functions and relations form a double category categor fun and pro funs or
5:29:48
double category uh ring modules metric spaces and post sets for example um
5:29:56
genuinely this perspective is a generalization both by categories and two categories in sort of orthogonal
5:30:03
directions and that the pro arrows um well sorry a by category is a
5:30:10
double category where all the ordinary arrows are merely trivial a two category is a double category where all the pro
5:30:16
arrows are trivial and likewise any double category has an underlying by category which you like Pro arrows and
5:30:22
an underlying two category just G byy the ordinary arrows I I like to think of these as the Rel like the relational
5:30:29
part and the functional part so that is kind of what's going on here that we're
5:30:35
looking for some kind of double category with some extra structure that gives us something like a double categorical olog
5:30:42
that would generalize both the relational ol and the functional OLS so that's what we're going to work
5:30:48
up to um we need some more structure though right
5:30:54
the the relational olog for in particular cartisian by categories and
5:31:00
that structure is used so we want something like a cartisian double category well what is
5:31:05
that well there the definition of such things um we want the diagonal and the unique
5:31:12
double functors to each have uh certain right ad
5:31:17
joints in a nice two category um it's also convenient to ask
5:31:23
that these double categories are equipments which just mean that the source Target projection of from the
5:31:31
category Pro arrows and cells then pair here of ordinary object arrows is a
5:31:37
vibration or equivalently a b fibration um and so we're going to cook
5:31:43
up to a nice generalization here and the theor that I think kind of makes this possible is that if you have a locally
5:31:50
subal cartisian equipment right having this and this
5:31:55
then the by category underline it is a local post and so on the basis of this result
5:32:02
we make a definition that a double category relations is a localle par equipment
5:32:08
satisfying um in its underlying B category the extra axing that Carboni
5:32:14
Walter asked for that distinguishes by category
5:32:21
relationship and we'll Define a double categorical olog to be um a small instance of suchal
5:32:29
thing so there are equipment structure and the cartisian structure will
5:32:35
actually do some work for us shortly so let's say what this means a little bit more precisely um basically this is
5:32:42
equivalent to saying that every Niche and every Co Niche fills in and has a
5:32:48
either restriction or an extension respectively the Restriction here fills
5:32:54
in the the niche with a cartisian cell cartisian relative to the source
5:33:02
and Target projection whereas on the other hand the uh
5:33:08
each is filled in with an extension cell here which is off C with respect to the
5:33:13
source Target projection meaning that these two cells each have nice lifting properties this is abstract so in
5:33:21
relations for example you can actually compute these by hand right uh
5:33:26
extensions well sorry restrictions roughly speaking pullbacks extensions roughly speaking are
5:33:36
in now these constructions will let us do some
5:33:42
manipulation of data and also help us to form more complex ideas
5:33:48
and propositions in our o logs um so let's also talk about uh the
5:33:57
cartisian structure briefly um this is basically saying that the categories of
5:34:02
ordinary of ordinary arrows and then of cells each have finite products
5:34:08
and these are related to each other nicely with all the external uh structure functors and as a result a rean category
5:34:15
Pro Arrows with fixed objects right has local products in the sense that um it
5:34:22
has well each one has all finite products and for example you can construct the terminal as a restriction
5:34:29
and you can construct the uh local product of two pro arrows let's say m and n by a restriction along the
5:34:36
diagonals which are given to you by the fact that category d0 has those
5:34:43
products great so uh now that we know a bit about
5:34:49
um abstractly right the sort of framework um I want to try to illustrate
5:34:56
the use of the framework in some examples uh so um I don't really like
5:35:02
the employee employer examples necessarily so I'm drawn to one of my favorite shows on television Stargate
5:35:09
sg1 you haven't seen Stargate that's fine you don't need to have seen it roughly speaking
5:35:15
Wormhole exploration of not just this galaxy but other galaxies Ancient Aliens
5:35:21
and stuff it pretty fun so if you haven't seen it it's highly recommended highly recommended um so uh first of all
5:35:30
I want you to make a point first point is that the structur genuinely more
5:35:35
expressive than just a functional or just a relational perspective so I'm going to
5:35:43
posit a a concept of system Lord the concept of Jaffa system Lords are
5:35:48
basically the rulers of the Ancient Aliens and the Jaffa are their Warrior slaves um who they need to actually
5:35:54
enforce their rule across this galaxy anyway and I'm going to deposit some
5:36:00
individuals here um one is a system Lord and one is a Java these I intend to be
5:36:07
sort of functional errors right these are constants in the logic that are picking out
5:36:13
individuals now let's say we want to a s of fact so every system Lord has a first
5:36:20
Prime which is sort of like the leading Warrior among all of his Jaffa armies
5:36:26
and so there is an attribute here system Lords jafa first Prime right thinking of
5:36:32
every one first Prime and so I'm going to assert the fact that
5:36:42
shil is first Prime of a propus that is just to require this diagram to be
5:36:47
commuted right so this is kind of the functional way of doing right and this
5:36:52
makes perfect sense but maybe the relationship between system jafa of first Prime is not necessarily or we
5:36:59
don't want to think of it as being functional why would we maybe not want to do that well there are certain system
5:37:06
who've been deposed Maybe they've lost all their jafa armies and so they wouldn't have first Prime at that point maybe jafa had served more than one
5:37:13
system Lord as first Prime maybe the system Lord still has all his armies but he's between first primes or something
5:37:19
like that maybe he's had multiple first primes well let's stipulate that instead we're interested in the relation first
5:37:26
Prime but we still have these individuals that we want to talk about well the individuals are the functions
5:37:31
and the relations of the relations and now we can actually look at a genuinely double category oral uh statement here
5:37:39
something like tilt is the first Prime of apus we
5:37:45
can actually form the proposition here with the Restriction sell is it true
5:37:50
well not necessarily but we can uh in the Old Log stipulate is Truth by just
5:37:55
requiring that that uh Pro arrow is in factal to terminal um in the H category
5:38:04
but you could also for example analyze right the extent of which this is true if we didn't want to stipulate or
5:38:10
require is truth um great so in other words right
5:38:15
genuinely we can build things that are very expressive and
5:38:22
rather well rather straightforward um so
5:38:27
what about manipulating data uh I believe the Innovation here is in some kind of loose sense like model
5:38:34
Theory um so structure preserving funter from some category C with structure to
5:38:40
Dort of a model of C in D um and so let's just Define WR a data instance for
5:38:46
a double O log would be a cartisian double functor Val relations um if you want to think of everything here being
5:38:52
sort of strict that's fine with me in implementation everything uh is usually
5:38:59
um we don't need to ask that the equipment structure is preserved because double functors automatically do
5:39:05
that um and so the last or not the last but the second point
5:39:11
that I want to make is that data manipulation in particular quing in
5:39:16
relational databases comes down to functorial semantics um how this is done right now
5:39:23
as far as I understand it although I'm not necessarily up to dat with state of the art um is in the functional
5:39:30
Viewpoint if you wanted to do a query or manipulate or sort of migrate your data
5:39:35
somehow which you would have to do is set up another
5:39:41
scheme introduce some kind of translation funter that would actually perform the operation and then you can
5:39:47
use these external ad joints in the co- categories to perform the manipulation
5:39:53
that you're looking for and so substitution fun is like a select and the right and left ad joints are various
5:40:00
joins tables um from an implementation perspective as far as I understand it
5:40:05
this setup is again computationally expensive so the thing with double
5:40:14
categories is that all of these operations of querying and manipulating can be done just by working
5:40:21
in the double category and then just trusting that the instancing funter does
5:40:28
what it's supposed to do so um in particular right select the
5:40:36
filter and join are basic just um Computing cir and extensions in Real
5:40:44
uh which as we've seen are pullbacks images um so let's let's look at another
5:40:50
example here um I like this example in that it sort of illustrates um a little
5:40:57
bit of My Philosophy double categories here um I like to think of the pro arrows
5:41:05
actually as being um second class of objects rather than second class of
5:41:10
arrows um and so this might Elevate mission to some kind of well it's a
5:41:17
concept right but also it's kind of an entity right and it's being tagged with dependency on other Concepts a mission
5:41:26
should occur on some date it should occur somewhere it should be to some end
5:41:31
and it and someone can carry it out right um so we can instance this with a
5:41:38
table for example and here are my rows um these are
5:41:44
all I think those are the air dates of the
5:41:49
episodes um these are some planets where the missions actually take place uh
5:41:54
simia for example was a a nice uh spot where uh sg1 assisted the simians in
5:42:01
their um Uprising against the system Lord who actually controlled that particular planet um there's a search
5:42:09
and rescue operation here on net 2 also carried out by sg1 Etc right um so let's
5:42:16
say we've got this data table right in our in our database um and let's
5:42:22
say the purpose and location of a mission is irrelevant to us for whatever reason and we just want to know when more our teams off well we can do
5:42:31
that we can just project right ignoring the location and purpose and then we
5:42:39
extend and then we get a new proposition here da team and in order to figure out exactly what the data looks like we'll
5:42:46
just use the instance and we'll compute the image uh in relations which is a new
5:42:52
table just date and team and this is precisely a select operation in a SQL
5:42:57
for example great say on the other hand we only care
5:43:04
uh about sg1 Miss right they are for example the particular team of the show
5:43:10
uh so we could stipulate an individual constant right sg1 of Team type um and
5:43:17
we form a restriction cell and this gives us um well essentially a new
5:43:23
proposition here we're just looking for SG sg1 missions and again the instancing data is going to preserve the
5:43:29
restriction and the cartisian structure and so this returns a new table sg1
5:43:34
missions right where we now just have the rows corresponding to um the submissions S1
5:43:42
carried out and this is filter in s in
5:43:49
great one last example of computations this one's a little
5:43:56
harder um this is the join of two tables that have a column in common um
5:44:03
abstractly this is kind of what it would look like imagine that P and Q are tables and they share a
5:44:12
column B in common um if we were to join these we would form a restriction here
5:44:18
along the diagonal from B um I want to uh apply
5:44:27
this so here's another ol log um again I think instancing the flexibility right
5:44:35
of this uh framework we have one uh one pro arrow for expertise and one pro
5:44:42
arrow for membership um every person hope hopefully every person
5:44:47
has has a skill right might have um you know multiple skills um but not every
5:44:54
person is on a team although each person who is on a team should probably just be on one team right um so whoops uh so I'm
5:45:01
going to instance instance these with some tables here Hammond his skill is command we got law combat command
5:45:09
archaeology sociopathy I guess a skill um uh yeah
5:45:16
I'm editorializing a little bit uh now all of these persons right and the
5:45:22
membership table are also persons in the expertise table however not however not
5:45:27
every person on the expertise table is on a team right only these five uh
5:45:33
individuals are actually on teams so perhaps right we have this have these two tables and we want to look at the
5:45:39
person and expertise in which team they're well that would be a join right each of these has a person uh instance
5:45:46
in common and so we're going to form this restriction cell where we pull back
5:45:52
along the diagonal for person type um and then we'll just compute the
5:46:00
corresponding pullback in relations uh to see what the table looks like and this is a the join right of the
5:46:08
expertise in membership tables where we're just getting the individual with
5:46:13
um his skill and which team great um yeah and uh this right is the uh join in
5:46:23
C um okay fantastic so um we have shown
5:46:28
right that uh we have lots of uh ability
5:46:34
to do standard operations in like relational databas in SQL for example um I like to call this this data munging
5:46:42
which I think is a phrase that's used in the field which I think is kind of funny [Music]
5:46:48
um so where are we with the implementation on this stuff um Evan and I spent the summer like I said in 2022
5:46:55
work material um we programmed a bunch of generalized algebra theories uh a bunch of gats uh
5:47:05
in the cat lab um implementing data structures based on well we did uh Carion double
5:47:13
categories and equipments mostly uh we revamped the double category stuff um there was a bunch of stuff there already
5:47:20
on categories on ordinary categories monal categories and by categories um this is
5:47:26
all all done in the Julia language which is like a high functioning
5:47:31
fast uh language for data science
5:47:40
um lot of questions outer joins there are people who think that's
5:47:46
not a super important problem but I
5:47:51
don't know maybe uh we'd like to program relation value double funs I'm very interested in whether there's some kind
5:47:57
of hurry language here in background I honestly have no idea um we'd like to do
5:48:02
full data migrations at some point which would involve being able to actually set up translation funkers between ologs and
5:48:11
then the migration would actually be operations using ads between virtual
5:48:16
double categories black double funter which is a lot of work so that's kind of on the horizon um like I said this is
5:48:23
basically an advertisement for a paper draft which should appear in the proceedings for now
5:48:31
it's on the archive under this address I also wrote a blog post on this about 18 months ago that's still
5:48:37
up um and you can take a look at that that's um all I have to say so thanks a lot
5:49:08
uh well for example we use
5:49:14
uh right here yeah yeah and which fin series is it Atlantis or the other one
5:49:20
that we don't talk about uh Atlantis is Banger is great but
5:49:27
uh star Universe lot
5:49:37
l so one is um aggregation and other is like nested
5:49:54
types um so relations have a domain and co- domain yeah but it seemed like the
5:50:00
semantics we were using didn't really distinguish between those yeah so uh
5:50:06
could you maybe speak for that why why should you arrange things in this relation thing and do you need ever to
5:50:13
put things on the other side at this point there is not a hug need to do that
5:50:19
we use double category relations there end being compact structure
5:50:25
but but I think there relation where it kind of buz in that semantically right a
5:50:31
relation like parent of I think that kind of matters um like you what want the
5:50:38
individuals who are the parents and individuals who are children sort of on opposite sides and right otherwise it
5:50:45
seem like it might not matter in some Grand sense but if you want to convey an
5:50:52
idea right of parent wood is sort of reling like this way maybe it matters in
5:50:58
which case there's a lot more Theory there and I don't exactly know what it looks like
5:51:05
one yeah yeah um if you instead of you
5:51:11
take theisms are morphisms for like theun
5:51:21
that sends a set to the set finite subsets of that is that still an
5:51:26
equipment um up I have idea okay okay because I feel like in database Theory
5:51:33
like a table is always finite yeah yeah right but if you only do relations
5:51:39
that's a good point yeah good question I suppose this uh framework
5:51:53
allows I don't know I suggest that we keep the rest of the questions for you know my
5:52:11
guys so there's here
5:52:51
better the second time
5:53:11
so we don't to much um our next speaker with afternoon
5:53:17
is CH ner and he will tell us about partial and relational
5:53:22
up Hello thank you for being here um my name is Chad I'm a research fellow at
5:53:29
the University of tartu which is a city in Estonia which is a country in Europe and i' i' just like to uh use this
5:53:37
opportunity to say that I'm actively looking for PhD students in t this sounds fun to you ask me um so what's
5:53:44
this talk about uh and or what's this talk going to be about the goal of this talk is going to be to come up with or construct satisfying Notions of theory
5:53:51
in the sense of algebraic Theory but instead of having models in sets and partial functions I want my theories to
5:53:56
have models in sorry in sets and functions I want I want my theories to have models in sets and partial
5:54:02
functions for the partial theories and sets and relations for the relation
5:54:08
and uh so a thing about it that differentiates it from other stories like this is that um we're going to use
5:54:13
string diagrams instead of the sort of classical terms that you see in like algebra one for example so this is an
5:54:19
exciting novelty and uh So the plan I'm going to give you three sort of stories
5:54:24
that rhyme with each other one just about algebraic theories that some of you probably know then one about partial
5:54:29
algebraic theories and one about relational algebraic theories and since these stories are all the same our notion of partial and relational
5:54:34
algebraic theories must be good um and we need to start off with finite
5:54:40
products because this is what altic theories are about and as you may know categories with finite products are
5:54:45
equivalently symmetric monoidal categories with a copying morphism for every object a which we're going to draw
5:54:51
as a string diagram like this R from top to bottom and a deleting morphism um for every object a which the copying that's
5:54:58
how we're going to draw that deleting that's so we're going to draw that and both of these have to sort of copy and delete correctly so uh these are sort of
5:55:05
we're insisting that Al together the copying and deleting morphisms form natural transformations of some sort so they have to if I do F and then I copy
5:55:11
its output that has to be the same thing as copying the input and doing F to we copy in this so sets and functions is a
5:55:18
great example of one of these uh and then these copying and leading operations in addition to these
5:55:23
naturality equations um they form what's called a commutative choid which just means that if I make three copies of something they're all the same thing and
5:55:30
that you they work like copying deleting op and two but this is kind of how we can capture that algebraically they have
5:55:36
to satisfy a coherence condition but it's fine um and then an algebraic Theory so
5:55:42
the one you've seen is maybe this isn't the way you've seen it but it's secretly just a small category call it x with
5:55:48
finite products any one of these we'll see an example can be thought of as an algebraic Theory and any algebraic
5:55:53
Theory these and then a model of an algebraic theory is simply a functor
5:55:59
from the theory category with finite products X into the category set that preserves the finite product structure
5:56:05
so preserves copying and observes the deleting and then a model morphism turns
5:56:10
out to be a natural transformation so you know this is a very nice story this is due to build
5:56:16
L uh so for example uh we could take the theory of monoids which is a very simple sort of algebraic Theory and the theory
5:56:22
of monoids is generated by a signature that has two function symbols in it a sort of multiplication which we'll write
5:56:28
infix sity 2 pictured here on the left and then also we're going to draw string diagrams but for now just read the text
5:56:34
part and also a unit of multiplication Z which we write e and then we construct
5:56:39
terms over this signature and we subject those terms to some equations particular we subject them to the associativity
5:56:44
equation and the two Unity equations right so pictured here below now I could
5:56:50
also do this by generating the free category with finite products over this
5:56:56
sort of string diagrammatic signature for those by adding um these two generators and question being by all the
5:57:02
equations you see here um to get so a category that will play the role of
5:57:08
terms and we'll see more of that in a moment and then I can quoti that
5:57:13
category of these sort of string diagrammatic terms by the smallest congruence generated by these string
5:57:18
diagrammatic equations so you can see how if uh does this light up or
5:57:24
anything yes so so if this one is sort of takes X1 and X2 to X1 times X2 and
5:57:30
this just spits out the unit then well this would be the associativity equation this would be unit equation in this
5:57:37
video and so I've made that exp so and so you can actually read off terms from
5:57:43
string diagrams they really in cartisan categories they really do uh correspond so the the copying well the input wires
5:57:49
are all the variables that our term is over and the output wires well each one is a term in those variables so really
5:57:55
they're tuples of terms which is convenient and um so the copying one it just takes you know it copies its input
5:58:01
variable variables are terms that's fine the deleting one doesn't have any output wires we don't need to give it any terms
5:58:07
and uh like I said pointing at the previous slide for the sort of multiplication and unit generators and then we have a more complicated example
5:58:14
where if I had some variable X1 sort of coming into my my thing on the the left input wire here and X2 was coming in on
5:58:21
the the right input wire then well this is sort of you saying you know take a copy of X1 and then apply one of the
5:58:27
copies of X1 to X2 using the multiplication and then appli the result of that using the multiplication to the
5:58:32
other copy of X1 and so the the term corresponding to that output wire would be this
5:58:38
one um and then when you do this you can do substitution by composing morphisms
5:58:43
in your category so so if you this string diagram is the bottom part of
5:58:49
this one and so if I wanted to substitute say the the unit e for X1 in that diagram
5:58:56
and X1 time X2 for x2 in that diagram well I would take the wires corresponding to those terms and sort of
5:59:03
mush them together in the composing way with the wires corresponding to the variables I substitute those terms for
5:59:10
and then the result of Performing that substitution is going to correspond to a diagram that is equal to the one that I
5:59:16
constructed so this is this is a good idea to do um and uh so then more classical
5:59:24
algebraic Theory stories so we we've sort of constructed the theory of monoids um we've gestured it how to
5:59:29
construct it and then if we take the uh so the models of the theory of monoids the funs from that category with finite
5:59:36
products into set sets of functions um are exactly the monoids and the model morphism so the natural Transformations
5:59:42
between those functors are exactly the monoid homomorphisms and so we get the category of homomorphisms and categories
5:59:50
like that that arise as the models and model morphisms of some algebraic Theory are called varieties um we're going to
5:59:56
have an analog of this later and then there's a result that we're going to have an analog of later where if we have two algebraic theories categories with
6:00:03
finite products then we know that they present equivalent varieties so they're equivalent categories if and only if the
6:00:09
categories like the theories as categories have equivalent item put and splitting conclusions by which I mean the Kuan blosi can whatever want um so I
6:00:18
guess this is AD can resist um okay so that's our recap of
6:00:24
algebraic theories that's all you need to know about them next we're g to move on
6:00:29
to to partial algebraic theories because I'm interested in theories whose operations are partial fun fun instead
6:00:36
of functions represent these things and so instead of having models in sets and
6:00:41
functions I want to have models in sets and partial functions and we a problem
6:00:47
occurs if you just try to do this naively with classical term syntax because classical serm term syntax is
6:00:52
bound very tightly to finite product structure and uh sets and paral functions does not have finite product
6:00:59
structure in the way that we want it to it does what it's um and so if we try to use classical serm syntax naively to
6:01:07
express you know things in sets and partial functions it's it's painful you can do it but we're not going to instead
6:01:15
we're going to use string diagrammatic syntax and we're going to sort of get to our notion of partial Theory by looking at what properties the cartisian product
6:01:22
of sets has in the category of sets and partial functions um so this is you know
6:01:27
in these category of sets and total functions um the properties that it has are this copying and deleting you know
6:01:33
it's finite product structure but it says in partial functions it has something else so it still has copying
6:01:38
um we we still have a copying operation in set and partial function that's total partial function that copies its input
6:01:44
and uh this is still natural in the sense that the input copies correctly instead of a deleting operation we have
6:01:50
what I'm going to be calling a restriction operation and it's a different thing than what Michael was just talking about I think restriction
6:01:56
means something else here um but just like in sets this is the thing that Maps an element of any set a to the one
6:02:02
element of the one element set which is our monal unit but we don't have that
6:02:07
this is natural and we'll see more that soon um and then also relevant is that
6:02:13
on the cartisan product of sets in sets and partial functions we have a sort of sameness partial function that Maps a
6:02:19
pair of things in which both of them are the same thing to either thing because they're the same and is otherwise
6:02:25
undefined so we couldn't have this in sets and functions because it's obviously a partial function we do have it in sets and partial
6:02:31
functions and um so as before the copying and restriction operator form a
6:02:37
commu of colonoid and the samess operator is a commu of semi- group so it's associative and copying and samess
6:02:44
interact they form they satisfy the sort of the equations of a communitive special fbus algebra although we have to say that we don't have one of those
6:02:51
because our we don't have a unit for our semi-group thing so that's not where we are and uh if you have a symmetric modal
6:02:59
category that has all of this structure and satisfies a cerence condition just like for categories with finite products you get what's called a discret
6:03:05
cartisian restriction for so oh and uh
6:03:11
so talking about the the Restriction operator in one of these discrete cartisian restriction categories the
6:03:17
arrow where I do some partial function f or abstract partial function f and then
6:03:22
I use restriction this tells me about the domain of definition of f we can think of this as the part of the domain
6:03:28
of F that f is defined on and so in particular I can talk about two things having the same domain of definition
6:03:34
being defined on the same part of their domain by saying that F instruction is equal to G instuction and this will turn
6:03:41
to be very useful and so these discret cartisian restriction categories arrived at in
6:03:48
this talk by sort of abstracting this structure that the cartisian product of sets has sets of partial functions it's
6:03:53
just going to be our notion of partial algebraic Theory um so we'll take a small one of those and then a model is
6:04:00
going to be a symmetrical fun from the theory into sets and partial functions that Preserve reses this structure so
6:04:07
the the copying and the restriction and the same it's just like a model of an algebraic Theory preserves the copying and finite product structure and then
6:04:14
model morphisms are a bit different instead of natural Transformations we have to take what I'm going to call a monoidal LAX transformation so just like
6:04:21
a natural transformation um so funter F to a funter g this will be a family of morphisms in the codomain category um
6:04:28
but instead of the naturality square just commuting it commutes up to inequality where here the relevant inequality is it's called extension of
6:04:35
partial functions so f is less than or equal to G if where they're both defined they're the same andbe G is defined on
6:04:42
more things um and it has to be monoidal so just like a monoidal natural
6:04:47
transformation the uh the components at X or Y have it has to be the component at x t are the component at Y and it has
6:04:54
to work with the units so now I can use the string
6:04:59
diagrammatic syntax to present these partial algebraic theories just like I kind of presented the the the algebraic
6:05:05
the of monoids to you before um because I've the of partial theory is kind of a
6:05:11
structured monoidal category and so a separation algebra for those who don't know is a partial commutative monoid
6:05:16
that is right cancell or left can because it's commutative and so to present this well I can take the free
6:05:22
discret cartisian restriction category so I add all of this string diagrammatic structure with the black dots um on two
6:05:28
generators one that's going to be the the partial multiplication of my partial monoid and one that's going to be the
6:05:33
unit I subject them to five equations so this is associativity unity and
6:05:38
commutativity the one in the bottom left says that the unit the the map that gives me the unit is a total map so that
6:05:45
it's defined on its whole domain of definition somehow um and we need that
6:05:51
notably we're not insisting that the multiplication is a total map because we want a partial monoid and here we have
6:05:56
to insist that things are total or else fault and then the one in the bottom right is the cancity equation so cancity
6:06:04
you sort of you imagine the a in can so cancel ativity says that a * Cals B * C
6:06:09
if and only if a equals B so you imagine the a coming in along the first wire the B coming in along the second wire and
6:06:15
the C coming in along the third wire and then this says that well a *
6:06:21
C the letters may be wrong but the idea is it says that so these two are equal
6:06:28
these two are the same really the part of the domain of definition where this times this is the same as this one times
6:06:35
this one is the same as the part of the domain of definition where those second two are just the same so in this way I can
6:06:41
express cancity although I'm not sure it's right yes this is more expressive
6:06:46
than ESS Al this will turn out to be exactly the same essentially Al but good question
6:06:55
yes so everybody here's favorite uh essentially algebraic Theory but uh thing that we can capture like this is
6:07:01
the theory of categories a difference is that now we have two sorts before we've just had one sort but now we need a sort a um of arrows and a sort o of objects
6:07:09
so these just become two generating objects of the category that we'll be Theory the generating morphisms map um
6:07:16
well arrows to their source and targets doain and codomain and objects to the identity morphism on that object and uh
6:07:22
two arrows so notably these are all total but this one is partial this is the composition operations so in
6:07:28
particular we ask for a bunch of equations but maybe the most interesting one is the first one here on the left so composition is partial and we're saying
6:07:35
that well the part of the domain of definition of this where so two arrows
6:07:40
where composition is defined is the part of the domain of defin sorry the domain where the target of the first arrow is
6:07:47
equal to the source of the second because that's when I can compose things and that's kind of the role that our
6:07:53
sess generator and the like partiality is playing in the definition and then the rest are the rest are the rest of
6:07:59
the
6:08:08
okay we have um right so categories cancity
6:08:16
these are good examples that you cannot do with mere algebraic
6:08:24
theories so obviously this says that models are
6:08:29
small categories that's obviously wrong what this should say is that um oh no
6:08:35
wait no this is fine models of the two sorted Theory sorry I got confused of the two sorted theory of categories are
6:08:42
indeed small categories and then it turns out that
6:08:49
mod morphisms so if you remember a model morphism is a monoidal LAX transformation but but in this case a
6:08:54
model morphism is also a funter because we're working in models of the theory of categories okay yeah um you know mod to
6:09:01
be exactly funter and this is an interesting place to to stop and sort of think about why we have to use these LAX
6:09:07
Transformations instead of just strict natural Transformations say and um so if
6:09:12
Alpha is our functor and so this is sort of like oh yeah um if this was an
6:09:18
equation instead of being an inequation so the the left hand side is uh like the
6:09:24
action of the functor on F composed with G and the right hand side is the action of the fun on F composed with the action
6:09:31
of the functor on G and if these were equal our functor would reflect composability so if I which is wrong
6:09:37
right so so if I could compose uh Alpha a and Alpha a g if this
6:09:44
was an equation I would also have to be able to compose f and g and that's not how functors work so it is actually important that this is an
6:09:50
in I've seen people get this wrong in the definition of category as a theory but so yes this is a bit about why lacks
6:09:59
Transformations um right so and then there are some results about this notion of partial
6:10:04
algebra Theory and uh so them theorems and the first theorem of them is that the categories that you get as the
6:10:11
models and model morphisms so the things that play the role of varieties this story are precisely what people call the
6:10:16
locally finitely presentable if you know great if you don't that's fine and then
6:10:22
also we have an analog of this Morita equivalence condition for algebraic theories where two of these partial
6:10:27
algebraic theories correspond to the same locally finitely presentable category so the same equivalent
6:10:33
precisely in case they have equivalent item potent splitting and this is exting if you're into
6:10:41
this why so if I take the these things these partial theories the discret cartisian Restriction categories so DCR
6:10:48
down here they uh their structure preserving functors and the monax Transformations organize themselves into
6:10:53
a strict two category if I restrict to the sub two category of that on the Zero
6:10:58
cells with split item potent then that's strict to equivalent to Lex the category of size constraint
6:11:05
you pick one and you get it on both sides so say small categories with finite buts there natural
6:11:10
Transformations and then uh so to answer davage question for a bunch of reasons on this
6:11:16
slide we know that these are equivalently expressive to essentially because usually that just means category with finite limits DCR
6:11:23
DCR is discreet cartisian restriction for the technical name of the thing that the partial theories
6:11:31
are okay enough about partial theories I'll tell you the same story again but this one will be shorter for relations
6:11:37
um so again instead of models in sets and functions I models in sets and
6:11:43
binary relations the same category Michael was just telling us about although this is a
6:11:48
category um and once again I'm going to just say that classical syntax is
6:11:53
unsuitable or at least more difficult than the string diagrammatic syntax we're going to use instead and I'm going to arrive at that
6:12:00
syntax in this talk by looking at what structure the cartisian product of set has in the category Rel of sets and
6:12:07
relations and I'm going to give up on cute names it's it's got a CO multiplication
6:12:13
and a multiplication the co- multiplication has a co-unit the multiplication has a unit and there are these relations so that the diagonal
6:12:19
relation the relational Converse of the diagonal the one that relates everything to the one element of the one element
6:12:24
set and its relational and we'll talk about more equations but these are going to turn out to be the cartisian by
6:12:30
categories of relations of Carboni and Walters which Michael talked about as ction by categories of relations so
6:12:36
there's some relationship um so we have these sort of
6:12:42
generating structure things and the things that they need that they satisfy that we're going to ask our relational theories to satisfy is that well co-
6:12:48
multiplication and co-unit are going to need to be a commu of choid just like before then upside down multiplication
6:12:53
and unit will need to be a monoid uh and together they'll form a communative
6:12:59
special forus algebra so that's these equations and then it turns out so normally when we present cartisian
6:13:05
categories of relations we do a lot more work it turns out you can skip a lot of that work and ask for this subject to
6:13:10
this equation for all and you know we don't even need this equation because it's captur um so that's a fun result but uh
6:13:18
we can replace all of that by this um and then we do what we did
6:13:24
before so I'm going to define a relational algebraic Theory to be a small in this case symmetric noal category with all that structure we just
6:13:30
saw so relations and then a model of this is going to be a structure
6:13:36
preserving fun from the theory that is the category into the category of sets and Bin relations that
6:13:42
preserves reses the structure results all that stuff and then again model morphisms are monoidal LAX
6:13:48
Transformations where here the inequation is inclusion of relations there's a more principled way
6:13:53
to get to the inequation we don't have time um so the best example that tells you everything you need to know of this
6:13:59
is the relational algebraic theory of non-empty sets um so
6:14:07
it it has one generating object and it has no generating morphisms and we
6:14:13
impose one equation on it where we ask that composing the co-unit with the unit
6:14:18
is equal to the identity on the unit of the monoidal structure monoidal category structure why because relational
6:14:25
composition involves existential quantification and it just turns out that this is insisting that the set that
6:14:32
you map the generating object of your Theory Category 2 is inhabit and then
6:14:37
yeah just cute and then um model morphisms of this are just functions
6:14:43
total functions they're not relations or anything but so total functions between the carrier
6:14:50
of right I have another example okay another thing that you can do with this essentially we'll be that
6:14:56
second another thing that we can do with this is the theory of regular semigroups uh so a regular semigroup is a semigroup
6:15:02
so we ask for one generating object and one generating morphism that's going to be the operation of our semigroup we
6:15:09
need to ask first so we need to ask that it's a function so now our operations
6:15:14
when we interpret them will be relations by default instead of functions or partial functions so here we're asking that it's functional or deterministic or
6:15:22
single valued all of those did the same thing except single valued and here we're asking that it's total in the sense that it once it's we're asking
6:15:30
that it's a total function these and um then it's a semigroup so we're asking
6:15:35
associative and then the interesting part is the regularity which is that for all a in the carrier there is a b such
6:15:41
that AA is a and then uh so here's our our a here's our exists a b that's how
6:15:47
we're using this new generator we've added this unit to our monoidal structure on the cartisian product of
6:15:53
sets and um we are just asserting that but using the string diagrams instead of
6:15:59
the classical terms I guess anybody wants to and then we have these two theorems again um so
6:16:07
the categories that arise as the models and monomorphisms of some relational algebraic theory in this sense turn out to be what and R call the definable
6:16:14
categories if these people are listening this is the worst name you
6:16:19
could but they they're a technical thing and you can go about them um and then uh
6:16:26
just like partial theories partial algebraic theories and the classical algebraic theories two of these relational algebraic theories turn out
6:16:32
to present equivalent definable categories if only if they have equivalent ENT splitting completions so
6:16:38
again the fact that this part of the story Rhymes is very exciting because it does call us and um why okay so again
6:16:46
these these relational algebraic theories um their morphisms and The Lax monoidal Transformations organize
6:16:52
themselves into a strict two category CW and if we restrict to the zero cells with split item potents that turns out
6:16:59
to be strict to equivalent to the category X which is exact categories so regular categories satisfying a
6:17:04
condition and uh their fun in that's all that's all I wanted to say
6:17:12
um there are some references so this this is all done in excruciating detail in my PhD thesis which is available on
6:17:18
the internet now there are some papers published in computer science conferences where the the main results
6:17:23
are um and then so this uh this project kind of got started by phip Bank D
6:17:30
pavich and pinski the um the idea to sort of take the cartisian categor as a
6:17:35
relation seriously as a notion of theory um they have some notes on it that are on the archive um and then uh these
6:17:42
papers by caronian Walters and uh for theion by categories of relations that's kind of where they come from and then um
6:17:50
there's a paper that more people should read on Range categories by Robin cocket shishan gu and Peter HRA where the
6:17:55
discret partisan restriction and then there are lots of other things like but these are
6:18:14
number so I don't know what one of those is but you mean this generalized
6:18:19
algebraic theory is a specific techn
6:18:39
[Music] only constru categories not
6:18:45
categor so is it the case that like theories like as soon as you have
6:18:53
category you know what the products are right is it the case that you can recover the rest of
6:19:01
this oh I do do think that it's all uniquely
6:19:06
determined once you know that there is one then there can only be that one so
6:19:23
yes by the item FY AUM you mean
6:19:29
this the way to think of this is in terms of the meat in cartisian by categories of relation so
6:19:38
um yeah yeah so you can Define the order by saying that F me G is f if only if f
6:19:43
is less than or equal to and um so you I guess in Tech like what you ask for is a
6:19:48
hypergraph and then a hypergraph category that satisfies this axium for allf just is the same thing as aan by
6:19:54
category of relations where you get the ordering because you can Define
6:20:02
it thanks this the same story
6:20:11
yes this is a very good question and I I know uh I hope so but I don't know how
6:20:17
you would do it I don't really have any ideas that seem good um but I would like to
6:20:23
know yeah potentially related to that actually but I saw I mean so so you had
6:20:29
what looked kind of like Gabriel uality with we do have to use that to get
6:20:37
theem one right and then and then it looks like you're doing an analog of
6:20:42
that although I don't know the analogous result where you have exact categories instead of yeah yeah so it is it might
6:20:49
even be yeah but there is a g Duality for it's really for exact categories but for regular cat right regular the
6:20:55
definable categories are the thing on the other side really that's where they come from yeah yeah yeah so so right so
6:21:01
there were those kind of results from categorical model theory that yeah
6:21:10
yeah how do you know if they have equivalent item potent splitting how do
6:21:15
you know if two have same oh you have to check uh yeah I don't know that seems
6:21:23
like it's probably not I mean you can do sneaky things right like I don't know if there's like
6:21:28
an algorithm that will do it for you in a reasonable amount of time or not uh
6:21:34
like is it decidable uh I don't know I don't want to commit either
6:21:47
way you've done Lex you've done exact
6:21:53
uh oh so one answer is Nathan
6:21:59
Hayden but uh one answer is that in a recent paper
6:22:05
so so some people Nathan Hayden alandro and hinski have a paper kind of
6:22:11
extending this to full forter logic in an interesting way so that would get you like Boolean hyperd doctrines I guess
6:22:16
they're just called Boolean categories we push it down um I guess I don't I don't know I don't know how you do more
6:22:24
or even in like um you can go to more and more structured allegories up to like a power power allegory say if you
6:22:29
want tooy and I don't know how you do that in terms of the monoidal structure like this um but I think it's an
6:22:35
interesting
6:22:42
question oh not really but there is I don't know
6:22:50
we have some observations maybe we can talk later about this hle diagram of 16 options you get you know what I mean
6:22:57
yeah so no but let's thank our speaker again
6:23:10
bre here set
6:23:42
that's
6:24:21
the
6:24:46
I
6:25:07
okay great so we're getting ready for the last
6:25:13
um for the first part of our afternoon session and our speaker
6:25:19
is speaking on preparational perspectives on determin determinization of finan
6:25:28
[Music]
6:25:34
I didn't know what affiliations consequently but that's that's that's where you can find
6:25:41
me sorry and
6:25:52
speak so
6:26:02
okay sorry
6:26:20
it's okay it turned itself off again maybe
6:26:27
slow and by hear that
6:26:36
yeah so um the purpose of this work was to try to constru a discrimination for a like a categorical
6:26:44
model of autom using Universal constructions so some of the objectives
6:26:49
of the paper Associated to this has been to like carefully investigate the universal chy of determinization
6:26:56
construction in terms of simulations between ala um and I present two different
6:27:03
determin constru one that generalizes an existing constructions to the B of
6:27:09
presented by andberg and then I also proposed an
6:27:14
alternative construction that is path relevant but might be a little bit Orthodox you know
6:27:21
what is but this one because it's path relevant has stronger Universal
6:27:27
Property so I assume that not everyone's familiar with automata but the main idea
6:27:33
is that you have a finite directed graph and you fix an alphabet so the edges of
6:27:41
this directed graph are labeled by letters in this alphabet and we refer to
6:27:47
them as Transitions and dieres are refer to so usually you would point out one
6:27:54
state to be the initial State and a couple states to be your final States
6:28:00
and so if you give a word to this automaton and your alphabet then it
6:28:06
tries to see if there is a transition over this word from the initial stage at
6:28:12
to a final State and if so it accepts the input and if not it rejected so in
6:28:18
this example Aon here our alphabet is Theos of the letter of a we had two
6:28:24
states one and two and this little arrow over there denotes initial State and
6:28:29
like R denotes the final state so it recogn
6:28:34
like words like a a and so
6:28:40
on this um in this example this example of tomaton can also be seen as a or we
6:28:47
can en code this or model this using a funer from the fre monoid over a into the category of relations by mapping the
6:28:56
the unique object to the set of states in this case just call them one and two
6:29:01
so State the except one and two each morphism is mapped to the corresponding
6:29:07
relation so since there's a transition or a from one to one one is
6:29:12
related to one and also one from one to two so one is also Rel to two and like a
6:29:18
relation is same for B and everything else so this view of funs from a monoid
6:29:27
into the category relations as knowledge of Tom was first presented by Tom and
6:29:35
Daniel from in a paper from 2020 I think and they they also add some
6:29:42
extra like they ask some extra structure to The Domain category to denote the
6:29:47
initial and the final St but for the purpose of this talk and for what I've been doing this is not very
6:29:53
important um but this definition is very like language oriented and models nicely
6:29:59
logical aspects of AA but since we're mapping into the category or we're
6:30:04
marking into real it only interprets U transitions of as ways of relating
6:30:12
States under a label if we go back to this here we see that there are two ways
6:30:18
to do a like one looping over a and then going to over B and the other one going
6:30:25
from one to to over a and then looping over B but since we're mapping into R
6:30:31
this doesn't really recognize that
6:30:37
see but for this model of automat of autom there exists a determinization
6:30:45
construction if we take our example Aton again we can also see it as a functor or
6:30:52
we can model it as a functor over this stre monoid um where the domain is then
6:30:58
given by category over the underlying graph with the fun denoting like a labeling
6:31:07
funter if you want to model automata as funs over a fre monoid category then you
6:31:13
need to make sure that if you have a transition over a word then this word
6:31:18
can be split into like sub transitions over each sers and this for this we can
6:31:23
po the condition on the funer to be Ulf or have the unique lifting ofation property and this view of is presented
6:31:31
by medas and silberer they also add some data to know do know like the initial
6:31:38
State and the final States but for the purpose of this talk this is not very
6:31:43
important by a relatively classical result we can see a new fun into a
6:31:49
category C as a pseudo funter from C into span set and this pseudo funter
6:31:56
that Maps uh each object to its set fiber and each morphism also to its set
6:32:02
fiber with source and tget projections so consequently we can see UF automata over I have this B Sigma is
6:32:12
just like the fre monoid or alphab Sigma these correspond to pseudo Sigma two SP
6:32:19
set if you want to impose the finite condition you can also impose the
6:32:25
condition of factoring and so we see here that real is a subcategory of spet
6:32:32
consequently the notion of will subsumes the notion
6:32:38
of and if we look at our nice running it will be a running example so our nice
6:32:45
example here then dispensed approach actually recognizes that there two ways
6:32:50
to do the transition AB but I said that we were going to talk
6:32:57
be talking about determinization so consequently we also like model of
6:33:03
deterministic automaton the difference between a deterministic automaton and a
6:33:08
non-deterministic one is that for the complete deterministic autom we if we
6:33:14
pick and a state and a word that from this state we have a unique way to do a
6:33:21
transition from this state over this word and this if you look at this from a factorial perspective this is exactly
6:33:28
the property of the AET vibration and by the gross and deconstruction any this frion correspond
6:33:35
to a functor into such we also since we're talking about
6:33:42
determinization we need like language preservation we need a way to relate automatic to each other say that they're
6:33:48
similar similar stuff so we borrow some Notions from label transition systems so
6:33:56
if we have to label transition systems which essentially automat are then we
6:34:01
say that a relation on the set of states of these is a simulation if we if we
6:34:08
have a transition over a word w from Q to Q Prime in L1 and something B State s
6:34:17
that relates to Q using this relation if there exists an S Prime so
6:34:22
that A S and S has a transition over W to S Prime S Prime relates to Q Prime
6:34:29
then this is a simulation or L2 simulates L1 and we can also do it the
6:34:34
other way around if we can do it the other way it's referred to as a b simulation consequently we can we can
6:34:43
define a type of simulation or we could say that a simulation from an aaton or a
6:34:48
spensive aaton F2 fime is a natural transformation between these two
6:34:55
factors and the explanation for this or to explain this we can look at the LAX
6:35:02
naturality Square for this transition for given for w so if you look at we
6:35:10
have a pair q q Prime and S with a lift
6:35:15
in of in this band that's given by this side of the diagram then we necessarily
6:35:21
have a state q and the alaton F such that there is a transition over W from Q
6:35:29
to Q Prime and also um there is a left of the pair S and S Prime or S and Q in
6:35:38
the span Alpha and since we have this two morphism here we also get an S State
6:35:46
S Prime and fime such that there is a transition from s to S Prime over W and
6:35:53
there is a lift of the pair S Prime Q Prime in
6:35:59
Alpha so if we impose other conditions on this we can also get different Terps
6:36:05
of simulations so if this natural transformation in that is pseudo we also have the here we can look at other side
6:36:12
of the span and get this data and since this is pseudo we can we also get a Q
6:36:19
and F such that there is a transition over W from Q to Q Prime and s s QR has
6:36:28
a lift an output so spans set is
6:36:34
on the one morphism side a dagger or has a dag dagger structure so if we're given
6:36:39
a LAX natural transformation whose um dagger version also is a LAX naal
6:36:46
transformation then this constitute in a b simulation because if you look
6:36:52
at this side of the square then you're given this data and you can fill it out
6:36:57
because well this is just the LA naturality of alpha dagger
6:37:05
so I mentioned before that real aomata has a determinization procedure and if you think about it
6:37:13
determinization
6:37:33
um this also gives us a canonical simulation of RA automata by its
6:37:39
determinization is given by the co-unit we also get a unique factorization of
6:37:45
simulations from a deterministic automaton to a non-deterministic automon by the canonical simulation which is
6:37:52
essentially what this equality of natural transformation
6:37:57
say so um I thought we wanted to ulate this and um it turns out to be quite
6:38:04
non-trivial because fans is a by category oroma or pseudo
6:38:11
functors so this is it's not easy to map back into set but it turns out that
6:38:16
there is a local Junction between spanset and R so a local adjunction between two by categories is comprised
6:38:23
of two fun one in each direction that induces a family of adjunctions of this
6:38:29
form between the home categories and this these family of adjunction JS are natural
6:38:35
in so the local adjunction between spanset and R is induced by the inclusion of R into spanset and the kind
6:38:43
of forgetful image fun of spanset R which is given by like it's identity an
6:38:49
object and then each span gives a if you look at the span as something like this
6:38:58
is said
6:39:06
by these are the two lines that we Master
6:39:14
image and this will be a relation see of
6:39:20
the co- domain of the both on both
6:39:26
domains so this gives us a relation and the on what it does on the two morphisms
6:39:32
just follows by doing this definition
6:39:38
sorry um which one this one yeah um yeah
6:39:44
it's a it's it's a I mean it's a functor between five categories so it needs
6:39:49
something but it's just inclusion
6:39:55
composition uh no no it's a last fter that way of course um so to determin IE a spans set
6:40:04
automaton we can just post compos by this image funter and then do the
6:40:09
automaton determinization procedure and we get here a canonical simulation by
6:40:16
pacing the co-unit of this adjunction and we're given a l natural
6:40:23
transformation by the local adjunction with the like two Arrow given by um the
6:40:29
data of the local Junction and it turns out that we can Define like one-sided inverses at least
6:40:38
for each of these two so each uh morphism or like each simulation of SP
6:40:44
that looks like this also has this nice factorization
6:40:50
property but so what does this determinization actually do on like much level so if we
6:40:57
look at our nice running example it Maps the set of states to the power set of
6:41:02
the States and each letter since it factors for R we map it
6:41:08
to the corresponding relation there's a lack of space so just denoted it by Delta a and dis is ma to a
6:41:16
function from the power set to the power set that Maps each subset to the set of
6:41:22
all things that are a related to anything in s so if you know a little
6:41:29
bit about Ala Theory then you might recognize this uh that or you might see
6:41:35
that this resembles um the classical terization procedure and indeed and when
6:41:42
we're taking pronoid as our base category then this actually recovers the clal
6:41:49
determinization algorithm but to see this more clearly you can also just turn our
6:41:55
set into using the grth construction into like approach look at whats look
6:42:01
like domain which is the
6:42:06
relations so I started this talk by saying that mapping into real kind of
6:42:13
identifies paths and this is not always something that you would
6:42:18
want so we could also um imagine like a
6:42:24
path relevant determinization procedure by using multisets instead of subsets
6:42:30
sorry how long have 19 okay yeah then I have
6:42:35
time um so something ask questions yeah okay yeah um so one key observation here
6:42:45
is that a span like this of finite sets always gives a function of this form
6:42:54
so so we we uh this is important for
6:43:00
later on um but we can define a multi set relative moad from pet by this m
6:43:08
and consequently if we're looking at only span fin or like
6:43:16
a CO domain or like the span of finite
6:43:21
sets then we can Define like a multi set determinization that factors through the
6:43:28
relative PCY category of the multiset relative monad instead of
6:43:34
R um and we also get this nice canonical simulation but this time we have pseudo
6:43:43
natural Transformations instead of LAX natural transformation so this gives a stronger like canonical simulation
6:43:51
and to Define this pseudo natural transformation we needed this
6:43:57
observation that each orphism of this form is kind of a span up to your choice
6:44:05
of Apex and again this we can again Define
6:44:12
like a one-sided inverse for each of these pseudo natural Transformations so again we get this nice factorization
6:44:21
property so some conclusions and takeaways is that oh we actually succeeded to express determinization as
6:44:29
a universal construction so that's very nice and we can something that is also
6:44:36
quite interesting is that we can recover the determinization algorithm via such a
6:44:41
construction using the power set monad and if we would like to have a notion of
6:44:47
PA relevance we can determin using like a multi set
6:44:53
set thanks for listening
6:45:08
is the the multi set at the path relevance like counting how many ways
6:45:14
there are to get from one state to the other is that um it's yeah that but
6:45:21
like yeah you can it's all the ways and like a multiplicity yeah
6:45:28
than um yeah first the next very interesting I thought that the gr
6:45:34
deconstruction played a role as well the vibration persp so I was wondering whether you could apply that to I mean
6:45:41
you were looking at like and M as
6:45:46
well to them and get things like vibrations your push that even further and get a
6:45:53
completely vibration based view like
6:45:59
what so there was this idea that vibration also would a toate like
6:46:06
Epsilon transitions that are like
6:46:13
like transitions that don't have a label since we can use not some more to
6:46:21
Identity um but I don't know whether this exp
6:46:29
for yes um so [Music]
6:46:44
kinding have this sort of IDE also that yeah of
6:46:52
course you have basically this is just the property of Junctions like you have F you have a
6:47:01
junction between like domain
6:47:24
enjoy this I was wondering about in the in the vibrational perspective with
6:47:29
Ence where where do the LA natur where
6:47:35
that um if want to like transform Sim
6:47:41
approach like spans of the bunkers with some type ofal property on but it's a
6:47:49
little bit
6:47:58
more and that's here again
6:48:08
now hope and show for
6:48:45
as about
6:49:15
position
6:49:48
that
6:50:08
should have a talk down here where
6:50:35
I saw down
6:51:03
yeah so [Music]
6:51:33
you
6:52:38
[Music]
6:52:47
mod
6:53:11
[Music]
6:53:26
to know where to know in advance where the program may not actually work what try to figure out it's like
6:53:47
in
6:53:53
this not because there's
6:54:09
[Music]
6:54:57
yeah structure
6:55:16
surpr
6:55:42
well right so my the idea that I have here um
6:56:02
all
6:56:57
oh I see that was PL see
6:57:13
so cool the summer come
6:57:46
want to ask you [Music]
6:58:23
the
6:58:56
password okay and they do the construction
6:59:30
yeah just just this
7:00:24
little
7:00:33
really
7:00:49
[Music]
7:01:03
sleep
7:01:34
where
7:02:00
sign e
7:02:39
than
7:03:30
h
7:04:00
e e
7:05:03
is this
7:05:09
yeah I see there's a safety limit with a number
7:05:16
of people I see I see
7:05:39
this
7:06:30
yeah e
7:07:09
you
7:08:00
e e
7:09:00
this e
7:10:00
it's
7:10:12
okay
7:10:50
it's
7:11:05
than
7:11:49
feel still me
7:12:07
good
7:12:37
this
7:13:28
it's oh all
7:14:30
what
7:14:42
[Music]
7:14:51
[Music]
7:14:59
for for
7:15:57
time
7:16:07
it's
7:16:50
right so
7:17:14
repres
7:17:46
never
7:18:17
set
7:18:23
up oh yeah yeah
7:18:49
it's all in
7:19:25
first you have half hour
7:19:55
oh these are microphones so they you up ideally but or you be loud
7:20:07
[Music]
7:20:13
hi everyone welcome to the last session of today thank you for laughing my joke
7:20:19
that wasn't actually J so the first talk is presenting funs
7:20:25
and I believe Gabriel is going to give it very much Prof Prof so hi every
7:20:30
everyone um thanks for attending this talk I'm really happy to be here um so U
7:20:36
the organizers of this uh meeting have been very smart because thanks to the talks that you've already seen I'll be
7:20:42
able to go a little bit faster the beginning um so the general context of
7:20:49
this computational category theories so that means that we're going to be interested in describing categories in
7:20:56
some syntactic way and operating on them right um the good news is that there are
7:21:02
canonical ways of doing this for categories category presentations that have already been mentioned today um for
7:21:10
funter between them and also for pre- sheets and co- pre sheets so all of this is standard uh but when you go to Pro
7:21:18
functors uh there hasn't been like such a detailed study of what the right way to present them is um and well a pro
7:21:27
functor between categories C and D can be thought of for instance as either a
7:21:32
fun from cop times D to set or as a functor from cop to the copi category on
7:21:39
d uh but the thing that we're going to explore here is that this although these are equivalent at a semantic level
7:21:46
they're not equalent at the syntactic level if we uh take the natural syntactic uh representation of these two
7:21:52
things and this is a subtle that so first of all I'll give some
7:21:57
motivation for uh the context in which we're iting which is categorical database Theory right um and well this
7:22:06
is uh the same idea that Michael already mentioned and I really like this phrase
7:22:11
of David uh that if you follow this idea and every theorem about small categories becomes a theorem of databases second I
7:22:18
think that's that's kind of like um the the original idea or at least the way I
7:22:23
take it um and by the way uh this is not the like the last version let's say of
7:22:31
the of the data model uh like there are some additional nities that we can work
7:22:36
with and that will be the content of ulio talk next uh but we'll stick to this uh relatively simpler uh model
7:22:43
because the salty we're interested in is already here so um this is the part that
7:22:48
I'll go a little bit faster um so a category presentation consists of objects generating arrows and equations
7:22:56
between parallel paths there and our beloved example I'm not going to do targate I'm sorry um I'm going to stick
7:23:04
with this and unrelated stuff um but yeah so this is just suggestively
7:23:10
labeled uh things right like this is just um objects or sorts and generating arrows or function symbols in logical
7:23:17
Dragon um and we have some equations um would say that manager Works in after
7:23:24
Works in after manager right we're writing from left to right and the dot is concatenation of paths is equal to
7:23:30
just works in and well we'll talk about that later maybe if it's really important um and yeah there you can see
7:23:38
it what I'm talking about and every category presentation will give a
7:23:43
category which we denote like that and which we'll call the sematics of C or the interpretation of C uh where the
7:23:49
morphisms are paths coed by a relation that we'll call provable equality right
7:23:54
because we're really working with equational logic here and um the equations generate well anything you can
7:24:01
prove from those equations will be provably equal um and um so here's an
7:24:07
example worked out where you can see how the morphisms well we can group them by the length of the path that they came
7:24:13
from um and the only really important thing that I wanted to mention here is that this is infinite right after in
7:24:18
this case after you reach this employee object you can start looping your manager and you'll never get the same
7:24:24
morph morphism as before so that's why we use presentations they give you a way of working with infinite stuff in a
7:24:31
finite way uh and the difference with uh well Michael there's plenty of
7:24:37
differences with Michael but we will be focusing on the syntactic aspect like we're going to go there um so um we I'm
7:24:46
going to explain a little bit more in detail what a morphism of category representations is because I will uh
7:24:53
omit the precise definitions of all other morphisms in the context because we don't have time um but essentially
7:25:00
what you're doing is assigning objects to objects and generating arrows to paths uh in the Target presentation um
7:25:08
and in this case we are looking at an endomorphism of our beloved example um
7:25:13
which is sending each object to itself it's sending the Aros manager and works
7:25:18
in to the uh to themselves too and the only interesting thing is it's sending
7:25:23
the secretary uh AR which I also like because you could read it a section because requiring that it's a section uh
7:25:31
it's mapped to well sex followed by manager and for this to be a well- defined morphism uh you have to uh
7:25:37
require that every equation in the domain C representation is respected
7:25:43
which means that when you map the two paths uh well you have to check that those are pro and well you can just
7:25:49
check that uh in this case I won't go through the example and in this way we obtain a category presentation uh sorry
7:25:55
a category of category presentations um which has its uh natural obious
7:26:01
semantics fun into the category of small categories this is also something that I
7:26:07
might be able to skip or let go much faster uh this is an example cop shift and this is really the point where you
7:26:13
you really understand why this would be a data database scheme right until now this were just um suggestively named uh
7:26:22
things now you can see that you can populate this objects with with some actual elements um and you can visualize
7:26:30
with tables in the way that Michael explained and that already gives you a notion of database in which uh every
7:26:37
object has a table and every uh column is either like the elements of the set
7:26:44
or the um well where that element is being mapped to by the functions in the coper Sheep um and yeah these are like
7:26:52
foreign keys in the sense that they refer to tables to to objects who sorry to rows in other tables um but now we
7:26:59
want to go to um presentations for these things too because we also want to be able to work with infinite um poets and
7:27:07
now I will start using instance and coper shief interchangeably because it gets it's a bit more of a marful to be
7:27:13
saying coper shief every time um but those are the same thing right
7:27:18
um okay so um we Define instance presentations this would be like actions
7:27:26
of mono or groups but similar idea if you've ever seen presentation for those but maybe they are not so common uh so
7:27:34
we're going to give some generators which have uh are labeled by an object the C representations and some equations
7:27:40
this is the simplest possible example with one generator uh in object employee
7:27:45
and no equations and then you can figure out what the table is um this is just
7:27:51
essentially applying um all POS map you generate new elements by uh declaring
7:27:57
those elements to be the images that you get from starting from this unique generator and since there are no equations we're not identifying
7:28:04
identifying anything with anything else so these are all distinct um and um here is the formal
7:28:11
definition of what I just showed or the one that we will use it's not necessarily the most obvious one to
7:28:16
anyone but it will be useful for us later to do it this way um which is that
7:28:22
we take our original category representation and we extend it with a unique object asterisk um and we add sorry arrows
7:28:31
coming out of it and equations that must involve those arrows um so for instance
7:28:36
here this is this was kind of like uh very simple no equations but we can add the equation that e do manager equals e
7:28:43
and that is valid what I said because it's an equation between paths that uh involve this these arrow and the
7:28:50
morphism of these things uh well it should assign these new these new generators or arrows that I added to
7:28:56
some of something else they should assign them to paths in a way this is also could be said to be a category
7:29:04
presentation morphism which is required to to keep uh to be like the identity of
7:29:11
to fix the Asis and be the identity that's another way of saying um now um we should also talk
7:29:18
about manipulating data because that's part of what a data model
7:29:23
uh slide oh so these aren't no these are no
7:29:29
longer like natural Transformations between the instances these are yes I mean these are
7:29:35
presentations of natural Transformations yes yes that's a very good point to clarify what we're doing
7:29:41
but yeah I mean the uh when you take this um the semantics of
7:29:46
isation this coincides with like the normal defition yeah yeah it coincides we're trying to re reproduce everything
7:29:52
in syntactic terms not do something new in that sense um and now uh we uh get to
7:30:00
the part where I talk about uh querying or transforming these database instances
7:30:05
and I didn't include a slide on the data migration funes that Michael showed
7:30:10
because here we're going to talk about something completely different or well not completely different which is using
7:30:15
Pro functors uh so we'll get to that but we first start with just instances right
7:30:21
so um what we will do is to consider a finally presented instance as a conjunctive Verve
7:30:28
and um and use the idea that uh conjunctive
7:30:33
queries are things that can be represented by mapping out of some model
7:30:38
right so uh we're going to map out of an instance and that will be our our query
7:30:44
right and in this example um the first example uh well I just I don't know it's
7:30:50
it's it's a it's presenting a representable uh right like Yetta says that representa are the free C shifts on
7:30:57
one generator quite literally so the result of quering with this will just well just give me the set of all
7:31:03
employees uh if we're interpreting it that way the second one will give you like a filter on that it will say give
7:31:09
you the set of employees who are their own managers um and this is this is good
7:31:15
this is intuitive we can program with this but it's not expressive enough uh for example in comparison to the data
7:31:21
migration functors because it really gives you a set as an answer and you might want something more structured so
7:31:27
uh you might want to produce a new instance um and we can do that following the same idea of conjunctive queries but
7:31:33
now having like more than one uh at the same time and that's a way of seeing Pro functors so um we will think of a of the
7:31:42
profunctor as a functor from cop to the C category and d and Define its
7:31:48
evaluation functor using this construction that is kind of like appears uh all around um but it's uh
7:31:56
well uh you can see how uh this contains a previous situation when the domain is the terminal category right now you're
7:32:03
just um so this C copy shift right once you have plugged in a dcop shift given
7:32:10
some objects in C well you get gives you the ways of mapping PC J
7:32:17
um and an important thing for us is composing that composing uh queries is
7:32:22
important because you uh want to be able to manipulate in different ways your queries before uh having the data come
7:32:29
in right before actually looking and this is all fine because profun composition uh which well I just we're
7:32:37
using this notation in this order and I just put here for reference the uh the Coen Formula One can use for composition
7:32:44
um well this this works in the sense that you can compose the evaluations and it's the same as the evaluation of the
7:32:51
composite um and this would be a great moment for an example but um uh I need
7:32:57
to introduce some stuff and then it will be easier uh so our first attempt at a profunctor
7:33:03
presentation will be just uh generalizing the instance presentations that I show so now we will have a
7:33:11
category presentation that extends simultaneously to category C and D with
7:33:16
some profunctor symbols that uh have a domain in C and A co- domain in D plus some equations between parallel cross
7:33:22
paths where cross path means that it has to go through some of these symbols atamp um and this notion turn out to be
7:33:30
equivalent to C op times D instance presentations uh I can't really Define
7:33:35
that because I I'd have to say what op and times means for presentations but you can and uh you can Define morphisms
7:33:42
to obtain a category uh which uh enjoys this sematics functor also uh
7:33:48
straightforwardly now there's a problem with this kind of um of presentation and
7:33:54
for that let me say uh this definition so if uh we have to fin category
7:34:00
presentations and a profunctor between the presented categories uh if it admits a finite profunctor presentation a
7:34:07
finite uncured profun presentation um then we say it's finitely uncured
7:34:12
presentable and the problem is this theorem it says that the class of finitely unar presentable Pro functors
7:34:19
is not closed under composition and the proof of this is through a very like a minimal country
7:34:25
example so I'll go through it quickly um in this example there are three category presentations which have one object each
7:34:31
and no equations at all and we have two uncured profun representations which have one generator
7:34:38
one profunctor symbol each and um no uh no equs at all I think I didn't mention
7:34:43
the F uh I skipped that there's one morph one morphism or generating arrow
7:34:50
in only one of the C representations and then you just uh look at the semantics
7:34:55
of all this uh and so you find out that uh this is presenting a profun with an infinite
7:35:01
number of elements these are all distinct the same happens with Q I'm going to go through this quickly um and
7:35:08
when you look at the composite of these Pro functors it also gives you an infinite number of elements now this
7:35:13
profunctor uh has a domain a ceg with no with no morphisms or no Arrow present
7:35:20
generating arrows and these one either so it's not possible to present that with a finite number of uh of symbols um
7:35:29
now the solution in this case will be to remember that we have this equivalents
7:35:34
um and go to the other way of thinking about this so we will take this idea and
7:35:42
kind of unflatten it into two layers so now we'll think of one part of this data
7:35:47
as being um an instance presentation and then this part is another instance presentation and we will have
7:35:55
um morphisms of presentations between them um these morphisms uh well of
7:36:02
course they will have to preserve satisfy the equations in the domain in a
7:36:07
suitable sense uh and we call this cured profound representations by the way I'm
7:36:12
skipping through many of the formal definitions but that's just in the interest of time it's not that they're particularly like I If I had an hour I
7:36:19
would just give them but um I think it's it's clear enough this way
7:36:25
um again you can Define morphisms between this um and you obtain a
7:36:30
category of cured perun representation between c and d uh and its semantic
7:36:36
funter uh now the the way this solves a problem is by giving an explicit
7:36:41
syntactic composite construction which we denote with this uh circled asterisk
7:36:47
and this is obtained by following like an algorithm that is uh known as sub
7:36:52
queron nesting or be unfolding and it's already like sketched in the categorical database literature here um
7:37:00
and the important thing to know about this firstly is that it preserves finiteness right if we start with finite things we will keep the finiteness uh so
7:37:09
we prove that this is a functorial construction um well we prove more more things in terms of like well these are
7:37:16
uh there there's a double category here and so on but that didn't get into the talk um and um and there is a natural
7:37:25
isomorphism uh telling us that this is correct with respect to the seman of profun composition um which is
7:37:33
fundamental because otherwise I wouldn't be even be talking about this construction um and as a corollary the
7:37:40
class of finally curried presentable Pro functors is closed under composition which is great for
7:37:47
implementation uh and now I would like to give an example uh which is kind of
7:37:52
like more complicated uh well it has a new a new category presentation which is like a duplication of this one it except
7:37:59
for one morphism that is like going uh I think I might not have time to really go
7:38:06
through it I already like 20 minutes um but the idea is that um you can define a
7:38:13
a curvy profun representation here so you do that by defining instance presentations corresponding to each of
7:38:19
the objects here I've actually included the generators in this pictorial form even though this is not uner this is
7:38:25
defined in this way and then you have to Define some some more pH here uh and
7:38:31
then uh to Showcase how you compose I said okay let's go to the instance presentation that I showed you before
7:38:37
but now we see it as a pro funter um and then we can compose and the more like
7:38:43
the idea more or less uh is that we look at uh pairs of composable generators and
7:38:51
we tenser them so we obtain a new generator that we WR write like this with a answer notation um and then we
7:39:00
have to uh take all the equations around there that could be relevant and we obtain equations of the right type let's
7:39:07
say by tensoring them on the left and on the right with the generators uh with all possible choices um and um yeah I I
7:39:17
think I don't have time to go through this in more detail um but it's a Once you define how to extend the tensor
7:39:24
operation to paths in general and not just generators um this is really
7:39:31
um and the result and this is also something that I wanted to emphasize is that um you get uh well what since what
7:39:38
you get has a domain which is presenting the terminal category this is an instance too um and as an instance we
7:39:45
can think of it as a conjunctive query and what it's saying it's it's like a migration well it is a migration of the
7:39:52
original uh query the original conjunctive query that was on this schema to this new schema the regional
7:39:59
conjunctive query said give me all the man all the employees who are their own managers and now we're saying give me all the pairs of an administrator and a
7:40:06
teacher I didn't explain it but uh such that the the the manager of the teacher is the administrator and the
7:40:12
administrator is its own manager their own so it kind of makes sense intuitively and the last thing that I
7:40:18
want to talk about is a little bit of um what failed in the uncured case and how
7:40:25
we could fix that by adding some conditions on what an uncured presentation should uh should satisfy so um this might be a
7:40:34
point that is well it's really a very interesting point I hope I can get it across um so the point is that we want
7:40:40
to guarantee the existence of a finite set of generators uh for a composite and
7:40:46
the syntactic construction that I showed is one way of achieving that and we what
7:40:51
we did was pair generators from two pro funes p and Q uh into new generators now
7:40:58
why did this work it worked because uh second profun
7:41:04
presentation has these morphisms Q of f for every uh generating arrow in the
7:41:09
intermediate category and these morphisms always tell you how you can um in a sense look at a cross path that
7:41:17
that doesn't start with a with a Q with a profun symbol from q and move to the
7:41:23
right what like apply in a sense this F to q and obtain A New Path that's starts
7:41:29
with a profun symbol um you need that because
7:41:35
um because otherwise uh when you write this little thing this is now a an atom
7:41:40
right is you can't have things in the middle between the p and the Q you need that um so uh when we look at an uncured
7:41:49
presentation we don't have the morphisms like that but we could still require that every cross path can be Rewritten
7:41:55
to start with a profun symbol uh and there's a uh a simpler way of stating
7:42:00
this but that's it yet and in that case we say that the uncured profun presentation is non-generative
7:42:08
um it turns out that we need another condition because we also want the number of equations to be finite
7:42:14
otherwise it would just finally generated and um to do this uh we look
7:42:19
at this little definition which is that uh starting from on on unur presentation
7:42:25
we can look at instances inside so we we we pick one of the objects in the and forget about everything else in the
7:42:31
domain and this is kind of a restriction we call it that way um and suppose that
7:42:37
for every pair of paths that start in this C that we chose uh if these are
7:42:43
provably equal in the larger context then they are already proly equal inside this instance Al using all the equations
7:42:49
here and if that happens we say that well that has to happen for all choices of c and in that case we say this is
7:42:56
conservative in the sense of like conservative EX exensions um and we said that P is cable if it's both
7:43:02
conservative and non-generative and um now I would this will help this
7:43:08
will give us an equivalence right but to to state it uh we can look at a a transformation in the opposite direction
7:43:15
which is like the flattening of a a cur into noncured presentation this uh determines a
7:43:21
functor which restricts to the finate and also preserves the semantics which is which are the two things that we're interested in and now we have to ident
7:43:29
ify sort of like the Target that we want to look at uh um which we will Define
7:43:35
like this cable from ctd to be a non-fall subcategory uh of the uncured
7:43:40
presentations where we look at the curable presentations so that once that satisfy these two conditions plus a
7:43:46
little technical condition the morphisms that says that all cross path are T to path that start with a pro symble um and
7:43:54
then when we co- restrict we restrict the co- domain of this functor then it's an equival of categories so we have
7:44:00
captured um uh the the essential composability or finite composability of
7:44:07
this um of the Cur presentations using uncured presentations and uh here's like
7:44:13
a ending remark that there are different ways to State this so we could drop this
7:44:18
technical condition and take cable to be a full subcategory which will be nicer conceptually but that leads us to a b
7:44:25
equivalence we have to weaken a little bit uh the risk like the notion of equivalence uh but I didn't talk about
7:44:32
the the like two-dimensional part of this which is uh uh simply the the
7:44:37
provable equalities like when these two syntactic objects behaves in the same way semantically well those are those
7:44:44
constitute the two cells uh and in that way uh many of this could be considered by
7:44:51
categories or something sat and reached that would be another way of think about
7:44:57
them um and we have a b equivalence between them and that's maybe nicer it depends right we you could want the more
7:45:04
concrete thing thinking in terms of implementations or the more abstract thing if you like it and um yeah that's
7:45:12
that's it
7:45:20
okay so have a few for questions
7:45:36
yes I mean I actually originally the first slide said category presentation
7:45:41
but it's a it's a matter of just like going between the syntax and the semantics uh you can Define you can
7:45:47
Define like that data model completely without any syntax and for the audience here that would be
7:45:54
acceptable it's actually I I don't know if that answer
7:46:00
any more yeah so you can take the further
7:46:06
and say that aond to appr as well and if you do things
7:46:13
that way then between so might there may also be a
7:46:22
way um you think that would uh I mean
7:46:27
yes I guess you could try to come up with a syntactic notion that would capture Co continuous functors between
7:46:34
coip categories but I feel like I mean if there was a different way of doing
7:46:39
that this was just an idea I wasn't yeah yeah I mean I I I I haven't thought about
7:46:46
that but you have the composite of pro
7:46:52
presentations as a fin presentation have you thought about doing the pro the one
7:46:57
given by the that's the oh
7:47:05
um yeah there's no I I don't think we have um I
7:47:13
don't yes there are some like dualities like that that lead to kind of like
7:47:19
similar but unexpected things but um um there is something well will mention
7:47:26
that aside like we don't just have the evaluation the profun we also have the
7:47:32
co- evaluation of the pro funer which behaves in a interesting way we won't get into that though um but this is
7:47:38
different because you're changing the notion of composition I that might be interesting to thank you okay one last
7:47:47
question that needed to set a finite number of
7:47:55
transitions on the slide where you were talking about the
7:48:00
limitations process number um yes two like if you want the
7:48:08
composite finite somehow extract the finite number generators which are yeah
7:48:15
like generating arrows yeah is that a man process like up to the Implement U
7:48:22
no uh like in the context this is like for this construction it's just take all
7:48:29
uh symbols from the first pro functor and all the symbols from the second and where whenever they're composed of it
7:48:35
like they match in the appropriate sense you just create a new profun symbol by
7:48:41
them but what I was trying to say in that sentence was that if you wanted to come up with some other way of composing
7:48:48
presentations even if it was by hand you would have to do it
7:48:54
somehow okay thank you again
7:49:11
you speak 25 minutes
7:49:41
can uh somebody help us with the email
7:50:26
yes maybe I'll get
7:50:36
yeah I'll give you an minute right so um the next talk is proc
7:50:45
par queries all right so we've seen um
7:50:53
querying algebraic theories Pro functors so now we get to put them all together
7:50:59
uh in this talk so that's it's really nice how that all lined up um so today
7:51:04
I'm going to talk about some work in progress uh in categorical database Theory uh this work expands the
7:51:10
algebraic model of uh categorical database theory developed in these two papers algebraic databases and algebraic
7:51:17
data integration um so some of the main results um are that we introduce U these
7:51:24
things called Pro queries non-strict Pro queries um and these are d Transformations that are reminiscent of
7:51:31
conjunctive queries um we prove correctness of pro query presentation
7:51:36
composition so that's what um Gabriel was talking about and we're going to introduce Pro queries which are data
7:51:42
transformation similar to unions of conjunct queries in relational database Theory and prove also correctness of
7:51:49
their composition so here's uh the story of categorical database Theory I'm not
7:51:55
going to go through all of this but it kind of goes back a little uh for me it was surprising how far I went back of
7:52:01
the overlap of category Theory and database Theory uh and then there's like a ton of work here on the sketch data
7:52:06
model uh but I didn't include all of that and here's kind of like the modern uh iteration of it and we're just going
7:52:13
to focus on these two papers uh so we already saw a bunch
7:52:20
about this so I get to go through really quickly database schemas small categories database instances coprs uh
7:52:27
category present ation only thing different here is that I use dep instead of Works in I have a different equation
7:52:35
but basically the same uh examples sorts equations function
7:52:40
symbols this is the um the semantics of category presentation provable equality
7:52:46
relation we've all seen this now here's the semantics um here we
7:52:52
actually end up with a worried about 44 slides but yeah no no you shouldn't be worried I'll just skip the whole talk at
7:52:59
this point so morphism of category
7:53:05
presentations uh as before but again let me just reiterate that um we take equations which is a finite amount of
7:53:11
data here and then we need to prove their equival so they don't have to be taken to actual
7:53:18
equation um now instance presentations as we saw are category presentations
7:53:24
with um this extra bit here and we think of uh these arrows here as generators so
7:53:30
this instance presentation has two generators and it has an extra equation here and we write it like
7:53:39
this and we can display it as a table so here we have four employees e e's
7:53:46
manager d uh the secretary of the department D um the manager of that
7:53:53
secretary and what's interesting here is that um this is very similar to what is
7:53:59
known in relational database Theory as incomplete uh databases um incomplete
7:54:05
information also called labeled mes um so given an instance presentation
7:54:13
uh we obtain semantics uh in basically the same way we already saw this um now
7:54:19
however that there's no actual data here right so this is just um keeping track
7:54:24
of primary keys and foreign keys so these are just kind of sets Okay but in
7:54:30
real life uh we want more than just sets we want to add in data and so that was
7:54:35
uh one of the ideas behind the algebraic uh data model so here's the algebraic data model so now algebraic theories um
7:54:46
so an algebraic signature consists of sorts and function symbols kind of like category presentations but now function
7:54:53
symbols are allowed to have higher errity so they can have uh more more inputs and we call the zero are function
7:55:00
symbols constant symbols so the algebraic signature for groups has one sort G and it has function symbols M
7:55:08
which is multiplication e which is the unit and I which is inverse and you can visualize these um
7:55:16
and when you put together these function symbols we call those terms
7:55:23
okay and uh we'll use this notation to represent um
7:55:28
these which I know might offend you guys because you love string diagrams
7:55:34
but so much um but here these are like free
7:55:40
variables okay so you imagine the free variables coming in uh
7:55:47
there and so some terminology the whole thing here is the term the free
7:55:52
variables we call the context and the the domain is the co- domain is is called its sort
7:55:58
so an algebraic presentation consists of an algebraic signature and a set of equations between
7:56:04
terms so here's the uh algebraic presentation of groups and we add in
7:56:10
these uh equations now if I have an algebraic
7:56:16
presentation we can also Define another equivalence relation provable equality we'll call it that again on terms rather
7:56:22
than just um paths and we get um a semantic uh category um
7:56:29
so the function symbols produce morphisms and now when I have the higher aity we think of that as
7:56:36
product um and morphisms of algebraic presentation send sorts to sorts function symbols to terms and instead of
7:56:44
uh setting equations on the nose we again do it up to provable equality and um so what we'll get is a
7:56:51
functor from algebraic presentation to uh finite product
7:56:57
categories so now for the rest of this talk let's fix an algebraic presentation
7:57:02
that we're going to call the type side and we want to think of this as the the kind of stuff you would see in U SQL
7:57:09
okay so um maybe we want strings and we want integers and we want some operations uh so we have plus successor
7:57:17
zero um taking two strings and concatenating them and then all the letters as constants and imagine there's
7:57:24
like a whole bunch of other stuff that you might might want um and we're going to use this to Define an algebraic
7:57:30
schema presentation um so an algebraic schema presentation you over a fixed type side
7:57:37
consists of this stuff that we already saw um so and we're going to call like we have a category presentation here
7:57:45
we're going to call those objects uh entities we're going to call the morphisms there they have to be unary on
7:57:51
this side forign keys and then we're going to have equations between them these are the same equations from before
7:57:56
I just now used a different um notation
7:58:02
okay um and we're also going to have the type side here on the right so you imagine that there's like a whole bunch
7:58:08
of different stuff here um and also it's equations that I haven't written down um
7:58:15
but now we also have uh attributes and these are going from the category
7:58:21
presentation to the algebraic presentation okay in One Direction so here's the salary of the employee here's
7:58:28
the name of employees the name of departments and these we think of as actual data okay and then we also have a
7:58:36
equations um that involve these attributes so here this is saying that
7:58:41
the salary of the manager of this employee is 100 plus the salary of the
7:58:46
employee which the sad fact of life um so we're typically not going to denote
7:58:52
all of the operations on the type side uh it gets a little messy so this is what we'll think of as um an algebraic
7:58:59
schema presentation so we want to talk about the semantics that reflect this
7:59:04
structure so given two categories um a bipartite category um uh here consists
7:59:12
of another category e equipped with a funter to the category with uh two
7:59:18
objects and one non-identity morphism going between them um so zero to one
7:59:23
there's one morphism and then the fibers should be C and D so this is a profunctor but let's not confuse that
7:59:31
with how Gabriel was thinking of a profunctor just think of this as like two categories with a bridge between
7:59:36
them okay so uh if use a schema presentation then we actually obtain a
7:59:42
bipartite category so the left side we call the entity category and the right
7:59:47
side we call the type side um and this inspires the definition
7:59:53
so given a fixed finite product category which I'll still think of the as a typ
7:59:59
side a schema is a bipartite category such that the inclusion from the typ
8:00:04
side preserves finite products now an instance is just a
8:00:10
functor to set that preserves the finite products of the type sign so I don't care about um products or anything on
8:00:17
the on the entity side but I do care about the products on the type side and we can Define schema
8:00:24
presentations and instance presentations just as we did before um and this allows us to input schemas and presentations
8:00:30
into a computer so in the algebraic model instances can Now display data other
8:00:37
than labeled nulls um while still having incomplete information so here uh I'm
8:00:43
going to give you a presentation for this database okay so I have three
8:00:49
generators of employees and I have two generators of departments and now the
8:00:54
actual data that's going to come here is given by equations okay so uh this employee zero
8:01:04
uh her name is Alice and then we have the name Bob and Charlie okay um and two
8:01:11
departments computer science and math and these equations tell us who's everybody's uh everybody's manager is so
8:01:18
we see that Alice is her own manager Bob's manager is Charley and Charlie is his own manager but uh notice here that
8:01:25
we don't know Bob's salary and we also don't know Charlie's salary
8:01:31
but we do know Alice's salary okay so we see that we have a mix of um of uh of
8:01:38
information of data and incomplete information now if we really want to call ourselves database the we need to
8:01:44
be able to query our data and this is where Pro functors come in which uh Gabriel talked about now um we can't
8:01:51
just naively use Pro functors because now we have this kind of um algebraic aspect to it so the idea Pro queries is
8:01:58
simply um what's the the correct notion of pro functors between algebraic
8:02:04
schemas okay so if I have a schema U and I have um an object in U I'm G to let y
8:02:12
you denote the following U instance which is just given by um so it's a representable instance on you and we can
8:02:19
present this super easily with just a generator and and no
8:02:25
equations so um suppose I have two schemas u and v a strict Pro query is a
8:02:32
functor from UOP to V instances um I should have said
8:02:38
morphisms of instances are just natural Transformations um and I require that if
8:02:44
I input a type um that I get a representable instance so I can put in
8:02:51
an entity or I can put in a type and I get an instance and every time I put in a type I get a representable
8:02:58
uh in general we want to allow Pro queries to only be isomorphic um but
8:03:04
every Pro query is actually isomorphic to a strict one so let's not worry about that right now so we're only going to
8:03:09
consider strict Pro queries um so this is the analog of thinking of pro functors um a seop uh indexed family of
8:03:18
copreps but this extra twist comes in now because we're we're talking about
8:03:25
attributes okay but let's let's just uh look at an example here so suppose I want to do the following uh thing I want
8:03:32
to select from my favorite uh example here I want to select the name and the
8:03:38
salary of all the employees who are their own manager and I want to add 50 to their salary I'm giving them a
8:03:44
promotion maybe they're promoting themselves um so I do um I can write
8:03:50
this in select from wear notation so I want to select the
8:03:55
name and the salary plus 50 so this is a this is a perfectly good term um and I
8:04:03
want to call that name new name and the new salary new cell and um so all I need
8:04:09
is one generator e and I want uh e to be uh their own
8:04:14
manager okay so here's the schema that I'm querying so what I do is I create a
8:04:19
new schema over here and for every select Clause here I'm going to have a new attribute here so here's new name
8:04:26
and new salary and for every generator I draw an arrow like this
8:04:33
okay so this is how I could give a appropriate
8:04:39
presentation so what I get is a diagram of U instance
8:04:44
presentations now uh I have one entity here so I have one entity in two types
8:04:51
So to that entity I stick in this instance presentation and when I stick
8:04:57
in a type I know I have to get representes and I have two morphisms going like that okay so that means that
8:05:04
I have a representable so it has one generator and one generator Here and Now morphisms of instance presentations just
8:05:11
say that for every generator I have to send it to a term here um so what I'll
8:05:18
do is I'll send n to this term uh and that's the term of e salary
8:05:25
plus 50 and then the new name will just be E's name
8:05:33
okay okay so that's how uh Pro query presentations work now given a pro query
8:05:40
um we get a funter uh as we saw in in Gabriel's talk and we call this evaluation so if I have an instance I on
8:05:49
V and an object U uh then I can uh get
8:05:56
the set um on you and this functor also has a left adint which we call Co
8:06:02
evaluation and there's actually a theorem from the algebraic databases paper that says if I'm a funter between
8:06:09
instance categories such that I preserve the type side H algebras uh and it's
8:06:16
also a right adjoint functor then there exists a pro query P such that f is
8:06:21
isomorphic to evaluation of that Pro query this is a very nice theorem that we're going to try and general
8:06:29
later um so this is what it looks like to evaluate so this uh is our V instance
8:06:35
I this is the example we saw before and now I want to I want to query this data
8:06:41
so I want to transfer this instance from V to you okay and that's what this looks
8:06:47
like so Alice is her own manager and Charlie is his own manager and they get
8:06:52
a promotion so Alice's salary is now 150 and Charlie's is whatever his salary was
8:06:57
plus 50 because we don't know what a salary was and this is what um that will
8:07:02
look like and you can do this all on the on the presentation Lev so that's uh all uh done already so
8:07:11
let's talk about what we're doing in this new work um so given two pro queries we can
8:07:18
compose them uh using um basically the same thing as uh profunctor composition
8:07:24
but we have to be a little bit careful here um because this is a set and this
8:07:30
is an instance on W okay so this is just the
8:07:36
tensoring of a set with an instance and then I take the coent and this coent is
8:07:41
taken in W instances so this could be a very very nasty Co liit um pretty much
8:07:48
arbitrary as I change the as the type side um and as Gabriel said this is very
8:07:54
similar to something done in in database Theory where you uh OS conjunctive queries um and so I want to tell you a
8:08:02
little bit about the history of of this idea um so there's these two papers on
8:08:07
the algebraic data model however what's interesting is that they give two inequivalent Notions of pro query
8:08:14
presentation so these are called B modules in the algebraic databases paper
8:08:20
uh B module presentation and they're called Uber flowers in uh the algebraic
8:08:25
data instance uh to explain select from where can also be
8:08:30
called flower notations it's like uber flower anyway so the point is is that um
8:08:36
in this paper they they come up they they sketch a way of composing Uber flowers but they never prove it
8:08:42
semantically correct and then for B modules uh there just there's no no composition given so this is kind of
8:08:48
what motivated us to write presenting Pro functors we're trying to understand this and we realized they weren't equivalent um and that's why this is
8:08:55
work in progress because we stop working on this to write a whole other paper um
8:09:01
so yeah now uh so it turns out that a semantically correct and finite
8:09:07
preserving composition operation cannot be given for B module presentation and that follows directly from what Gabriel
8:09:13
talked about and that was our motivation to to write the paper um so our new
8:09:20
contribution is to give a fully specified definition of pro query presentation prove uh correctness of
8:09:26
composition we also introdu Pro queries so these are
8:09:33
similar to Pro queries using this analogy conjunctive queries unions of conjunctive queries um here's the formal
8:09:40
definition I'm not going to read it but it's basically an instance and a pro query um and they have to interact in a
8:09:47
certain way and you also get an evaluation functor and this just looks like taking evaluation of the pro but
8:09:53
you're taking a sum over uh the set from the
8:09:59
instance so this is what a procy presentation would look like so it' be
8:10:04
like select the name and the salary plus 50 of all the employees are are their own manager or the name and salary of
8:10:11
all the employees in the math department so I just take this and I Union it with
8:10:16
this um but notice here that the select Clause um the the things you're saving
8:10:23
the maths are the same but I can choose different terms now when I select um and
8:10:29
that's impossible to do with appropriate with appropriate you only get uh one one
8:10:34
select Clause so here's my my old uh instance
8:10:40
and now when I eval it using this Pro query I have this and I have this new
8:10:47
stuff and notice Charlie's here twice uh here I've added 50 to a salary but here
8:10:52
I have so you can do kind of interesting things by combining um these different
8:10:58
uh database instances um so as a summary in the new
8:11:03
work we're going to Define we Define a definition um procy presentation semantics and composition prove
8:11:10
correctness uh prove that praries can also equivalently be described uh as funs between instance uh categories that
8:11:19
preserve type algebras and are parametric right adjoints also called Prof functors so before we saw that when
8:11:26
this is actually a right ad joint it comes from a pro query now we want to
8:11:32
say we weaken that to okay it's a parametric right ad joint then it comes from a c
8:11:37
query um and if you don't know what that is a functor uh between categories is a
8:11:42
parametric right ad joint when C has a terminal object and when given this
8:11:48
factorization that always exists this functor has the right ad joint and these
8:11:53
have started to show up all over the place in category Theory especially po funs
8:12:00
um and that's it thank
8:12:10
you I should also mention that you can you can go to this website and play around with cql um the language that
8:12:17
Conexus uses for their software and you can actually um like evaluate Pro
8:12:23
queries using that software um Pro queries are the next thing to be
8:12:29
question any
8:12:34
questions yeah you really get what Co evaluation was back to that yeah I
8:12:40
didn't explain it
8:12:45
um yeah co- evaluation is just this left ad joint of of gamma um but Co
8:12:50
evaluation actually turns out to be exactly composition so I can take a pro query I can take
8:12:59
um yeah how do I explain um it turns out that you can take a pro query think of
8:13:05
as UOP to V instances take Lambda of Q that's a functor from V instances to W
8:13:11
instances and then when I compose those as funs it turns out to be exactly this
8:13:17
so co- valuation you can think of um as as composition you can also think of it
8:13:23
as certain kind of data migration operations so I didn't mention the the delta Sigma Pi um which was kind of the
8:13:29
old way of thinking about data migration so coal is kind of thinking of it like
8:13:35
Sigma and evaluation kind of like Delta and Pi I mean there's there's a specific
8:13:41
there's there's an actual theorem that's of how they're related I won't go into it right
8:13:46
now yeah at the end about joints uh the notion really reminds me
8:13:53
of thetion Po funs or containers
8:14:00
just is deeper connection I think that's equivalent
8:14:06
right yeah I'm not 100% sure what you mean but I'm pretty sure that's that's
8:14:13
[Music] equivalent C
8:14:22
and yeah maybe maybe you don't even need a terminal object you say for like all objects something like that
8:14:35
yeah yeah any other
8:14:45
questions yeah so your ability to write unions of
8:14:51
conjunctive quaries how does that relate to the duck quaries that's this joint
8:14:57
Union of uh conjunctive queries yeah we just I just like the name better Pro
8:15:03
query but yeah same
8:15:11
thing okay thanks again
8:15:31
so we got five minutes before the next Talk starts so stand up
8:16:14
I don't
8:16:26
[Music]
8:16:56
you
8:17:12
is it okay to be here
8:17:37
you see no
8:18:15
and
8:18:45
[Music] only you're an experience
8:19:23
real yeah yeah
8:19:30
is
8:19:57
get
8:20:52
am
8:20:57
and
8:21:32
Okay so we've now got the last Talk of the
8:21:40
section okay thank you thanks for being here thanks to the organizers um okay pattern runs on
8:21:47
matter the fre monad Monet as a module over the first we I so I always I mean I
8:21:53
always start with this very like metaphysics you thing because that's what motivated me to entire time it's what's going on with this and why is
8:21:59
math being created at all and why does the universe blah blah blah blah blah so what let's just talk naively in some
8:22:05
sense we're material I have a body right and in some and in some sense uh I I something
8:22:14
things are occurring on that body I have beliefs I have habits and know how where someone says something to me I'm running
8:22:19
a script right now with this talk right something's running and and it'll stop and when someone says turn on the stove
8:22:25
and you know put a pan on it and stuff these are little scripts that I can run and the scripts somehow run on the
8:22:30
matter okay so that's what we're talking about here uh computers also they're material things but they're running scripts or our cells you know they're
8:22:38
material things but they're running protein production scripts Etc so whether you think matter and pattern are
8:22:43
the right words here for this uh that's Up For Debate but what I'm trying to talk about is a is a straightforward
8:22:49
math idea basically that free monads differ from but interact with co- free comonads and that I want to let you in
8:22:57
this uh both formally and see why it might be useful so here's four examples
8:23:02
sorry I forgot to mention this is Joint with Sophie Lipkin and these are four examples that um we think are example of
8:23:09
pattern running on matter that interviews run on people you can't have interview
8:23:16
running that uh that programs run on operating systems that voting schemes run on voters and the games run on
8:23:22
players finite things happening on things that go on uh so we'll explain
8:23:27
those in our paper if you're interested in those examples I'll kind of talk about this programs one briefly or something but the main point is this
8:23:34
modle structure uh that the co- free comonad on P and the free monad on Q
8:23:41
tenser together and map to the Prem monad on P1 Cu uh and so that is a
8:23:47
module in a sense I'll explain so I I uh I love polinomial functors because it
8:23:53
hits a sweet spot of Elementary expressive and elegant it's Elementary in that if you if you want to you can
8:23:58
just think of sets and functions all day long you're just doing sets and functions nothing nothing uh difficult At All by the way polinomial punctures
8:24:05
and containers are the same thing um not only Elementary it's also expressive so
8:24:14
uh the whole um categorical databases story uh and data Pro functors between
8:24:19
them are colonoids in poly and the bom mod ma um uh and um uh dynamical systems
8:24:27
that you put in wiring diagrams Etc and programming language Theory um all of these things can be very nicely
8:24:33
expressed using and it's elegant in that it has tons of just very standard monoidal structure monid infinitely many
8:24:41
monoidal Clos structures you know that that sort of thing like you it's hard to think of a category with two monoidal CL structures this one has to many so it's
8:24:48
got a lot of weird uh uh surprises um if you don't know what a polinomial funter is all it is is a funter from set to set
8:24:56
that is is a co-product of representable functors if you like it better to say it's a functor that preserves connected
8:25:01
limits that's that's fine too whichever one you prefer and a map of polinomial functions is just a natural
8:25:07
transformation uh but because of the unal Lemma because it's a co-product of representa bles and the co-product has
8:25:13
Universal property and everything's representa bles have a nice mapping out property uh uh a map between polinomial
8:25:20
funes unwinds into what looks more complicated there than natural transformation but it just says for all
8:25:26
I pick a j and for all B pick an a um and so it's the IJ goes forward in the
8:25:32
alphabetic direction and the B A goes backwards inal Direction but um the
8:25:37
point is that you can just go functions from sets to sets basically in one in
8:25:43
two cond variants um so it has tons of structure all I'm really going to work
8:25:48
on use today it has co-products and products which I used to even talk about it at all but there's another distributive monoidal close structure um
8:25:56
I said there are infinitely many here's one of them that's called tensor or deay product you'll use the same structure in
8:26:02
his talk which has a monoidal closure um and there's a fourth monoidal structure
8:26:08
called composition and I use a little triangle symbol I'll I'll explain it minut um so machines I'm going to be
8:26:16
talking a little bit about more meing machines today uh um I decided to skip the wiring diagrams part for a time but
8:26:23
uh machines of type AB input lists of A's and produce lists of B's so imagine a machine that could take a list of A's
8:26:29
and as you give it to it it starts producing B's uh what is a more machine or Mele machine it has a set of States s
8:26:36
just a set and a war machine is a map that takes an element of s and outputs a b and then listens for an A and uses it
8:26:43
to update its s a meing machine is a t a function that takes a state s listens
8:26:48
for an A and uses that a to Output a b and a new state they're slightly
8:26:53
different there but they're both polinomial functors so for any polinomial p a p machine is just what's
8:26:59
called coalgebra of M set and a map from s to P of s um now uh when p is the
8:27:06
polinomial B * y to the a I'll explain the notation a little bit these give more machines and when p is B to the a y
8:27:12
the a meing machines um not going to talk about uh wiring diagrams uh tensor product which
8:27:20
you would use to make a wiring diagram like that um has a m closure and the machine meing machines are the Universal
8:27:27
other they're The Duel they're the HS into the monoidal unit of more machines
8:27:33
so if you had a more machine in a box was in a closed world with something else the universal other thing it could
8:27:38
be is the meelee machine uh so that's kind of the how they
8:27:43
relate um so polinomial funs there's lots of different ways to represent them this one looks like a polinomial this
8:27:50
one is a bundle this came up in various people's talk and this looks like a Corolla like a bunch of little teeny
8:27:56
trees and these are representing the same thing uh basically down here if you apply if you apply this polinomial to
8:28:02
one you get 1+ 3+ 2 is six and those are the six dots and the six dots here and
8:28:07
above every dot is some number of directions uh um or elements of the fiber and that's two one one one and 0 0
8:28:15
so that two there represents y to the power zero it's the co-product of y the0 y zero okay so composite of polinomial
8:28:23
functors is again polinomial it's given in the usual like y^2 compos y + 1 is y + 1^ 2 y^ 2 +
8:28:31
2 y + 1 uh and here's an example of that so if you took P of Q um what you do is
8:28:38
you write it's how you get it is you take all the ways of putting the qish things on top of the pish things and so
8:28:44
that triangle shape I like I like uh for non-symmetric monal products I like nonsymmetric symbols because they let
8:28:50
you reverse them easily and they're kind of nice uh so that triangle represents kind of trees moving up trees growing
8:28:57
upward and so you can kind of count there it's y 6 Plus y Cub plus y Cub
8:29:02
plus y 0 plus y Cub plus y if you count how many leaves there are 6 3 3 0 two 0
8:29:09
and that's the composite y the 6 plus 3 y Cube plus two uh so that's this kind of trees growing up picture and and you
8:29:16
kind of going to need that for the free Moneta and Pro free Comet to understand them they are going to be trees so um
8:29:22
any questions about how the trees work is Corolla for us kind of Nest on top of each
8:29:29
other uh great so what's a monad so why try this triangle symbol is composing
8:29:36
and so that's a monoidal structure on poly and so a monoid for that would be ammona it's um a map from y to some
8:29:43
polinomial and a map from that polinomial compos itself to itself and aoid is ma from the some polinomial to Y
8:29:50
and M from that poomi 2 C compos C and so since triangle of composition of
8:29:56
functors even though we think of polinomial as objects in a category there're objects inory as functors uh
8:30:03
these would be monads and they just monoids these are monoids in poly with
8:30:09
this structure they're monads on set so a poal monad is a variant of an operad a
8:30:16
one object multicategory um so we'll be into some
8:30:21
Prem monads which are kind of like flowchart languages these are imagine you some flowchart like you know are you
8:30:28
happy or sad and then you like pick your directions like what's your favorite color and you kind of move through the
8:30:33
language of those flowcharts would be uh when you this flowchart should finish in finite time otherwise you're not going
8:30:39
to be happy using the flowchart uh those will be like PR monads elements of the pr monad and the um a poom comonad turns
8:30:47
out to be exactly the same thing as the category not just a variant of but literally a category and uh that's kind
8:30:53
of the basis for um and so like the the comodules between them would be the pro
8:30:58
funter that uh prer is the Amilia talked about but we're not going to go into
8:31:03
that today uh we're just going to talk about co- pronets which are special um
8:31:09
uh they're not flowchart languages they're the machines so these are going to be more like the matter and these are going to be more like the
8:31:16
pattern so the freon ad to build the Freemon ad you could either read Kelly's
8:31:21
uh paper on free uh monoids or you could read shulman's uh wonderful like
8:31:27
condensation of it on nlab but still you're going to be working super hard compared to what you have to for polinomial fun they're actually quite
8:31:34
easy so the free add on them so what do you do you have a polinomial fun p and you say uh I'm G to find Define p sub Z
8:31:42
to be just y the the monoidal unit and I'm going to Define p sub I +1 to be y
8:31:47
plus P composed the previous and so what this does it's going to just build pre but I'll show that in a minute uh I just
8:31:54
want to do show the formal St for a second and there's a map from P sub I to P sub I +1 that includes y y + P at the
8:32:03
base case and then when you have the map from P sub I to P sub I + 1 then y + P
8:32:09
composed pi to y + P composed p+ one is just given an inductive and now you define p sub
8:32:17
Infinity P sub Omega to be the co liit of all these P so y Maps into y + P
8:32:23
which Maps into y plus P composed y + p etc etc and you take the col of that and
8:32:28
when p is finitary like when all those little trees only have finitely many branches then you're done that is the
8:32:35
free Monet and when p is Capa small in other words they have less than you know Alf one many uh branches then you're
8:32:43
going to need a few directed Co limits along the way you just kind of every time you hit a limit ordinal you just take a CO limit and it's very simple and
8:32:50
now uh you just take the co liit of all of those things and um that's the premon the free Comon is almost the same
8:32:57
sort of thing so P super Z will be y p super I + 1 will be y * P composed pi
8:33:03
and again you can project out to the Y and you can uh Define things inductively
8:33:09
and then you define CP the co- fre comonad to be the limit of that sequence y from y * P from y * P composed y * P
8:33:16
etc etc and unlike M you can stop there you don't need any higher ordinals as soon as you've done this uh um you know
8:33:24
infinite limit here directed limit you're done um if you remember P machines like
8:33:30
Mele machines and more machines they have these different pols uh a position of C subp will be an
8:33:38
initialized P machine so it's like a coalgebra where you pick an initial
8:33:43
State up to behavioral equivalence you don't care if two states act exactly the same way in terms of their observable
8:33:49
Behavior then they are the same as an element or position in CP by the way CP
8:33:55
it's position CP of one is uh is the terminal Co you like those words so M
8:34:02
the free monad of zero gives you the initial algebra the co-free common of one gives you the terminal coalgebra so
8:34:08
if you like initial algebras and terminal coalgebras I tend to think that the freem monad and terminal co-rec are
8:34:14
kind of better Concepts because you can recover the other ones from it um but you but not like first e so uh so these
8:34:22
things are pretty different seeming and yet they both have this wide Plus versus times P compos the old one how similar
8:34:29
are they how different are they um basically they one way to see it is they both have a tree representation they're
8:34:35
both polinomial and so what I want to Define is something called a p tree A P tree for a polinomial p will be a rooted
8:34:41
tree where every node in that tree is labeled by a position in p and then branches accordingly so let's say your
8:34:48
polinomial was y^2 + y Cub + 1 but every time I see a y squ I'm going to write an
8:34:53
A and every time I see a y cub write a beat writing this this is a brilliant I don't know who invented this I feel like
8:34:59
I saw from Andrea CH I don't know this idea of labeling maybe maybe it's super old and everyone knows it but me but
8:35:04
like uh uh taking a one element set and multiplying it by something as a way of labeling is just a very convenient uh
8:35:11
cool trick so um uh so um the pictures
8:35:17
of the free monad are going to be trees that don't all like they're not you know they they can have different depths at
8:35:23
different points or different heights at different places um and this would be an example of an element of the free monad or a position
8:35:29
in the free monad um this would be these the for the co- comonad they keep going up and up
8:35:35
and up forever now whenever they hit a c no matter whether you're in the freem monad or the whenever you h a see
8:35:42
you stop because there's no branches here it's y to the power zero there's no branches but but for the free monad you
8:35:48
have to stop at some point every one of these flowcharts ends whereas over here you keep going and going unless you
8:35:54
happen to for some it all CES um so the the pictures pictures are
8:36:00
quite similar in some ways they're both P trees but these end and these don't oh I forgot to see what the
8:36:06
directions are so what are the what are the directions on on this thing the directions are just the open leaves so
8:36:12
there's one two three four five of them whereas on this one the directions are every single
8:36:18
node so quite different as a polinomial a polinomial when I say positions and directions I mean uh uh
8:36:26
the elements of P of One Versus the exponent there of the representable anyway so this thing the directions are
8:36:32
quite different in these two things so these are kind of very similar the fre monad and copr comonad they're like
8:36:38
lists and streams for example they're so similar and yet so different um what is what else do we know about them uh first
8:36:46
of all the free monad is a functor from poly to poly and that thing is itself a
8:36:51
monad that's why it could be called the free monad monad um
8:36:56
uh so that means that for every P there's a map from P to the pronat on P and there's a map from the pron on the
8:37:02
pron pron on P and a polinomial monad is is just anything that is an algebra of
8:37:08
of this monad um and the co- fre comonad is the comonad on poly so there's the
8:37:13
co-free comonad comonad it takes every polinomial P to the co-free comonad
8:37:20
P um right so there's various interactions
8:37:25
between freem monads and co- comonads um one is that the co-free comonad just
8:37:31
interacts with itself and that is LAX monoidal there's a map from C P to P Prime to P tensor P Prime
8:37:39
but the pre monan is not LAX noal for tensor it's laal for some other monal products but not for tensor so um uh
8:37:48
that's not so interesting for now but one thing I want to say is that because C is LAX noal it makes sense to say what
8:37:55
a uh to to say that something is a module over it and so this is the nlab
8:38:00
definition of if you look at module over a monal funter that's what I mean by module here so there's a natural map
8:38:05
that takes the copr element of that copr Comon that infinitely branching tree that goes on forever not infinitely branching that tree that goes on forever
8:38:12
of shape p and that a free monad mq that kind of stops at some point and gives
8:38:17
you a free element of free monad on P um and then you just check two
8:38:22
diagrams like uh if you take two different C things you can combine them into one and then use your F map or you
8:38:29
could hit the F map and then the other F map and those each other uh so how do you use it um well
8:38:37
think of T think of an element of CP as a tree is like a machine that's going to run forever it's just listening for
8:38:43
inputs and then giving outputs a repple if you know what that is like it prints something in the in the terminal you
8:38:49
type in something it reads it changes States and prints something in the terminal and just keeps going forever and ever so it's like an operating
8:38:55
system it's more like the matter part of that otomy and an element of mq of one would
8:39:01
be a terminating program it'll be a finite flowchart okay and so what happens when you do this thing you take
8:39:07
your operating system you take your script and you get something here what
8:39:12
happens is you lay the tree T next to the tree U and you move through forward through them both in tandem what do I
8:39:19
mean the root of the tandem thing is the pair of roots and a branch of the tandem thing
8:39:24
is a pair of branches so you you just keep branching according to p and Q what do you put there you put whatever was
8:39:30
there on P and whatever the the P thing and whatever was there in in in you what
8:39:36
is a leaf whenever you your terminating thing hits a leaf you hit a leaf you
8:39:41
stop right there and that and and you just return to the remainder of your uh of your infinite
8:39:48
treat so when I said before what's a more machine I said an AB more machine sends a lists to B lists a lot of times
8:39:55
when you hear people explain that they're going to do an inductive thing and they're going to leave the world of coalgebras and they're going to start
8:40:00
writing some equations that look like they're talking to you know a much younger student um what what happened
8:40:06
why couldn't they stay where they were uh the way to let them stay where they were is to use this uh this thing so a
8:40:13
more machine since a an initialized AB more machine is a position in this uh
8:40:19
co-free thing so that's your machine and an A-list is a position in uh the free
8:40:26
monad on ay so the free monad on ay is list a of Y times y um so uh so there's
8:40:34
a map from b y the a tensor a y a polinomial map that's just b a y the A
8:40:40
and then you can kind of cancel this out there's a polinomial map to be y you
8:40:45
just kind of have to believe me if you can't read it right now um and so you can get from you know your machine and
8:40:52
your list you can apply the P map put them together run the list Against the
8:40:59
Machine and then get your your b list so this is a way that you don't have to leave the land of to algebras to talk
8:41:06
about how a more machine uh runs a list um okay so programs run on
8:41:12
operators here's a guessing game you input a Max guesses and a goal if they're out of guesses you return false
8:41:18
they lost and if you read in a guess which is a natural number if the guess equals the goal return true uh and if it
8:41:26
doesn't return the guessing game with the same goal but with one less guess and the way you get this thing is you
8:41:32
define this polinomial which is the sum over Max guesses which is natural number and goals which is a natural number of a
8:41:38
thing that receives a Boolean did you win or lose and uh you define a map from R to the natural numbers that play this
8:41:45
game uh basically a position in my y to the N what is I guess I'm I probably no
8:41:52
time to say this my to the N sorry the free moad on y the N is is a branching
8:41:58
tree that at every moment accepts a guess and then moves possibly to a new branching tree or stops so it says
8:42:04
what's your guess at one it's like ah stop there at two it's like new branching tree like natural number of
8:42:10
many branches I shouldn't uh and so this G
8:42:15
this program right here is actually of an on map from this R thing to branching
8:42:22
trees basically if your BR number of guest is zero you have the empty tree and if your number of guesses is more
8:42:29
than zero you you basically ask for a guess which is a natural number branching and then if that Branch equals
8:42:36
the goal you stop and return true you come back to r with true and if it's uh
8:42:43
if it is not the right answer then you repeat this story with one fewer guess
8:42:48
you might have another branching tree another branching tree forever anyway so a position uh Sigma is is some big um
8:42:56
stream of natural numbers it's the thing inputting guesses maybe it's the operating system maybe it's you you're operating this this game uh and then
8:43:04
since y the naturals and Naturals y are dual one is uh kind of the universal
8:43:10
other for the other um you can you can kind of use what we've done so far you
8:43:16
use your stream of guesses and your program to turn to get a map from R to
8:43:22
the natural numbers times why what this is going to do is take your max guesses
8:43:27
and your goal run it with the operator who's giving the guesses and give the number of natur give the natural number
8:43:33
of moves that happen until they until they either win or lose and return back to R the Boolean of whether they won or
8:43:40
lost so that's what this machine would do it kind of use so um now we've seen
8:43:45
two different examples of how to use this spy map one just say in this thing how to use more machines turn lists to
8:43:51
lists and here how to run some guessing game um so in conclusion uh one might be
8:43:59
interested in how What's the diff what's going on with pattern and matter whether you like those terms or not we're thinking of patterns as terminating
8:44:05
programs little scripts you can run we're thinking of matter as the Dynamics of something that kind of persists a
8:44:11
machine and what does it mean to run pattern on matter so one thing one way to think of it is to think of these
8:44:17
scripts as a monad and matter as konad and then we've constructed the co-free or free monad or konad on any pomal p
8:44:24
and showed how they look like two different types of P tree and there's lots of different interactions between CP and MP um many I didn't talk about
8:44:32
today but one thing is that matter can run on matter can run on matter in this sense there is LAX monoidal fun from CP
8:44:39
tensor CP to CP tensor piece you can run two material things with each other and get another material thing uh pattern
8:44:46
does not run on pure pattern in that sense there is no manal LAX noal structure on MP but uh m is a c module
8:44:54
so this is what we mean by uh matter runs on sorry pattern runs on matter so
8:45:00
that's it thank you thank
8:45:09
you any questions is there a Dynam version of this that did like continuous
8:45:15
time Dynamics thiss it's hard to Define it it can do
8:45:20
so much it's very polinomial in a lot of ways except there's no composition or substitution that I've heard of okay so yeah you
8:45:28
can't get like because when I hear this how run and still keep my try thing I'd be
8:45:35
like there but because I love try this composition so much I can't quite escape
8:45:40
the finite uh finite State machine and one way to do that was with something with that Kevin Uh Kevin arand or
8:45:49
Kevin Carlson is and I that's that's to short yeah yeah so so um my question is
8:45:56
about mixed distributive laws at two different levels um firstly at the level of monads and comonads in this setting
8:46:03
where you get something interesting there and then secondly the free and co-free constructions also you said give
8:46:09
examples of mons ands and do you get something interesting there people could freee sorry give you so mixed
8:46:14
distributive laws either at the level of your poal mon or at the level of you know these free and co-free
8:46:21
constructions interacting with one another giving yeah so so given any map from P composed Q to Q composed p i can
8:46:30
get a distributive law from pron P compos Q to Q compos pron P so
8:46:40
they're mixed distributive law between p and Q right um that's one answer um I'm
8:46:46
not sure of other answers I don't know if that answered both of your questions simultaneously or only one of them the
8:46:52
first one so so the free construction do they also interact nicely with like if I
8:46:57
first take the free Mon and then take the free know I want I would love to know
8:47:03
yeah yeah I I yeah I think I thought about that and I didn't directly but maybe maybe there are things I didn't
8:47:09
see yeah yeah
8:47:15
[Music]
8:47:25
oh your
8:47:32
matter conce the matter the if it hits at all points in the tree
8:47:40
it has no option is left it can if it can no longer receive for some reason
8:47:46
then it would stop but in general they would just keep going forever
8:47:51
yeah the way you just described it sounds like it backs up in the tree if
8:47:57
it it's a LEF no if it hits a leaf it's dead it's it's this infinitely branching tree maybe you are like some path
8:48:03
through this tree uh the only way and you could hit a place where you have zero options you
8:48:09
don't have even the option to just listen to the pal of existence forever
8:48:14
you get no options uh then then you would stop yeah I'm curious about this comment
8:48:21
that you made at the end about matter running on matter should I think that as like an emergent system sort of thing
8:48:27
where like um proteins run on chemistry and cells runs on proteins and so like
8:48:32
there's there's levels that contain other levels or is it more like two things at the same level that are more
8:48:37
like two things at the same level yeah mentioned this Duality do
8:48:44
more meeting machines a few times was that just so you got a p and a q that you could together like a Clos thing or
8:48:51
is there more going on for any more machine yeah I think of it the interface as a y to the B yeah in hom from that to
8:48:59
Y is the meing machine that okay no is it was that just as example to get a p
8:49:06
and a q that you could T it together in your in your little subscrip here yeah
8:49:12
yeah these are these both but these are both the sort of thing where it take the Cod free on them which is a melee machine or more machine up to behavioral
8:49:18
equivalence and give it a list you could use this to get the output list yeah
8:49:25
CH yeah yeah thanks um it asra effects people talk about co-
8:49:31
models and Runners and use some of the same
8:49:37
words yeah there's a very tight connection uh with uh stuff with from too and
8:49:44
uh uh oh these people uh it's almost I think it's basically they like a choose
8:49:49
style where they kind of take CP tensor mq to R or something like that this is
8:49:56
just uh it turns out very very similar result maybe it's the same uh but um but
8:50:03
uh in not not in The Chew style in the module style so I think that the diagrams we want improve we want to
8:50:08
consider our um are different but they may be the same maybe time for one more
8:50:15
question yeah yeah when you talked about putting the two uh two corers in T and
8:50:23
then progressing like what so what does that I understand that that's like a metaphor from running what does that
8:50:29
correspond to that the module action that's the module action yeah um is that
8:50:35
your whole question yeah did I answer it or tell me yeah if you take five steps up both trees that's yeah so this little
8:50:44
this little tree here could be running next to that other tree and what you do is like you have a comma a as the root
8:50:51
it would Branch four different times uh two times to and then the result would be BB b a a and a a I guess and you keep
8:51:01
going but once you hit these leaves you would also make a leaf for this tensored
8:51:06
thing right this one ready to keep this one would end in the leaf this one would say aha you left me off the operating
8:51:13
system would say you left me off in this state one more then uh so you you showed that uh that
8:51:21
there was this Co modal structure but you only used a LAX um fun structure of the
8:51:29
cofree but the mon but NP and CP are mon
8:51:34
andas respectively and the tensor that c is LA with respect to is do with the
8:51:43
composition of which they are moloids and does the structures
8:51:49
interact modu structure okay yeah that that that
8:51:55
doality that you're talking about is what uh is important in in in all this
8:52:01
yeah okay thank you very much
8:52:13
David uh tomorrow we're not here so we're going to go to the uh well
8:52:20
it's not quite as straightforward as that but tomorrow morning we're going to go to the Martin Woods at Complex Center which is um just along that the road
8:52:27
that way where keable Road meets Park Road on the parks road side the road
8:52:33
there's a map online so that's at 9ine o'clock tomorrow but it's not so simple as that because at two I think one o'clock
8:52:40
Southern Midlands Southern and Midlands logic seminar is meeting in this room H
8:52:46
you're all apparently space you're welcome to to G Crush that otherwise stay in the Martin
8:52:53
wood complex at 4:00 act splits into two tracks and one
8:52:59
of the tracks will be in El to a downstairs and one will be in the mar and wood complex and the southern Midland logical carry
8:53:09
on to recover from that information okay but it's all going to
8:53:15
be fine because you all be at the Martin wood complex at 9 o'
8:53:24
and tonight uh Bob cook is performing live in the Jericho Tavern which is a
8:53:30
famous Oxford venue radio had did their early gigs that it's a it's a great place that's a Jericho is a good area to
8:53:36
go to for dinner and drinks even if you decide you don't want to stay for the
8:54:06
and
8:54:53
I know what I'm
8:55:10
doing you come to the bring to you come with
8:55:49
yeah okay great well I'll start talking
8:55:55
he's probably jealous already he out he should have stayed for some more things yeah that's great are you
8:56:05
going yeah I I thought this for us to talk
8:56:44
triple
8:57:34
[Music]
8:57:47
B
8:58:04
it's
8:58:21
not in our we have this
8:58:45
you can do a little from
8:59:04
[Music]
8:59:19
in
8:59:32
handle it like in the outer you have
9:00:13
I
9:00:29
I didn't do I also uh but I'm not officially involved in um
9:00:37
I'm giving a presentation on I have AER that
9:00:43
is related
9:00:51
it's the pattern is like instuction sequences
9:01:01
and it's like yeah yeah so okay yeah let me just
9:01:07
say that the mon part is the instruction sequence
9:01:12
say and then the part is an implementation of you
9:01:18
run I'm not going to say much about this s that's not the that was the or form I
9:01:26
[Music] hadiz other the other post is related
9:01:35
but separate we noticed that it you work in the RO construction of
9:01:43
the over the category of on the index category of Al
9:02:03
so so I wrote this blog post and I posted on
9:02:13
and have I thought you had like
9:02:22
genderized how many symbol
9:02:39
very I think theog was
9:02:45
somehow publishing separately I haven't done this yet I'm not sure why I haven't
9:02:53
been [Music]
9:03:07
yours much more minimiz thing
9:03:15
exem so what's going on in this world of math science Society
9:03:27
I also think like kind of inverse problem inverse
9:03:34
problem yeah right and then some people in the
9:03:39
society they found some ways to actually map yeah to kind of solve this inverse
9:03:45
problem and then order to sort a new inverse problem you're trying to modify this a little bit this um procedure that
9:03:52
was found to solve the first problem so yeah and a good talk a good book or
9:03:59
good presentation or good whatever is one that not only achieves whatever it but also that other people look at it
9:04:05
and say I could use a similar thing to do my and this should not just be something we notice in our own minds it
9:04:11
should be something that's part right
9:04:19
right I gu I me okay hi yeah um we can yeah I
9:04:26
guess I should continue questions maybe but then anyone wants hang out
9:04:36
afterward oh I'm
9:04:50
glad my own or it
9:05:05
[Music]
9:05:22
you oh you
9:05:36
[Music]
9:05:48
[Music] let you
9:06:11
[Music]
9:06:19
start slow
9:06:49
are there paper or your most
9:07:09
say just saying hi good talk oh thanks thanks yeah looking forward to next week yes yeah
9:07:16
yeah yeah what's next week um CB is coming as a intern yes
9:07:33
matter what you they Emer fin
9:07:39
question is it cool if I just like in my
9:07:44
for the G Construction of the well it's like the DU of the like the
9:07:51
Cod and I just say
9:07:57
like it's not like the fundamental I
9:08:09
want I wasn't one detail about this talk that
9:08:15
it's very impressive that you got the explanation of category five minutes
9:08:24
yeah yeah I mean being able to say things quickly
9:08:30
is it's like can you like basically like uh you're all given a certain amount of
9:08:35
space to talk in this and so if you can say your points quickly then you get to see more
9:08:44
points I thought the transition were saying yeah let's talk and then suddenly
9:08:50
from one sentence to the next you made it into categ yeah
9:08:55
yeah no one wants to hang out in flossy for too long without something sorry no
9:09:02
one that's what happened to me yeah yeah it's great like floss is so cool does anyone wait how can we be disagreeing
9:09:09
person like or something I used to disagre with people for like so long and my my thinking would never converge and
9:09:14
like so annoying that's that that's what's so surprising about this field I asked you in the in the toos Workshop
9:09:22
when we were talking oh you know baby SE a marker and all this like just kind of felt like by and then and okay let's get
9:09:29
down to Real World what do you mean this is real and then yeah right right you just like it's EAS to slide into from
9:09:37
that to to to a
9:09:46
talk yeah it' be nice if like you could actually have philos philosophical conversations that matter to us but
9:09:52
without having
9:10:01
I I have a technical question
