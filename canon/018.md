\title{
Compiling to Categories
}

\author{
CONAL ELLIOTT, Target, USA
}

It is well-known that the simply typed lambda-calculus is modeled by any cartesian closed category (CCC). This correspondence suggests giving typed functional programs a variety of interpretations, each corresponding to a different category. A convenient way to realize this idea is as a collection of meaningpreserving transformations added to an existing compiler, such as GHC for Haskell. This paper describes such an implementation and demonstrates its use for a variety of interpretations including hardware circuits, automatic differentiation, incremental computation, and interval analysis. Each such interpretation is a category easily defined in Haskell (outside of the compiler). The general technique appears to provide a compelling alternative to deeply embedded domain-specific languages.

CCS Concepts: $\bullet$ Theory of computation $\rightarrow$ Lambda calculus; $\bullet$ Software and its engineering $\rightarrow$ Functional languages; Compilers;

Additional Key Words and Phrases: category theory, compile-time optimization, domain-specific languages

ACM Reference Format:

Conal Elliott. 2017. Compiling to Categories. Proc. ACM Program. Lang. 1, ICFP, Article 27 (September 2017), 27 pages.

https://doi.org/10.1145/3110271

\section{INTRODUCTION}

As discovered by Joachim Lambek $[1980,1986]$, the models of the simply typed $\lambda$-calculus (STLC) are exactly the cartesian closed categories (CCCs). Moreover, there is a simple, compositional, syntactic transformation from STLC to the vocabulary of CCCs. Each CCC, i.e., each interpretation of this vocabulary, thus gives an interpretation of the STLC. This paper explores a practical application of Lambek's discovery for giving a variety of principled non-standard interpretations of Haskell programs by means of a fairly simple GHC plugin that performs the needed sourceto-source transformation to generalize Haskell code to categories other than the usual one. The interpretations we show include compiling Haskell to massively parallel hardware, linear maps (generalized "matrices"), automatic differentiation, incremental computation, and interval analysis, as well as a textual presentation of CCC expressions for debugging. Moreover, these CCCs combine easily and usefully. For instance, one can describe a function directly in Haskell and then apply a few interpretations to compute the exact $n$th derivative (as a linear map), incrementally, and in hardware. As we'll see, we can sometimes get away with less power, particularly with a cartesian category (without closure), although at the cost of larger categorical translations. We can also make use of additional power, particularly bi-cartesian (coproducts), allowing translation of sum types and case expressions.

The GHC plugin that implements categorical interpretation is modular in that it has no knowledge of specific target categories. To introduce a new interpretation, one simply defines a type and few

Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s).

(c) 2017 Copyright held by the owner/author(s).

2475-1421/2017/9-ART27

https://doi.org/10.1145/3110271

Proc. ACM Program. Lang., Vol. 1, No. ICFP, Article 27. Publication date: September 2017.

Haskell class instances for it-all in regular Haskell code without no exposure to compiler internals. Each interpretation corresponds to a (possibly closed) cartesian functor, and this property makes for a simple specification, useful in calculating the needed class instances.

\section{CARTESIAN CLOSED CATEGORIES}

There are many introductions to category theory [Awodey 2006; Lawvere and Schanuel 2009]. For the purposes of this paper, a brief description of interfaces will do. The basic category interface, along with its instance for functions, is as follows:

![](https://cdn.mathpix.com/cropped/2023_10_29_09ed653e2c9e391c4cbfg-02.jpg?height=188&width=1237&top_left_y=642&top_left_x=172)

The category laws state that $i d$ is the identity for composition and that composition is associative.

A cartesian category adds products, with one introduction and two elimination operations:

infixr $3 \Delta$

class Category $k \Rightarrow$ Cartesian $k$ where

$$
\begin{aligned}
& (\triangle)::\left(a^{\prime} k^{\prime} c\right) \rightarrow\left(a^{\prime} k^{\prime} d\right) \rightarrow\left(a^{\prime} k^{c}(c \times d)\right) \\
& \operatorname{exl}::(a \times b){ }^{\prime} k^{c} a \\
& \operatorname{exr}::(a \times b)^{\prime} k^{\prime} b
\end{aligned}
$$
instance Cartesian $(\rightarrow)$ where

$$
\begin{aligned}
f \Delta g & =\lambda x \rightarrow(f x, g x) \\
\text { exl } & =\lambda(a, b) \rightarrow a \\
\text { exr } & =\lambda(a, b) \rightarrow b
\end{aligned}
$$

In this paper, " $a \times b$ " is a synonym for the Haskell notation " $(a, b)$ ". More generally, each category can have its own product construction, and the category's "objects"/types do not need to have kind * (classifying values). Both forms of added generality are quite useful, but they add complexity that would distract from the main topic of this paper (though see Section 12).

The Cartesian operations must satisfy a universal property:

$\forall h . h \equiv f \Delta g \Longleftrightarrow \operatorname{exl} \circ h \equiv f \wedge \operatorname{exr} \circ h \equiv g$

The names of these operators and style of writing the universal property, as well as those below, are adopted from Gibbons [2002]. A "terminal" category $k$ has a designated object 1 (corresponding to Haskell's () type), such that there is exactly one $k$-arrow from $a$ to 1 for any object $a$ in $k$ :

$$
\begin{array}{c|c}
\text { class Category } k \Rightarrow \text { Terminal } k \text { where } & \text { instance Terminal }(\rightarrow) \text { where } \\
\text { it }:: a^{\prime} k^{\prime} 1 & \text { it }=\lambda a \rightarrow()
\end{array}
$$

Finally, a cartesian closed category (CCC) adds "exponential" types $a \mapsto b$ (representing morphisms as values/objects) with three operations:

![](https://cdn.mathpix.com/cropped/2023_10_29_09ed653e2c9e391c4cbfg-02.jpg?height=191&width=647&top_left_y=1828&top_left_x=173)

instance Closed $(\rightarrow)$ where

$$
\begin{array}{ll}
\operatorname{apply}(f, x) & =f x \\
\text { curry } f & =\lambda a b \rightarrow f(a, b) \\
\text { uncurry } g & =\lambda(a, b) \rightarrow f a b
\end{array}
$$

The notation " $(\Leftrightarrow)$ " is a synonym for $(\rightarrow)$ in this paper but serves to remind us that each CCC can have its own notion of exponentials. The universal property:

apply $\circ($ curry $f \circ \operatorname{exl} \triangle \operatorname{exr}) \equiv f$

We will also want to consider categorical re-interpretations of some primitive constants. For base-typed primitives such as booleans and numbers, we'll use arrows from terminal objects:
class Terminal $k \Rightarrow$ ConstCat $k b$ where

unitArrow : $b \rightarrow\left(1{ }^{\prime} k\right.$ ' $\left.b\right)$ instance ConstCat $(\rightarrow) b$ where

unitArrow $b=\lambda() \rightarrow b$

It's sometimes more convenient to allow non-terminal domains:

const :: ConstCat $k b \Rightarrow b \rightarrow\left(a^{\prime} k\right.$ ' $\left.b\right)$

const $b=$ unitArrow $b \circ$ it

Function-valued primitives may have interpretations in other categories, which can be captured in additional ad hoc Category subclasses. For instance,

![](https://cdn.mathpix.com/cropped/2023_10_29_09ed653e2c9e391c4cbfg-03.jpg?height=291&width=553&top_left_y=681&top_left_x=172)

In a more general setting, different categories can have different Bools. Note that primitives are in uncurried form.

\section{CHANGING VOCABULARIES: CARTESIAN CLOSED CATEGORIES}

The first step in compiling to categories is a syntactic transformation that converts the language of the simply typed $\lambda$-calculus (STLC) to a particular point-free form, corresponding to the vocabulary of cartesian closed categories (CCCs). For expository purposes, we will stay within the function category. The generalization to other categories will come in Section 4. A translation from the STLC can be defined in terms of typing contexts [Chu-Carroll 2012; Curien 1986]. In the translation as described below, however, every translated term is instead a closed, explicit $\lambda$-abstraction-an especially convenient style for use from within GHC's simplifier (Section 5), which does not provide typing contexts. Translation occurs via a small collection of equivalences, presented below, to be used as rewrite rules, with each one taking us closer to a pure CCC expression. The $\lambda$-calculus terms are expressed in Haskell notation. Since we are translating function-typed terms, we can assume that we have an explicit abstraction, $\lambda(x:: \tau) \rightarrow U$ for some term $U$; otherwise, simply $\eta$-expand. Thus we need consider only a small number of cases-one for each kind of lambda term that appears in the body of a $\lambda$-abstraction.

First consider the case that the abstraction body is a variable. Since our terms are closed and well-typed, there is only one possible variable choice, so we must have the identity function on $\tau$ : $(\lambda x \rightarrow x) \equiv i d:: \tau \rightarrow \tau$.

Translating an application (as abstraction body) is a little more involved, involving the Category, Cartesian, and Closed instances for functions:

![](https://cdn.mathpix.com/cropped/2023_10_29_09ed653e2c9e391c4cbfg-03.jpg?height=328&width=622&top_left_y=1969&top_left_x=185)

If the body of an abstraction is an abstraction, we can curry a translation of the uncurried form:

$$
\begin{aligned}
& \lambda x \rightarrow \lambda y \rightarrow U \\
\equiv & \{\text { - definition of curry on }(\rightarrow)-\} \\
& \text { curry }(\lambda(x, y) \rightarrow U)
\end{aligned}
$$

For case expressions, suppose the scrutinee expression has a product type:

$$
(\lambda x \rightarrow \text { case scrut of }\{(u, v) \rightarrow r h s\}) \equiv(\lambda x \rightarrow \text { let }\{w=\operatorname{scrut} ; u=\operatorname{exl} z ; v=\operatorname{exr} z\} \text { in rhs })
$$

These let bindings are often then eliminated by GHC's simplifier. Other single-constructor data types are handled by conversion (Section 9). Translating multi-constructor data types requires a distributive category (Section 8).

The remaining case is a constant as abstraction body, i.e., $\lambda x \rightarrow c$. There are two possibilities. For simple types like Bool and Int, use const:

$$
(\lambda x \rightarrow c) \equiv \text { const } c
$$

If $c$ has function type and an interpretation via BoolCat, NumCat, etc, translate it accordingly:

$$
\begin{aligned}
& (\lambda x \rightarrow \neg) \equiv \text { constFun notC } \\
& (\lambda x \rightarrow(\wedge)) \equiv \text { constFun }(\text { curry and } C) \\
& \ldots
\end{aligned}
$$

where

$$
\begin{aligned}
& \text { constFun }:: \text { Closed } k \Rightarrow\left(a^{\prime} k^{\prime} b\right) \rightarrow\left(z^{\prime} k^{\prime}(a \boxminus b)\right) \\
& \text { constFun } f=\text { curry }(f \circ \text { exr })
\end{aligned}
$$

To see the translation to $\mathrm{CCC}$ form in practice, consider the following definitions:

![](https://cdn.mathpix.com/cropped/2023_10_29_09ed653e2c9e391c4cbfg-04.jpg?height=302&width=715&top_left_y=1316&top_left_x=170)

Type-specialized to a primitive type like Double, and converted to CCC form:

$$
\begin{aligned}
& s q r=\operatorname{mulC} \circ(i d \triangle i d) \\
& \text { magSqr }=\operatorname{add} C \circ(\text { mulC } \circ(\operatorname{exl} \triangle \operatorname{exl}) \triangle \operatorname{mulC} \circ(\operatorname{exr} \triangle \operatorname{exr})) \\
& \operatorname{cosSinProd}=(\cos C \triangle \sin C) \circ \operatorname{mulC}
\end{aligned}
$$

The $s q r$ conversion, step-by-step:

![](https://cdn.mathpix.com/cropped/2023_10_29_09ed653e2c9e391c4cbfg-04.jpg?height=414&width=674&top_left_y=1883&top_left_x=185)

\begin{tabular}{lll} 
& $\mathcal{H}$ exl $\equiv$ exl & $\mathcal{H}$ apply \\
$\mathcal{H}$ id $\equiv$ id & $\mathcal{H}$ exr $\equiv$ exr & $\mathcal{H}($ curry $f) \equiv$ curry $(\mathcal{H} f)$ \\
$\mathcal{H}(g \circ f) \equiv \mathcal{H} g \circ \mathcal{H} f$ & $\mathcal{H}(f \Delta g) \equiv \mathcal{H} f \Delta \mathcal{H} g$ & $\mathcal{H}$ (uncurry $g) \equiv$ uncurry $(\mathcal{H} g)$
\end{tabular}
(a) Category
(b) Cartesian
(c) Closed

Fig. 1. Homomorphism properties

$$
\begin{aligned}
\equiv & \{- \text { Law for closed categories }-\} \\
& m u l C \circ(i d \triangle i d)
\end{aligned}
$$

That last law, apply $\circ($ curry $h \Delta g) \equiv h \circ(i d \Delta g)$, is a consequence of the universal property for exponentials. This law, like many others, is implemented as a GHC rewrite rules in syntactic form [Peyton Jones et al. 2001]. Without such optimizations, $\cos \operatorname{Sin} P r o d$ becomes $(\cos C \triangle \sin C) \circ$ mulC $\circ$ $($ exl $\triangle$ exr $)$.

\section{CHANGING CATEGORIES: CLOSED CARTESIAN FUNCTORS}

In the previous section, we saw how to re-express STLC programs (with pairing and constants) into categorical vocabulary without changing their meaning. The value in doing so is that it then becomes easy to generalize beyond the original $(\rightarrow)$ category to other categories. Each such other category becomes an alternative interpretation of the STLC, and hence of Haskell programs, as we will see. To switch from $(\rightarrow)$, simply replace the $(\rightarrow)$ instances of all categorical operations in the translation above with corresponding instances for another category $k$.

The consistent replacement of interpretations while keeping vocabulary intact is equivalent to applying a homomorphism. Figure 1 shows the homomorphism properties for Category, Cartesian, and Closed. ${ }^{1}$ Note that the identity and composition on the left are for one category, and on the right for another.

Together with the translation from STLC to CCC form, these homomorphism equations are the key to compiling to alternative categories, thus giving sound, non-standard interpretations of functional programs. Interpreted as rewrite rules (oriented left-to-right), the homomorphism equations spell out a simple, systematic transformation from one category (initially $(\rightarrow)$ ) to another. It is important to note that both of these translation steps are syntactic (source code) transformations. Although homomorphisms are semantic properties, their mechanical application requires access to and manipulation of syntax. For this reason, the general technique described in this paper is most naturally implemented as part of compilation rather than a shallow or deep DSL embedding.

\section{TRANSFORMING GHC CORE TO CCC}

The Glasgow/Glorious Haskell Compiler (GHC) compiles Haskell for execution on CPUs. One of the major design choices in implementing GHC is to translate the large source language (Haskell) to a much smaller language, called "GHC Core". The Core language is a typed $\lambda$-calculus with a powerful type system, namely "System FC" (System F with constraints) [Sulzmann et al. 2007].

Since Core has a far more expressive type system than the STLC, it is not immediately obvious that the translation from STLC to CCC form as described above is applicable. A few techniques are required for bridging this gap, the most important being monomorphization. Polymorphism is extremely useful in writing modular programs, but if the ultimate function being compiled is
\footnotetext{
${ }^{1}$ The properties are also known as "functor", "cartesian functor", and "closed cartesian functor", respectively. Note that Haskell's standard library comes with a Functor type class, but it is restricted to endofunctors on the standard Haskell category (i.e., from $(\rightarrow)$ to $(\rightarrow)$ ).
}
monomorphic, then the intermediate polymorphism can be removed by monomorphizing, i.e., inlining polymorphic definitions and substituting monotypes for the type variables involved. Due to the presence of polymorphic recursion in Haskell, monomorphizing does not always have a finite result, and so sometimes the translation fails to terminate. In the many cases in which monomorphization does succeed, it has the additional benefit of leading to very efficient code. For instance, all dictionaries (used to implement type classes) are statically eliminated by partial evaluation, similarly to the treatment by Jones [1994].

In practice, the transformation to CCC form and conversion from $(\rightarrow)$ to other CCCs are implemented as GHC rewrite rules [Peyton Jones et al. 2001]. Transformation is guided by the presence of a pseudo-function: ${ }^{2}$

$c c c::(a \rightarrow b) \rightarrow\left(a^{\prime} k^{c} b\right)$

One might expect there to be a constraint on $k$, such as Closed. Instead, the rewrite rules introduce their own constraints as needed, allowing flexibility and extensibility. For instance, some target categories are cartesian but not cartesian closed, so alternative translations are needed. Homomorphism rules similar to the $\mathcal{H}$ examples of Section 2 push applications of $c c c$ inward, eventually disappearing, as in the rules for $i d$, exl, exr, and apply above as well as the similar rules for primitives like $a d d C$ and mulC. Other rules prepare for homomorphism applicability.

Operating inside of GHC's simplifier allows translation to be done one fragment at a time with many other useful simplifications being applied to the results. This synergy between $c c c$-specific transformation and more general transformations helps considerably in making the implementation simple, efficient, and effective.

Rather than dozens of small rewrite rules, most of the rewriting is done in the form of a single GHC "built-in rule", which is a Haskell-defined function of type CoreExpr $\rightarrow$ Maybe CoreExpr, injected into GHC's simplifier [Peyton Jones and Marlow 2002], whose job is to massage Core expressions into more efficient form. This choice loses some modularity while gaining efficiency via faster matching. More importantly, built-in rules remove some problematic limitations of the rewrite rule source language, including lack of side conditions.

The argument to $c c c$ is typically a lambda expression, although it might need to be converted to one via $\eta$-expansion or inlining. In such cases, the $c c c$ rule will perform one transformation step described in Sections 3 and 4.

For instance, the currying transformation from Section 3, for a target category $k$, is

$$
\operatorname{ccc}(\lambda x \rightarrow U V) \longmapsto \operatorname{apply}_{k} \circ\left(\operatorname{ccc}(\lambda x \rightarrow U) \Delta_{k} c c c(\lambda x \rightarrow V)\right)
$$

where the $k$ subscripts here indicate type applications of the categorical methods to the target category $k$. Since Core lacks the type classes and methods found in the Haskell source language, the transformation rule must construct instances explicitly and insert the $k$-specific versions of the general methods. These instances are in the form of "class dictionaries", which are simply records of methods (Core value identifiers). GHC performs many other simplifications, including method name inlining. The generated $c c c$ calls, if any, are then further transformed by future applications of the $c c c$ transformation. Some transformations introduce no new $c c c$ calls, instead generating simple expressions like $i d_{k}$ and $e x l_{k}$.

Adding a new built-in rule to GHC requires writing a Core-to-Core plugin [GHC Team 2016], packaging some code that alters the normal compiler flow. While plugins can be arbitrarily complex, a very simple one suffices, attaching one the transformation function to the $c c c$ identifier for GHC's Core simplifier to find and apply during normal compilation. Most of the implementation work is

![](https://cdn.mathpix.com/cropped/2023_10_29_09ed653e2c9e391c4cbfg-06.jpg?height=47&width=1389&top_left_y=2208&top_left_x=150)
complicates translation, but is quite useful (as alluded to in Section 12) and will be addressed in a later paper.
}

![](https://cdn.mathpix.com/cropped/2023_10_29_09ed653e2c9e391c4cbfg-07.jpg?height=217&width=649&top_left_y=287&top_left_x=193)

Fig. 2. magSqr

![](https://cdn.mathpix.com/cropped/2023_10_29_09ed653e2c9e391c4cbfg-07.jpg?height=179&width=632&top_left_y=306&top_left_x=851)

Fig. 3. cosSinProd

in this transformation function, which takes a Core expression $e$ and successfully yields an altered expression fust $e^{\prime}$ or fails (Nothing) based on the structure of $e$, as described above.

\section{CONSTRAINED CATEGORIES}

The classes in Section 2 (Category, Cartesian, etc) are too simplistic for many useful target categories. It's often necessary to allow categories to restrict the domain and codomain types. For instance, general differentiable functions require a vector space structure, since derivative values are linear maps between vector spaces having a shared scalar field [Elliott 2009; Spivak 1971]. Similarly, hardware generation requires representability of types as collections of wires. A similar issue was explored for constrained monads, leading to a normal form for applicative and monadic computations that allowed use of the usual, unconstrained Monad class [Sculthorpe et al. 2013a]. It wasn't apparent, however, how to apply that solution to categories.

Since different categories will restrict their types ("objects") differently, let's add an associated type predicate (function from type to constraint) to the Category class from Section 2 (via the "constraint kinds" language extension [Bolingbroke 2011]) and use it to constrain the types involved:

infixr $9 \circ$

class Category $k$ where

type $O k$ k $a::$ Constraint

type $O k k a=() \quad--$ default vacuous constraint

id :: Ok $k a \Rightarrow a^{\prime} k^{\prime} a$

(o) :: $O k_{3} k a b c \Rightarrow\left(b^{\prime} k^{c} c\right) \rightarrow\left(a^{\prime} k^{c} b\right) \rightarrow\left(a^{\prime} k^{c} c\right)$

We'll see a lot of these $O k$ constraints, so define some helpers, as used in the signature for (o):

type $O k_{2} k a b=(O k k a, O k k b)$

type $O k_{3} k a b c=\left(O k_{2} k a b, O k k c\right)$

The types involved in Cartesian and Closed methods are similarly constrained by $O k$.

\section{SOME APPLICATIONS}

\subsection{Computation Graphs and Compiling Haskell to Hardware}

It can be illuminating to visualize programs as computation graphs, revealing potential for parallel evaluation. For instance, the magSqr and cosSinProd examples in Section 3 can be visualized as in Figures 2 and 3.

Underlying these diagrams is a Kleisli-like category of directed graphs based on a simple state monad that supplies output ports and a list of instantiated primitive components. ${ }^{3}$

$$
\text { data Graph a } b=\operatorname{Graph}(\text { Ports } a \rightarrow \operatorname{Graph} M(\text { Ports } b))
$$
\footnotetext{
${ }^{3}$ We could instead combine State Port with Writer [Comp], but the form shown more easily extends optimizations discussed below.
}

![](https://cdn.mathpix.com/cropped/2023_10_29_09ed653e2c9e391c4cbfg-08.jpg?height=110&width=1140&top_left_y=289&top_left_x=172)

The port collections are represented as a generalized algebraic data type (GADT):

![](https://cdn.mathpix.com/cropped/2023_10_29_09ed653e2c9e391c4cbfg-08.jpg?height=382&width=672&top_left_y=460&top_left_x=173)

The Category, Cartesian, Terminal, and Closed instances are very like that of Kleisli [Hughes 1998], but they must also manage the isomorphisms between pair ports and port pairs and between function ports and Graphs: ${ }^{4}$

![](https://cdn.mathpix.com/cropped/2023_10_29_09ed653e2c9e391c4cbfg-08.jpg?height=667&width=1159&top_left_y=991&top_left_x=173)

All that remains is to define instances for primitive operations, each of which simply adds a component (graph node) defined by an operation name and a typed collections of ports for the inputs and the outputs.

These Ports structures are generated from their types, with no ports for 1, one for Bool, Int, and Double, and pairs of recursively generated structures for pairs:

genPort:: GraphM Port -- single port

genPort $=$ do $\{(o$, comps $) \leftarrow$ get $;$ put $(o+1$, comps $)$; return o $\}$

class GenPorts a where genPorts :: GraphM (Ports a)

instance GenPorts 1 where genPorts = return UnitP

instance GenPorts Bool where genPorts = fmap BoolP genPort

instance GenPorts Int $\quad$ where genPorts $=$ fmap IntP genPort

instance GenPorts Double where genPorts $=$ fmap DoubleP genPort
\footnotetext{
${ }^{4}$ The "<=<" operator is Kleisli composition, of type Monad $m \Rightarrow(b \rightarrow m c) \rightarrow(a \rightarrow m b) \rightarrow(a \rightarrow m c)$.
}
instance (GenPorts $a$, GenPorts $b) \Rightarrow$ GenPorts $(a \times b)$ where genPorts $=$ lift $A_{2}$ PairP genPorts genPorts

To add a new graph component, generate output ports by type, associate with the primitive and inputs, and to the accumulating component list:

$$
\begin{aligned}
& \text { genComp }:: \text { GenPorts } b \Rightarrow \text { String } \rightarrow \text { Graph } a b \\
& \text { genComp name }= \\
& \quad \text { Graph }(\lambda a \rightarrow \text { do }\{b \leftarrow \text { genPorts; modify }(\text { second }(\text { Comp name } a b:)) ; \text { return } b\})
\end{aligned}
$$

Now we have everything we need to easily instantiate the operation-specific classes:

![](https://cdn.mathpix.com/cropped/2023_10_29_09ed653e2c9e391c4cbfg-09.jpg?height=421&width=833&top_left_y=699&top_left_x=174)

The $E q$ and Num constraints aren't strictly necessary in this simple implementation, but they serve to remind us of the expected translation from $E q$ and Num methods.

Notice that the Category, Cartesian, and Closed methods produce no components. Instead, Category manages connectivity, Cartesian discards and replicates signals, and Closed generates sub-graphs.

The simple representation and implementation outlined above can be improved by adding a few optimizations. The simplest is constant folding: when an operation is fed by only constant components (having no inputs), perform the operation during graph construction, and generate another constant component. Other algebraic simplifications can be applied, such as addition with zero, multiplication by one, double negation, etc. Finally, redundant computation can be eliminated via hash-consing during graph construction, at the cost of tracking more information about the graph as it is being generated. In practice, these optimizations are quite worthwhile [Elliott 2017].

Once a computation graph is constructed, it can be rendered into a picture, as in the illustrations in this paper. Additionally, graphs can be rendered into machine-readable circuit descriptions in a hardware description language such as Verilog or VHDL. There is a library operation (unknown to the compiler plugin) that generates a picture and a Verilog source file, roughly:

$$
\text { mkCircuit }:: \mathrm{Ok}_{2} \text { Graph a } b \Rightarrow \text { String } \rightarrow \text { Graph } a b \rightarrow I O()
$$

To make a circuit, one applies the pseudo-function $c c c$ to a monomorphic Haskell function and gives the result to mkCircuit, e.g., the magSqr example defined in Section 3, type-specialized to 32-bit integers:

$$
\text { main = mkCircuit "magSqr" (ccc (magSqr @Int)) }
$$

Type inference determines the target category to be Graph, and the plugin's added transformation rules push the $c c c$ application progressively inward, inlining where needed, with many of GHC's standard simplifications tidying things up along the way. When this main is compiled and run, it generates the picture in Figure 2 and the following Verilog implementation:

![](https://cdn.mathpix.com/cropped/2023_10_29_09ed653e2c9e391c4cbfg-10.jpg?height=508&width=693&top_left_y=290&top_left_x=192)

One can convert graphs to other forms as well. For instance, it was easy to translate to shader programs for parallel execution on graphics processors, as in Vertigo [Elliott 2004], and to SMT (satisfiability modulo theories) problems to be solved by Z3 [De Moura and Bjørner 2008].

\subsection{Syntax}

Section 3 showed CCC expressions for three simple functions (sqr, magSqr, and cosSinProd). Those expressions were generated by interpreting the corresponding functions in a syntactic category, which we'll now see. Since CCC expressions can get large, we'll want multi-line pretty-printing, for which we can use a common library [Hughes 1995; Hughes and Peyton Jones 2007].

Start with a simple type of untyped syntax trees:

type DocTree $=$ Tree PDoc

where Tree $a$ is a standard type of rose trees having a value of type $a$ at each node, and PDoc is a pretty-printing document, parametrized by contextual binding precedence (for inserting parentheses as needed):

$$
\begin{aligned}
& \text { data Tree } a=\text { Node } a[\text { Tree } a] \\
& \text { type } P D o c=\text { Rational } \rightarrow \text { Doc }
\end{aligned}
$$

One could use String in place of PDoc in the definition of DocTree, but PDoc allows for complex constant values that can be pretty-printed and parenthesized in context. CCC expressions are very simple, and so is pretty-printing, which handles infix operators and general applications:

$$
\begin{aligned}
& \text { prettyTree }:: \text { DocTree } \rightarrow \text { PDoc } \\
& \text { prettyTree }(\text { Node } d[u, v]) p \mid \text { fust }(q,(\text { lf }, \text { rf })) \leftarrow \text { lookup name fixities }= \\
& \quad \text { maybeParens }(p>q) \$ \operatorname{sep}[\text { prettyTree } u(l f q)<+>\text { text name },(\text { prettyTree } v)(r f q)] \\
& \quad \text { where name }=\operatorname{show~}(d 0) \\
& \quad \text { fixities }=\text { fromList }[(" \circ ",(9, \text { assocR })),(" \triangle ",(3, \text { assocR })),(" \nabla ",(2, \text { assocR }))] \\
& \text { prettyTree }(\text { Node } f \text { es }) p= \\
& \quad \text { maybeParens }(\neg(\text { null es }) \wedge p>10)(\operatorname{sep}(f 10: \text { map }(\lambda e \rightarrow \text { prettyTree e 11) es }))
\end{aligned}
$$

Operator fixity is represented by a number (with higher numbers for tighter binding and 10 for function application) together with a pair of functions that adjust for left and right arguments:

$$
\begin{aligned}
& \text { type Prec }=\text { Rational } \\
& \text { type Fixity }=(\text { Prec }, \text { Assoc }) \\
& \text { type Assoc }=(\text { Prec } \rightarrow \text { Prec }, \text { Prec } \rightarrow \text { Prec })
\end{aligned}
$$

assocL, assocR, nonassoc :: Assoc

assocL $=($ id, succ $)$

assocR $=($ succ, id $)$

nonassoc $=($ succ, succ $)$

Next, wrap up these untyped expressions in a phantom-typed representation [Hinze 2003; Leijen and Meijer 1999], representing an arrow from $a$ to $b$, and define some utility functions:

$$
\begin{aligned}
& \text { newtype Syn } a b=\text { Syn DocTree } \\
& \text { atom }:: \text { Pretty } a \Rightarrow a \rightarrow \text { Syn } a b \\
& \text { atom } a=\text { Syn }(\text { Node }(\text { ppretty } a)[]) \\
& \text { appt }:: \text { String } \rightarrow[\text { DocTree }] \rightarrow \text { DocTree } \\
& \text { appt } s \text { ts }=\text { Node }(\text { const }(\text { text } s)) \text { ts }
\end{aligned}
$$

$$
\begin{aligned}
& a p p_{0}:: \text { String } \rightarrow \text { Syn } a b \\
& a p p_{0} s=\text { Syn }(\text { appt } s[]) \\
& a p p_{1}:: \text { String } \rightarrow \text { Syn } a b \rightarrow \text { Syn } c d \\
& a p p_{1} s(\text { Syn } p)=\text { Syn }(\text { appt } s[p]) \\
& a p p_{2}:: \text { String } \rightarrow \text { Syn } a b \rightarrow \text { Syn } c d \rightarrow \text { Syn ef } \\
& a p p_{2} s(\text { Syn } p)(\text { Syn } q)=\text { Syn }(\text { appt } s[p, q])
\end{aligned}
$$

With these utilities in hand, categorical operations come easily, e.g.,

instance Category Syn where

id $=a p p_{0} " \mathrm{id} "$

$(\circ)=a p p_{2} " \circ "$

instance Cartesian Syn where

exl $=a p p_{0} "$ exl"

exr $=a p p_{0} "$ exr"

$(\Delta)=a p p_{2} " \Delta "$ instance Closed Syn where

apply $=a p p_{0}$ "apply"

curry $=a p p_{1}$ "curry"

uncurry $=a p p_{1}$ "uncurry"

instance BoolCat Syn where

and $C=a p p_{0}$ "andC"

orC $=a p p_{0} "$ orC"

..

These "instances", while useful for debugging, fail to satisfy the categorical axioms.

\subsection{Products of Categories}

Why give only one interpretation to a functional program when we can give two, such as a graph and the corresponding syntactic form? We could compile twice, each with a different target category, but a more convenient and efficient alternative is to compile once to a product of categories. The arrows of such a product is represented by an arrow from each category, acting completely independently:

infixl $7 \otimes$

data $(p \otimes q) a b=p a b \otimes q a b$

Categorical operations simply combine the corresponding operations of each category, e.g.,

instance (Category $k$, Category $\left.k^{\prime}\right) \Rightarrow$ Category $\left(k \otimes k^{\prime}\right)$ where

type $O k\left(k \otimes k^{\prime}\right) a=\left(O k k a, O k k^{\prime} a\right)$

$i d=i d \otimes i d$

$\left(g \otimes g^{\prime}\right) \circ\left(f \otimes f^{\prime}\right)=(g \circ f) \otimes\left(g^{\prime} \circ f^{\prime}\right)$

instance $\left(\right.$ Cartesian $k$, Cartesian $\left.k^{\prime}\right) \Rightarrow$ Cartesian $\left(k \otimes k^{\prime}\right)$ where

$e x l=e x l \otimes e x l$

exr $=$ exr $\otimes$ exr

$\left(f \otimes f^{\prime}\right) \Delta\left(g \otimes g^{\prime}\right)=(f \Delta g) \otimes\left(f^{\prime} \Delta g^{\prime}\right)$

As an identity for $(\otimes)$, there is a category with exactly one arrow for each domain/codomain pair and trivial instances of Category, Cartesian, etc.

\subsection{Linear Maps}

Although we usually represent functions as code, sometimes we can use data instead. For linear functions, one can instead use a very compact data representation. For instance, any linear function from $\mathbb{R}^{2}$ to $\mathbb{R}^{3}$ can be represented as a matrix of six numbers. Moreover, since the identity function is linear, and the composition of linear functions is linear, we have a category. For linearity to be meaningful, we need vector space over a scalar field (or just a (semi)module over a (semi)ring). There are various ways to formulate vector spaces over a scalar field $s$. A particularly elegant choice is that of free vector spaces, each of which is isomorphic to the space of functions $f:: A \rightarrow s$ from some index set $A$. Rather than mapping to functions, however, we can use a memoized form for these functions as tries, composed from some basic functor building blocks [Hinze 2000].

Conversion to and from (representable endo)functor form is managed by the following class:

![](https://cdn.mathpix.com/cropped/2023_10_29_09ed653e2c9e391c4cbfg-12.jpg?height=184&width=578&top_left_y=893&top_left_x=174)

Some instances:

![](https://cdn.mathpix.com/cropped/2023_10_29_09ed653e2c9e391c4cbfg-12.jpg?height=573&width=808&top_left_y=1147&top_left_x=172)

The $\operatorname{Par}_{1}, U_{1}$ and $(x)$ type constructors are identity functor, unit functor, and cartesian functor product, taken from GHC.Generics [Magalhães et al. 2010; Magalhães et al. 2011]:

![](https://cdn.mathpix.com/cropped/2023_10_29_09ed653e2c9e391c4cbfg-12.jpg?height=145&width=388&top_left_y=1825&top_left_x=170)

A linear map from $a$ to $b$ is represented by a vector of vectors, i.e., a "matrix", in row-major form (though we could as easily use column-major):

newtype $a \multimap_{s} b=\operatorname{LMap}(V s b(V s a s))$

The categorical instances are as follows, with auxiliary definitions given in Appendix A:

instance Num $s \Rightarrow$ Category $\left(\sim_{s}\right)$ where

type $O k\left(\multimap_{s}\right) a=(\operatorname{HasV} s a, O k L F(V s a))$

![](https://cdn.mathpix.com/cropped/2023_10_29_09ed653e2c9e391c4cbfg-13.jpg?height=392&width=619&top_left_y=289&top_left_x=172)

Linear map application is a functor from $\left(-_{s}\right)$ to $(\rightarrow)$, serving as a semantic function for $\left(-_{s}\right)$ :

lapply:: $\left(\mathrm{Ok}_{2}\left(\bigcirc_{s}\right) a b\right.$, Num $\left.s\right) \Rightarrow\left(a \multimap_{s} b\right) \rightarrow(a \rightarrow b)$

lapply $($ LMap ba $)=$ unV $\circ$ lapplyL $b a \circ$ toV

Conversely, given a function $f:: a \rightarrow b$, we can construct a linear map $f^{\prime}:: a \multimap_{s} b$ such that lapply $f^{\prime} \equiv f$ if $f$ is linear:

linear :: $\left(\mathrm{Ok}_{2}\left(\multimap_{s}\right)\right.$ a b, HasL $(V s a)$, Num $\left.s\right) \Rightarrow(a \rightarrow b) \rightarrow\left(a \multimap_{s} b\right)$

linear $h=L M a p($ linearL $($ toV $\circ h \circ u n V))$

\subsection{Automatic Differentiation}

Next, let's consider how to differentiate functions exactly. To handle multi-dimensional types, assume that our functions map between free vector spaces over a common scalar field. In this general setting, derivative values are linear maps [Elliott 2009; Spivak 1971]. We thus have the following type for differentiation:

$$
\operatorname{deriv}::(a \rightarrow b) \rightarrow\left(a \rightarrow\left(a \multimap_{s} b\right)\right)
$$

Although $\operatorname{deriv} f$ is not computable from the function $f$, we can construct it homomorphically from a categorical recipe for $f$ by compiling to a suitable category. Consider the chain rule in terms of derivatives as linear maps [Spivak 1971, Theorem 2-2]:

$\operatorname{deriv}(g \circ f) a \equiv \operatorname{deriv} g(f a) \circ \operatorname{deriv} f a$

While the composition in the left side of the chain rule is on functions, the composition on the right is on linear maps. The notion of derivatives as linear maps subsumes various representations including scalar values, vectors, covectors, matrices, and higher dimensional counterparts. Likewise, this one general chain rule subsumes many specific variations involving scalar multiplication, inner products, outer products, matrix products, Hessians, etc.

Note that the derivative of $g \circ f$ depends not only on the derivatives of $g$ and $f$, but also on $g$ itself, so deriv is not a functor. All is not lost though, as we can instead compositionally construct a combination of functions and their derivatives. A straightforward pairing of the two leads to the following representation of differentiable functions between vector spaces over a field $s$ :

$$
\text { data } \left.a \sim_{s} b=D(a \rightarrow b)\left(a \rightarrow\left(a \multimap_{s} b\right)\right)\right) \quad \text {-- first try }
$$

This representation, however, prevents exploiting the considerable amount of computation that functions and their derivatives typically have in common. Fortunately, there is a simple solution, using the $(\triangle)$ isomorphism from Cartesian to combine functions and their derivatives:

$$
\text { data } a \sim_{s} b=D\left\{u n D:: a \rightarrow b \times\left(a \multimap_{s} b\right)\right\} \quad \text {-- allows work sharing }
$$

Our goal is to implement the following functor

andDeriv $::(a \rightarrow b) \rightarrow\left(a \sim_{s} b\right)$

andDeriv $f=D(f \Delta \operatorname{deriv} f) \quad-$ specification

This combination forms a local affine (first order) approximation of $f$ at every point in $a$. Once we implement andDeriv faithfully to this specification, we will have a simple implementation of deriv:

$\operatorname{deriv} f=\operatorname{snd} \circ \operatorname{unD}(\operatorname{andDeriv} f)$

Linear functions are trivial to differentiate, since they are their own (perfect) linear approximations. The following helper function takes a linear function and its linear map counterpart: ${ }^{5}$

linearD $::\left(N u m s, O k_{2}\left(\multimap_{s}\right) a b\right) \Rightarrow(a \rightarrow b) \rightarrow\left(a \multimap_{s} b\right) \rightarrow\left(a \sim_{s} b\right)$

linearD $f f^{\prime}=D\left(f \triangle\right.$ const $\left.f^{\prime}\right)$

Now we're ready to define the Category instance for differentiable functions. The identity function is linear, and composition follows from the chain rule:

instance Num $s \Rightarrow$ Category $\left(\sim_{s}\right)$ where

type $O k\left(\sim_{s}\right) a=O k\left(\multimap_{s}\right) a$

id $=$ linearD id id

$D g \circ D f=D\left(\lambda a \rightarrow\right.$ let $\left\{\left(b, f^{\prime}\right)=f a ;\left(c, g^{\prime}\right)=g b\right\}$ in $\left.\left(c, g^{\prime} \circ f^{\prime}\right)\right)$

Product operations are handled similarly. For $(\triangle)$, we'll need a counterpart to the chain rule:

$\operatorname{deriv}(f \Delta g) a=\operatorname{deriv} f a \triangle \operatorname{deriv} g a$

Assembling the pieces,

instance Num $s \Rightarrow$ Cartesian $\left(\sim_{s}\right)$ where

exl $=$ linearD exl exl

exr $=$ linearD exr exr

$D f \Delta D g=D\left(\lambda a \rightarrow\right.$ let $\left\{\left(b, f^{\prime}\right)=f a ;\left(c, g^{\prime}\right)=g a\right\}$ in $\left.\left((b, c), f^{\prime} \Delta g^{\prime}\right)\right)$

Knowledge of derivatives for numerical operations lives in instances of NumCat, FloatingCat, etc:

instance (Num $\left.s, V s s \sim \operatorname{Par}_{1}, \mathrm{Ok}\left(\multimap_{s}\right) s\right) \Rightarrow \mathrm{NumCat}\left(\sim_{s}\right) s$ where

negate $C=$ linearD negateC $($ linear negateC $)$

add $C=$ linearD addC (linear addC)

mulC $=D($ mulC $\triangle \lambda(a, b) \rightarrow \operatorname{linear}(\lambda(d a, d b) \rightarrow d a * b+d b * a))$

Figures 4 and 5 shows the result of andDeriv on the magSqr and cosSinProd examples from Section 3. Because magSqr $:: \mathbb{R}^{2} \rightarrow \mathbb{R}$, its derivative values have type $\mathbb{R}^{2} \multimap \mathbb{R} \mathbb{R}$, represented by a pair of reals. Because cosSinProd $:: \mathbb{R}^{2} \rightarrow \mathbb{R}^{2}$, its derivative values have type $\mathbb{R}^{2} \multimap \mathbb{R}^{2}$, represented by a pair of pairs of reals. Figures 6 and 7 shows the results of deriv on the same two examples.

The Category and Cartesian instances for $\left(\sim_{s}\right)$ rely on $\left(-_{s}\right)$ only for its instances of these same classes. Thanks to this simple relationship, we can easily generalize from $\left(-_{s}\right)$ to an arbitrary bicartesian category, re-specializing to $\left(\sim_{s}\right)$ :

newtype $G D k a b=D\left(a \rightarrow b \times\left(a^{\prime} k^{c} b\right)\right)$

type $\left(\sim_{s}\right)=G D\left(\multimap_{s}\right)$

The instances for $G D k$ are as they were for $D$ above, except for adding the properties required of $k$ :
\footnotetext{
${ }^{5}$ One could compute either the function or the map from the other, using lapply or linear above, but the two-argument linear $D$ allows for a useful generalization below.
}

![](https://cdn.mathpix.com/cropped/2023_10_29_09ed653e2c9e391c4cbfg-15.jpg?height=312&width=591&top_left_y=315&top_left_x=190)

Fig. 4. andDeriv magSqr

![](https://cdn.mathpix.com/cropped/2023_10_29_09ed653e2c9e391c4cbfg-15.jpg?height=373&width=705&top_left_y=287&top_left_x=778)

Fig. 5. andDeriv cosSinProd

![](https://cdn.mathpix.com/cropped/2023_10_29_09ed653e2c9e391c4cbfg-15.jpg?height=196&width=398&top_left_y=837&top_left_x=192)

Fig. 6. deriv magSqr

![](https://cdn.mathpix.com/cropped/2023_10_29_09ed653e2c9e391c4cbfg-15.jpg?height=349&width=882&top_left_y=758&top_left_x=604)

Fig. 7. deriv cosSinProd

instance Category $k \Rightarrow$ Category $(G D k)$ where $\ldots$ instance Cartesian $k \Rightarrow$ Cartesian $(G D k)$ where ...

\subsection{Incremental Computation}

When a function is applied to the same argument twice, it performs the same work, unless the function is memoized. When a function is applied to two similar arguments, memoization is no help, and the second application must start from scratch even though some of the work may be repeated between the two applications. The idea of incremental computation (IC) is to make a small amount of extra effort on the first invocation so that invocations on similar arguments may be done incrementally and thus inexpensively.

To formulate IC, define an interface for incremental value changes, mimicking Cai et al. [2014]:

infixl $6 \ominus, \oplus$

class HasDelta $a$ where

type $\Delta a$

$$
\begin{aligned}
& (\oplus):: a \rightarrow \Delta a \rightarrow a \\
& (\ominus):: a \rightarrow a \rightarrow \Delta a \\
& 0 \quad:: \Delta a
\end{aligned}
$$

The unit type has a trivial instance, since there can be no changes; a change for pairs is a pair of changes; and a change for functions is a function to changes. Atomic types can be handled in various ways, including simply a Maybe giving a new value if changed.

A change morphism says how to map changes to changes:

newtype $\operatorname{DelX} a b=\operatorname{DelX}(\Delta a \rightarrow \Delta b)$

It's easy to then give Category and Cartesian instances for DelX:

![](https://cdn.mathpix.com/cropped/2023_10_29_09ed653e2c9e391c4cbfg-16.jpg?height=189&width=1041&top_left_y=345&top_left_x=174)

Since DelX is a cartesian functor, we can use it with generalized automatic differentiation to get a category of incremental computation:

type $I n c=G D$ DelX

\subsection{Interval Analysis}

Interval analysis (IA) is a technique for computing bounds on functions, mapping domain intervals to codomain intervals [Moore 1966], with applications including root finding, minimization, and error management. Given a function $f$, a corresponding interval function $\hat{f}$ has the property that for any domain interval $I, \forall x \in I$. $f x \in \hat{f} I$. The compositional nature of IA makes it a natural fit for a categorical interface. Different types have different interval representations, with atomic values having lower and upper bound, while product intervals are products of intervals ("boxes"), and function intervals are functions between intervals:

$$
\begin{aligned}
& \text { data IFun a } b=\text { IFun }(\text { Interval } a \rightarrow \text { Interval } b) \\
& \text { type family Interval } a \\
& \text { type instance Interval Double }=\text { Double } \times \text { Double } \\
& \text { type instance Interval Int }=\text { Int } \times \text { Int } \\
& \text { type instance Interval }(a \times b)=\text { Interval } a \times \text { Interval } b \\
& \text { type instance Interval }(a \rightarrow b)=\text { Interval } a \rightarrow \text { Interval } b
\end{aligned}
$$

Instances for the basic category classes are as simple as can be:

instance Category IFun where id $=$ IFun id IFun $g \circ$ IFun $f=$ IFun $(g \circ f)$

instance Cartesian IFun where

exl $=$ IFun exl

exr $=$ IFun exr

IFun $f \triangle I F u n g=\operatorname{IFun}(f \Delta g)$ instance Closed IFun where

apply $=$ IFun apply

$\operatorname{curry}(\operatorname{IFun} f)=$ IFun $($ curry $f)$

uncurry $($ IFun $g)=$ IFun (uncurry $g)$

instance Interval $b \sim(b \times b) \Rightarrow$ ConstCat IFun $b$ where unitArrow $b=$ IFun (unitArrow $(b, b)$ )

The real work is done in numeric operations:

![](https://cdn.mathpix.com/cropped/2023_10_29_09ed653e2c9e391c4cbfg-16.jpg?height=189&width=1255&top_left_y=1875&top_left_x=172)

\subsection{Other Examples}

Kmett [2011] shows how to form a simple cartesian category of entailments between Haskell's type constraints. The natural formulation of this category has kind Constraint $\rightarrow$ Constraint $\rightarrow *$,
relying on the poly-kinded generalization mentioned in Section 12. (It can be encoded somewhat less directly via values that can be converted to and from constraint dictionaries, similarly to the conversion to and from functor representations of linear maps in Section 7.4). This category is useful for boosting the power of GHC's type inference, particularly to overcome the lack of universally quantified constraints. Other deductive systems may be possible as well, including disjunction (coproduct) and implication (exponential).

Just as linear maps form a (bi)cartesian category, so do polynomials (including product domains and ranges). As long as one uses only addition and multiplication as primitives, functional programs can be compiled into polynomials, which can then be analyzed efficiently and exactly, finding roots, minima and maxima, derivatives, and integrals, as well as evaluated efficiently in parallel using parallel prefix algorithms [Blelloch 1990; Elliott 2017]. Power series (infinite polynomials) can probably be treated the same way (allowing operations beyond addition and multiplication), perhaps using the operations elegantly defined by McIlroy [1999].

\section{COCARTESIAN AND DISTRIBUTIVE CATEGORIES}

Although not used in the examples of this paper, another common and useful concept is that of cocartesian categories. The interface is exactly dual to that of Cartesian, with sums in place of cartesian products, having two introduction and one elimination operation:

infixr $2 \nabla$

class Category $k \Rightarrow$ Cocartesian $k$ where

inl :: $O k_{2} k a b \Rightarrow a^{\prime} k^{\prime}(a+b)$

inr :: $O k_{2} k a b \Rightarrow b^{\prime} k^{c}(a+b)$

$(\nabla):: O k_{3} k$ a c $d$

$\Rightarrow\left(c^{\prime} k^{\prime} a\right) \rightarrow\left(d^{\prime} k^{\prime} a\right) \rightarrow\left((c+d)^{'} k^{\prime} a\right)$ instance Cocartesian $(\rightarrow)$ where

inl $=$ Left

inr $=$ Right

$(f \nabla g)($ Left $a)=f a$

$(f \nabla g)($ Right $b)=g b$

The universal property is also dual to that of Cartesian:

$$
\forall h . h \equiv f \nabla g \Longleftrightarrow h \circ \text { inl } \equiv f \wedge h \circ \text { inr } \equiv g
$$

Again, details are adopted from Gibbons [2002]. As with products, it will be useful to generalize the notion of coproducts beyond sum types. A "bicartesian" category is both cartesian and cocartesian.

A distributive category is one that enables distribution of products over coproducts:

class (Cartesian $k$, Cocartesian $k) \Rightarrow$ Distrib $k$ where distl :: $\mathrm{Ok}_{3} \mathrm{k}$ a $\mathrm{u}$

$$
\Rightarrow(a \times(u+v))^{'} k^{\prime}(a \times u+a \times v)
$$

distr :: $\mathrm{Ok}_{3}$ k a $u$

$$
\Rightarrow(a \times u+a \times v)^{'} k^{\varsigma}(a \times(u+v))
$$

instance Distrib $(\rightarrow)$ where

$$
\begin{aligned}
& \operatorname{distl}(a, \text { Left } \quad u)=\text { Left } \quad(a, u) \\
& \operatorname{distl}(a, \text { Right } v)=\text { Right }(a, v) \\
& \operatorname{distr}(\text { Left } \quad u, b)=\text { Left } \quad(u, b) \\
& \operatorname{distr}(\text { Right } v, b)=\operatorname{Right}(v, b)
\end{aligned}
$$

We can define either distl or distr in terms of the other (exercise), so only one need be primitive.

Distributive categories enable translation of definition by cases. Consider only case over binary sums $a+b$ for now. Other multi-constructor data types will be translated into binary sums, as described in Section 9. Transform such expressions (in context) as follows:

$$
(\lambda x \rightarrow \text { case scrut of }\{\text { Left } u \rightarrow U ; \text { Right } v \rightarrow V\}) \equiv(\lambda x \rightarrow(U \nabla V) \text { scrut })
$$

We already know how to handle applications under an outer abstraction. For the rest,

$$
(\lambda x \rightarrow U \nabla V) \equiv \text { curry }((\text { uncurry }(\lambda x \rightarrow U) \nabla \operatorname{uncurry}(\lambda x \rightarrow V)) \circ \text { distl })
$$

The proof is left as an exercise.

\section{NON-STANDARD TYPES}

So far, we're only handling "standard" types, i.e., primitive types like 1, Bool, Int, and Double, along with products of standard types and functions between standard types. In practice, most programs also involve algebraic data types. Such "non-standard" types are always isomorphic to standard types. To assist with these isomorphisms, define a class of types with alternative representations. Rather than go all the way to and from standard types in one step, take just a single step at a time:

![](https://cdn.mathpix.com/cropped/2023_10_29_09ed653e2c9e391c4cbfg-18.jpg?height=189&width=706&top_left_y=568&top_left_x=174)

For instance, for a uniform pair type data Pair $a=P a$,

instance HasRep (Pair a) where

type Rep (Pair a) $=a \times a$

$\operatorname{repr}\left(P \quad a a^{\prime}\right)=\left(a, a^{\prime}\right)$

abst $\left(a, a^{\prime}\right)=P a a^{\prime}$

The key to translating non-standard types is observing that (a) they are constructed in exactly one way, namely constructor application, and (b) they are consumed in exactly one way, namely as the scrutinee of a case expression. (Haskell's lambda and let patterns become simple variable lambda and let, together with case expressions in GHC Core.) Constructor application becomes abst applications, and case consumption becomes repr applications, both by means of the absto repr law above and some selective inlining.

Given a saturated constructor application Con $e_{1} \ldots e_{n}$, rewrite it to abst (inline repr (Con $\left.e_{1} \ldots e_{n}\right)$ ), where inline e tells GHC's simplifier to inline the expression $e .{ }^{6} \mathrm{GHC}$ 's usual simplifications then eliminate the constructor, leaving only abst behind. For example,

![](https://cdn.mathpix.com/cropped/2023_10_29_09ed653e2c9e391c4cbfg-18.jpg?height=502&width=815&top_left_y=1406&top_left_x=187)

Dually, consider a case expression case scrut of $\left\{p_{1} \rightarrow r h s_{1} ; \ldots ; p_{n} \rightarrow r h s_{n}\right\}$, where (the scrutinee) scrut has a non-standard type with a HasRep instance. Rewrite scrut to inline abst (repr scrut) (this time inlining abst instead of repr). GHC's usual simplifications will then replace the case over a non-standard type with a case over a standard type or one closer to standard. For instance,

$$
\begin{aligned}
& \text { case } p \text { of } P x y \rightarrow x+y \\
\equiv & \{-a b s t \circ \text { repr } \equiv i d-\}
\end{aligned}
$$
\footnotetext{
${ }^{6} \mathrm{~A}$ "saturated" constructor application is one that is applied to the maximal number of arguments, or equivalently, one that has a non-function type. Unsaturated constructor applications can be $\eta$-expanded until saturated.
}

![](https://cdn.mathpix.com/cropped/2023_10_29_09ed653e2c9e391c4cbfg-19.jpg?height=505&width=970&top_left_y=290&top_left_x=187)

These two transformations occur only in the context "ccc $(\lambda x \rightarrow \ldots)$ ". The remaining occurrences of abst (for construction) and repr (for consumption) become part of the categorical vocabulary as a generalization of HasRep:

![](https://cdn.mathpix.com/cropped/2023_10_29_09ed653e2c9e391c4cbfg-19.jpg?height=139&width=1273&top_left_y=945&top_left_x=174)

During conversion to categorical form (as in Section 3), repr and abst are replaced by their generalizations $r e p r C$ and $a b s t C$. When changing categories (as in Section 4), the occurrences of $r e p r C$ and $a b s t C$ in $(\rightarrow)$ are replaced by the same methods in another category.

Multi-constructor algebraic data types pose no additional difficulty. Their Rep encodings involve sums (and often products as well), and GHC's case-of-case and case-of-known-constructor transformations handle the resulting multi-branch case expressions. Conversion to categories other than $(\rightarrow)$ requires support for cocartesian categories, adding coproducts, as well as distributive categories, as described in Section 8.

As an example, the Graph category in Section 7.1 handles non-standard types via an additional Ports constructor:

$$
\text { data Ports }:: * \rightarrow * \text { where }
$$

$$
\text { AbstP }:: \text { Ports }(\text { Rep } a) \rightarrow \text { Ports } a
$$

The RepCat methods add and remove AbstP ports:

instance HasRep $a \Rightarrow$ RepCat Graph a where

abst $C=\operatorname{Graph}(\lambda r \rightarrow \operatorname{return}($ AbstP $r))$

reprC $=\operatorname{Graph}(\lambda($ AbstP $r) \rightarrow$ return $r)$

Elliott [2017] showed many examples with non-standard types compiled to Graph.

\section{SOME IMPLEMENTATION ISSUES}

\subsection{Unboxed Operations}

Performance of numeric operations under GHC is considerably improved by using unboxed number representations inside of boxed wrappers [Peyton Jones and Launchbury 1991]. For instance, the (boxed) Int type is defined as a wrapping of an unboxed field:

$$
\text { data } I n t=I \text { Int }_{\#}
$$

Numerical operations on Int are then defined to unwrap (unbox) arguments, apply unboxed operations on the contents, and rewrap (rebox) the unboxed results:

![](https://cdn.mathpix.com/cropped/2023_10_29_09ed653e2c9e391c4cbfg-20.jpg?height=281&width=534&top_left_y=389&top_left_x=173)

When boxed operations are combined, inlined, and optimized, most unwrapping and rewrapping code is eliminated, thanks to the GHC's case-of-known-constructor optimization. For instance, for variables $a, b::$ Int the expression " $a+3 * b$ " optimizes to the following:

case $a$ of $I_{\#} x \rightarrow$ case $b$ of $I_{\#} y \rightarrow I_{\#}\left(x+\# 3_{\#} *_{\#} y\right)$

Even recursive definitions involving numbers can be handled efficiently, via the general worker wrapper transformation [Gill and Hutton 2009].

Although unboxing speeds up execution of Haskell programs with the usual interpretation (the $(\rightarrow$ ) category), it complicates conversion to categorical form and hence compiling to other categories. Recall the signatures involved in the generalized Num class:

class NumCat $k$ a where

add $C$, subC, mulC :: $(a \times a)^{\prime} k$ ' $a$

While these methods are polymorphic over number type/object, the polymorphism is implicitly restricted to kind $*$, i.e., boxed types. The original boxed versions of addition and multiplication used in our example above have disappeared from the optimized form and must be recovered in order to satisfy the implicit kind restriction (or some other boxed form, which is probably no easier).

After much experimentation, a simple solution to reboxing emerged. The first step is to find applications of constructors like $I_{\#}$, such as $I_{\#}\left(x+_{\#} 3_{\#} *_{\#} y\right)$ in the optimized example above. Replace those outer constructors with a synonym defined as follows:

$$
\begin{aligned}
& \text { boxI }:: \text { Int }_{\#} \rightarrow \text { Int } \\
& \text { boxI = I\# } \\
& \{-\# \text { INLINE [0] boxI \#-\}}
\end{aligned}
$$

The INLINE pragma causes these synonyms to be replaced by their original constructors only at the end of compilation (phase zero) if any still exist (which happens with constant expressions). The example above becomes

case $a$ of $I_{\#} x \rightarrow$ case $b$ of $I_{\#} y \rightarrow b o x I\left(x t_{\#} 3_{\#} *_{\#} z\right)$

The boxing synonyms trigger cascaded application of step-by-step reboxing rewrite rules such as the following:

$$
\begin{array}{ll}
\forall u . \quad \text { boxI }(\text { negateInt } u) & =\text { negateC }(\text { boxI } u) \\
\forall u v . \operatorname{boxI}\left(u+_{\#} v\right) & =\operatorname{addC}(\operatorname{boxI} u, \text { boxI } v) \\
\forall u v . \operatorname{boxI}\left(u-_{\#} v\right) & =\operatorname{subC}(\operatorname{boxI} u, \text { boxI } v) \\
\forall u v . \operatorname{boxI}\left(u *_{\#} v\right) & =\operatorname{mulC}(\operatorname{boxI} u, \operatorname{boxI} v)
\end{array}
$$

Introducing category-generalized names rather than the usual versions $((+),(*)$, etc) in these rules avoids having GHC's simplifier re-inline the usual versions back into unboxed form. Note here
how the reboxing rules push boxI applications inward as long as there are unboxed operations being applied. This recursive transformation ends in literals and variables. Our example becomes

case $a$ of $I_{\#} x \rightarrow$ case $b$ of $I_{\#} y \rightarrow$ addC (boxI $x$, mulC (boxI $3_{\#}$, boxI $\left.y\right)$ )

One more transformation eliminates the unboxing case scrutinees: transform an expression like "case $a$ of $I_{\#} x \rightarrow$...boxI $x \ldots$.." to "let $x^{\prime}=a$ in ... $x^{\prime} \ldots$... By applying the previous transformations (converting boxing constructors and recursive reboxing) before this unboxing scrutinee elimination, all occurrences of $x$ will be of the form boxI $x$, so no unboxed variables remain. The let bindings are removed by GHC's simplifier unless doing so replicates nontrivial work. After all of these transformations, our example involves only (a) numeric operations in category-generalized form, (b) variables of boxed types, and (c) boxing synonyms applied to unboxed literals:

$\operatorname{add} C\left(a, \operatorname{mulC}\left(\operatorname{boxI} 3_{\#}, b\right)\right)$

\subsection{Translation without Closure}

Some categories are cartesian but not cartesian closed, e.g., vector spaces with linear maps. Most of the rules for converting to $\mathrm{CCC}$ form rely on closure, which poses a problem for non-closed categories. If, however, the overall function being converted does not involve functions in its domain or codomain, then the corresponding closure-dependent CCC form can often (or perhaps always) be converted to a form free of the Closed operations (apply, curry, and uncurry)-assuming that none of the primitive operations ( $a d d C$, mulC, eq, etc) involve exponentials in their types either (which seems a harmless restriction). An easy way to eliminate the Closed operations is by converting first to CCC form in the $(\rightarrow)$ category (which is closed) as suggested in Section 3, applying some rewrite rules that follow from general category laws, and then homomorphically converting to the desired non-closed category, as suggested in Section 4. (For cartesian closed categories, compilation can take a more direct route of fusing the vocabulary change with the category change, although homomorphism application is fairly simple and inexpensive.) These closure-eliminating rules include the following:

$$
\begin{aligned}
& \forall f . \quad \text { uncurry }(\text { curry } f)=f \\
& \forall g . \quad \text { curry }(\text { uncurry } g)=g \\
& \forall g f . \text { apply }(\text { curry }(g \circ \text { exr }) \Delta f)=g \circ f
\end{aligned}
$$

\subsection{Postponing Inlining}

In the current GHC (8.0.2), class methods are always inlined early. This behavior, if not somehow avoided would have some unfortunate consequences:

- The reboxing rewrite rules of Section 10.1 would get undone immediately, through inlining and subsequent simplification, leading to an infinite transformation loop.

- Homomorphism rules (the heart of transforming to non-standard interpretations, outlined in Section 3) would never fire, since they are written in terms of class methods.

- For the same reason, simplification rules that apply category theory laws would never fire.

Fortunately, there is a simple and effective work-around for premature method inlining. All categorical class methods have corresponding late-inlining aliases with the same names but placed in a dedicated module, along with rewrite rules involving them. Conversion to categorical form (Section 3) uses these aliases instead of the module that defines the classes and methods. If a future version of GHC allows delayed method inlining, the aliases can be removed.

\section{RELATED WORK}

The Categorical Abstract Machine (CAM) is an execution model for terms from the language of cartesian closed categories [Cousineau et al. 1987], emerging from the categorical combinators of Curien [1986], and used as the basis of an implementation of the Caml dialect and of the ML programming language [Weis et al. 2005]. Like the work described in this paper, the CAM is based on CCCs and its relation to the $\lambda$-calculus. It does not appear to have been used to give multiple CCC-based interpretations (each with its own semantics and notion of execution).

There has been a lot of work in describing and synthesizing hardware via functional programming, beginning with muFP [Sheeran 1984], which was based on FP [Backus 1978] (close to the language of cartesian categories), extended with streams for synchronous circuits. The muFP language was followed by Ruby, adding a relational perspective [Jones and Sheeran 1990], and then by Lava [Bjesse et al. 1998], which was embedded in Haskell. This very fruitful line of research has focused on describing hardware but also explored elegant expression of hardware-friendly algorithms.

$\mathrm{C} \lambda \mathrm{aSH}$ is a compiler from Haskell to hardware, also using $\mathrm{GHC}$ as a front end and also working by successive program transformation [Baaij and Kuper 2014]. Like the work in this paper, $\mathrm{C} \lambda \mathrm{aSH}$ compiles a somewhat restricted form of Haskell rather than being a Haskell library implemented as a deep or shallow embedding. Unlike the present paper, $\mathrm{C} \lambda \mathrm{aSH}$ only compiles to hardware.

Cai et al. [2014] describes how to convert a typed $\lambda$-calculus into an incremental version by a process very similar to differentiation. That work informed the incremental CCC of Section 7.6 as another instance of a generalized differentiation CCC, which may have other useful specializations as well (in addition to differential calculus). Also related is the work on self-adjusting computation in a functional setting [Acar 2005; Acar et al. 2005] and the monadic/applicative formulation in Haskell [Carlsson 2002]. Both require using a supporting library, with considerable impact of programming style.

Automatic differentiation (AD) has a long and rich history dating back to Wengert [1964] and including modern, functional formulations [Elliott 2009; Karczmarczuk 1998]. Siskind and Pearlmutter $[2005,2008]$ suggest that it is difficult and perhaps impossible to give a correct implementation of $\mathrm{AD}$ in purely functional languages such as Haskell, in particular pointing out the danger of "perturbation confusion" when nesting the differentiation operator. The technique in this paper used with the AD CCC given in Section 7.5 side-steps these difficulties by providing a correctly implemented differentiation operator by means of compile-time transformation. It also does not appear to fall naturally into the usual classification of forward vs reverse vs mixed modes, although perhaps it could become any of those modes by applications of the associativity law for composition.

Cartesian categories are closely related to arrows [Hughes 1998], but the latter's inclusion of arr (with roughly the the same signature as the $c c c$ pseudo-function of Section 5) precludes instances for many cartesian categories.

\section{FUTURE WORK}

There are several possible improvements to the scheme described in this paper:

- The categorical classes have fixed versions of product, unit, exponential, coproduct, etc. We can gain much useful flexibility by replacing these fixed type constructions with associated types [Chakravarty et al. 2005a,b]. For instance, linear maps and polynomials have a "direct sum" coproduct whose representation is the cartesian product rather than a sum (tagged union). The types managed by the compiler plugin become somewhat more complicated, but it seems quite doable, and work is in progress.

- The types involved in categorical operations are restricted to having kind $*$. The only reason for this restriction is the fixed versions of product, unit, etc. Once those types are generalized,
they can easily become poly-kinded [Hinze 2004; Yorgey et al. 2012]. For instance, the functor versions of linear map representation and operations in Appendix A form a cartesian category, using functor product as the associated categorical product. A related example is the category of natural transformations.

- Implemented as described above, compilation to categories is costly for large computations, with a great deal of inlining, simplification, translation to CCC form, and conversion to alternative categories. When a top-level definition is used more than once, this processing occurs redundantly. Within a given compilation run, perhaps some sort of memoization can be done to reuse work, but the same issue exists between successive compilations and across many modules. An earlier implementation of compiling to categories applied an effective and fairly simple form of separate compilation. For each top-level binding $f @ v_{1} \ldots @ v_{n}=r h s$ (where $v_{1}, \ldots, v_{n}$ are type variables), the compiler plugin generated a rewrite rule $c c c\left(f @ v_{1} \ldots @ v_{n}\right)=c c c$ rhs. The right-hand side of this rule simplifies to some term $r h s^{\prime}$, often containing residual $c c c$ applications due to polymorphism. Later, when a type instance $c c c\left(f @ \tau_{1} \ldots @ \tau_{n}\right)$ is encountered (for types $\tau_{1}, \ldots, \tau_{n}$, possibly containing other type variables), including in a different module, GHC's simplifier would find and apply the generated rule, substituting $\tau_{1}, \ldots \tau_{n}$ for $v_{1}, \ldots v_{n}$ in $r h s^{\prime}$, and then continue, making further progress with the unfinished transformation. This experiment in separate compilation worked because the earlier plugin supported only a single CCC, namely Graph from Section 7.1. It is not so clear how to adapt this scheme to support multiple categories, including ones not yet defined when a library module is compiled, since translation depends on the instances available for a particular target category.

- In the presence of recursive definitions, repeated inlining and simplification can easily cause the compiler not to terminate. Fortunately, recursion is explicit in GHC Core, so it is easy for a compiler plugin to notice and handle with care. It may thus be possible to translate to a categorical interface for fixed points [Barr 1990; Mulry 1990; Simpson and Plotkin 2000], to then be interpreted in different categories.

Considering the broad applicability of category theory, it seems likely that the applications given in this paper barely skim the surface of the interesting and useful interpretations of functional programs made possible by compiling to categories.

\section{CONCLUSIONS}

The method described in this paper constitutes a new angle on domain-specific embedded languages (DSELs) and provides a compelling alternative to the "deep embedding" technique often used to enable analysis and optimization [Boulton et al. 1993; Elliott et al. 2003; Gibbons and Wu 2014; Gill 2014]. In deep embeddings, a DSEL/library implementation includes a syntactic representation to be manipulated by the library (at its run time), in addition to the host compiler's syntactic representation (GHC Core). Sharing is lost and must be rediscovered by some form of common subexpression elimination (CSE), an awkward and expensive phase to define in a purely functional language like Haskell [Claessen and Sands 1999; Elliott et al. 2003; Gill 2009]. The library implementation generally also includes various optimizations on its syntactic representation for the sake of performance, as well as some form of back-end code generation. In these ways, a deep DSEL implementation replicates much of the work of its host compiler, making such implementations difficult, and rarely as high-quality as a host language compiler such as GHC.

In spite of all the effort required, the programming interface of a deep DSEL still has some shortcomings. Instead of manipulating simple values such as Bool and Int directly, one must operate on some sort of expression type. This fact can be partially hidden by means of overloading, e.g., via
instances of the numeric classes. Some operations, however, have insufficiently flexible types for the required overloading, such as equality and inequalities, as well as if ... then ... else. Additional efforts can hide more of these symptoms, but the cracks still show, and each new coping mechanism leads to increasingly mysterious type errors. Moreover, deep embeddings cannot support definitionby-cases-a commonly used style in functional programming.

The compiling-to-categories technique described in this paper avoids the shortcomings of shallow and deep embeddings. Unlike shallow embeddings, we can get static analysis-even inside of functions-including aggressive optimization. Unlike deep embeddings, one uses Haskell's standard types and notation (directly and without compromise), gets the full power of the host compiler for optimization, and sees the usual type error messages. Moreover, much implementation work is saved. There is no additional syntactic representation to define, and sharing needn't be recovered, since it is never lost. While the compiler plugin implementation is non-trivial, working with internal compiler representations, it is done once per host compiler (e.g., GHC) rather than per DSEL.

The technique described in this paper has been illustrated in terms of Haskell and its compiler $\mathrm{GHC}$, but it could be applied to other languages and compilers as well. A few properties of GHC have been particularly helpful:

- The whole source language reduces to a small core typed $\lambda$-calculus [Sulzmann et al. 2007].

- Optimization mainly comprises transformations on this core language [Peyton Jones 1996].

- The set of such transformations is extensible by a compiler plugin [GHC Team 2016]. (Given the generality of the needed additional transformations, however, one might build them into an existing compiler instead of writing a plugin. Doing so would require coordination with and support from the compiler's implementors, maintainers, and community.)

As for language features, type classes are very useful as a way to package the required interfaces (Category, Cartesian, NumCat, etc) as well as their specific definitions for alternative interpretations/categories. Especially important here is that these interpretations are defined in familiar-looking source code entirely outside of the compiler and plugin, making it easy to add new interpretations, such as those of Section 7, without any knowledge of compiler internals (including the existence or particulars of GHC Core).

\section{A LINEAR MAP DETAILS}

The linear map category in Section 7.4 is based on free vector spaces (or modules or semimodules) over a scalar $s$, as representable functors, i.e., functors isomorphic to $A \rightarrow s$ for some type $A$. For instance, the vector space $\mathbb{R}^{2}$ over $\mathbb{R}$ is represented as a Pair $\mathbb{R}$, where Pair is a uniform pair functor, which is isomorphic to Bool $\rightarrow \mathbb{R}$. This representation enables simple and general definitions of "vector" operations, e.g.,

$$
\begin{aligned}
& \text { scaleV }::(\text { Functor } f, \text { Num } s) \Rightarrow s \rightarrow f s \rightarrow f s \quad \text {-- Scale a vector } \\
& s^{\prime} \text { scaleV }^{\prime} v=(s *) \nless v \\
& \operatorname{addV}::(\operatorname{Zip} f, \text { Num } s) \Rightarrow f s \rightarrow f s \rightarrow f s \quad \text {-- Add vectors } \\
& \text { addV }=\text { zipWith }(+) \\
& \operatorname{dot} V::(\text { Zip } f, \text { Foldable } f, \text { Num } s) \Rightarrow f s \rightarrow f s \rightarrow s \quad \text {-- Dot product } \\
& x^{\prime} \operatorname{dot} V^{\prime} y=\operatorname{sum}(z i p W i t h(*) x y) \\
& \text { zeroV }::(\text { Pointed } f, \text { Num } a) \Rightarrow f a \quad \text {-- Zero vector } \\
& \text { zeroV }=\text { point } 0
\end{aligned}
$$

The Pointed class has one method point :: $s \rightarrow f s$ that fills a structure with a given value. The Zip class provides zipWith :: $(s \rightarrow t \rightarrow u) \rightarrow f s \rightarrow f t \rightarrow f u$ for combining structures element-wise. These operations can be generalized easily and usefully from Num to semirings.

A functor version of linear maps from $a s$ to $b s$ in row-major form (though column-major can work as well):

infixr $1 \rightarrow$

type $(a \rightarrow b) s=b(a s)$

To apply a linear map $a s:: b(a s)$ to a vector $a:: a s$, form the inner product of each row in $a s$ with $a$ :

lapplyL :: (Zip a, Foldable a, Zip $b$, Num $s) \Rightarrow(a \rightarrow b) s \rightarrow a s \rightarrow b s$

lapplyL as $a=\left({ }^{\prime} \operatorname{dot} V^{\prime} a\right) \ll$ as

The identity and composition for this linear map representation are fairly simple:

$$
\begin{aligned}
& i d L::(\text { Diagonal a,Num } s) \Rightarrow(a \rightarrow a) s \\
& i d L=\text { diag } 01 \\
& \text { compL }::(\text { Zip a, Zip b, Pointed a, Foldable b, Functor } c, \text { Num } s) \\
& \quad \Rightarrow(b \rightarrow c) s \rightarrow(a \rightarrow b) s \rightarrow(a \rightarrow c) s \\
& b c \text { 'compL' } a b=(\lambda b \rightarrow \operatorname{sumV}(\text { zipWith scaleV } b a b)) \ll b c
\end{aligned}
$$

The Diagonal class has diag :: $s \rightarrow s \rightarrow f(f s)$, with diag $z$ o having $o$ ("one") on the diagonal and $z$ ("zero") elsewhere. While idL and compL are used in the Category instance in Section 7.4, the following three are used in the Cartesian instance:

![](https://cdn.mathpix.com/cropped/2023_10_29_09ed653e2c9e391c4cbfg-25.jpg?height=301&width=930&top_left_y=1328&top_left_x=171)

Finally, the function linearL that converts from a function (presumably linear) to the functor representation of linear maps:

class $O k L F f \Rightarrow$ HasL $f$ where linearL $:: \forall s g .(N u m s, O k L F g) \Rightarrow(f s \rightarrow g s) \rightarrow(f \rightarrow g) s \quad--$ Law: linearL $\circ$ lapplyL $\equiv$ id instance HasL $U_{1} \quad$ where linearL $h=$ const $U_{1} \$ h U_{1}$ instance HasL Par $_{1}$ where linearL $h=$ Par $_{1} \$ h\left(\right.$ Par $\left._{1} 1\right)$

instance $($ HasL $f$, HasL $g) \Rightarrow$ HasL $(f \times g)$ where linearL $h=$ zipWith $(\mathbf{x})($ linearL $(h \circ(\times$ zeroV $)))($ linearL $(h \circ($ zeroV $\mathbf{x})))$

The constraint used in defining $O k\left(-_{s}\right)$ conjoins required class constraints:

type OkLF $a=($ Foldable a, Pointed a, Zip a, Diagonal a $)$

\section{ACKNOWLEDGMENTS}

Steve Teig first suggested to me the project of compiling Haskell to (dynamically reprogrammable) hardware, which was the main focus of my work at Tabula. I'm grateful to Steve for inspiration, support, and many conversations during the first phase of this work. Anshul Malvi implemented the graph-to-Verilog translation (also at Tabula). The Kansas University Haskell folks, especially Andrew Farmer and Andy Gill, helped me considerably with their HERMIT system [Farmer et al. 2012; Sculthorpe et al. 2013b], on which earlier implementations of the Haskell-to-hardware plugin were built. Simon Peyton Jones suggested structuring the compiler plugin as a "built-in" GHC rewrite rule. Tim Sears has provided helpful discussions, encouragement, and suggestions throughout the project. Finally, my thanks to Target, for supporting continued development.

\section{REFERENCES}

Umut Acar. Self-adjusting computation. PhD thesis, School of Computer Science, Carnegie Mellon University, May 2005. Umut A. Acar, Guy E. Blelloch, Matthias Blume, and Robert Harper. Self-adjusting programming. In ML Workshop, 2005. Steve Awodey. Category theory, volume 49 of Oxford Logic Guides. Oxford University Press, 2006.

Christiaan Baaij and Jan Kuper. Using rewriting to synthesize functional languages to digital circuits. In Trends in Functional Programming, Lecture Notes in Computer Science, pages 17-33, 2014.

John Backus. Can programming be liberated from the von Neumann style? A functional style and its algebra of programs. Communications of the ACM, 21(8):613-641, August 1978.

Michael Barr. Fixed points in cartesian closed categories. Theoretical Computer Science, 70(1):65-72, 1990.

Per Bjesse, Koen Claessen, Mary Sheeran, and Satnam Singh. Lava: hardware design in Haskell. In ICFP, 1998.

Guy E. Blelloch. Prefix Sums and Their Applications. Technical Report CMU-CS-90-190, School of Computer Science, Carnegie Mellon University, November 1990.

Max Bolingbroke. Constraint kinds for GHC. http://blog.omega-prime.co.uk/?p=127, 2011.

Richard Boulton, Andrew Gordon, Mike Gordon, John Harrison, John Herbert, and John Van Tassel. Experience with embedding hardware description languages in HOL. In Proceedings of the IFIP TC10/WG 10.2 International Conference on Theorem Provers in Circuit Design: Theory, Practice and Experience, volume A-10, pages 129-156, 1993.

Yufei Cai, Paolo G. Giarrusso, Tillmann Rendel, and Klaus Ostermann. A theory of changes for higher-order languages: incrementalizing $\lambda$-calculi by static differentiation. In PLDI '14, pages 145-155, 2014.

Magnus Carlsson. Monads for incremental computing. In ICFP, pages 26-35, 2002.

Manuel M. T. Chakravarty, Gabriele Keller, and Simon Peyton Jones. Associated type synonyms. In ICFP, 2005a.

Manuel M. T. Chakravarty, Gabriele Keller, Simon Peyton Jones, and Simon Marlow. Associated types with class. In Principles of Programming Languages, 2005b.

Mark Chu-Carroll. Interpreting lambda calculus using closed cartesian categories. http://goodmath.scientopia.org/2012/03/ 11/interpreting-lambda-calculus-using-closed-cartesian-categories/, March 2012.

Koen Claessen and David Sands. In Asian Computing Science Conference, 1999.

Guy Cousineau, Pierre-Louis Curien, and Michel Mauny. The categorical abstract machine. Science of Computer Programming, 8, 1987.

Pierre-Louis Curien. Categorical combinators. Information and Control, 69(1-3):188-254, 1986.

Leonardo De Moura and Nikolaj Bjørner. Z3: An efficient SMT solver. In Theory and Practice of Software, International Conference on Tools and Algorithms for the Construction and Analysis of Systems, pages 337-340, 2008.

Conal Elliott. Programming graphics processors functionally. In Haskell Workshop, 2004.

Conal Elliott. Beautiful differentiation. In International Conference on Functional Programming, 2009.

Conal Elliott. Generic functional parallel algorithms: Scan and FFT. Proc. ACM Program. Lang., 1(ICFP), September 2017.

Conal Elliott, Sigbjørn Finne, and Oege de Moor. Compiling embedded languages. Fournal of Functional Prog., 13(2), 2003.

Andrew Farmer, Andy Gill, Ed Komp, and Neil Sculthorpe. The HERMIT in the machine: A plugin for the interactive transformation of GHC core language programs. Haskell Symposium, pages 1-12, 2012.

GHC Team. The Glorious Glasgow Haskell compilation system user's guide, version 8.0.1. https://downloads.haskell.org/ $\sim$ ghc/latest/docs/html/users_guide, 2016.

Jeremy Gibbons. Calculating functional programs. In Algebraic and Coalgebraic Methods in the Mathematics of Program Construction, volume 2297 of Lecture Notes in Computer Science. Springer-Verlag, 2002.

Jeremy Gibbons and Nicolas Wu. Folding domain-specific languages: Deep and shallow embeddings. In ICFP, 2014.

Andy Gill. Type-safe observable sharing in Haskell. In Haskell Symposium, pages 117-128, September 2009.

Andy Gill. Domain-specific languages and code synthesis using Haskell. ACM Queue, 12(4), April 2014.

Andy Gill and Graham Hutton. The worker/wrapper transformation. fournal of Functional Prog., pages 227-251, 2009.

Ralf Hinze. Memo functions, polytypically! In Workshop on Generic Programming, pages 17-32, 2000.

Ralf Hinze. Fun with phantom types. In The fun of programming. Palgrave, 2003.

Ralf Hinze. Polytypic values possess polykinded types. In Science of Computer Programming, pages 2-27, June 2004.

John Hughes. The design of a pretty-printing library. In Advanced Functional Programming, pages 53-96, 1995.

John Hughes. Generalising monads to arrows. Science of Computer Programming, 37:67-111, 1998.

John Hughes and Simon Peyton Jones. The pretty package. https://hackage.haskell.org/package/pretty, November 2007. Haskell library.

Geraint Jones and Mary Sheeran. Circuit design in Ruby. Formal methods for VLSI design, 1, 1990

Mark P. Jones. Dictionary-free overloading by partial evaluation. In In ACM SIGPLAN Workshop on Partial Evaluation and Semantics-Based Program Manipulation, pages 107-117, 1994.

Jerzy Karczmarczuk. Functional differentiation of computer programs. In ICFP, pages 195-203, 1998.

Edward Kmett. What constraints entail: Part 1. http://comonad.com/reader/2011/what-constraints-entail-part-1/, 2011.

Joachim Lambek. From $\lambda$-calculus to cartesian closed categories. In J.P. Seldin and J.R. Hindley, editors, To H.B. Curry: Essays on Combinatory Logic, Lambda Calculus, and Formalism. Academic Press, 1980.

Joachim Lambek. Cartesian closed categories and typed lambda-calculi. In Thirteenth Spring School of the LITP on Combinators and Functional Programming Languages, pages 136-175, 1986.

F. William Lawvere and Stephen H. Schanuel. Conceptual Mathematics: A First Introduction to Categories. Cambridge University Press, 2nd edition, 2009.

Daan Leijen and Erik Meijer. Domain specific embedded compilers. In Conference on Domain-Specific Languages, pages 109-122, 1999.

José Pedro Magalhães, Atze Dijkstra, Johan Jeuring, and Andres Löh. A generic deriving mechanism for Haskell. In Haskell Symposium, pages $37-48,2010$.

José Pedro Magalhães et al. GHC.Generics, 2011. URL https://wiki.haskell.org/GHC.Generics. Haskell wiki page.

M. Douglas Mcllroy. Power series, power serious. Journal of Functional Programming, 9(3):325-337, 1999.

R.E. Moore. Interval analysis. Series in automatic computation. Prentice-Hall, 1966.

Philip S. Mulry. Categorical fixed point semantics. Theoretical Computer Science, 70(1):85-97, January 1990.

Simon Peyton Jones and John Launchbury. Unboxed values as first class citizens in a non-strict functional language. In Functional programming languages and computer architecture, pages 636-666, 1991.

Simon Peyton Jones and Simon Marlow. Secrets of the Glasgow Haskell compiler inliner. Fournal of Functional Programming, 12(5), July 2002.

Simon Peyton Jones, Andrew Tolmach, and Tony Hoare. Playing by the rules: Rewriting as a practical optimisation technique in GHC. In Haskell Workshop, pages 203-233, 2001.

Simon L. Peyton Jones. Compiling Haskell by program transformation: A report from the trenches. In European Symposium on Programming, pages 18-44, 1996.

Neil Sculthorpe, Jan Bracker, George Giorgidze, and Andy Gill. The constrained-monad problem. In International Conference on Functional Programming, pages 287-298, 2013a.

Neil Sculthorpe, Andrew Farmer, and Andy Gill. The HERMIT in the tree: Mechanizing program transformations in the GHC core language. In Symposium on Implementation and Application of Functional Languages, pages 86-103, 2013b.

Mary Sheeran. muFP, a language for VLSI design. In Symposium on LISP and Functional Programming, pages 104-112, 1984.

Alex Simpson and Gordon Plotkin. Complete axioms for categorical fixed-point operators. In Logic in Computer Science, pages $30-41,2000$.

Jeffrey Mark Siskind and Barak A. Pearlmutter. Perturbation confusion and referential transparency: Correct functional implementation of forward-mode AD. In Implementation and Application of Functional Languages, pages 1-9, 2005.

Jeffrey Mark Siskind and Barak A. Pearlmutter. Nesting forward-mode AD in a functional framework. Higher Order Symbolic Computation, 21(4):361-376, 2008.

Michael Spivak. Calculus on Manifolds: A Modern Approach to Classical Theorems of Advanced Calculus. HarperCollins Publishers, 1971.

Martin Sulzmann, Manuel M. T. Chakravarty, Simon L. Peyton Jones, and Kevin Donnelly. System F with type equality coercions. In Types In Languages Design And Implementation, pages 53-66, 2007.

Pierre Weis et al. A history of Caml, 2005. URL https://caml.inria.fr/about/history.en.html. Last updated 2005-01-28.

R. E. Wengert. A simple automatic derivative evaluation program. Communications of the ACM, 7(8):463-464, 1964.

Brent A. Yorgey, Stephanie Weirich, Julien Cretin, Simon L. Peyton Jones, Dimitrios Vytiniotis, and José Pedro Magalhães. Giving Haskell a promotion. In Types In Languages Design And Implementation, pages 53-66. ACM, 2012.