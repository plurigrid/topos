\title{
Prophecy Made Simple
}

\author{
Leslie Lamport and Stephan Merz
}

22 June 2020

\begin{abstract}
Prophecy variables were introduced in the paper The Existence of Refinement Mappings by Abadi and Lamport. They were difficult to use in practice. We describe a new kind of prophecy variable that we find much easier to use. We also reformulate ideas from that paper in a more mathematical way.
\end{abstract}

\section*{Contents}
1 Introduction ..... 1
2 Preliminaries ..... 3
2.1 States, Behaviors, and Specifications ..... 3
2.2 State Machines ..... 4
2.3 Internal Variables ..... 5
3 Implementation and Refinement Mappings ..... 6
3.1 Specification $\mathcal{A}$ ..... 6
3.2 Specification $\mathcal{B}$ ..... 6
3.3 Implementation and a Refinement Mapping ..... 7
3.4 Finding the Refinement Mapping ..... 10
3.5 Generalization ..... 11
4 Auxiliary Variables ..... 12
4.1 History Variables ..... 14
4.2 Simple Prophecy Variables ..... 15
4.3 Predicting the Impossible ..... 16
4.4 A Sequence of Prophecies ..... 18
4.5 A Set of Prophecies ..... 20
4.6 Further Generalizations of Prophecy Variables ..... 21
4.7 Stuttering Variables ..... 22
5 Verifying Linearizability ..... 24
6 Prophecy Constants ..... 27
7 The Existence of Refinement Mappings ..... 29
References ..... 30

\section*{1 Introduction}

Refinement mappings are used to verify that one specification implements another. They generalize to systems the concept of abstraction function, introduced by Hoare to define what it means for one input/output relation to implement another [9]. Refinement mappings are a central concept in extending Floyd-Hoare state-based reasoning to concurrent systems. They are crucial to making verification of those systems tractable, whether verification is by rigorous proof or model checking.

The Existence of Refinement Mappings by Abadi and Lamport [2] has become a standard reference for verifying implementation with refinement mappings in state-based formalisms. That paper, henceforth called ER, was mostly a synthesis of work that had been done in the preceding decade or so. It was well known that being able to construct a refinement mapping often requires adding to a specification a history variable that remembers information from previous states. The major new concept ER introduced was prophecy variables that predict future states, which may also be required to define a refinement mapping. ER showed that refinement mappings can always be found by adding history and prophecy variables for specifications satisfying certain conditions.

The prophecy variables defined by ER were elegant, looking like history variables with time running backwards. In practice, they turned out to be difficult to use. Defining the prophecy variable needed to verify implementation was challenging even in simple examples. We were never able to do it for realistic examples. Here, we describe a new kind of prophecy variable that we find easier to understand and to use. It makes simple examples simple and realistic examples not too hard.

We were motivated to take a fresh look at prophecy variables by a relatively recent paper of Abadi [1]. It describes techniques to make ER's prophecy variables easier to use, but we found those techniques hard to understand and prophecy variables still too hard to use. Our experience writing specifications with TLA, which was developed after ER was written, gave us a powerful new way to think about prophecy.

TLA is a linear-time temporal logic. A formula in such a logic is a predicate on sequences of states. In other temporal logics, formulas are built from predicates on states. TLA formulas are built from actions, which are predicates on pairs of states. This makes it easy to write as TLA formulas the state-machine specifications on which ER is based. Earlier temporal logics could also express actions, but not as conveniently as TLA. They therefore did not lead one to think in terms of actions.

Thinking in terms of actions led us quickly to the simple idea of letting the value of a prophecy variable predict which one of a set of actions will be the next one satisfied by a pair of successive states. For example, if each of the actions describes the sending of a different message, the value of the prophecy variable predicts which message is the next one to be sent. It was easy to generalize this idea to a prophecy variable that makes multiple predictions-even infinitely many.

In addition to explaining our new prophecy variables, we recast the concepts from ER in terms of temporal logic formulas. TLA is an obvious logic to use since it was devised for representing state machines, but the concepts should be applicable to any state-based formalism. We assume no prior knowledge of TLA or of ER. For readers who are familiar with ER, we point out the correspondence between our definitions and those of ER.

Sections 2, 3, and 4.1 explain how specifications are written, what it means for one specification to implement another, refinement mappings, and history variables. They correspond to Sections 2,3 , and 5.1 of ER. The rest of Section 4 explains prophecy variables and stuttering variables, which provide part of the functionality of ER's prophecy variables. Section 5 shows how a prophecy variable can be used to verify that a concurrent algorithm implements the specification of a linearizable object [7]. Our method should be useful for verifying linearizable specifications of other systems.

Section 6 shows how a very simple case of our prophecy variables are present in TLA and other temporal logics. Section 7 sketches a proof that the refinement mapping required to verify an implementation can always, in theory, be obtained by adding history and stuttering variables and those preexisting simple prophecy variables. However, our more general prophecy variables are usually more convenient in practice.

Our exposition is as informal as we can make it while trying to be rigorous. $\mathrm{TLA}^{+}$is a complete specification language based on TLA [11]. Most of what we describe here has been explained elsewhere in excruciating detail for $\mathrm{TLA}^{+}$users [12]. It is easy to write our examples in TLA ${ }^{+}$, and their correctness has been checked with the $\mathrm{TLA}^{+}$tools. Since the examples are written somewhat informally here, we cannot be sure that they have no errors.

\section*{2 Preliminaries}

\subsection*{2.1 States, Behaviors, and Specifications}

Following Turing and ER, we model the execution of a discrete system as a sequence of states, which we call a behavior. For mathematical simplicity, we define a state to be an assignment of values to all possible variables. Think of a behavior as representing a history of the entire universe. We specify a system as a predicate on behaviors, which is satisfied by those behaviors that represent a history in which the system executes the way it should. Traditional verification methods consider only behaviors that represent possible executions of a system. We consider all behaviors, where a behavior is any sequence of states, and a state is any assignment of any values to variables.

Only a finite number of variables are relevant to a system; the system's specification allows behaviors in which other variables can have any values. For example, if we represent its display with the variable $h r$, a 12-hour clock that displays the hour is satisfied by behaviors of the form

$$
\begin{equation*}
[h r: 12],[h r: 1],[h r: 2], \ldots \tag{1}
\end{equation*}
$$

where $[h r: i]$ can be any state that assigns the value $i$ to $h r$. We call each pair of successive states in a behavior a step of the behavior. A state of ER corresponds to an assignment of values to only the variables of the specification.

Common sense dictates that a specification of an hour clock should not say that the clock has no alarm, or no radio, or no display showing minutes. However, between any two steps that change the value of $h r$, a behavior representing a universe in which our hour clock also displays minutes must contain 59 steps in which the minute display changes and the value of $h r$ remains the same. Therefore, in addition to allowing behaviors of the form (1), a specification of an hour clock must allow steps in which the value of $h r$ does not change.

We define a stuttering step of a specification to be one in which both states assign the same values to the specification's variables. Two behaviors are said to be stuttering-equivalent for a specification iff (if and only if) they both have the same sequence of non-stuttering steps. We often don't mention the specification when it is clear from context. We write only specifications that are stuttering-insensitive, meaning that if two behaviors are stuttering-equivalent, then one satisfies the specification iff the other does. All behaviors are infinite. An execution in which a system stops is
represented by a behavior ending in an infinite sequence of stuttering steps of its specification. (The rest of the universe needn't also stop.)

An event $e$ in an event-based formalism corresponds to a step that satisfies some predicate $E$ on pairs of states. If the events are generated by transitions in an underlying state machine, then transitions that produce no event correspond to stuttering steps. In a purely event-based formalism, special "nothing happened" events correspond to stuttering steps.

Writing stuttering-insensitive specifications allows a simple definition of implementation (also called refinement). We say that a specification $\mathcal{S}_{1}$ implements a specification $\mathcal{S}_{2}$ iff every behavior satisfying $\mathcal{S}_{1}$ also satisfies $\mathcal{S}_{2}$. When predicates on behaviors are formulas in a temporal logic, $\mathcal{S}_{1}$ implements $\mathcal{S}_{2}$ means that the formula $\mathcal{S}_{1} \Rightarrow \mathcal{S}_{2}$ is valid (satisfied by all behaviors).

\subsection*{2.2 State Machines}

Following Turing, ER, and common programming languages, we write our specifications in terms of state machines. A state machine is specified with two formulas: a predicate Init on states that describes the possible initial states and a predicate Next on pairs of states that describes how the state can change. We call a predicate $A$ on pairs of states an action, and we call a step satisfying $A$ an $A$ step. Let $\mathbf{x}$ be the list $x_{1}, \ldots, x_{n}$ of all variables of the specification, and let $\mathrm{UC}\langle\mathbf{x}\rangle$ be the action satisfied only by stuttering steps-that is, steps leaving the variables $\mathbf{x}$ unchanged. The state machine specified by Init and Next is satisfied by a behavior $s_{1}, s_{2}, \ldots$ iff

SM1. $s_{1}$ satisfies Init, and

SM2. For all $i$, the step $s_{i}, s_{i+1}$ satisfies Next $\vee \mathrm{UC}\langle\mathbf{x}\rangle$.

The disjunct UC $\langle\mathbf{x}\rangle$ in SM2 ensures that the specification is stutteringinsensitive. The predicate on behaviors described by SM1 and SM2 is written in TLA as this formula:

$$
\text { (2) Init } \wedge \square[N e x t]_{\langle\mathbf{x}\rangle}
$$

In TLA, an action is written as an ordinary mathematical formula that may contain primed and unprimed variables. Unprimed variables refer to the values of the variables in the first state of a pair of states, and primed variables refer to their values in the second state. (An action with no primed variables is a predicate on states.) Thus, UC $\langle\mathbf{x}\rangle$ equals $\left(x_{1}^{\prime}=x_{1}\right) \wedge \ldots \wedge\left(x_{n}^{\prime}=x_{n}\right)$. Our hour-clock specification can be written in TLA as

$$
\left.(h r=12) \wedge \square\left[h r^{\prime}=\text { if } h r=12 \text { then } 1 \text { else } h r+1\right)\right]_{\langle h r\rangle}
$$

This specification allows behaviors in which, at some point, the values of the variables $\mathbf{x}$ never again change - that is, behaviors in which the clock halts. Allowing halting is a feature, not a problem. Formula (2) expresses a safety property. If we want the system also to satisfy a liveness property ${ }^{1} L$, we specify it as

\section*{(3) Init $\wedge \square[N e x t]_{\langle\mathbf{x}\rangle} \wedge L$}

Letting $L$ be the TLA formula $\mathrm{WF}_{\langle h r\rangle}(N e x t)$ makes (3) assert that the state machine never halts in a state in which a non-stuttering step is possible. For the hour clock, this implies that the clock never stops.

Safety and liveness properties are verified differently, so it is best to keep them separate in a specification. We will be concerned only with the safety part, so we don't care how $L$ is written. We don't even require $L$ to be a liveness property. Following ER, we call it a supplementary property.

\subsection*{2.3 Internal Variables}

Specifying a system with a state machine often requires the use of variables that do not represent the actual state of the system, but serve to describe how that state changes. We call the variables describing the system's state external variables, and we call the additional variables internal variables. In our specifications, we want to hide the internal variables, leaving only the external variables visible.

In a linear-time temporal logic, we hide a variable $y$ in a formula $\mathcal{F}$ with the temporal existential quantifier $\exists$. The approximate definition is that $\exists y: \mathcal{F}$ is true of a behavior $\sigma$ iff there exist assignments of values to $y$ in the states of $\sigma$ (a separate assignment for each state of $\sigma$ ) that make the resulting behavior satisfy $\mathcal{F}$. This definition is wrong because it doesn't ensure that that $\exists y: \mathcal{F}$ is stuttering-insensitive. The correct definition is that $\sigma$ satisfies $\exists y: \mathcal{F}$ iff there is a behavior $\tau$ stuttering-equivalent for $\mathcal{F}$ to $\sigma$ and assignments of values to $y$ that makes $\tau$ satisfy $\mathcal{F}$. For a list $\mathbf{y}$ of variables $y_{1}, \ldots y_{m}$, we define $\boldsymbol{\exists} \mathbf{y}: \mathcal{F}$ to equal $\boldsymbol{\exists} y_{1}: \ldots \boldsymbol{\exists} y_{m}: \mathcal{F}$.

We generalize the form (3) of a specification $\mathcal{S}$ to $\exists \mathrm{y}: \mathcal{I S}$, where

$$
\begin{equation*}
\mathcal{I S} \triangleq \text { Init } \wedge \square[N e x t]_{\langle\mathbf{x}, \mathbf{y}\rangle} \wedge L \tag{4}
\end{equation*}
$$

and $\mathbf{x}$ and $\mathbf{y}$ are lists of variables that may appear in Init, Next, and $L$. The external variables $\mathbf{x}$ are assumed to be different from the internal variables $\mathbf{y}$. We call $\mathcal{I} \mathcal{S}$ the internal specification of $\mathcal{S}$.
\footnotetext{
${ }^{1}$ The definitions of safety and liveness can be found elsewhere [4]; they are not needed here.
}

\section*{3 Implementation and Refinement Mappings}

We explain refinement mappings with an example consisting of a specification $\mathcal{A}$, a specification $\mathcal{B}$ that implements $\mathcal{A}$, and a refinement mapping that can be used to verify $\mathcal{B} \Rightarrow \mathcal{A}$.

\subsection*{3.1 Specification $\mathcal{A}$}

Specification $\mathcal{A}$ describes a system that receives as input a sequence of integers and, after receipt of each integer, outputs the average of all the integers received thus far. Receipt of an integer $i$ is represented by the value of the variable in changing from the special value rdy to $i$, where we assume rdy is not a number. Producing an output is represented by the value of in changing back to rdy and the value of out being set to the output. Initially, in $=\mathrm{rdy}$ and out $=0$. Here is the beginning of a behavior that satisfies $\mathcal{A}$ :

$$
\begin{align*}
& {[\text { in : rdy, out }: 0],[\text { in : } 3, \text { out }: 0],[\text { in : rdy, out }: 3],}  \tag{5}\\
& {[\text { in : }-2, \text { out }: 3],\left[\text { in : rdy, out }: \frac{1}{2}\right], \ldots}
\end{align*}
$$

$\mathcal{A}$ is defined to equal $\exists$ sum, num $: \mathcal{I A}$, where num is the number of outputs that have been produced and sum is the sum of the inputs that produced the most recent output. Here is a behavior satisfying $\mathcal{I} \mathcal{A}$ which shows that behavior (5) satisfies $\mathcal{A}$ :

(6) [in : rdy, out : 0, num : 0, sum : 0],

[in : 3, out : 0 num : 0 , sum : 0],

[in : rdy, out : 3, num : 1, sum : 3],

[in : -2 , out : 3 , num : 1, sum : 3],

[in : rdy, out : $\frac{1}{2}$, num : 2 , sum : 1$], \ldots$

The complete specification $\mathcal{A}$ is defined in Figure 1, where Int is the set of all integers. A step satisfies the action Next $_{\mathcal{A}}$ iff it is an Input $_{\mathcal{A}}$ step or an Output $_{\mathcal{A}}$ step. An Input $_{\mathcal{A}}$ step represents the receipt of an input and an Output $_{\mathcal{A}}$ step represents the production of an output.

\subsection*{3.2 Specification $\mathcal{B}$}

Specification $\mathcal{B}$, is a different way of writing the same specification as $\mathcal{A}$. Instead of variables that record the number of inputs and their sum, the internal specification $\mathcal{I B}$ has a single internal variable seq that records the entire sequence of inputs received so far. Specification $\mathcal{B}$ has the same form

$$
\begin{aligned}
& \mathcal{A} \quad \triangleq \exists \text { num, sum }: \mathcal{I A} \\
& \mathcal{I A} \quad \triangleq \text { Init }_{\mathcal{A}} \wedge \square\left[N e x \mathcal{A}_{\mathcal{A}}\right]_{\langle\text {in, out }, \text { num }, \text { sum }\rangle} \\
& \text { Init }_{\mathcal{A}} \triangleq(\text { in }=\mathrm{rdy}) \wedge(o u t=n u m=\operatorname{sum}=0) \\
& \text { Next }_{\mathcal{A}} \triangleq \text { Input }_{\mathcal{A}} \vee \text { Output }_{\mathcal{A}} \\
& \operatorname{Input}_{\mathcal{A}} \triangleq(\text { in }=\mathrm{rdy}) \wedge\left(\text { in }{ }^{\prime} \in \text { Int }\right) \wedge \mathrm{UC}\langle\text { out, num, sum }\rangle \\
& \text { Output }_{\mathcal{A}} \triangleq \quad(\text { in } \neq \mathrm{rdy}) \wedge\left(\text { in }{ }^{\prime}=\mathrm{rdy}\right) \\
& \wedge\left(s u m^{\prime}=s u m+i n\right) \wedge\left(n u m^{\prime}=n u m+1\right) \\
& \wedge\left(\text { out }^{\prime}=\text { sum }^{\prime} / \text { num }^{\prime}\right)
\end{aligned}
$$

Figure 1: The definition of specification $\mathcal{A}$.

$$
\begin{aligned}
& \mathcal{B} \quad \triangleq \exists s e q: \mathcal{I B} \\
& \mathcal{I B} \quad \triangleq \operatorname{Init}_{\mathcal{B}} \wedge \square\left[\text { Next }_{\mathcal{B}}\right]_{\langle\text {in,out }, \text { seq }\rangle} \\
& \text { Init }_{\mathcal{B}} \triangleq(\text { in }=\mathrm{rdy}) \wedge(\text { out }=0) \wedge(\operatorname{seq}=\langle\rangle) \\
& \text { Next }_{\mathcal{B}} \triangleq \text { Input }_{\mathcal{B}} \vee \text { Output }_{\mathcal{B}} \\
& \text { Input }_{\mathcal{B}} \triangleq(\text { in }=\mathrm{rdy}) \wedge\left(i n^{\prime} \in \text { Int }\right) \\
& \wedge\left(s e q^{\prime}=\operatorname{Append}\left(\operatorname{seq}, i n^{\prime}\right)\right) \wedge\left(\text { out }^{\prime}=\text { out }\right) \\
& \text { Output }_{\mathcal{B}} \triangleq \quad(\text { in } \neq \text { rdy }) \wedge\left(\text { in }{ }^{\prime}=\text { rdy }\right) \\
& \wedge\left(o u t^{\prime}=\operatorname{Sum}(\operatorname{seq}) / \operatorname{Len}(s e q)\right) \wedge\left(s e q^{\prime}=s e q\right)
\end{aligned}
$$

Figure 2: The definition of specification $\mathcal{B}$.

as $\mathcal{A}$ except its action Input $_{\mathcal{B}}$ appends the value being input to seq, and its Output $\mathcal{B}$ action outputs the average of the numbers in the sequence seq.

To write $\mathcal{B}$, we introduce some notation for sequences. We enclose sequences in angle brackets $\langle$ and $\rangle$, so \langle\rangle is the empty sequence. We define $\operatorname{Len}(s q)$ to equal the length of sequence $s q$ and Append $(s q, e)$ to be the sequence obtained by appending $e$ to the end of sequence $s q$, so $\operatorname{Len}(\langle 3,1\rangle)$ equals 2 and Append $(\langle 3,1\rangle, 42)$ equals $\langle 3,1,42\rangle$. We also define $\operatorname{Sum}(s q)$ to be the sum of the elements of $s q$, so Sum $(\langle 3,1,42\rangle$ equals 46 (which equals $3+1+42)$ and Sum $(\langle\rangle)$ equals 0 . Specification $\mathcal{B}$ is defined in Figure 2.

\subsection*{3.3 Implementation and a Refinement Mapping}

To show $\mathcal{B} \Rightarrow \mathcal{A}$, we must show $(\mathcal{\exists} s e q: \mathcal{I B}) \Rightarrow \mathcal{A}$. The quantifier $\boldsymbol{\exists}$ obeys the same rules as the quantifier $\exists$ of ordinary math. By those rules, since
seq is not a variable of $\mathcal{A}$, to show $(\exists$ seq $: \mathcal{I B}) \Rightarrow \mathcal{A}$ it suffices to show $\mathcal{I B} \Rightarrow \mathcal{A}$. (This is also easy to see from the definition of $\exists$.)

For any state $s$, let $s \llbracket n u m \leftarrow u$, sum $\leftarrow v \rrbracket$ be the state that is the same as $s$ except that it assigns the value $u$ to variable num and the value $v$ to variable sum. Since $\mathcal{A}$ equals $\exists$ num, sum: $\mathcal{I} \mathcal{A}$, to show $\mathcal{I B} \Rightarrow \mathcal{A}$, it suffices to assume that a behavior $s_{1}, s_{2}, \ldots$ satisfies $\mathcal{I B}$ and find sequences of values $\overline{\text { uum }}_{1}, \overline{\text { num }}_{2}, \ldots$ and $\overline{\text { sum }}_{1}, \overline{\text { sum }}_{2}, \ldots$ such that the behavior

![](https://cdn.mathpix.com/cropped/2024_06_18_297ef490dcef3ba9183fg-10.jpg?height=58&width=1180&top_left_y=795&top_left_x=516)

satisfies $\mathcal{I} \mathcal{A}$. We are free to let each $\overline{n u m}_{i}$ and $\overline{s u m}_{i}$ depend on the entire behavior $s_{1}, s_{2}, \ldots$ However, we are going to make them depend only on the state $s_{i}$. We do that by finding expressions $\overline{n u m}$ and $\overline{s u m}$, containing only the variables in, out, and seq of $\mathcal{I B}$, and let $\overline{n u m}_{i}$ and $\overline{s u m}_{i}$ be the values of these expressions in state $s_{i}$.

More precisely, if $u$ and $v$ are expressions (formulas that need not be Boolean-valued), then let $s \llbracket n u m \leftarrow u$, sum $\leftarrow v]$ be the state that is the same as $s$ except that it assigns to the variables num and sum the values of $u$ and $v$ in state $s$, respectively. To show $\mathcal{I B} \Rightarrow \boldsymbol{\exists}$ num, sum: $\mathcal{I A}$, it suffices to find expressions $\overline{n u m}$ and $\overline{s u m}$, containing only the (unprimed) variables of $\mathcal{I H}$, such that:

RM. If a behavior $s_{1}, s_{2}, \ldots$ satisfies $\mathcal{I B}$, then the behavior

![](https://cdn.mathpix.com/cropped/2024_06_18_297ef490dcef3ba9183fg-10.jpg?height=57&width=1101&top_left_y=1457&top_left_x=588)
satisfies $\mathcal{I} \mathcal{A}$.

From conditions SM1 and SM2 of Section 2.2 and the definitions of $\mathcal{I} \mathcal{A}$ and $\mathcal{I B}$, we see that RM is implied by:

RM1. For any state $s$, if $s$ satisfies Init $_{\mathcal{B}}$, then $s[\llbracket$ num $\leftarrow \overline{n u m}$, sum $\leftarrow$ $\overline{s u m}]$ satisfies Init $_{\mathcal{A}}$.

RM2. For any states $s$ and $t$, if step $s, t$ satisfies $N e x t_{\mathcal{B}} \vee \mathrm{UC}\langle$ in, out, seq $\rangle$, then the pair of states

$$
s \llbracket[n u m \leftarrow \overline{n u m}, \text { sum } \leftarrow \overline{\text { sum }} \rrbracket, t[n n u m \leftarrow \overline{n u m}, \text { sum } \leftarrow \overline{s u m} \rrbracket]
$$

satisfies $N e x t_{\mathcal{A}} \vee \mathrm{UC}\langle$ in, out, num, sum $\rangle$.

Because $\overline{n u m}$ and $\overline{\text { sum }}$ contain only the variables in, out, and seq of $\mathcal{I B}$, if the step $s, t$ satisfies UC $\langle$ in, out, seq $\rangle$, then the step

$$
s \llbracket n u m \leftarrow \overline{n u m}, \text { sum } \leftarrow \overline{\text { sum }} \rrbracket, t[n u m \leftarrow \overline{n u m}, \text { sum } \leftarrow \overline{\text { sum }}]
$$

satisfies UC $\langle$ in, out, num, sum $\rangle$. Therefore, RM2 is automatically satisfied if $s, t$ is a UC $\langle$ in, out, seq $\rangle$ step. This means we can simplify RM2 to:

RM2. For any states $s$ and $t$, if the step $s, t$ satisfies $N e x t_{\mathcal{B}}$, then the pair of states

$$
\begin{aligned}
& s \llbracket n u m \leftarrow \overline{n u m}, \text { sum } \leftarrow \overline{s u m} \rrbracket, t[\llbracket n u m \leftarrow \overline{n u m}, \text { sum } \leftarrow \overline{s u m}]
\end{aligned}
$$

![](https://cdn.mathpix.com/cropped/2024_06_18_297ef490dcef3ba9183fg-11.jpg?height=51&width=696&top_left_y=625&top_left_x=579)

Let's consider RM1. Since $\operatorname{Init}_{\mathcal{A}}$ is the formula

$$
(\text { in }=\mathrm{rdy}) \wedge(\text { out }=n u m=s u m=0)
$$

![](https://cdn.mathpix.com/cropped/2024_06_18_297ef490dcef3ba9183fg-11.jpg?height=57&width=1196&top_left_y=844&top_left_x=432)

$$
\begin{equation*}
(\text { in }=\mathrm{rdy}) \wedge(o u t=\overline{n u m}=\overline{s u m}=0) \tag{7}
\end{equation*}
$$

This is the formula obtained by substituting the expression $\overline{\text { num }}$ for the vari-

![](https://cdn.mathpix.com/cropped/2024_06_18_297ef490dcef3ba9183fg-11.jpg?height=52&width=1255&top_left_y=1042&top_left_x=430)
Let's call that formula (Init $\mathcal{A}_{\mathcal{A}}$ with $n u m \leftarrow \overline{n u m}$, sum $\leftarrow \overline{\text { sum }}$ ). RM1 asserts that every state satisfying $\operatorname{Init}_{\mathcal{B}}$ satisfies (7). Therefore, it is equivalent to

$$
\text { RM1. Init } \mathcal{B} \Rightarrow\left(\text { Init }_{\mathcal{A}} \text { with } n u m \leftarrow \overline{n u m}, \text { sum } \leftarrow \overline{\text { sum }}\right)
$$

As a sanity check on this condition, observe that because the variables in expressions $\overline{\text { num }}$ and $\overline{\text { sum }}$ are variables of Init $_{\mathcal{B}}$, and the other variables in and out of $\operatorname{Init}_{\mathcal{A}}$ are also variables of $\operatorname{Init}_{\mathcal{B}}$, the formula (Init $\mathcal{A}_{\mathcal{A}}$ with ...) in RM1 contains only variables in Init $_{\mathcal{B}}$. Therefore, RM1 asserts that Init $_{\mathcal{B}}$ implies a formula containing only variables of Init $_{\mathcal{B}}$.

Applying the same reasoning to RM2, and performing the substitution in the expression UC $\langle$ in, out, num, sum $\rangle$, we see that RM2 is equivalent to

$$
\begin{aligned}
& \text { RM2. Next }{ }_{\mathcal{B}} \Rightarrow
\end{aligned}
$$

![](https://cdn.mathpix.com/cropped/2024_06_18_297ef490dcef3ba9183fg-11.jpg?height=60&width=1106&top_left_y=1737&top_left_x=602)

Substituting an expression like $\overline{n u m}$ for num in $\operatorname{Next}_{\mathcal{A}}$ means replacing $n u m^{\prime}$ by $\overline{n u m}{ }^{\prime}$. The expression $\overline{n u m}{ }^{\prime}$ represents the value of $\overline{n u m}$ in the second state of a step. It is the expression obtained by priming all the variables in $\overline{n u m}$.

The substitutions num $\leftarrow \overline{\text { num }}$, sum $\leftarrow \overline{\text { sum }}$ of expressions containing variables of $\mathcal{I B}$ for the internal variables of $\mathcal{I A}$ is what we call a refinement mapping. In ER, a state of $\mathcal{I A}$ or $\mathcal{I B}$ would be an assignment of values to that specification's variables. The mapping from states of $\mathcal{I B}$ to states of $\mathcal{I} \mathcal{A}$

![](https://cdn.mathpix.com/cropped/2024_06_18_297ef490dcef3ba9183fg-11.jpg?height=54&width=1255&top_left_y=2190&top_left_x=430)
mapping. Thinking of refinement mappings in terms of formulas instead of states is better when writing proofs, since proofs are written with formulas.

\subsection*{3.4 Finding the Refinement Mapping}

Let's now find the expressions $\overline{\text { num }}$ and $\overline{\text { sum }}$ for the actual formulas defined in Figures 1 and 2 that satisfy RM1 and RM2. RM2 asserts that a step satisfying $N e x t_{\mathcal{B}}$ simulates a step satisfying $N e x t_{\mathcal{A}}$ or a stuttering step, where the values of num and sum are simulated by the values of $\overline{\text { num }}$ and $\overline{s u m}$. In this simulation, the variables in and out are simulated by themselves. This implies that an Input $_{\mathcal{B}}$ step must simulate an Input $_{\mathcal{A}}$ step, leaving $\overline{n u m}$ and

![](https://cdn.mathpix.com/cropped/2024_06_18_297ef490dcef3ba9183fg-12.jpg?height=57&width=1258&top_left_y=801&top_left_x=431)
we should verify RM2 by verifying these two formula:

(8) Input $_{\mathcal{B}} \Rightarrow$ (Input $\mathcal{A}_{\mathcal{A}}$ with num $\leftarrow \overline{n u m}$, sum $\leftarrow \overline{\text { sum }}$ )

(9) $\quad$ Output $\mathcal{B}_{\mathcal{B}} \Rightarrow\left(\right.$ Output $_{\mathcal{A}}$ with num $\leftarrow \overline{\text { num }}$, sum $\left.\leftarrow \overline{\text { sum }}\right)$

It's pretty clear that, after an output step, $\overline{n u m}$ should equal Len(seq) and $\overline{\text { sum }}$ should equal Sum $($ seq $)$. Since in equals rdy after an Output ${ }_{\mathcal{A}}$ step, this leads to the following definitions:

$$
\begin{aligned}
& \overline{n u m} \triangleq \text { if in }=r d y \text { then } \operatorname{Len}(\text { seq }) \text { else } \operatorname{Len}(\operatorname{Front}(\text { seq })) \\
& \overline{\operatorname{sum}} \triangleq \text { if } \text { in }=r d y \text { then } \operatorname{Sum}(\text { seq) } \text { else } \operatorname{Sum}(\operatorname{Front}(\text { seq }))
\end{aligned}
$$

where $\operatorname{Front}(s q)$ is defined to equal the sequence consisting of the first $\operatorname{Len}(s q)-1$ elements of sequence $s q$, and $\operatorname{Front}(\langle\rangle)$ is defined to equal \langle\rangle .

It's easy to verify RM1, which asserts

$$
\begin{aligned}
& (\text { in }=\text { rdy }) \wedge(\text { out }=0) \wedge(\text { seq }=\langle\rangle) \Rightarrow \\
& \quad(\text { in }=\text { rdy }) \wedge(\text { out }=\overline{n u m}=\overline{s u m}=0)
\end{aligned}
$$

It's not hard to verify (8), since $\operatorname{Input}_{\mathcal{B}}$ implies Front $(s e q$ ' $)=$ seq. Many readers will also be able to convince themselves that (9) is valid. Those readers are wrong. For example, there's no way to show that (9) is true if $i n^{\prime}=42$ and $s e q=\langle$ rdy $\rangle$, since we don't know what Sum(〈rdy $\left.\rangle\right)$ and Sum( $\langle$ rdy, 42$\rangle$ ) equal.

We expect many readers will object that seq can't equal $\langle$ rdy $\rangle$. But, why can't it? Nothing in (9) or Figure 2 asserts that seq doesn't equal 〈rdy $\rangle$. What is true is that the value of seq can't equal $\langle$ rdy $\rangle$ in any state of any behavior satisfying $\mathcal{I B}$. To show implementation, we don't have to show that RM2 is true for all pairs of states. It need only be true for reachable states, which are states that can occur in a behavior satisfying $\mathcal{I B}$. In fact, every reachable state of $\mathcal{I B}$ satisfies the following formula Inv:

$$
\begin{aligned}
\operatorname{Inv} \triangleq & (\text { in } \in \operatorname{Int} \cup\{\mathrm{rdy}\}) \wedge(\text { out } \in \operatorname{Int}) \wedge\left(\text { seq } \in \operatorname{Int}^{*}\right) \wedge \\
& ((\text { in } \neq r d y) \Rightarrow(\operatorname{seq} \neq\langle\rangle) \wedge(\text { in }=\operatorname{Last}(s e q)))
\end{aligned}
$$
where Int* is the set of finite sequences of integers and Last(sq) denotes the last element of a non-empty sequence $s q$. A formula that is true in every reachable state of a specification is called an invariant of the specification. In temporal logic, the formula $\square I n v$ is satisfied by a behavior iff every state of the behavior satisfies Inv. Therefore, the assertion that Inv is an invariant of $\mathcal{I B}$ is expressed by $\mathcal{I B} \Rightarrow \square I n v$.

Since Inv contains only variables of $\mathcal{I B}$, its value is left unchanged by steps that leave those variables unchanged. To show that $I n v$ is an invariant of $\mathcal{I B}$, by induction it suffices to show:

$$
\begin{aligned}
& \text { I1. } \text { Init }_{\mathcal{B}} \Rightarrow \text { Inv } \\
& \text { I2. } \operatorname{Inv} \wedge \operatorname{Next}_{\mathcal{B}} \Rightarrow \text { Inv }^{\prime}
\end{aligned}
$$

(Remember that $I n v^{\prime}$ is the formula obtained by priming all the variables in Inv.) Because Inv is an invariant of $\mathcal{I B}$, instead of showing RM2 we need only show:

$$
\begin{align*}
& \operatorname{Inv} \wedge \operatorname{Inv}^{\prime} \wedge \text { Next }_{\mathcal{B}} \Rightarrow  \tag{10}\\
& \left(\text { Next }_{\mathcal{A}} \text { with } \text { num } \leftarrow \overline{n u m}, \text { sum } \leftarrow \overline{\text { sum }}\right) \vee \mathrm{UC}\langle\text { in, out }, \overline{n u m}, \overline{\text { sum }}\rangle
\end{align*}
$$

We leave this to the reader.

Proving invariance by proving I1 and I2 underlies all state-based methods for proving correctness, including the Floyd-Hoare $[6,10]$ and OwickiGries [13] methods. ER avoids the explicit use of invariants by restricting a specification's set of states to ones that satisfy the needed invariant.

\subsection*{3.5 Generalization}

We now generalize what we have done in this section to arbitrary specifications $\mathcal{S}_{1}$ and $\mathcal{S}_{2}$, with external variables $\mathbf{x}$, defined by

$$
\begin{align*}
& \mathcal{I S _ { 1 }} \triangleq \operatorname{Init}_{1} \wedge \square\left[N e x t_{1}\right]_{\langle\mathbf{x}, \mathbf{y}\rangle} \wedge L_{1}  \tag{11}\\
& \mathcal{I S _ { 2 }} \triangleq \text { Init }_{2} \wedge \square[\text { Next }]_{\langle\mathbf{x}, \mathbf{z}\rangle} \wedge L_{2}
\end{align*}
$$

![](https://cdn.mathpix.com/cropped/2024_06_18_297ef490dcef3ba9183fg-13.jpg?height=62&width=689&top_left_y=1953&top_left_x=520)

where the lists $\mathbf{y}$ and $\mathbf{z}$ of internal variables of $\mathcal{S}_{1}$ and $\mathcal{S}_{2}$ contain no variables of $\mathbf{x}$. To verify $\mathcal{S}_{1} \Rightarrow \mathcal{S}_{2}$, we first define a state predicate Inv, with variables in $\mathbf{x}$ and $\mathbf{y}$, and show it is an invariant of $\mathcal{I} \mathcal{S}_{1}$ by showing:

$$
\begin{aligned}
& \text { I1. } \text { Init }_{1} \Rightarrow I n v \\
& \text { I2. } \operatorname{Inv} \wedge \text { Next }_{1} \Rightarrow I n v^{\prime}
\end{aligned}
$$

Then, if $\mathbf{z}$ is the list $z_{1}, \ldots, z_{m}$ of variables, we find expressions $\overline{z_{1}}, \ldots$, $\overline{z_{m}}$ with variables $\mathbf{x}$ and $\mathbf{y}$ and show the following; where $\mathbf{z} \leftarrow \overline{\mathbf{z}}$ means $z_{1} \leftarrow \overline{z_{1}}, \ldots, z_{m} \leftarrow \overline{z_{m}}:$

RM1. Init ${ }_{1} \Rightarrow\left(\right.$ Init $_{2}$ with $\left.\mathbf{z} \leftarrow \overline{\mathbf{z}}\right)$

RM2. Inv $\wedge I n v^{\prime} \wedge$ Next $_{1} \Rightarrow\left(\left(\right.\right.$ Next $_{2}$ with $\left.\left.\mathbf{z} \leftarrow \overline{\mathbf{z}}\right) \vee \mathrm{UC}\langle\mathbf{x}, \overline{\mathbf{z}}\rangle\right)$

RM3. Init ${ }_{1} \wedge \square\left[N e x t_{1}\right]_{\langle\mathbf{x}, \mathbf{y}\rangle} \wedge L_{1} \Rightarrow\left(L_{2}\right.$ with $\left.\mathbf{z} \leftarrow \overline{\mathbf{z}}\right)$

When RM1-RM3 hold, we say that $\mathcal{I} \mathcal{S}_{1}$ implements $\mathcal{I} \mathcal{S}_{2}$ under the refinement mapping $\mathbf{z} \leftarrow \overline{\mathbf{z}}$.

\section*{4 Auxiliary Variables}

Sometimes, one specification implements another, but there does not exist a refinement mapping that shows it. For example, while we showed above that $\mathcal{B}$ implies $\mathcal{A}$, the two specifications are actually equivalent. However, $\mathcal{I A}$ does not implement $\mathcal{I B}$ under any refinement mapping because there is no way to define $\overline{\text { seq }}$ in terms of the variables of $\mathcal{A}$.

To show $\mathcal{A} \Rightarrow \mathcal{B}$, we construct a specification $\mathcal{A}^{a}$ from $\mathcal{A}$ containing an additional variable $a$ such that $\mathcal{A}$ is equivalent to $\exists a: \mathcal{A}^{a}$, and we show $\mathcal{A}^{a} \Rightarrow \mathcal{B}$. This shows $\mathcal{A} \Rightarrow \mathcal{B}$, assuming that $a$ is not an (external) variable of $\mathcal{B}$. Constructing $\mathcal{A}^{a}$ such that $\exists a: \mathcal{A}^{a}$ is equivalent to $\mathcal{A}$ is called adding the auxiliary variable $a$ to $\mathcal{A}$. We define three kinds of auxiliary variables: history, prophecy, and stuttering variables.

Let specification $\mathcal{S}$ have internal specification $\mathcal{I S}$ defined by (4). We define $\mathcal{S}^{a}$ to equal $\exists \mathrm{y}: \mathcal{I} \mathcal{S}^{a}$ and define

$$
\begin{equation*}
\mathcal{I S}^{a} \triangleq \operatorname{Init}^{a} \wedge \square\left[N e x t^{a}\right]_{\langle\mathbf{x}, \mathbf{y}, a\rangle} \wedge L \tag{12}
\end{equation*}
$$

where Init $^{a}$ and Next ${ }^{a}$ are obtained from Init and Next by adding specifications of the initial value of $a$ and how $a$ changes. To show that $\mathcal{S}^{a}$ is obtained by adding $a$ as an auxiliary variable - that is, $\exists a: \mathcal{S}^{a}$ is equivalent to $\mathcal{S}$-we show that $\exists a: \mathcal{I} \mathcal{S}^{a}$ is equivalent to $\mathcal{I S}$. Since $\mathcal{I} \mathcal{S}^{a}$ and $\mathcal{I S}$ have the same supplementary property $L$, it suffices to show their equivalence with $L$ removed. That is, we only have to show that if we hide the variable $a$, the state machines of $\mathcal{I} \mathcal{S}^{a}$ and $\mathcal{I} \mathcal{S}$ are equivalent. This requires verifying two conditions:

AV1. Any behavior satisfying SM1 and SM2 for $\mathcal{I} \mathcal{S}^{a}$ satisfies them for $\mathcal{I S}$.

AV2. From any behavior $\sigma$ satisfying SM1 and SM2 for $\mathcal{I} \mathcal{S}$, we can obtain a behavior $\sigma^{a}$ satisfying SM1 and SM2 for $\mathcal{I} \mathcal{S}^{a}$ by adding stuttering steps and assigning new values to the variable $a$ in the states of the resulting behavior.

For all our auxiliary variables, Init ${ }^{a}$ is defined by

$$
\begin{equation*}
\text { Init }^{a} \triangleq \text { Init } \wedge J \tag{13}
\end{equation*}
$$

where $J$ is an expression containing the variables $\mathbf{x}, \mathbf{y}$, and $a$. To define Next ${ }^{a}$, we write Next as a disjunction of elementary actions, where we consider existential quantification to be a disjunction. For example, we can consider the elementary actions of

$$
\begin{equation*}
U \vee V \vee \exists i \in \text { Int }: W(i) \tag{14}
\end{equation*}
$$

to be $U, V$, and all $W(i)$ with $i \in I n t$. (We could also consider $U \vee V$ and $\exists i \in$ Int: $W(i)$ to be the elementary actions of (14).) We define Next ${ }^{a}$ by replacing every elementary action $A$ of $N e x t$ with an action $A^{a}$. For history and prophecy variables, $A^{a}$ is defined by letting

$$
\begin{equation*}
A^{a} \triangleq A \wedge B \tag{15}
\end{equation*}
$$

where $B$ is an action containing the variables $\mathbf{x}, \mathbf{y}$, and $a$ (which may appear primed or unprimed), and letting $a$ be left unchanged by stuttering steps of $\mathcal{I S}$. Condition AV1 is implied by (13) and (15). Condition AV2 is implied by:

AX. For any behavior $s_{1}, s_{2}, \ldots$ satisfying SM1 and SM2 for $\mathcal{I S}$, there exists a behavior $s_{1}^{a}, s_{2}^{a}, \ldots$ such that each $s_{i}^{a}$ is the same as $s_{i}$ except for the value it assigns to $a$, and: (1) $s_{1}^{a}$ satisfies Init $^{a}$ and (2) for each elementary action $A$ and each step $s_{i}, s_{i+1}$ that satisfies $A$, the step $s_{i}^{a}, s_{i+1}^{a}$ satisfies $A^{a}$.

We can show that history and prophecy variables satisfy AX. Stuttering variables can be shown to satisfy AV1 and AV2 directly.

Inspired by Abadi [1], we explain prophecy variables in terms of examples in which a specification with an undo action that reverses the effect of some other action implements the same specification without the undo action. However, there is nothing about undo that makes our prophecy variables work especially well. We find them just as easy to use on other kinds of examples.

\subsection*{4.1 History Variables}

We use a history variable $h$ to show $\mathcal{A} \Rightarrow \mathcal{B}$. A history variable stores information from the current and previous states. To be able to find a refinement mapping that shows $\mathcal{I A}^{h} \Rightarrow \exists$ seq $: \mathcal{I B}$, we let $h$ record the sequence of values input thus far. The initial value of $h$ should obviously be the empty sequence, so we define

$$
\text { Init }_{\mathcal{A}}^{h} \triangleq \text { Init }_{\mathcal{A}} \wedge(h=\langle\rangle)
$$

The elementary actions of $\operatorname{Next}_{\mathcal{A}}$ are Input $_{\mathcal{A}}$ and Output $\mathcal{A}_{\mathcal{A}}$. We let Input ${ }_{\mathcal{A}}^{h}$ append the new input value to $h$ and Output $_{\mathcal{A}}$ leave $h$ unchanged:

$$
\begin{aligned}
& \text { Input }_{\mathcal{A}}^{h} \triangleq \text { Input }_{\mathcal{A}} \wedge\left(h^{\prime}=\operatorname{Append}\left(h, \text { in }^{\prime}\right)\right) \\
& \text { Output }_{\mathcal{A}}^{h} \triangleq \text { Output }_{\mathcal{A}} \wedge\left(h^{\prime}=h\right)
\end{aligned}
$$

Finally, we define

$$
\begin{aligned}
& \text { Next }_{\mathcal{A}}^{h} \triangleq \text { Input }_{\mathcal{A}}^{h} \vee \text { Output }_{\mathcal{A}}^{h} \\
& \mathcal{I A}^{h} \triangleq \operatorname{Init}_{\mathcal{A}}^{h} \wedge \square\left[N e x t_{\mathcal{A}}^{h}\right]_{\langle\text {in, out,sum,num }, h\rangle} \\
& \mathcal{A}^{h} \triangleq \boldsymbol{g} \text { sum, num }: \mathcal{I A}^{h}
\end{aligned}
$$

Condition AX is satisfied because, for any behavior $s_{1}, s_{2}, \ldots$ satisfying SM1 and SM2 for $\mathcal{I} \mathcal{A}$, we can inductively define the required states $s_{i}^{h}$ as follows: The value of $h$ in $s_{1}^{h}$ is determined by the condition $h=\langle\rangle$. For each $i$, a nonstuttering step $s_{i}, s_{i+1}$ is a step of one of the two elementary actions, and we let $s_{i+1}^{h}$ assign to $h$ the value of $h^{\prime}$ determined by the $h^{\prime}=\ldots$ condition of that action. For a stuttering step, $h^{\prime}=h$.

![](https://cdn.mathpix.com/cropped/2024_06_18_297ef490dcef3ba9183fg-16.jpg?height=48&width=1195&top_left_y=1689&top_left_x=492)
mapping seq $\leftarrow h$. We must find an invariant Inv of $\mathcal{I} \mathcal{A}^{h}$ and show:

$$
\begin{align*}
& \text { Init }_{\mathcal{A}}^{h} \Rightarrow\left(\text { Init }_{\mathcal{B}} \text { with seq } \leftarrow h\right)  \tag{16}\\
& I n v \wedge \text { Inv }^{\prime} \wedge \text { Input }_{\mathcal{A}}^{h} \Rightarrow\left(\text { Input }_{\mathcal{B}} \text { with seq } \leftarrow h\right) \\
& I n v \wedge \text { Inv }^{\prime} \wedge \text { Output }_{\mathcal{A}}^{h} \Rightarrow\left(\text { Output }_{\mathcal{B}} \text { with } s e q \leftarrow h\right)
\end{align*}
$$

This is a standard exercise in assertional reasoning. Formulas (16) imply RM1 and RM2, which imply $\mathcal{A}^{h} \Rightarrow \mathcal{B}$.

The generalization to an arbitrary internal specification (4) is simple. We define

$$
\text { Init }^{h} \triangleq \text { Init } \wedge(h=f)
$$
where $f$ is an expression that can contain the variables $\mathbf{x}$ and $\mathbf{y}$. For an elementary action $A$ of Next, we define

$$
A^{h} \triangleq A \wedge\left(h^{\prime}=F\right)
$$

where $F$ is an expression that can contain the variables $\mathbf{x}$ and $\mathbf{y}$, both unprimed and primed, and the unprimed variable $h$. The general verification of AX is essentially the same as for our example. (If a step satisfies more than one elementary action, the value of $h^{\prime}$ determined by $A^{h}$ for any of those actions can be used.)

\subsection*{4.2 Simple Prophecy Variables}

We now define $\widetilde{\mathcal{A}}$ to be the same spec as $\mathcal{A}$ except with the variable in hidden. That is, $\widetilde{\mathcal{A}}$ equals $\exists i n: \mathcal{A}$, which equals $\boldsymbol{\exists}$ in, num, sum: $\mathcal{I} \mathcal{A}$. Thus, $\widetilde{\mathcal{A}}$ is the same as $\mathcal{A}$ except we consider input actions to be internal to the system.

We define $\mathcal{C}$ to be the same as $\widetilde{\mathcal{A}}$, except that after an input is received, the input action can be "undone", setting in to rdy, without producing any output for that input. The definition of $\mathcal{C}$ is in Figure 3.

Since out is the only external variable, it's clear that $\mathcal{C}$ allows the same externally visible behaviors as $\widetilde{\mathcal{A}}$. An Input $\mathcal{A}_{\mathcal{A}}$ step followed by an Undoc step produce no change to out, so viewed externally they're just stuttering steps. It's obvious that $\widetilde{\mathcal{A}}$ implements $\mathcal{C}$ because $\mathcal{I} \mathcal{A}$ implies $\mathcal{I} \mathcal{C}$. (A behavior allowed by $\mathcal{I} \mathcal{A}$ is allowed by $\mathcal{I C}$ because $\mathcal{I C}$ does not require that any Undo $o_{\mathcal{C}}$ steps occur.) However, we can't show $\mathcal{C} \Rightarrow \widetilde{\mathcal{A}}$ with a refinement mapping, even by adding history variables.

We can verify that $\mathcal{C}$ implements $\widetilde{\mathcal{A}}$ by adding a prophecy variable $p$ to $\mathcal{C}$ and showing that $\mathcal{I C}^{p}$ implements $\mathcal{I} \mathcal{A}$ under a refinement mapping. The variable $p$ predicts whether or not an input value will be output. More precisely, its value predicts whether the next Output $_{\mathcal{A}} \vee U_{n d o}^{\mathcal{C}}$ step will be an Output $_{\mathcal{A}}$ step or an $U n d o_{\mathcal{C}}$ step. The initial predicate makes the first prediction. The next prediction is made after the currently predicted Output $_{\mathcal{A}}$ or $U n d o_{\mathcal{C}}$ step occurs.

$$
\begin{aligned}
\mathcal{C} & \triangleq \exists \text { in, num, sum }: \mathcal{I C} \\
\mathcal{I C} & \triangleq \text { Init }_{\mathcal{A}} \wedge \square\left[\text { Next }_{\mathcal{C}}\right]_{\langle\text {out }, \text { in }, \text { num }, \text { sum }\rangle} \\
\text { Next }_{\mathcal{C}} & \triangleq N e x t_{\mathcal{A}} \vee U n d o_{\mathcal{C}} \\
U n d o_{\mathcal{C}} & \triangleq(\text { in } \neq \mathrm{rdy}) \wedge\left(\text { in }^{\prime}=\mathrm{rdy}\right) \wedge \mathrm{UC}\langle\text { out }, \text { num }, \text { sum }\rangle
\end{aligned}
$$

Figure 3: The definition of specification $\mathcal{C}$.

$$
\begin{aligned}
& \mathcal{C}^{p} \quad \triangleq \exists \text { in, num, sum: } \mathcal{I C}^{p} \\
& \mathcal{I C}^{p} \quad \triangleq \operatorname{Init}_{\mathcal{C}}^{p} \wedge \square\left[\text { Next }{ }_{\mathcal{C}}^{p}\right\}_{\langle o u t, i n, \text { num }, \text { sum }, p\rangle} \\
& \operatorname{Init}_{\mathcal{C}}^{p} \triangleq(p \in\{\mathrm{do}, \text { undo }\}) \wedge \operatorname{Init}_{\mathcal{A}} \\
& \operatorname{Next}_{\mathcal{C}}^{p} \triangleq \operatorname{Input}_{\mathcal{A}}^{p} \vee \text { Output }{ }_{\mathcal{A}}^{p} \vee \operatorname{Undo}_{\mathcal{C}}^{p} \\
& \operatorname{Input}_{\mathcal{C}}^{p} \triangleq\left(p^{\prime}=p\right) \wedge \text { Input }_{\mathcal{A}} \\
& \text { Output }_{\mathcal{C}}^{p} \triangleq(p=\mathrm{do}) \wedge\left(p^{\prime} \in\{\mathrm{do}, \text { undo }\}\right) \wedge \text { Output }_{\mathcal{A}} \\
& U n d o_{\mathcal{C}}^{p} \triangleq(p=\text { undo }) \wedge\left(p^{\prime} \in\{\mathrm{do}, \text { undo }\}\right) \wedge U n d o_{\mathcal{C}}
\end{aligned}
$$

Figure 4: The definition of specification $\mathcal{C}^{p}$.

The specification $\mathcal{C}^{p}$ is defined in Figure 4. The value of the prophecy variable $p$ is always either do or undo. Initially, $p$ can have either of those values. If $p$ equals do, then the next Output $_{\mathcal{A}}$ or $U n d o_{\mathcal{C}}$ step must be an Output $_{\mathcal{A}}$ step; it must be an $U n d o_{\mathcal{C}}$ step if $p$ equals undo. In either case, after that step is taken, $p$ is set to either do or undo. Condition AX is satisfied because for any behavior $s_{1}, s_{2}, \ldots$ satisfying $\mathcal{I C}$, there is a corresponding behavior $s_{1}^{p}, s_{2}^{p}, \ldots$ satisfying $\mathcal{I C}^{p}$ in which $p$ always makes the correct prediction.

It's not hard to see that $\mathcal{I C ^ { p }}$ implements $\mathcal{I A}$ under this refinement mapping:

$$
\text { in } \leftarrow \text { if } p=\text { undo then rdy else } \text { in, }, \text { num } \leftarrow n u m, \text { sum } \leftarrow \text { sum }
$$

The generalization from this example is straightforward. Suppose the nextstate action Next is the disjunction of elementary actions that include a set of actions $A_{i}$ for $i$ in some set $P$. A simple prophecy variable $p$ that predicts for which $i$ the next $A_{i}$ step occurs is obtained by:

1. Conjoining $p \in P$ to the initial predicate Init.

2. Replacing each $A_{i}$ by $(p=i) \wedge\left(p^{\prime} \in P\right) \wedge A_{i}$.

3. Replacing each other elementary action $B$ by $\left(p^{\prime}=p\right) \wedge B$.

Generalizations of simple prophecy variables and of the prophecy variables described in Sections 4.4 and 4.5 are discussed in Section 4.6.

\subsection*{4.3 Predicting the Impossible}

What if we obtain $\mathcal{S}^{p}$ by adding a prophecy variable $p$ in this way to a specification $\mathcal{S}$, and $p$ makes a prediction that cannot be fulfilled? It may
seem impossible for $\exists p: \mathcal{S}^{p}$ to be equivalent to $\mathcal{S}$ if this can happen. To see why this doesn't affect the equivalence of the two specifications, let's consider an especially egregious example. Define $\mathcal{S}$ by:

$$
\mathcal{S} \triangleq(x=0) \wedge \square\left[x^{\prime}=x+1\right]_{\langle x\rangle}
$$

Since $x^{\prime}=x+1$ equals $\left(x^{\prime}=x+1\right) \vee$ FALSE, we can rewrite this as:

$$
\mathcal{S} \triangleq(x=0) \wedge \square\left[\left(x^{\prime}=x+1\right) \vee \text { FALSE }\right]_{\langle x\rangle}
$$

Following the procedure above, we add a prophecy variable $p$ that predicts if the next $\left(x^{\prime}=x+1\right) \vee$ FALSE step is an $x^{\prime}=x+1$ step or a FALSE step.

$$
\begin{aligned}
& \mathcal{S}^{p} \triangleq \operatorname{Init}^{p} \wedge \square\left[\text { Next }^{p}\right]_{\langle x, p\rangle} \\
& \text { Init }^{p} \triangleq(p \in\{\text { go, stop }\}) \wedge \text { Init } \\
& \text { Next }^{p} \triangleq \quad\left((p=\text { go }) \wedge\left(x^{\prime}=x+1\right) \wedge\left(p^{\prime} \in\{\text { go }, \text { stop }\}\right)\right) \\
& \vee\left((p=\text { stop }) \wedge \text { FALSE } \wedge\left(p^{\prime} \in\{\text { go }, \text { stop }\}\right)\right)
\end{aligned}
$$

If $p$ ever becomes equal to stop, then no further Next ${ }^{p}$ step is possible (since no step can satisfy FALSE), at which point the behavior must consist entirely of stuttering steps. In other words, the behavior describes a system that has stopped. But that's fine because $\mathcal{S}$ allows such behaviors. If we don't want $\mathcal{S}$ to allow such halting behaviors, we must conjoin to it a supplementary property such as $\mathrm{WF}_{\langle x\rangle}\left(x^{\prime}=x+1\right)$. In that case, $\mathcal{S}^{p}$ becomes

$$
\begin{equation*}
\operatorname{Init}^{p} \wedge \square\left[\text { Next }{ }^{p}\right]_{\langle\mathbf{x}, p\rangle} \wedge \mathrm{WF}_{\langle x\rangle}\left(x^{\prime}=x+1\right) \tag{17}
\end{equation*}
$$

The conjunct $\mathrm{WF}_{\mathbf{x}}\left(x^{\prime}=x+1\right)$ implies that a behavior must keep taking steps that increment $x$. Formula (17) thus rules out any behavior in which $p$ ever equals stop, so $\exists p: \mathcal{S}^{p} \wedge \mathrm{WF}_{\langle x\rangle}\left(x^{\prime}=x+1\right)$ is equivalent to $\mathcal{S} \wedge \mathrm{WF}_{\langle x\rangle}\left(x^{\prime}=x+1\right)$.

A reader who finds this hard to understand is making the mistake of thinking of a specification like (17) as a rule for generating behaviors. It's not. It's a predicate on behaviors - a formula that is either satisfied or not satisfied by a behavior.

A reader who finds the specification (17) weird is making no mistake. It is weird. In the terminology introduced by ER, it is weird because it is not machine closed. (Machine closure is explained in ER; it originally appeared under the name feasibility [5].) Except in rare cases, system specifications should be machine closed. However, a specification obtained by adding a prophecy variable is not meant to specify a system. It is used only to verify the system. Its weirdness is harmless.

\subsection*{4.4 A Sequence of Prophecies}

We generalize a simple prophecy variable that makes a single prediction to one that makes a sequence of consecutive predictions. As an example, let $\mathcal{D}$ be the specification that is the same as $\widetilde{\mathcal{A}}$ except instead of alternating between input and output actions, it maintains a queue inq of unprocessed input values. An input action appends a value to the end of inq, and an output action removes the value at the head of the queue and changes sum and out as in our previous specifications. An input action can be performed anytime, but an output action can occur only when inq is not empty. The definition of $\mathcal{D}$ is in Figure 5, where for any nonempty sequence $s q$ of values, $\operatorname{Head}(s q)$ is the first element of $s q$ and $\operatorname{Tail}(s q)$ is the sequence obtained from $s q$ by removing its first element, with $\operatorname{Tail}(\langle\rangle)=\langle\rangle$.

As for our previous example, we implement $\mathcal{D}$ with a specification $\mathcal{E}$ which also contains an undo action that throws away the first input in inq instead of processing it. It is specified in Figure 6.

To define a refinement mapping under which $\mathcal{E}$ implements $\mathcal{D}$, we add a prophecy variable whose value is a sequence of predictions, each one predicting whether the corresponding value of inq will be processed by an output action or thrown away by an undo action. Each prediction is made when the value is added to inq by an input action. The prediction is forgotten when the predicted action occurs. The definition of $\mathcal{E}^{p}$ is in Figure 7.

For sequences $v s q$ and $d s q$ of the same length, let $O n l y D o(v s q, d s q)$ be the subsequence of $v s q$ consisting of all the elements for which the corresponding element of $d s q$ equals do. For example:

$$
\begin{aligned}
& \operatorname{Only} D o(\langle 3,2,1,4,7\rangle,\langle\text { do, undo, undo, do, undo }\rangle)=\langle 3,4\rangle \\
& \mathcal{D} \quad \triangleq \exists \text { inq, num, sum: } \mathcal{I D} \\
& \mathcal{I D} \quad \triangleq \text { Init }_{\mathcal{D}} \wedge \square\left[\text { Next }_{\mathcal{D}}\right]_{\langle\text {inq, out }, \text { num }, \text { sum }\rangle} \\
& \text { Init }_{\mathcal{D}} \triangleq(\text { inq }=\langle\rangle) \wedge(\text { out }=\text { num }=\text { sum }=0) \\
& \text { Next }_{\mathcal{D}} \triangleq \text { Input }_{\mathcal{D}} \vee \text { Output }_{\mathcal{D}} \\
& \text { Input }_{\mathcal{D}} \triangleq \exists n \in \text { Int }:\left(i n q^{\prime}=\operatorname{Append}(\text { inq }, n)\right) \wedge \mathrm{UC}\langle\text { out }, \text { num, sum }\rangle \\
& \text { Output }_{\mathcal{D}} \triangleq \quad(i n q \neq\langle\rangle) \wedge\left(i n q^{\prime}=\operatorname{Tail}(i n q)\right) \\
& \wedge\left(\operatorname{sum}^{\prime}=\operatorname{sum}+\operatorname{Head}(\operatorname{inq})\right) \wedge\left(n u m^{\prime}=n u m+1\right) \\
& \wedge\left(\text { out }^{\prime}=\text { sum }^{\prime} / \text { num }^{\prime}\right)
\end{aligned}
$$

Figure 5: The definition of specification $\mathcal{D}$.

$$
\begin{aligned}
\mathcal{E} & \triangleq \exists i n q, \text { num, sum }: \mathcal{I E} \\
\mathcal{I E} & \triangleq \text { Init }_{\mathcal{D}} \wedge \square[\text { Next }]_{\langle\text {inq, out, num }, \text { sum }\rangle} \\
\text { Next } & \triangleq \operatorname{Next}_{\mathcal{D}} \vee U n d \mathcal{E}_{\mathcal{E}} \\
U n d o_{\mathcal{E}} & \triangleq(i n q \neq\langle\rangle) \wedge\left(i n q^{\prime}=\operatorname{Tail}(\text { inq })\right) \wedge \mathrm{UC}\langle\text { out }, \text { sum, num }\rangle
\end{aligned}
$$

Figure 6: The definition of specification $\mathcal{E}$.

$$
\begin{aligned}
& \mathcal{E}^{p} \quad \triangleq \exists i n q, \text { num, sum : } \mathcal{I E}^{p} \\
& \mathcal{I E}^{p} \quad \triangleq \operatorname{Init}_{\mathcal{E}}^{p} \wedge \square\left[N e x t_{\mathcal{E}}^{p}\right]_{\langle\text {inq }, \text { out }, \text { num }, \text { sum }, p\rangle} \\
& \operatorname{Init}_{\mathcal{E}}^{p} \triangleq(p=\langle\rangle) \wedge \text { Init }_{\mathcal{D}} \\
& \text { Next } t_{\mathcal{E}}^{p} \triangleq \text { Input } \mathrm{E}_{\mathcal{E}}^{p} \vee \text { Output } \mathrm{E}_{\mathcal{E}}^{p} \vee \text { Undo }_{\mathcal{E}}^{p} \\
& \operatorname{Input}_{\mathcal{E}}^{p} \triangleq\left(\exists d \in\{\mathrm{do}, \text { undo }\}: p^{\prime}=\operatorname{Append}(p, d)\right) \wedge \operatorname{Input}_{\mathcal{D}} \\
& \text { Output } \mathcal{E}_{\mathcal{E}}^{p} \triangleq(\operatorname{Head}(p)=\mathrm{do}) \wedge\left(p^{\prime}=\operatorname{Tail}(p)\right) \wedge \text { Output }_{\mathcal{D}} \\
& \operatorname{Undo} o_{\mathcal{E}}^{p} \triangleq(\operatorname{Head}(p)=\text { undo }) \wedge\left(p^{\prime}=\operatorname{Tail}(p)\right) \wedge \operatorname{Undo\mathcal {E}}
\end{aligned}
$$

Figure 7: The definition of specification $\mathcal{E}^{p}$.

Specification $\mathcal{I E}$ implements $\mathcal{I D}$ under this refinement mapping:

$$
\text { inq } \leftarrow O n l y D o(i n q, p), \text { sum } \leftarrow \operatorname{sum}, \text { num } \leftarrow \text { num }
$$

The generalization from this example is straightforward, if we take $p=\langle\rangle$ to mean that there is no prediction being made. Let the next-state action Next be the disjunction of elementary actions that include a set of actions $A_{i}$ for $i$ in a set $P$. Here is how we add a prophecy variable $p$ that makes a sequence of predictions of the $i$ for which the next $A_{i}$ step occurs:

1. Conjoin $p=\langle\rangle$ to the initial predicate Init.

2. Replace each $A_{i}$ by $(p=\langle\rangle \vee \operatorname{Head}(p)=i) \wedge\left(p^{\prime}=\operatorname{Tail}(p)\right) \wedge A_{i}$.

3. Replace each other elementary action $B$ by either $\left(p^{\prime}=p\right) \wedge B$ or $\left(\exists i \in P: p^{\prime}=\operatorname{Append}(p, i)\right) \wedge B$.

As with simple prophecy variables, $\mathrm{AX}$ is satisfied with the required behavior $s_{1}^{p}, s_{2}^{p}, \ldots$ being one in which all the right predictions are made.

In our definition of $\mathcal{E}^{p}$, we could eliminate the $p=\langle\rangle$ of condition 2 from the definitions of Output $t_{\mathcal{E}}^{p}$ and $U n d o_{\mathcal{E}}^{p}$ because $\mathcal{I} \mathcal{E}^{p}$ implies that $p$ is always the same length as inq, and Output $_{\mathcal{D}}$ and $U n d o_{\mathcal{E}}$ both imply inq $\neq\langle\rangle$.

\subsection*{4.5 A Set of Prophecies}

Our next type of prophecy variable is one that makes a set of concurrent predictions. Our example specification $\mathcal{F}$ is similar to $\mathcal{D}$, except that instead of a queue inq of inputs, it has an unordered set inset of inputs. An output action can process any element of inset. Formula $\mathcal{F}$ is defined in Figure 8, where $\backslash$ is the set difference operator, so Int $\backslash$ inset is the set of all integers not in inset.

As before, we add an undo action that can throw away an element in inset so it is not processed by an output action. The resulting specification $\mathcal{G}$ is defined in Figure 9.

To show that $\mathcal{G}$ implements $\mathcal{F}$, we add a prophecy variable $p$ whose value is always a function with domain inset. For any element $n$ of inset, $p(n)$ predicts whether that element will be undone or produce an output. To write the resulting specification $\mathcal{G}^{p}$, we need some notation for describing functions:

EmptyFcn The (unique) function whose domain is the empty set.

$\operatorname{Extend}(f, v, w)$ The function $\widehat{f}$ obtained from function $f$ by adding $v$ to its domain and defining $\widehat{f}(v)$ to equal $w$.

Remove $(f, v)$ The function obtained from function $f$ by removing $v$ from its domain.

The specification $\mathcal{G}^{p}$ is defined in Figure 10. As before, AX holds with $s_{1}^{p}, s_{2}^{p}, \ldots$ a behavior having all the right predictions. Specification $\mathcal{I G}^{p}$ implements $\mathcal{I F}$ under this refinement mapping:

$$
\begin{aligned}
& \text { inset } \leftarrow\{n \in \text { inset }: p(n)=\text { do }\} \text {, sum } \leftarrow \text { sum, num } \leftarrow \text { num } \\
& \mathcal{F} \quad \triangleq \exists \text { inset, num, sum }: \mathcal{I F} \\
& \mathcal{I F} \quad \triangleq \operatorname{Init}_{\mathcal{F}} \wedge \square\left[N e x t_{\mathcal{F}}\right]_{\langle\text {inset, out, num }, \text { sum }\rangle} \\
& \text { Init }_{\mathcal{F}} \quad \triangleq(\text { inset }=\{\}) \wedge(\text { out }=\text { num }=\text { sum }=0) \\
& \operatorname{Next}_{\mathcal{F}} \quad \triangleq\left(\exists n \in \operatorname{Int} \backslash \text { inset }: \operatorname{Input}_{\mathcal{F}}(n)\right) \vee\left(\exists n \in \operatorname{inset}: \operatorname{Output}_{\mathcal{F}}(n)\right) \\
& \operatorname{Input}_{\mathcal{F}}(n) \triangleq(\text { inset }=\text { inset } \cup\{n\}) \wedge \mathrm{UC}\langle\text { out, num, sum }\rangle \\
& \text { Output }_{\mathcal{F}}(n) \triangleq \quad\left(\text { inset }^{\prime}=\text { inset } \backslash\{n\}\right) \\
& \wedge\left(s u m^{\prime}=s u m+n\right) \wedge\left(n u m^{\prime}=n u m+1\right) \\
& \wedge\left(\text { out }{ }^{\prime}=\text { sum }^{\prime} / \text { num }^{\prime}\right)
\end{aligned}
$$

Figure 8: The definition of specification $\mathcal{F}$.

$$
\begin{aligned}
& \mathcal{G} \quad \triangleq \exists \text { inset, num, sum }: \mathcal{I G} \\
& \mathcal{I G} \quad \triangleq \operatorname{Init}_{\mathcal{F}} \wedge \square\left[\operatorname{Next}_{\mathcal{G}}\right]_{\langle\text {inset }, \text { out }, \text { num }, \text { sum }\rangle} \\
& \text { Next }_{\mathcal{G}} \quad \triangleq \operatorname{Next}_{\mathcal{F}} \vee(\exists n \in \text { inset }: \operatorname{Undo}(n)) \\
& \operatorname{Undo}_{\mathcal{G}}(n) \triangleq\left(\text { inset }^{\prime}=\text { inset } \backslash\{n\}\right) \wedge \mathrm{UC}\langle\text { out, sum, num }\rangle
\end{aligned}
$$

Figure 9: The definition of specification $\mathcal{G}$

$$
\begin{aligned}
& \mathcal{G}^{p} \quad \triangleq \boldsymbol{\exists} \text { inset, num, sum }: \mathcal{I G}^{p} \\
& \mathcal{I G}^{p} \quad \triangleq \operatorname{Init}_{\mathcal{G}}^{p} \wedge \square\left[\text { Next } \mathcal{G}_{\mathcal{G}}^{p}\right]_{\langle\text {inset }, \text { out }, \text { num }, \text { sum }, p\rangle} \\
& \operatorname{Init}_{\mathcal{G}}^{p} \quad \triangleq(p=\text { EmptyFcn }) \wedge \operatorname{Init}_{\mathcal{F}} \\
& \operatorname{Next}_{\mathcal{G}}^{p} \quad \triangleq \quad\left(\exists n \in \operatorname{Int} \backslash \text { inset }: \operatorname{Input}_{\mathcal{G}}^{p}(n)\right) \\
& \vee\left(\exists n \in \text { inset }: \operatorname{Output}{ }_{\mathcal{G}}^{p}(n) \vee \operatorname{Undo}_{\mathcal{G}}^{p}(n)\right) \\
& \operatorname{Input} \mathcal{G}_{\mathcal{G}}^{p}(n) \triangleq\left(\exists d \in\{\mathrm{do}, \operatorname{undo}\}: p^{\prime}=\operatorname{Extend}(p, n, d)\right) \wedge \operatorname{Input}_{\mathcal{F}}(n) \\
& \operatorname{Output}_{\mathcal{G}}^{p}(n) \triangleq(p(n)=\operatorname{do}) \wedge\left(p^{\prime}=\operatorname{Remove}(p, n)\right) \wedge \operatorname{Output}_{\mathcal{F}}(n) \\
& \operatorname{Undo} o_{\mathcal{G}}^{p} \quad \triangleq(p(n)=\text { undo }) \wedge\left(p^{\prime}=\operatorname{Remove}(p, n)\right) \wedge \operatorname{Undo} o_{\mathcal{G}}(n)
\end{aligned}
$$

Figure 10: The definition of specification $\mathcal{G}^{p}$.

which assigns to the variable inset of $\mathcal{I F}$ the subset of inset consisting of all elements $n$ with $p(n)=$ do.

The only nontrivial part of the generalization from this example to an arbitrary set of prophecies is that $p$ should make no prediction for a value not in its domain. Usually, as in our example, the actions to which the prediction apply are not enabled for a value not in the domain of $p$. If that's not the case, then the condition conjoined to an action to enforce the prediction should equal TRUE if the prediction is being made for a value not in the domain of $p$.

\subsection*{4.6 Further Generalizations of Prophecy Variables}

Prophecy variables making sequences and sets of predictions can be generalized to prophecy variables whose predictions are organized in any data structure even an infinite one. A data structure can be represented as a function. For example, a sequence of length $n$ is naturally represented as a function with domain the set $\{1,2, \ldots, n\}$. The generalization is described in detail in [12]. The basic ideas are:
- A prediction predicts a value $i$ for which the next step satisfying an action $\exists i \in P: A_{i}$ satisfies $A_{i}$. To add the prophecy variable, each $A_{i}$ is modified to enforce this prediction.
- An action or an initial condition that makes a prediction must allow any value $i$ in $P$ to be predicted.
- Any action may remove a prediction and/or make a new prediction. An action that fulfills a prediction must remove that prediction and may replace it with a new prediction. Any other action may leave the prediction unchanged.

Whether or not a particular prophecy is made is often indicated by the data structure containing the prophecies. In the example of Section 4.5, whether a prediction is made for an integer $n$ depends on whether or not $n$ is in the domain of $p$. Sometimes it is convenient to indicate the absence of a prophecy by a special value none that is not an element of the set $P$ of possible predictions. In the example of a simple prophecy variable in Section 4.2, we could let the Output and Undo actions remove the prophecy by setting $p$ to none, and have the Input action make the prophecy by setting $p$ to do or undo. Handling none values is straightforward.

\subsection*{4.7 Stuttering Variables}

Usually, when $\mathcal{S}_{1}$ implements $\mathcal{S}_{2}$, specification $\mathcal{S}_{1}$ takes more steps than $\mathcal{S}_{2}$. Those extra steps simulate stuttering steps of $\mathcal{S}_{2}$ under a refinement mapping. If $\mathcal{S}_{2}$ takes more steps than $\mathcal{S}_{1}$ to perform some operation, then defining a refinement mapping requires an auxiliary variable that adds stuttering steps to $\mathcal{S}_{1}$. For example, our specification of an hour clock implements the specification of an hour-minute clock with the variable describing the minute display hidden. Defining a refinement mapping to show this requires an auxiliary variable that adds to the hour-clock specification 59 stuttering steps between every change to the variable $h r$. ER used prophecy variables for this. We find it more convenient to use another type of auxiliary variable, which we obviously call a stuttering variable.

It's easy to make up examples like the hour clock implementing the hourminute clock where a stuttering variable is clearly required. In practice, stuttering variables are often used in more subtle ways. A realistic use appears in Section 5 below. A more surprising use is that in the three examples of prophecy variables in Sections 4.2, 4.4, and 4.5, we can use stuttering variables instead of the prophecy variables. We simply add a
stuttering step before each output-action step, and we define the refinement mapping to make that stuttering step implement the input step. For the last two of those examples, the refinement mapping maps each behavior of the specification with undo to a behavior in which there is never more than one value in the internal queue or set. We can use stuttering variables instead of prophecy variables in these examples only because they unrealistically make input steps internal while output steps are externally visible.

We add stuttering steps before and/or after elementary actions of the next-state action. An easy way to do it is to let the value of the stuttering variable $s$ be a natural number. Normally $s$ equals 0 ; it is set to a positive integer to take stuttering steps, the value of $s$ being used to count the number of steps remaining. For example, consider the specification Init $\wedge \square[N e x t]_{\mathbf{x}}$, where $\mathbf{x}$ is the tuple of all the specification's variables (internal and external); and let $N e x t$ equal $A \vee B \vee C$. A stuttering variable $s$ that adds 4 stuttering steps after each $C$ step can be defined by:

$$
\begin{aligned}
& \text { Init }^{s} \triangleq \text { Init } \wedge(s=0) \quad N e x t^{s} \triangleq A^{s} \vee B^{s} \vee C^{s} \\
& A^{s} \triangleq\left(s=s^{\prime}=0\right) \wedge A \quad B^{s} \triangleq\left(s=s^{\prime}=0\right) \wedge B \\
& C^{s} \triangleq\left((s=0) \wedge\left(s^{\prime}=4\right) \wedge C\right) \vee\left((s>0) \wedge\left(s^{\prime}=s-1\right) \wedge \mathrm{UC}\langle\mathbf{x}\rangle\right)
\end{aligned}
$$

To add 4 stuttering steps before each $C$ step, we have to write $C$ in the form $E \wedge D$, where $E$ is a state predicate and $D$ is an action that is enabled in every state satisfying $E$-which means that for every state $s$ satisfying $E$ there is a state $t$ such that the step $s, t$ is a $D$ step. Most elementary actions in specifications can easily be written in this form. (In $\mathrm{TLA}^{+}$, we can always let $E$ equal EnABLEd $C$ and let $D$ equal $C$.) We can then define

$$
\begin{aligned}
C^{s} \triangleq \quad((s=0) & \left.\wedge E \wedge\left(s^{\prime}=4\right) \wedge \mathrm{UC}\langle\mathbf{x}\rangle\right) \\
\vee((s>1) & \left.\wedge\left(s^{\prime}=s-1\right) \wedge \mathrm{UC}\langle\mathbf{x}\rangle\right) \\
\vee((s=1) & \left.\wedge\left(s^{\prime}=0\right) \wedge D\right)
\end{aligned}
$$

It is not hard to see that both of these constructions satisfy AV1 and AV2.

We don't have to use natural numbers for counting stuttering states. For example, we can add stuttering steps both before and after an action by using negative integers to count the steps after the action, counting up to 0 . Often, we let $s$ take values that help define the refinement mapping. For example, suppose we want to take stuttering steps so the refinement mapping can implement an action by each process satisfying some condition. We can let $s$ always be a sequence of processes, where the empty sequence is the normal value of $s$, and counting down is done by $s^{\prime}=\operatorname{Tail}(s)$.

A single variable $s$ can be used to add stuttering steps before and/or after multiple actions. For example, we can let the normal value of $s$ be \langle\rangle , add stuttering steps to an action $A$ by letting $s$ assume values of the form 〈"A", $i$ 〉 for a number $i$, and add stuttering steps to an action $B$ by letting $s$ assume values of the form 〈"B", $q$ 〉 for $q$ a sequence of processes.

To handle the unusual case when $\mathcal{S}_{1}$ implements $\mathcal{S}_{2}$ but it has internal behaviors that halt while the corresponding internal behaviors of $\mathcal{S}_{2}$ must take additional steps, we add an infinite stuttering variable $s$ to $\mathcal{S}_{1}$ that simply keeps changing forever. We do this by conjoining $\mathrm{WF}_{\langle s\rangle}\left(s^{\prime} \neq s\right)$ to the internal specification of $\mathcal{S}_{1}$.

\section*{5 Verifying Linearizability}

Linearizability has become a standard way of specifying an object shared by multiple processes [7]. A process's operation $O p$ is described by a sequence of three steps: a BeginOp step that provides the operation's input, a DoOp step that performs the operation by reading and/or modifying the object, and an EndOp step that reports the operation's output. The BeginOp and EndOp steps are externally visible, meaning that they change external variables. The $D o O p$ step is internal, meaning it modifies only internal variables.

We illustrate our use of auxiliary variables for verifying a linearizability specification with the atomic snapshot algorithm of Afek et al. [3]. Our discussion is informal; formal $\mathrm{TLA}^{+}$specifications are in [12]. The algorithm implements an array of memory registers accessed by a set of writer processes and a set of reader processes, with one register for each writer. A writer can perform write operations to its register. A reader can perform read operations that return a "snapshot" of the memory - that is, the values of all the registers.

We let LinearSnap be a linearizable specification of what a snapshot algorithm should do. It uses an internal variable $m e m$, where $\operatorname{mem}(w)$ equals the value of writer $w$ 's register. A DoWrite step modifies mem $(w)$ for a single writer $w$. A single DoRead step reads the value of mem. Another internal variable maintains a process's state while it is performing an operation, including whether the $D o O p$ action has been performed and, for a reader, what value of mem was read by DoRead and will be returned by EndRead. An external variable describes the BeginOp and EndOp actions.

We consider a simplified version of the Afek et al. snapshot algorithm we call SimpleAfek. It maintains an internal variable imem. A writer $w$ writes a value $v$ on its $i^{\text {th }}$ write by setting $\operatorname{imem}(w)$ to the pair $\langle i, v\rangle$. A
reader does a sequence of reads of imem, each of those reads reading the values of $\operatorname{imem}(w)$ for all writers $w$ in separate actions, executed in any order. If the reader obtains the same value of imem on two successive reads, it returns the obvious snapshot contained in that value of imem. If not, it keeps reading. SimpleAfek does not guarantee termination. The actual algorithms add a way to have reading terminate after at most three reads, and a way to replace the unbounded write numbers by a bounded set of values. These more complicated versions can be handled in the same way as Simple Afek.

Simple Afek implements LinearSnap, but constructing a refinement mapping to show that it does requires predicting the future. To see why, assume a refinement mapping under which Simple Afek implements LinearSnap. Since Begin Op and EndOp actions of LinearSnap modify external variables, there is no choice of which SimpleAfek actions implement them under a refinement mapping. Only the choice of which Simple Afek action implements Do $O p$ depends on the refinement mapping. Consider the following scenario, in which we conflate actions of LinearSnap with the actions of Simple Afek that simulate them.

The scenario begins with no operation in progress. A reader performs a BeginRead action, completes one read of imem, and then begins its second read by reading $\operatorname{imem}(w)$, obtaining the same value as in its first read. Writer $w$ then performs its BeginWrite action, writes a new value in $\operatorname{imem}(w)$, and is about to perform its EndWrite action. If no other writer performs a DoWrite, then the reader will complete its second read of imem, obtaining a snapshot containing the old value of $\operatorname{mem}(w)$. This requires that its DoRead must occur before the DoWrite of $w$. However, suppose another writer $u$ does perform BeginWrite after $w$ performs the DoWrite and writes $\operatorname{imem}(u)$ before the reader reads it, and no more write operations are performed. In that case, the reader will read imem two more times and then return a snapshot containing the value of $\operatorname{imem}(w)$ just written by $w$, so the DoRead must occur after the DoWrite by $w$. Thus, knowing whether the DoRead occurs before or after the DoWrite of $w$ requires knowing what writes occur in the future. Constructing the refinement mapping requires predicting the future.

Linearizability provides a simple, uniform way of specifying data objects; but it provides little insight into what state must be maintained by an implementation. Whether this is a feature or a flaw depends on what the specification is used for. We present an equivalent snapshot specification NewLinearSnap that can make verifying correctness of an implementation easier. We verify that SimpleAfek implements LinearSnap by verifying
that it implements NewLinearSnap and that NewLinearSnap implements LinearSnap.

In addition to the internal variable mem of LinearSnap, NewLinearSnap uses an internal variable isnap such that $\operatorname{isnap}(r)$ is the sequence of snapshots (values of mem) that a read by $r$ can return. The BeginRead action sets $\operatorname{isnap}(r)$ to a one-element sequence containing the current value of mem. The writer actions are the same as in LinearSnap, except that a DoWrite action appends the new value of mem to $\operatorname{isnap}(r)$ for all readers $r$ that have executed a BeginRead action but not the corresponding EndRead. The EndRead action of reader $r$ returns a nondeterministically chosen element of the sequence $\operatorname{isnap}(r)$. There is no DoRead action.

To verify that Simple Afek implements NewLinearSnap, we add to it a history variable that has the same value as variable isnap of NewLinearSnap. Translating an understanding of why the algorithm is correct into an invariant of SimpleAfek and a refinement mapping under which it implements NewLinearSnap is then a typical exercise in assertional reasoning about concurrent algorithms, requiring no prophecy variable.

Although NewLinearSnap is equivalent to LinearSnap, to verify SimpleAfek we need only verify that it implements LinearSnap. This is done by first adding to it a prophecy variable $p$ so that $p(r)$ predicts which element of the sequence isnap $(r)$ of snapshots will be chosen by the EndRead action. The value of $p(r)$ is set to an arbitrary positive integer by $r$ 's BeginRead action and is reset to none by its EndRead action. We then add a stuttering variable that adds a single stuttering step after $r$ 's BeginRead action if $p(r)=1$ and adds stuttering steps after a DoWrite action-one stuttering step for every read $r$ for which the write adds the $p(r)^{\text {th }}$ element to $i s n a p(r)$. Each of those stuttering steps will simulate a DoRead step for one reader. To add the stuttering step after a BeginRead step, the stuttering variable simply counts down from 1 . To add the stuttering steps after a DoWrite step, it counts down using the set of readers whose DoRead the steps will simulate. Requiring the stuttering steps to simulate those DoRead actions makes it clear how to define the refinement mapping.

This technique of verifying linearizability by verifying an equivalent specification should often be applicable. In their paper defining linearizability [7], Herlihy and Wing specify a linearizable FIFO queue and show an implementation in which defining a refinement mapping requires predicting the future. As with the snapshot example, we can write a new specification of the queue that is equivalent to the linearizable specification, and then verify that the Herlihy-Wing algorithm implements that new specification without needing
a prophecy variable. Instead of maintaining a totally ordered queue of elements, the new specification maintains a partially ordered set, where the partial order describes constraints on the order in which items may be dequeued. To define a refinement mapping showing that the new specification implements the original one, we add a prophecy variable that predicts the order in which items will be dequeued.

\section*{6 Prophecy Constants}

In addition to variables and constants like 0 , a temporal logic formula can contain constant parameters. The sets of readers and writers in the SimpleAfek specification are examples of constant parameters. While the value of a variable can be different in different states of a behavior, the value of a constant parameter is the same throughout any behavior. (Logicians call constant parameters rigid variables, and what we call variables they call flexible variables.)

In addition to quantifiers over variables, temporal logic has quantifiers $\exists$ and $\forall$ over constant parameters. A behavior $\sigma$ satisfies the formula $\exists n: \mathcal{F}$ iff there is a value of the constant parameter $n$ (the same value in every state of $\sigma$ ) for which $\sigma$ satisfies $\mathcal{F}$. We let $\exists n \in P: \mathcal{F}$ equal $\exists n:(n \in P) \wedge \mathcal{F}$, where $P$ is a constant expression (one containing only constants and constant parameters) not containing $n$. The following simple rule of ordinary logic holds for any temporal logic formulas $\mathcal{F}$ and $\mathcal{G}$ and constant expression $P$.

$\exists$ Elimination To prove $(\exists n \in P: \mathcal{F}) \Rightarrow \mathcal{G}$, it suffices to assume $n \in P$ and prove $\mathcal{F} \Rightarrow \mathcal{G}$.

The following example from Section 5.2 of ER shows how this rule can be used to construct refinement mappings that require predicting the future, without adding a prophecy variable.

Specification $\mathcal{S}_{1}$ is satisfied by behaviors that begin with $x=0$, repeatedly increment $x$ by 1 , and eventually stop (take only stuttering steps). It has no internal variables. Specification $\mathcal{S}_{2}$ has external variable $x$ and internal variable $y$. Its internal specification is satisfied by behaviors that begin with $x=0$ and $y$ any element of the set Nat of natural numbers, take steps that increment $x$ by 1 and decrement $y$ by 1 , and stop when $y=0$. The TLA specifications of $\mathcal{S}_{1}$ and $\mathcal{S}_{2}$ are in Figure 11, where formula Stops asserts that the value of $x$ eventually stops changing.

Clearly $\mathcal{S}_{1}$ and $\mathcal{S}_{2}$ are equivalent, since both are satisfied by behaviors in which $x$ is incremented a finite number of times (possibly 0 times) and then

$$
\begin{aligned}
& \text { Init }_{1} \triangleq x=0 \quad N_{e x t_{1}} \triangleq x^{\prime}=x+1 \\
& \text { Stops } \triangleq \diamond \square\left[x^{\prime}=x\right]_{\langle x\rangle} \\
& \mathcal{S}_{1} \triangleq \text { Init }_{1} \wedge \square\left[\text { Next }_{1}\right]_{\langle x\rangle} \wedge \text { Stops } \\
& \text { Init }_{2} \triangleq(x=0) \wedge(y \in \text { Nat }) \\
& \text { Next }_{2} \triangleq(y>0) \wedge\left(x^{\prime}=x+1\right) \wedge\left(y^{\prime}=y-1\right) \\
& \mathcal{I S}_{2} \triangleq \text { Init }_{2} \wedge \square\left[\text { Next }_{2}\right]_{\langle x, y\rangle} \\
& \mathcal{S}_{2} \triangleq \exists y: \mathcal{I} \mathcal{S}_{2}
\end{aligned}
$$

Figure 11: The definitions of specification $\mathcal{S}_{1}$ and $\mathcal{S}_{2}$.

stop. ER observes that $\mathcal{S}_{1} \Rightarrow \mathcal{S}_{2}$ cannot be verified using their prophecy variables because $\mathcal{S}_{1}$ doesn't satisfy a condition they call finite internal nondeterminism. We can prove it using the $\exists$ Elimination rule.

Specification $\mathcal{S}_{1}$ implies that the value of $x$ is bounded, which means that there is some natural number $n$ for which $x \leq n$ is an invariant. This means that the following theorem is true:

$$
\begin{equation*}
\mathcal{S}_{1} \Rightarrow \exists n \in N a t: \square(x \leq n) \tag{18}
\end{equation*}
$$

Define $\mathcal{T}_{1}(n)$ to equal $\mathcal{S}_{1} \wedge \square(x \leq n)$. Formula (18) implies that $\mathcal{S}_{1}$ equals $\exists n \in$ Nat $: \mathcal{T}_{1}(n)$. By the $\exists$ Elimination rule, this implies that to prove $\mathcal{S}_{1} \Rightarrow \mathcal{S}_{2}$, it suffices to assume $n \in$ Nat and prove $T_{1}(n) \Rightarrow \mathcal{S}_{2}$, which can be done with the refinement mapping $\bar{y} \leftarrow n-x$. The proof of $\mathcal{S}_{1} \Rightarrow \mathcal{S}_{2}$ can be made completely rigorous in TLA and presumably in other temporal logics.

In general, we prove $\mathcal{S}_{1} \Rightarrow \mathcal{S}_{2}$ by finding a formula $\mathcal{T}_{1}(n)$ such that $\mathcal{S}_{1}$ implies $\exists n \in P: \mathcal{T}_{1}(n)$ for some constant set $P$, and we then prove $n \in P$ implies $T_{1}(n) \Rightarrow \mathcal{S}_{2}$. We can view this method in two ways. The first is that instead of proving $\mathcal{S}_{1} \Rightarrow \mathcal{S}_{2}$ with a single refinement mapping, we prove $\mathcal{T}_{1}(n) \Rightarrow \mathcal{S}_{2}$ by using a separate refinement mapping for each value of $n$. The second is that the constant parameter $n$ is equivalent to a simple prophecy variable that predicts an action that never occurs, so its value never changes. This is because $\exists n \in P: \mathcal{T}_{1}(n)$ is equivalent to

$$
\begin{equation*}
\exists p:(p \in P) \wedge \square\left[p^{\prime}=p\right]_{p} \wedge \mathcal{T}_{1}(p) \tag{19}
\end{equation*}
$$

For our example, this formula is equivalent to

$$
\exists p:\left((p \in \text { Nat }) \wedge \text { Init }_{1}\right) \wedge \square\left[\left(p^{\prime}=p\right) \wedge \text { Next }_{1}\right]_{\langle x, p\rangle} \wedge \text { Stops }
$$

This is the formula we get by observing that $N e x t_{1}$ is equivalent to

$$
N^{2} t_{1} \vee(\exists n \in \text { Nat }:(x=n) \wedge \text { FALSE })
$$

and adding a simple prophecy variable $p$ to predict for which value of $n$ the next $(x=n) \wedge$ FALSE step occurs.

When a constant parameter $n$ is used in this way, we call it a prophecy constant. The equivalence of (19) and $\exists n \in P: \mathcal{T}_{1}(p)$ means that a verification using a prophecy constant can be done using a simple prophecy variable, but there's no reason to do so.

Prophecy constants are useful for predicting the infinite future - that is, making predictions that depend on the entire behavior. Section 6 of ER provides an example in which they cannot prove $\mathcal{S}_{1} \Rightarrow \mathcal{S}_{2}$ with a refinement mapping because the supplementary property of $\mathcal{S}_{2}$ implies that the initial value of an internal variable depends on whether or not the behavior terminates, violating a condition they call internal continuity. It is easy to find the refinement mapping by adding a prophecy constant that predicts if the behavior terminates - a prediction about the entire behavior.

\section*{7 The Existence of Refinement Mappings}

There is a completeness result stating that for any specification $\mathcal{S}_{1}$ of the form $\exists \mathbf{y}:$ Init $\wedge \square[N e x t]_{\langle\mathbf{x}, \mathbf{y}\rangle} \wedge L$, if $\mathcal{S}_{1}$ implements $\mathcal{S}_{2}$, then we can add history, prophecy, and stuttering variables to $\mathcal{S}_{1}$ to obtain an equivalent specification $\mathcal{S}_{1}^{\text {a }}$ for which there exists a refinement mapping showing that $\mathcal{S}_{1}^{\text {a }}$ implements $\mathcal{S}_{2}$. In fact, we can use prophecy constants instead of prophecy variables. We need only assume that the language for defining auxiliary variables and writing proofs is sufficiently expressive. (TLA ${ }^{+}$is such a language.)

This result has been known for almost two decades. Our prophecy constants are essentially what Hesselink called eternity variables [8]. His proof of completeness for eternity variables can be translated directly to the following proof for prophecy constants.

Let $\mathcal{I} \mathcal{S}_{1}$ and $\mathcal{I} \mathcal{S}_{2}$ be the internal specifications of $\mathcal{S}_{1}$ and $\mathcal{S}_{2}$. To simplify the proof, we assume that the next-state action Next of $\mathcal{I} \mathcal{S}_{1}$ allows stuttering steps, replacing it by Next $\vee \mathrm{UC}\langle\mathbf{x}, \mathbf{y}\rangle$ if necessary; and we assume $\mathcal{I} \mathcal{S}_{1}$ never halts, adding an infinite stuttering variable if it may halt. ${ }^{2}$ Let $\mathcal{I} \mathcal{S}_{1}^{h}$ be obtained from $\mathcal{I} \mathcal{S}_{1}$ by adding a history variable $h$ that initially equals 1
\footnotetext{
${ }^{2}$ This also allows us to avoid Hesselink's "preservation of quiescence" assumption.
}
and is incremented by 1 with every Next step. Letting $\sigma_{[i]}$ be the $i^{\text {th }}$ state of a behavior $\sigma$, specification $\mathcal{I} \mathcal{S}_{1}^{h}$ equals

$$
\exists \sigma \in P: \mathcal{I} \mathcal{S}_{1}^{h} \wedge \square\left(\langle\mathbf{x}, \mathbf{y}\rangle=\sigma_{[h]}\right)
$$

where $P$ is the set of all behaviors satisfying $\mathcal{I} \mathcal{S}_{1}^{h}$. We define a refinement mapping that depends on the specific behavior $\sigma$.

Since $\mathcal{S}_{1}$ implements $\mathcal{S}_{2}$, for each $\sigma$ in $P$ there exists a behavior $f(\sigma)$ of $\mathcal{I} \mathcal{S}_{2}$ that $\sigma$ simulates. We define the refinement mapping for $\sigma$ so that it maps the state $\sigma_{[h]}$ in the behavior of $\mathcal{I} \mathcal{S}_{1}^{h}$ to the corresponding state $f(\sigma)_{[g]}$ of $\mathcal{I} \mathcal{S}_{2}$, for some $g$. In the absence of stuttering steps, $g$ would equal $h$. To define $g$ in general, we first make the externally visible steps of $\sigma$ and $f(\sigma)$ match up by adding stuttering steps to $\sigma$ and/or $f(\sigma)$. Since $\mathcal{I} \mathcal{S}_{2}$ is stuttering insensitive, we can assume that $f(\sigma)$ already has the necessary stuttering steps. We define $\mathcal{I} \mathcal{S}_{1}^{h s}$ by adding a stuttering variable $s$ to $\mathcal{I} \mathcal{S}_{1}^{h}$ that adds those stuttering steps needed to make the externally visible steps of $\sigma$ match those of $f(\sigma)$. We can then define $g$ to be a function of $h, s, \sigma$, and $f(\sigma)$.

What this proof shows is that prophecy constants allow embedding behavioral reasoning about a specification into state-based reasoning about another specification. That just places a state-based veneer over a behavioral proof, and presents a state-based tool like a model checker with a specification whose states are impossibly complex. It defeats the purpose of refinement mappings, which is to extend the Floyd-Hoare state-based approach to systems.

A prophecy constant makes a single prediction. When prophecy is needed in practice, as with the Afek et al. algorithm, repeated predictions are almost always required. Making multiple predictions with a constant parameter requires encoding some aspect of the system's behavior in the value of that constant. Our prophecy variables allow that to be avoided.

\section*{References}

[1] Martín Abadi. The prophecy of undo. In Alexander Egyed and Ina Schaefer, editors, Fundamental Approaches to Software Engineering, volume 9033 of Lecture Notes in Computer Science, pages 347-361, Berlin Heidelberg, 2015. Springer.

[2] Martín Abadi and Leslie Lamport. The existence of refinement mappings. Theoretical Computer Science, 82(2):253-284, May 1991.

[3] Yehuda Afek, Hagit Attiya, Danny Dolev, Eli Gafni, Michael Merritt, and Nir Shavit. Atomic snapshots of shared memory. Journal of the ACM, 40(4):873-890, September 1993.

[4] Bowen Alpern and Fred B. Schneider. Defining liveness. Information Processing Letters, 21(4):181-185, October 1985.

[5] Krzysztof R. Apt, Nissim Francez, and Shmuel Katz. Appraising fairness in languages for distributed programming. Distributed Computing, 2:226-241, 1988 .

[6] R. W. Floyd. Assigning meanings to programs. In Proceedings of the Symposium on Applied Math., Vol. 19, pages 19-32. American Mathematical Society, 1967.

[7] Maurice P. Herlihy and Jeannette M. Wing. Linearizability: A correctness condition for concurrent objects. ACM Transactions on Programming Languages and Systems, 12(3):463-492, January 1990.

[8] Wim H. Hesselink. Eternity variables to prove simulation of specifications. ACM Trans. Comput. Log., 6(1):175-201, 2005.

[9] C. A. R. Hoare. Proof of correctness of data representations. Acta Informatica, 1:271-281, 1972.

[10] C.A.R. Hoare. An axiomatic basis for computer programming. Communications of the ACM, 12(10):576-583, October 1969.

[11] Leslie Lamport. Specifying Systems. Addison-Wesley, Boston, 2003. Also available on the Web via a link at http://lamport.org.

[12] Leslie Lamport and Stephan Merz. Auxiliary variables in TLA+. arXiv:1703.05121 (https://arxiv.org/abs/1703.05121) Also available, together with $\mathrm{TLA}^{+}$specifications, at http://lamport. azurewebsites.net/tla/auxiliary/auxiliary.html.

[13] Susan Owicki and David Gries. Verifying properties of parallel programs: An axiomatic approach. Communications of the ACM, 19(5):279-284, May 1976 .