\title{
Seven Sketches in Compositionality: An Invitation to Applied Category Theory
}

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-001.jpg?height=499&width=1022&top_left_y=886&top_left_x=557)

Brendan Fong David I. Spivak

\section*{Preface}

Category theory is becoming a central hub for all of pure mathematics. It is unmatched in its ability to organize and layer abstractions, to find commonalities between structures of all sorts, and to facilitate communication between different mathematical communities.

But it has also been branching out into science, informatics, and industry. We believe that it has the potential to be a major cohesive force in the world, building rigorous bridges between disparate worlds, both theoretical and practical. The motto at MIT is mens et manus, Latin for mind and hand. We believe that category theory-and pure math in general—has stayed in the realm of mind for too long; it is ripe to be brought to hand.

\section*{Purpose and audience}

The purpose of this book is to offer a self-contained tour of applied category theory. It is an invitation to discover advanced topics in category theory through concrete real-world examples. Rather than try to give a comprehensive treatment of these topics-which include adjoint functors, enriched categories, proarrow equipments, toposes, and much more-we merely provide a taste of each. We want to give readers some insight into how it feels to work with these structures as well as some ideas about how they might show up in practice.

The audience for this book is quite diverse: anyone who finds the above description intriguing. This could include a motivated high school student who hasn't seen calculus yet but has loved reading a weird book on mathematical logic they found at the library. Or a machine-learning researcher who wants to understand what vector spaces, design theory, and dynamical systems could possibly have in common. Or a pure mathematician who wants to imagine what sorts of applications their work might have. Or a recently-retired programmer who's always had an eerie feeling that category theory is what they've been looking for to tie it all together, but who's found the usual books on the subject impenetrable.

For example, we find it something of a travesty that in 2018 there is almost no introductory material available on monoidal categories. Even beautiful modern introductions to category theory, e.g. by Riehl or Leinster, do not include anything on this rather central topic. The only exceptions we can think of are [CK17, Chapter 3] and [CP10], each of which has a very user-friendly introduction to monoidal categories; however, readers who are not drawn to physics may not think to look there.

The basic idea of monoidal categories is certainly not too abstract; modern human intuition seems to include a pre-theoretical understanding of monoidal categories that is just waiting to be formalized. Is there anyone who wouldn't correctly understand the basic idea being communicated in the following diagram?

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-004.jpg?height=677&width=1364&top_left_y=911&top_left_x=367)

Many applied category theory topics seem to take monoidal categories as their jumpingoff point. So one aim of this book is to provide a reference-even if unconventional-for this important topic.

We hope this book inspires both new visions and new questions. We intend it to be self-contained in the sense that it is approachable with minimal prerequisites, but not in the sense that the complete story is told here. On the contrary, we hope that readers use this as an invitation to further reading, to orient themselves in what is becoming a large literature, and to discover new applications for themselves.

This book is, unashamedly, our take on the subject. While the abstract structures we explore are important to any category theorist, the specific topics have simply been chosen to our personal taste. Our examples are ones that we find simple but powerful, concrete but representative, entertaining but in a way that feels important and expansive at the same time. We hope our readers will enjoy themselves and learn a lot in the process.

\section*{How to read this book}

The basic idea of category theory-which threads through every chapter-is that if one pays careful attention to structures and coherence, the resulting systems will be extremely reliable and interoperable. For example, a category involves several structures: a collection of objects, a collection of morphisms relating objects, and a formula for combining any chain of morphisms into a morphism. But these structures need to cohere or work together in a simple commonsense way: a chain of chains is itself a long chain, so combining a chain of chains should be the same as combining the long chain. That's it!

We will see structures and coherence come up in pretty much every definition we give: "here are some things and here are how they fit together." We ask the reader to be on the lookout for structures and coherence as they read the book, and to realize that as we layer abstraction upon abstraction, it is the coherence that makes all the parts work together harmoniously in concert.

Each chapter in this book is motivated by a real-world topic, such as electrical circuits, control theory, cascade failures, information integration, and hybrid systems. These motivations lead us into and through various sorts of category-theoretic concepts. We generally have one motivating idea and one category-theoretic purpose per chapter, and this forms the title of the chapter, e.g. Chapter 4 is "Collaborative design: profunctors, categorification, and monoidal categories."

In many math books, the difficulty is roughly a monotonically-increasing function of the page number. In this book, this occurs in each chapter, but not so much in the book as a whole. The chapters start out fairly easy and progress in difficulty.

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-005.jpg?height=436&width=1436&top_left_y=1549&top_left_x=342)

The upshot is that if you find the end of a chapter very difficult, hope is certainly not lost: you can start on the next one and make good progress. This format lends itself to giving you a first taste now, but also leaving open the opportunity for you to come back to the book at a later date and get more deeply into it. But by all means, if you have the gumption to work through each chapter to its end, we very much encourage that!

We include about 240 exercises throughout the text, with solutions in Appendix A. Usually these exercises are fairly straightforward; the only thing they demand is that the reader changes their mental state from passive to active, rereads the previous
paragraphs with intent, and puts the pieces together. A reader becomes a student when they work the exercises; until then they are more of a tourist, riding on a bus and listening off and on to the tour guide. Hey, there's nothing wrong with that, but we do encourage you to get off the bus and make direct contact with the native population and local architecture as often as you can.

\section*{Acknowledgments}

Thanks to Jared Briskman, James Brock, Ronnie Brown, Thrina Burana, David Chudzicki, Jonathan Castello, Margo Crawford, Fred Eisele, David Ellerman, Cam Fulton, Bruno Gavranović, Sebastian Galkin, John Garvin, Peter Gates, Juan Manuel Gimeno, Alfredo Gómez, Leo Gorodinski, Jason Grossman, Jason Hooper, Yuxi Liu, Jesús López, MTM, Nicolò Martini, Martin MacKerel, Pete Morcos, Nelson Niu, James Nolan, Dan Oneata, Paolo Perrone, Thomas Read, Rif A. Saurous, Dan Schmidt, Samantha Seaman, Marcello Seri, Robert Smart, Valter Sorana, Adam Theriault-Shay, Emmy Trewartha, Sergey Tselovalnikov, Andrew Turner, Joan Vazquez, Daniel Wang, Jerry Wedekind for helpful comments and conversations.

We also thank our sponsors at the AFOSR; this work was supported by grants FA9550-14-1-0031 and FA9550-17-1-0058.

Finally, we extend a very special thanks to John Baez for running an online course on this material and generating tons of great feedback.

\section*{Personal note}

Our motivations to apply category theory outside of math are, perhaps naively, grounded in the hope it can help bring humanity together to solve our big problems. But category theory is a tool for thinking, and like any tool it can be used for purposes we align with and those we don't.

In this personal note, we ask that readers try to use what they learn in this book to do something they would call "good," in terms of contributing to the society they'd want to live in. For example, if you're planning to study this material with others, consider specifically inviting someone from an under-represented minority-a group that is more highly represented in society than in upper-level math classes-to your study group. As another example, perhaps you can use the material in this book to design software that helps people relate to and align with each other. What's the mathematics of a well-functioning society?

The way we use our tools affects all our lives. Our society has seen the results-both the wonders and the waste-resulting from rampant selfishness. We would be honored if readers found ways to use category theory as part of an effort to connect people, to create common ground, to explore the cross-cutting categories in which life, society,
and environment can be represented, and to end the ignorance entailed by limiting ourselves to a singular ontological perspective on anything.

If you do something of the sort, please let us and the community know about it.

Brendan Fong and David I. Spivak

Cambridge MA, October 2018

\section*{Contents}
1 Generative effects: Orders and adjunctions ..... 1
1.1 More than the sum of their parts ..... 1
1.1.1 A first look at generative effects . ..... 2
1.1.2 Ordering systems ..... 5
1.2 What is order? ..... 7
1.2.1 Review of sets, relations, and functions ..... 7
1.2.2 Preorders ..... 12
1.2.3 Monotone maps ..... 18
1.3 Meets and joins ..... 23
1.3.1 Definition and basic examples ..... 23
1.3.2 Back to observations and generative effects ..... 26
1.4 Galois connections ..... 26
1.4.1 Definition and examples of Galois connections ..... 27
1.4.2 Back to partitions ..... 28
1.4.3 Basic theory of Galois connections ..... 30
1.4.4 Closure operators ..... 33
1.4.5 Level shifting ..... 35
1.5 Summary and further reading ..... 36
2 Resources: monoidal preorders and enrichment ..... 39
2.1 Getting from $a$ to $b$ ..... 39
2.2 Symmetric monoidal preorders ..... 41
2.2.1 Definition and first examples ..... 41
2.2.2 Introducing wiring diagrams ..... 43
2.2.3 Applied examples ..... 48
2.2.4 Abstract examples ..... 52
2.2.5 Monoidal monotone maps ..... 55
2.3 Enrichment ..... 57
2.3.1 $\mathcal{V}$-categories ..... 57
2.3.2 Preorders as Bool-categories ..... 58
2.3.3 Lawvere metric spaces ..... 59
2.3.4 $\mathcal{V}$-variations on preorders and metric spaces ..... 63
2.4 Constructions on $\mathcal{V}$-categories ..... 64
2.4.1 Changing the base of enrichment ..... 64
2.4.2 Enriched functors ..... 65
2.4.3 Product $\mathcal{V}$-categories ..... 66
2.5 Computing presented $\mathcal{V}$-categories with matrix mult. ..... 68
2.5.1 Monoidal closed preorders ..... 69
2.5.2 Quantales ..... 71
2.5.3 Matrix multiplication in a quantale ..... 73
2.6 Summary and further reading ..... 75
3 Databases: Categories, functors, and (co)limits ..... 77
3.1 What is a database? ..... 77
3.2 Categories ..... 81
3.2.1 Free categories ..... 82
3.2.2 Presenting categories via path equations ..... 84
3.2.3 Preorders and free categories: two ends of a spectrum ..... 85
3.2.4 Important categories in mathematics . ..... 86
3.2.5 Isomorphisms in a category ..... 88
3.3 Functors, natural transformations, and databases ..... 89
3.3.1 Sets and functions as databases . ..... 89
3.3.2 Functors ..... 91
3.3.3 Database instances as Set-valued functors ..... 93
3.3.4 Natural transformations ..... 95
3.3.5 The category of instances on a schema ..... 97
3.4 Adjunctions and data migration ..... 99
3.4.1 Pulling back data along a functor ..... 100
3.4.2 Adjunctions ..... 102
3.4.3 Left and right pushforward functors, $\Sigma$ and $\Pi$ ..... 104
3.4.4 Single set summaries of databases ..... 106
3.5 Bonus: An introduction to limits and colimits ..... 107
3.5.1 Terminal objects and products ..... 107
3.5.2 Limits ..... 110
3.5.3 Finite limits in Set ..... 111
3.5.4 A brief note on colimits ..... 113
3.6 Summary and further reading ..... 114
4 Co-design: profunctors and monoidal categories ..... 117
4.1 Can we build it? ..... 117
4.2 Enriched profunctors ..... 119
4.2.1 Feasibility relationships as Bool-profunctors ..... 119
4.2.2 $\mathcal{V}$-profunctors ..... 121
4.2.3 Back to co-design diagrams ..... 124
4.3 Categories of profunctors ..... 125
4.3.1 Composing profunctors ..... 125
4.3.2 The categories $\mathcal{V}$-Prof and Feas ..... 127
4.3.3 Fun profunctor facts: companions, conjoints, collages ..... 130
4.4 Categorification ..... 132
4.4.1 The basic idea of categorification ..... 133
4.4.2 A reflection on wiring diagrams ..... 134
4.4.3 Monoidal categories ..... 136
4.4.4 Categories enriched in a symmetric monoidal category ..... 138
4.5 Profunctors form a compact closed category ..... 139
4.5.1 Compact closed categories ..... 141
4.5.2 Feas as a compact closed category ..... 143
4.6 Summary and further reading ..... 145
5 Signal flow graphs: Props, presentations, \& proofs ..... 147
5.1 Comparing systems as interacting signal processors ..... 147
5.2 Props and presentations ..... 149
5.2.1 Props: definition and first examples ..... 149
5.2.2 The prop of port graphs ..... 151
5.2.3 Free constructions and universal properties ..... 153
5.2.4 The free prop on a signature ..... 155
5.2.5 Props via presentations ..... 158
5.3 Simplified signal flow graphs ..... 159
5.3.1 Rigs ..... 159
5.3.2 The iconography of signal flow graphs ..... 160
5.3.3 The prop of matrices over a rig ..... 164
5.3.4 Turning signal flow graphs into matrices ..... 165
5.3.5 The idea of functorial semantics ..... 168
5.4 Graphical linear algebra ..... 168
5.4.1 A presentation of $\operatorname{Mat}(R)$ ..... 168
5.4.2 Aside: monoid objects in a monoidal category ..... 172
5.4.3 Signal flow graphs: feedback and more ..... 174
5.5 Summary and further reading ..... 178
6 Circuits: hypergraph categories and operads ..... 181
6.1 The ubiquity of network languages ..... 181
6.2 Colimits and connection ..... 184
6.2.1 Initial objects ..... 184
6.2.2 Coproducts ..... 186
6.2.3 Pushouts ..... 188
6.2.4 Finite colimits ..... 191
6.2.5 Cospans ..... 194
6.3 Hypergraph categories ..... 197
6.3.1 Frobenius monoids ..... 197
6.3.2 Wiring diagrams for hypergraph categories ..... 200
6.3.3 Definition of hypergraph category ..... 201
6.4 Decorated cospans ..... 203
6.4.1 Symmetric monoidal functors ..... 204
6.4.2 Decorated cospans ..... 204
6.4.3 Electric circuits ..... 207
6.5 Operads and their algebras ..... 211
6.5.1 Operads design wiring diagrams ..... 211
6.5.2 Operads from symmetric monoidal categories ..... 214
6.5.3 The operad for hypergraph props ..... 216
6.6 Summary and further reading ..... 218
7 Logic of behavior: Sheaves, toposes, languages ..... 221
7.1 How can we prove our machine is safe? ..... 221
7.2 The category Set as an exemplar topos ..... 224
7.2.1 Set-like properties enjoyed by any topos ..... 225
7.2.2 The subobject classifier ..... 228
7.2.3 Logic in the topos Set ..... 230
7.3 Sheaves ..... 232
7.3.1 Presheaves ..... 232
7.3.2 Topological spaces ..... 234
7.3.3 Sheaves on topological spaces ..... 236
7.4 Toposes ..... 242
7.4.1 The subobject classifier $\Omega$ in a sheaf topos ..... 243
7.4.2 Logic in a sheaf topos ..... 245
7.4.3 Predicates ..... 247
7.4.4 Quantification ..... 248
7.4.5 Modalities ..... 250
7.4.6 Type theories and semantics ..... 251
7.5 A topos of behavior types ..... 252
7.5.1 The interval domain ..... 252
7.5.2 Sheaves on $\mathbb{I} \mathbb{R}$ ..... 253
7.5.3 Safety proofs in temporal logic ..... 255
7.6 Summary and further reading ..... 256
A Exercise solutions ..... 259
A. 1 Solutions for Chapter 1 ..... 259
A. 2 Solutions for Chapter 2 ..... 270
A. 3 Solutions for Chapter 3 ..... 277
A. 4 Solutions for Chapter 4 ..... 286
A. 5 Solutions for Chapter 5 ..... 293
A. 6 Solutions for Chapter 6 ..... 303
A. 7 Solutions for Chapter 7 ..... 312
Bibliography ..... 323
Index ..... 331

\section*{Chapter 1}

\section*{Generative effects: Orders and Galois connections}

In this book, we explore a wide variety of situations-in the world of science, engineering, and commerce-where we see something we might call compositionality. These are cases in which systems or relationships can be combined to form new systems or relationships. In each case we find category-theoretic constructs-developed for their use in pure math—which beautifully describe the compositionality of the situation.

This chapter, being the first of the book, must serve this goal in two capacities. First, it must provide motivating examples of compositionality, as well as the relevant categorical formulations. Second, it must provide the mathematical foundation for the rest of the book. Since we are starting with minimal assumptions about the reader's background, we must begin slowly and build up throughout the book. As a result, examples in the early chapters are necessarily simplified. However, we hope the reader will already begin to see the sort of structural approach to modeling that category theory brings to the fore.

\subsection*{1.1 More than the sum of their parts}

We motivate this first chapter by noticing that while many real-world structures are compositional, the results of observing them are often not. The reason is that observation is inherently "lossy": in order to extract information from something, one must drop the details. For example, one stores a real number by rounding it to some precision. But if the details are actually relevant in a given system operation, then the observed result of that operation will not be as expected. This is clear in the case of roundoff error, but it also shows up in non-numerical domains: observing a complex system is rarely enough to predict its behavior because the observation is lossy.

A central theme in category theory is the study of structures and structure-preserving maps. A map $f: X \rightarrow Y$ is a kind of observation of object $X$ via a specified relationship it has with another object, $Y$. For example, think of $X$ as the subject of an experiment
and $Y$ as a meter connected to $X$, which allows us to extract certain features of $X$ by looking at the reaction of $Y$.

Asking which aspects of $X$ one wants to preserve under the observation $f$ becomes the question "what category are you working in?." As an example, there are many functions $f$ from $\mathbb{R}$ to $\mathbb{R}$, and we can think of them as observations: rather than view $x$ "directly", we only observe $f(x)$. Out of all the functions $f: \mathbb{R} \rightarrow \mathbb{R}$, only some of them preserve the order of numbers, only some of them preserve the distance between numbers, only some of them preserve the sum of numbers, etc. Let's check in with an exercise; a solution can be found in Chapter 1.

Exercise 1.1. Some terminology: a function $f: \mathbb{R} \rightarrow \mathbb{R}$ is said to be

(a) order-preserving if $x \leq y$ implies $f(x) \leq f(y)$, for all $x, y \in \mathbb{R} ; 1$

(b) metric-preserving if $|x-y|=|f(x)-f(y)|$;

(c) addition-preserving if $f(x+y)=f(x)+f(y)$.

For each of the three properties defined above-call it foo-find an $f$ that is foopreserving and an example of an $f$ that is not foo-preserving.

In category theory we want to keep control over which aspects of our systems are being preserved under various observations. As we said above, the less structure is preserved by our observation of a system, the more "surprises" occur when we observe its operations. One might call these surprises generative effects.

In using category theory to explore generative effects, we follow the basic ideas from work by Adam [Ada17]. He goes much more deeply into the issue than we can here; see Section 1.5. But as mentioned above, we must also use this chapter to give an order-theoretic warm-up for the full-fledged category theory to come.

\subsection*{1.1.1 A first look at generative effects}

To explore the notion of a generative effect we need a sort of system, a sort of observation, and a system-level operation that is not preserved by the observation. Let's start with a simple example.

A simple system. Consider three points; we'll call them $\bullet, \circ$ and $*$. In this example, a system will simply be a way of connecting these points together. We might think of our points as sites on a power grid, with a system describing connection by power lines, or as people susceptible to some disease, with a system describing interactions that can lead to contagion. As an abstract example of a system, there is a system where $\bullet$ and $\circ$
\footnotetext{
${ }^{1}$ We are often taught to view functions $f: \mathbb{R} \rightarrow \mathbb{R}$ as plots on an $(x, y)$-axis, where $x$ is the domain (independent) variable and $y$ is the codomain (dependent) variable. In this book, we do not adhere to that naming convention; e.g. in Exercise 1.1, both $x$ and $y$ are being "plugged in" as input to $f$. As an example consider the function $f(x)=x^{2}$. Then $f$ being order-preserving would say that for any $x, y \in \mathbb{R}$, if $x \leq y$ then $x^{2} \leq y^{2}$; is that true?
}
are connected, but neither are connected to *. We shall draw this like so:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-015.jpg?height=168&width=179&top_left_y=339&top_left_x=973)

Connections are symmetric, so if $a$ is connected to $b$, then $b$ is connected to $a$. Connections are also transitive, meaning that if $a$ is connected to $b$, and $b$ is connected to $c$, then $a$ is connected to $c$; that is, all $a, b$, and $c$ are connected. Friendship is not transitive-my friend's friend is not necessarily my friend-but possible communication of a concept or a disease is.

Here we depict two more systems, one in which none of the points are connected, and one in which all three points are connected.
![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-015.jpg?height=184&width=630&top_left_y=973&top_left_x=736)

There are five systems in all, and we depict them just below.

Now that we have defined the sort of system we want to discuss, suppose that Alice is observing this system. Her observation of interest, which we call $\Phi$, extracts a single feature from a system, namely whether the point $\bullet$ is connected to the point *; this is what she wants to know. Her observation of the system will be an assignment of either true or false; she assigns true if $\bullet$ is connected to *, and false otherwise. So $\Phi$ assigns the value true to the following two systems:
![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-015.jpg?height=180&width=626&top_left_y=1625&top_left_x=739)

and $\Phi$ assigns the value false to the three remaining systems:
![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-015.jpg?height=184&width=872&top_left_y=1960&top_left_x=621)

The last piece of setup is to give a sort of operation that Alice wants to perform on the systems themselves. It's a very common operation-one that will come up many times throughout the book-called join. If the reader has been following the story arc, the expectation here is that Alice's connectivity observation will not be compositional with respect to the operation of system joining; that is, there will be generative effects. Let's see what this means.

Joining our simple systems. Joining two systems $A$ and $B$ is performed simply by combining their connections. That is, we shall say the join of systems $A$ and $B$, denote it $A \vee B$, has a connection between points $x$ and $y$ if there are some points $z_{1}, \ldots, z_{n}$ such that each of the following are true in at least one of $A$ or $B: x$ is connected to $z_{1}$, $z_{i}$ is connected to $z_{i+1}$, and $z_{n}$ is connected to $y$. In a three-point system, the above definition is overkill, but we want to say something that works for systems with any number of elements. The high-level way to say it is "take the transitive closure of the union of the connections in $A$ and B." In our three-element system, it means for example that

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-016.jpg?height=178&width=777&top_left_y=735&top_left_x=674)

and

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-016.jpg?height=179&width=777&top_left_y=992&top_left_x=674)

Exercise 1.4. What is the result of joining the following two systems?
![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-016.jpg?height=298&width=892&top_left_y=1298&top_left_x=606)

We are now ready to see the generative effect. We don't want to build it up too much—this example has been made as simple as possible-but we will see that Alice's observation fails to preserve the join operation. We've been denoting her observationmeasuring whether $\bullet$ and $*$ are connected-by the symbol $\Phi$; it returns a boolean result, either true or false.

We see above in Eq. (1.2) that $\Phi\left(\bullet^{\circ}\right)=\Phi(\%)=$ false: in both cases $\bullet$ is not connected to *. On the other hand, when we join these two systems as in Eq. (1.3), we see that $\Phi\left(\bigcirc^{\circ} \vee \%\right)=\Phi(\odot)=$ true: in the joined system, $\bullet$ is connected to $*$. The question that Alice is interested in, that of $\Phi$, is inherently lossy with respect to join, and there is no way to fix it without a more detailed observation, one that includes not only $*$ and $\bullet$ but also $\circ$.

While this was a simple example, it should be noted that whether the potential for such effects exist-i.e. determining whether an observation is operation-preservingcan be incredibly important information to know. For example, Alice could be in charge of putting together the views of two local authorities regarding possible contagion between an infected person $\bullet$ and a vulnerable person $*$. Alice has noticed that if they
separately extract information from their raw data and combine the results, it gives a different answer than if they combine their raw data and extract information from it.

\subsection*{1.1.2 Ordering systems}

Category theory is all about organizing and layering structures. In this section we will explain how the operation of joining systems can be derived from a more basic structure: order. We will see that while joining is not preserved by Alice's connectivity observation $\Phi$, order is.

To begin, we note that the systems themselves are ordered in a hierarchy. Given systems $A$ and $B$, we say that $A \leq B$ if, whenever $x$ is connected to $y$ in $A$, then $x$ is connected to $y$ in $B$. For example,

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-017.jpg?height=176&width=431&top_left_y=920&top_left_x=847)

This notion of $\leq$ leads to the following diagram:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-017.jpg?height=778&width=767&top_left_y=1243&top_left_x=668)

where an arrow from system $A$ to system $B$ means $A \leq B$. Such diagrams are known as Hasse diagrams.

As we were saying above, the notion of join is derived from this order. Indeed for any two systems $A$ and $B$ in the Hasse diagram (1.5), the joined system $A \vee B$ is the smallest system that is bigger than both $A$ and $B$. That is, $A \leq(A \vee B)$ and $B \leq(A \vee B)$, and for any $C$, if $A \leq C$ and $B \leq C$ then $(A \vee B) \leq C$. Let's walk through this with an exercise.

Exercise 1.6.

1. Write down all the partitions of a two element set $\{\bullet, *\}$, order them as above, and draw the Hasse diagram.

2. Now do the same thing for a four element-set, say $\{1,2,3,4\}$. There should be 15 partitions.

Choose any two systems in your 15-element Hasse diagram, call them $A$ and $B$.

3. What is $A \vee B$, using the definition given in the paragraph above Eq. (1.3)?

4. Is it true that $A \leq(A \vee B)$ and $B \leq(A \vee B)$ ?

5. What are all the systems $C$ for which both $A \leq C$ and $B \leq C$.

6. Is it true that in each case, $(A \vee B) \leq C$ ?

The set $\mathbb{B}=\{$ true, false $\}$ of booleans also has an order, false $\leq$ true:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-018.jpg?height=146&width=132&top_left_y=827&top_left_x=991)

Thus false $\leq$ false, false $\leq$ true, and true $\leq$ true, but true $\not$ false. In other words, $A \leq B$ if $A$ implies $B .{ }^{2}$

For any $A, B$ in $\mathbb{B}$, we can again write $A \vee B$ to mean the least element that is greater than both $A$ and $B$.

Exercise 1.7. Using the order false $\leq$ true on $\mathbb{B}=\{$ true, false $\}$, what is:

1. true $\vee$ false?

2. false $\vee$ true?

3. true $\vee$ true?

4. false $\vee$ false?

Let's return to our systems with $\bullet, 0$, and $*$, and Alice's " $\bullet$ is connected to $*$ " function, which we called $\Phi$. It takes any such system and returns either true or false. Note that the map $\Phi$ preserves the $\leq$ order: if $A \leq B$ and there is a connection between $\bullet$ and * in $A$, then there is such a connection in $B$ too. The possibility of a generative effect is captured in the inequality

$$
\begin{equation*}
\Phi(A) \vee \Phi(B) \leq \Phi(A \vee B) \tag{1.8}
\end{equation*}
$$

We saw on page 4 that this can be a strict inequality: we showed two systems $A$ and $B$ with $\Phi(A)=\Phi(B)=$ false, so $\Phi(A) \vee \Phi(B)=$ false, but where $\Phi(A \vee B)=$ true. In this case, a generative effect exists.

These ideas capture the most basic ideas in category theory. Most directly, we have seen that the map $\Phi$ preserves some structure but not others: it preserves order but not join. In fact, we have seen here hints of more complex notions from category theory, without making them explicit; these include the notions of category, functor, colimit, and adjunction. In this chapter we will explore these ideas in the elementary setting of ordered sets.
\footnotetext{
${ }^{2}$ In mathematical logic, false implies true but true does not imply false. That is " $P$ implies $Q$ " means, "if $P$ is true, then $Q$ is true too, but if $P$ is not true, I'm making no claims."
}

\subsection*{1.2 What is order?}

Above we informally spoke of two different ordered sets: the order on system connectivity and the order on booleans false $\leq$ true. Then we related these two ordered sets by means of Alice's observation $\Phi$. Before continuing, we need to make such ideas more precise. We begin in Section 1.2.1 with a review of sets and relations. In Section 1.2.2 we will give the definition of a preorder-short for preordered set-and a good number of examples.

\subsection*{1.2.1 Review of sets, relations, and functions}

We will not give a definition of set here, but informally we will think of a set as a collection of things, known as elements. These things could be all the leaves on a certain tree, or the names of your favorite fruits, or simply some symbols $a, b, c$. For example, we write $A=\{h, 1\}$ to denote the set, called $A$, that contains exactly two elements, one called $h$ and one called 1 . The set $\{h, h, 1, h, 1\}$ is exactly the same as $A$ because they both contain the same elements, $h$ and 1, and repeating an element more than once in the notation doesn't change the set. ${ }^{3}$ For an arbitrary set $X$, we write $x \in X$ if $x$ is element of $X$; so we have $h \in A$ and $1 \in A$, but $0 \notin A$.

Example 1.9. Here are some important sets from mathematics-and the notation we will use-that will appear again in this book.
- $\varnothing$ denotes the empty set; it has no elements.
- $\{1\}$ denotes a set with one element; it has one element, 1 .
- $\mathbb{B}$ denotes the set of booleans; it has two elements, true and false.
- $\mathbb{N}$ denotes the set of natural numbers; it has elements $0,1,2,3, \ldots, 90^{717}, \ldots$.
- $\underline{n}$, for any $n \in \mathbb{N}$, denotes the $n^{\text {th }}$ ordinal; it has $n$ elements $1,2, \ldots, n$. For example, $\underline{0}=\varnothing, \underline{1}=\{1\}$, and $\underline{5}=\{1,2,3,4,5\}$.
- $\mathbb{Z}$, the set of integers; it has elements ..., $-2,-1,0,1,2, \ldots, 90^{717}, \ldots$.
- $\mathbb{R}$, the set of real numbers; it has elements like $\pi, 3.14,5 * \sqrt{2}, e, e^{2},-1457,90^{717}$, etc.

Given sets $X$ and $Y$, we say that $X$ is a subset of $Y$, and write $X \subseteq Y$, if every element in $X$ is also in $Y$. For example $\{h\} \subseteq A$. Note that the empty set $\varnothing:=\{\}$ is a subset of every other set. ${ }^{4}$ Given a set $Y$ and a property $P$ that is either true or false for each element of $Y$, we write $\{y \in Y \mid P(y)\}$ to mean the subset of those $y^{\prime}$ s that satisfy $P$.

Exercise 1.10 .

1. Is it true that $\mathbb{N}=\{n \in \mathbb{Z} \mid n \geq 0\}$ ?
\footnotetext{
${ }^{3}$ If you want a notion where " $h, 1$ " is different than " $h, h, 1, h, 1$ ", you can use something called bags, where the number of times an element is listed matters, or lists, where order also matters. All of these are important concepts in applied category theory, but sets will come up the most for us.

${ }^{4}$ When we write $Z:=$ foo, it means "assign the meaning foo to variable $Z$ ", whereas $Z=$ foo means simply that $Z$ is equal to foo, perhaps as discovered via some calculation. In particular, $Z:=$ foo implies $Z=f o o$ but not vice versa; indeed it would not be proper to write $3+2:=5$ or \{\}$:=\varnothing$.
}

2. Is it true that $\mathbb{N}=\{n \in \mathbb{Z} \mid n \geq 1\}$ ?

3. Is it true that $\varnothing=\{n \in \mathbb{Z} \mid 1<n<2\}$ ?

If both $X_{1}$ and $X_{2}$ are subsets of $Y$, their union, denoted $X_{1} \cup X_{2}$, is also a subset of $Y$, namely the one containing the elements in $X_{1}$ and the elements in $X_{2}$ but no more. For example if $Y=\{1,2,3,4\}$ and $X_{1}=\{1,2\}$ and $X_{2}=\{2,4\}$, then $X_{1} \cup X_{2}=\{1,2,4\}$. Note that $\varnothing \cup X=X$ for any $X \subseteq Y$.

Similarly, if both $X_{1}$ and $X_{2}$ are subsets of $Y$, then their intersection, denoted $X_{1} \cap X_{2}$, is also a subset of $Y$, namely the one containing all the elements of $Y$ that are both in $X_{1}$ and in $X_{2}$, and no others. So $\{1,2,3\} \cap\{2,5\}=\{2\}$.

What if we need to union or intersect a lot of subsets? For example, consider the sets $X_{0}=\varnothing, X_{1}=\{1\}, X_{2}=\{1,2\}$, etc. as subsets of $\mathbb{N}$, and we want to know what the union of all of them is. This union is written $\bigcup_{n \in \mathbb{N}} X_{n}$, and it is the subset of $\mathbb{N}$ that contains every element of every $X_{n}$, but no others. Namely, $\bigcup_{n \in \mathbb{N}} X_{n}=\{n \in \mathbb{N} \mid n \geq 1\}$. Similarly one can write $\bigcap_{n \in \mathbb{N}} X_{n}$ for the intersection of all of them, which will be empty in the above case.

Given two sets $X$ and $Y$, the product $X \times Y$ of $X$ and $Y$ is the set of pairs $(x, y)$, where $x \in X$ and $y \in Y$.

Finally, we may want to take a disjoint union of two sets, even if they have elements in common. Given two sets $X$ and $Y$, their disjoint union $X \sqcup Y$ is the set of pairs of the form $(x, 1)$ or $(y, 2)$, where $x \in X$ and $y \in Y$.

Exercise 1.11. Let $A:=\{h, 1\}$ and $B:=\{1,2,3\}$.

1. There are eight subsets of $B$; write them out.

2. Take any two nonempty subsets of $B$ and write out their union.

3. There are six elements in $A \times B$; write them out.

4. There are five elements of $A \sqcup B$; write them out.

5. If we consider $A$ and $B$ as subsets of the set $\{h, 1,2,3\}$, there are four elements of $A \cup B$; write them out.

Relationships between different sets-for example between the set of trees in your neighborhood and the set of your favorite fruits-are captured using subsets and product sets.

Definition 1.12. Let $X$ and $Y$ be sets. A relation between $X$ and $Y$ is a subset $R \subseteq X \times Y$. A binary relation on $X$ is a relation between $X$ and $X$, i.e. a subset $R \subseteq X \times X$.

It is convenient to use something called infix notation for binary relations $R \subseteq A \times A$. This means one picks a symbol, say $\star$, and writes $a \star b$ to mean $(a, b) \in R$.

Example 1.13. There is a binary relation on $\mathbb{R}$ with infix notation $\leq$. Rather than writing $(5,6) \in R$, we write $5 \leq 6$.

Other examples of infix notation for relations are $=, \approx,<,>$. In number theory, they are interested in whether one number divides without remainder into another number;
this relation is denoted with infix notation |, so $5 \mid 10$.

Partitions and equivalence relations. We can now define partitions more formally.

Definition 1.14. If $A$ is a set, a partition of $A$ consists of a set $P$ and, for each $p \in P$, a nonempty subset $A_{p} \subseteq A$, such that

$$
\begin{equation*}
A=\bigcup_{p \in P} A_{p} \quad \text { and } \quad \text { if } p \neq q \text { then } A_{p} \cap A_{q}=\varnothing \tag{1.15}
\end{equation*}
$$

We may denote the partition by $\left\{A_{p}\right\}_{p \in P}$. We refer to $P$ as the set of part labels and if $p \in P$ is a part label, we refer to $A_{p}$ as the $p^{\text {th }}$ part. The condition (1.15) says that each element $a \in A$ is in exactly one part.

We consider two different partitions $\left\{A_{p}\right\}_{p \in P}$ and $\left\{A_{p^{\prime}}^{\prime}\right\}_{p^{\prime} \in P^{\prime}}$ of $A$ to be the same if for each $p \in P$ there exists a $p^{\prime} \in P^{\prime}$ with $A_{p}=A_{p^{\prime}}^{\prime}$. In other words, if two ways to divide $A$ into parts are exactly the same-the only change is in the labels-then we don't make a distinction between them.

Exercise 1.16. Suppose that $A$ is a set and $\left\{A_{p}\right\}_{p \in P}$ and $\left\{A_{p^{\prime}}^{\prime}\right\}_{p^{\prime} \in P^{\prime}}$ are two partitions of $A$ such that for each $p \in P$ there exists a $p^{\prime} \in P^{\prime}$ with $A_{p}=A_{p^{\prime}}^{\prime}$.

1. Show that for each $p \in P$ there is at most one $p^{\prime} \in P^{\prime}$ such that $A_{p}=A_{p^{\prime}}^{\prime}$

2. Show that for each $p^{\prime} \in P^{\prime}$ there is a $p \in P$ such that $A_{p}=A_{p^{\prime}}^{\prime}$.

Exercise 1.17. Consider the partition shown below:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-021.jpg?height=295&width=350&top_left_y=1479&top_left_x=882)

For any two elements $a, b \in\{11,12,13,21,22,23\}$, let's allow ourselves to write a twiddle symbol $a \sim b$ between them if $a$ and $b$ are both in the same part. Write down every pair of elements $(a, b)$ that are in the same part. There should be $10 .{ }^{5}$

We will see in Proposition 1.19 that there is a strong relationship between partitions and something called equivalence relations, which we define next.

Definition 1.18. Let $A$ be a set. An equivalence relation on $A$ is a binary relation, let's give it infix notation $\sim$, satisfying the following three properties:

(a) $a \sim a$, for all $a \in A$,

(b) $a \sim b$ iffa $b \sim a$, for all $a, b \in A$, and

(c) if $a \sim b$ and $b \sim c$ then $a \sim c$, for all $a, b, c \in A$.
\footnotetext{
${ }^{5}$ Hint: whenever someone speaks of "two elements $a, b$ in a set $A$ ", the two elements may be the same!
}

These properties are called reflexivity, symmetry, and transitivity, respectively.

${ }^{a}$ 'Iff' is short for 'if and only if'.

Proposition 1.19. Let $A$ be a set. There is a one-to-one correspondence between the ways to partition $A$ and the equivalence relations on $A$.

Proof. We first show that every partition gives rise to an equivalence relation, and then that every equivalence relation gives rise to a partition. Our two constructions will be mutually inverse, proving the proposition.

Suppose we are given a partition $\left\{A_{p}\right\}_{p \in P}$; we define a relation $\sim$ and show it is an equivalence relation. Define $a \sim b$ to mean that $a$ and $b$ are in the same part: there is some $p \in P$ such that $a \in A_{p}$ and $b \in A_{p}$. It is obvious that $a$ is in the same part as itself. Similarly, it is obvious that if $a$ is in the same part as $b$ then $b$ is in the same part as $a$, and that if further $b$ is in the same part as $c$ then $a$ is in the same part as $c$. Thus $\sim$ is an equivalence relation as defined in Definition 1.18.

Suppose given an equivalence relation $\sim$; we will form a partition on $A$ by saying what the parts are. Say that a subset $X \subseteq A$ is ( )-closed if, for every $x \in X$ and $x^{\prime} \sim x$, we have $x^{\prime} \in X$. Say that a subset $X \subseteq A$ is ( )-connected if it is nonempty and $x \sim y$ for every $x, y \in X$. Then the parts corresponding to $\sim$ are exactly the ( )-closed, $(\sim)$-connected subsets. It is not hard to check that these indeed form a partition.

Exercise 1.20. Let's complete the "it's not hard to check" part in the proof of Proposition 1.19. Suppose that $\sim$ is an equivalence relation on a set $A$, and let $P$ be the set of ( )-closed and $(\sim)$-connected subsets $\left\{A_{p}\right\}_{p \in P}$.

1. Show that each part $A_{p}$ is nonempty.

2. Show that if $p \neq q$, i.e. if $A_{p}$ and $A_{q}$ are not exactly the same set, then $A_{p} \cap A_{q}=\varnothing$.

3. Show that $A=\bigcup_{p \in P} A_{p}$.

Definition 1.21. Given a set $A$ and an equivalence relation $\sim$ on $A$, we say that the quotient $A / \sim$ of $A$ under $\sim$ is the set of parts of the corresponding partition.

Functions. The most frequently used sort of relation between sets is that of functions.

Definition 1.22. Let $S$ and $T$ be sets. A function from $S$ to $T$ is a subset $F \subseteq S \times T$ such that for all $s \in S$ there exists a unique $t \in T$ with $(s, t) \in F$.

The function $F$ is often denoted $F: S \rightarrow T$. From now on, we write $F(s)=t$, or sometimes $s \mapsto t$, to mean $(s, t) \in F$. For any $t \in T$, the preimage of $t$ along $F$ is the subset $\{s \in S \mid F(s)=t\}$.

A function is called surjective, or a surjection, if for all $t \in T$ there exists $s \in S$ with $F(s)=t$. A function is called injective, or an injection, if for all $t \in T$ and $s_{1}, s_{2} \in S$
with $F\left(s_{1}\right)=t$ and $F\left(s_{2}\right)=t$, we have $s_{1}=s_{2}$. A function is called bijective if it is both surjective and injective.

We use various decorations on arrows, $\rightarrow, \rightarrow, \hookrightarrow, \stackrel{\cong}{\rightrightarrows}$ to denote these special sorts of functions. Here is a table with the name, arrow decoration, and an example of each sort of function:

arbitrary function

$$
\text { surjective function }
$$

injective function

bijective function
![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-023.jpg?height=272&width=1286&top_left_y=636&top_left_x=428)

Example 1.23. An important but very simple sort of function is the identity function on a set $X$, denoted id $X$. It is the bijective function $\operatorname{id}_{X}(x)=x$.

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-023.jpg?height=215&width=203&top_left_y=1123&top_left_x=950)

For notational consistency with Definition 1.22, the arrows in Example 1.23 might be drawn as $\mapsto$ rather than $\rightarrow \rightarrow$. The $\cdots \rightarrow$-style arrows were drawn because we thought it was prettier, i.e. easier on the eye. Beauty is important too; an imbalanced preference for strict correctness over beauty becomes pedantry. But outside of pictures, we will be careful.

Exercise 1.24. In the following, do not use any examples already drawn above.

1. Find two sets $A$ and $B$ and a function $f: A \rightarrow B$ that is injective but not surjective.

2. Find two sets $A$ and $B$ and a function $f: A \rightarrow B$ that is surjective but not injective. Now consider the four relations shown here:
![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-023.jpg?height=208&width=1080&top_left_y=1894&top_left_x=518)

For each relation, answer the following two questions.

3. Is it a function?

4. If not, why not? If so, is it injective, surjective, both (i.e. bijective), or neither? $\diamond$

Exercise 1.25. Suppose that $A$ is a set and $f: A \rightarrow \varnothing$ is a function to the empty set. Show that $A$ is empty.

Example 1.26. A partition on a set $A$ can also be understood in terms of surjective functions out of $A$. Given a surjective function $f: A \rightarrow P$, where $P$ is any other set, the preimages $f^{-1}(p) \subseteq A$, one for each element $p \in P$, form a partition of $A$. Here is an example.

Consider the partition of $S:=\{11,12,13,21,22,23\}$ shown below:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-024.jpg?height=293&width=439&top_left_y=566&top_left_x=843)

It has been partitioned into four parts, so let $P=\{a, b, c, d\}$ and let $f: S \rightarrow P$ be given by

$$
f(11)=a, \quad f(12)=a, \quad f(13)=b, \quad f(21)=c, \quad f(22)=d, \quad f(23)=d
$$

Exercise 1.27. Write down a surjection corresponding to each of the five partitions in Eq. (1.5).

Definition 1.28. If $F: X \rightarrow Y$ is a function and $G: Y \rightarrow Z$ is a function, their composite is the function $X \rightarrow Z$ defined to be $G(F(x))$ for any $x \in X$. It is often denoted $G \circ F$, but we prefer to denote it $F ; G$. It takes any element $x \in X$, evaluates $F$ to get an element $F(x) \in Y$ and then evaluates $G$ to get an element $G(F(x))$.

Example 1.29. If $X$ is any set and $x \in X$ is any element, we can think of $x$ as a function $\{1\} \rightarrow X$, namely the function sending 1 to $x$. For example, the three functions $\{1\} \rightarrow\{1,2,3\}$ shown below correspond to the three elements of $\{1,2,3\}$ :
![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-024.jpg?height=200&width=772&top_left_y=1874&top_left_x=669)

Suppose given a function $F: X \rightarrow Y$ and an element of $X$, thought of as a function $x:\{1\} \rightarrow X$. Then evaluating $F$ at $x$ is given by the composite, $F(x)=x ; F$.

\subsection*{1.2.2 Preorders}

In Section 1.1, we several times used the symbol $\leq$ to denote a sort of order. Here is a formal definition of what it means for a set to have an order.

Definition 1.30. A preorder relation on a set $X$ is a binary relation on $X$, here denoted with infix notation $\leq$, such that

(a) $x \leq x$; and

(b) if $x \leq y$ and $y \leq z$, then $x \leq z$.

The first condition is called reflexivity and the second is called transitivity. If $x \leq y$ and $y \leq x$, we write $x \cong y$ and say $x$ and $y$ are equivalent. We call a pair $(X, \leq)$ consisting of a set equipped with a preorder relation a preorder.

Remark 1.31. Observe that reflexivity and transitivity are familiar from Definition 1.18: preorders are just equivalence relations without the symmetry condition.

Example 1.32 (Discrete preorders). Every set $X$ can be considered as a discrete preorder $(X,=)$. This means that the only order relations on $X$ are of the form $x \leq x$; if $x \neq y$ then neither $x \leq y$ or $y \leq x$ hold.

We depict discrete preorders as simply a collection of points:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-025.jpg?height=100&width=485&top_left_y=1110&top_left_x=820)

Example 1.33 (Codiscrete preorders). From every set we may also construct its codiscrete preorder $(X, \leq)$ by equipping it with the total binary relation $X \times X \subseteq X \times X$. This is a very trivial structure: it means that for all $x$ and $y$ in $X$ we have $x \leq y$ (and hence also $y \leq x$ ).

Example 1.34 (Booleans). The booleans $\mathbb{B}=\{$ false, true $\}$ form a preorder with false $\leq$ true.

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-025.jpg?height=192&width=174&top_left_y=1671&top_left_x=973)

Remark 1.35 (Partial orders are skeletal preorders). A preorder is a partial order if we additionally have that

(c) $x \cong y$ implies $x=y$.

In category theory terminology, the requirement that $x \cong y$ implies $x=y$ is known as skeletality, so partial orders are skeletal preorders. For short, we also use the term poset, a contraction of partially ordered set.

The difference between preorders and partial orders is rather minor. A partial order already is a preorder, and every preorder can be made into a partial order by equating any two elements $x, y$ for which $x \cong y$, i.e. for which $x \leq y$ and $y \leq x$.

For example, any discrete preorder is already a partial order, while any codiscrete preorder simply becomes the unique partial order on a one element set.

We have already introduced a few examples of preorders using Hasse diagrams. It will be convenient to continue to do this, so let us be a bit more formal about what we mean. First, we need to define a graph.

Definition 1.36. A graph $G=(V, A, s, t)$ consists of a set $V$ whose elements are called vertices, a set $A$ whose elements are called arrows, and two functions $s, t: A \rightarrow V$ known as the source and target functions respectively. Given $a \in A$ with $s(a)=v$ and $t(a)=w$, we say that $a$ is an arrow from $v$ to $w$.

By a path in $G$ we mean any sequence of arrows such that the target of one arrow is the source of the next. This includes sequences of length 1 , which are just arrows $a \in A$ in $G$, and sequences of length 0 , which just start and end at the same vertex $v$, without traversing any arrows.

Example 1.37. Here is a picture of a graph:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-026.jpg?height=266&width=482&top_left_y=1057&top_left_x=816)

It has $V=\{1,2,3,4\}$ and $A=\{a, b, c, d, e\}$. The source and target functions, $s, t: A \rightarrow$ $V$ are given by the following partially-filled-in tables (see Exercise 1.38):

\begin{tabular}{l||l|l} 
arrow $a$ & source $s(a) \in V$ & target $t(a) \in V$ \\
\hline$a$ & 1 & $?$ \\
$b$ & 1 & 3 \\
$c$ & $?$ & $?$ \\
$d$ & $?$ & $?$ \\
$e$ & $?$ & $?$
\end{tabular}

There is one path from 2 to 3 , namely the arrow $e$ is a path of length 1 . There are no paths from 4 to 3 , but there is one path from 4 to 4 , namely the path of length 0 . There are infinitely many paths $1 \rightarrow 2$ because one can loop and loop and loop through $d$ as many times as one pleases.

Exercise 1.38. Fill in the table from Example 1.37.

Remark 1.39. From every graph we can get a preorder. Indeed, a Hasse diagram is a graph $G=(V, A, s, t)$ that gives a presentation of a preorder $(P, \leq)$. The elements of $P$ are the vertices $V$ in $G$, and the order $\leq$ is given by $v \leq w$ iff 6 there is a path $v \rightarrow w$. For any vertex $v$, there is always a path $v \rightarrow v$, and this translates into the reflexivity
\footnotetext{
${ }^{6}$ The word 'iff' is a common mathematical shorthand for the phrase "if and only if", and we use it to connect two statements that each imply the other, and hence are logically equivalent.
}
law from Definition 1.30. The fact that paths $u \rightarrow v$ and $v \rightarrow w$ can be concatenated to a path $u \rightarrow w$ translates into the transitivity law.

Exercise 1.40. What preorder relation $(P, \leq)$ is depicted by the graph $G$ in Example 1.37? That is, what are the elements of $P$ and write down every pair $\left(p_{1}, p_{2}\right)$ for which $p_{1} \leq p_{2}$.

Exercise 1.41. Does a collection of points, like the one in Example 1.32, count as a Hasse diagram?

Exercise 1.42. Let $X$ be the set of partitions of $\{\bullet, 0, *\} ;$ it has five elements and an order by coarseness, as shown in the Hasse diagram Eq. (1.5). Write down every pair $(x, y)$ of elements in $X$ such that $x \leq y$. There should be 12 .

Remark 1.43. In Remark 1.35 we discussed partial orders-preorders with the property that whenever two elements are equivalent, they are the same-and then said that this property is fairly inconsequential: any preorder can be converted to a partial order that's "equivalent" category-theoretically. A partial order is like a preorder with a fancy haircut: some mathematicians might not even notice it.

However, there are other types of preorders that are more special and noticeable. For example, a total order has the following additional property:

(d) for all $x, y$, either $x \leq y$ or $y \leq x$.

We say two elements $x, y$ of a preorder are comparable if either $x \leq y$ or $y \leq x$, so a total order is a preorder where every two elements are comparable.

Exercise 1.44. Is it correct to say that a discrete preorder is one where no two elements are comparable?

Example 1.45 (Natural numbers). The natural numbers $\mathbb{N}:=\{0,1,2,3, \ldots\}$ are a preorder with the order given by the usual size ordering, e.g. $0 \leq 1$ and $5 \leq 100$. This is a total order: either $m \leq n$ or $n \leq m$ for all $m, n$. One can see that its Hasse diagram looks like a line:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-027.jpg?height=75&width=645&top_left_y=1765&top_left_x=737)

What made Eq. (1.5) not look like a line is that there are non-comparable elements $a$ and $b$-namely all those in the middle row-which satisfy neither $a \leq b$ nor $b \leq a$.

Note that for any set $S$, there are many different ways of assigning an order to $S$. Indeed, for the set $\mathbb{N}$, we could also use the discrete ordering: only write $n \leq m$ if $n=m$. Another ordering is the reverse ordering, like $5 \leq 3$ and $3 \leq 2$, like how golf is scored (5 is worse than 3 ).

Yet another ordering on $\mathbb{N}$ is given by division: we say that $n \leq m$ if $n$ divides into $m$ without remainder. In this ordering $2 \leq 4$, for example, but $2 \not 33$, since there is a remainder when 2 is divided into 3 .

Exercise 1.46. Write down the numbers $1,2, \ldots, 10$ and draw an arrow $a \rightarrow b$ if $a$ divides perfectly into $b$. Is it a total order?

Example 1.47 (Real numbers). The real numbers $\mathbb{R}$ also form a preorder with the "usual ordering", e.g. $-500 \leq-499 \leq 0 \leq \sqrt{2} \leq 100 / 3$.

Exercise 1.48. Is the usual $\leq$ ordering on the set $\mathbb{R}$ of real numbers a total order?

Example 1.49 (Partition from preorder). Given a preorder, i.e. a pre-ordered set $(P, \leq)$, we defined the notion of equivalence of elements, denoted $x \cong y$, to mean $x \leq y$ and $y \leq x$. This is an equivalence relation, so it induces a partition on $P$. (The phrase "A induces B" means that we have an automatic way to turn an A into a B. In this case, we're saying that we have an automatic way to turn equivalence relations into partitions, which we do; see Proposition 1.19.)

For example, the preorder whose Hasse diagram is drawn on the left corresponds to the partition drawn on the right.
![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-028.jpg?height=282&width=1026&top_left_y=1041&top_left_x=541)

Example 1.50 (Power set). Given a set $X$, the set of subsets of $X$ is known as the power set of $X$; we denote it $P(X)$. The power set can naturally be given an order by inclusion of subsets (and from now on, whenever we speak of the power set as an ordered set, this is the order we mean).

For example, taking $X=\{0,1,2\}$, we depict $P(X)$ as

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-028.jpg?height=352&width=534&top_left_y=1740&top_left_x=793)

See the cube? The Hasse diagram for the power set of a finite set, say $\mathrm{P}\{1,2, \ldots, n\}$, always looks like a cube of dimension $n$.
\footnotetext{
${ }^{a}$ Note that we omit the parentheses here, writing $\mathrm{PX}$ instead of $\mathrm{P}(\mathrm{X})$; throughout this book we will omit parentheses if we judge the presentation is cleaner and it is unlikely to cause confusion.
}

Example 1.52 (Partitions). We talked about getting a partition from a preorder; now let's think about how we might order the set $\operatorname{Prt}(A)$ of all partitions of $A$, for some set $A$. In fact, we have done this before in Eq. (1.5). Namely, we order on partitions by fineness: a partition $P$ is finer than a partition $Q$ if, for every part $p \in P$ there is a part $q \in Q$ such that $A_{p} \subseteq A_{q}$. We could also say that $Q$ is coarser than $P$.

Recall from Example 1.26 that partitions on $A$ can be thought of as surjective functions out of $A$. Then $f: A \rightarrow P$ is finer than $g: A \rightarrow Q$ if there is a function $h: P \rightarrow Q$ such that $f ; h=g$.

Exercise 1.53. For any set $S$ there is a coarsest partition, having just one part. What surjective function does it correspond to?

There is also a finest partition, where everything is in its own partition. What surjective function does it correspond to?

Example 1.54 (Upper sets). Given a preorder $(P, \leq$ ), an upper set in $P$ is a subset $U$ of $P$ satisfying the condition that if $p \in U$ and $p \leq q$, then $q \in U$. "If $p$ is an element then so is anything bigger." Write $U(P)$ for the set of upper sets in $P$. We can give the set $U$ an order by letting $U \leq V$ if $U$ is contained in $V$.

For example, if $(\mathbb{B}, \leq)$ is the booleans (Example 1.34), then its preorder of uppersets $U(\mathbb{B})$ is

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-029.jpg?height=293&width=301&top_left_y=1453&top_left_x=912)

The subset $\{$ false $\} \subseteq \mathbb{B}$ is not an upper set, because false $\leq$ true and true $\notin\{$ false $\}$.

Exercise 1.55. Prove that the preorder of upper sets on a discrete preorder (see Example 1.32) on a set $X$ is simply the power set $P(X)$.

$\diamond$

Example 1.56 (Product preorder). Given preorders $(P, \leq)$ and $(Q, \leq)$, we may define a preorder structure on the product set $P \times Q$ by setting $(p, q) \leq\left(p^{\prime}, q^{\prime}\right)$ if and only if $p \leq p^{\prime}$ and $q \leq q^{\prime}$. We call this the product preorder. This is a basic example of a more general construction known as the product of categories.

Exercise 1.57. Draw the Hasse diagram for the product of the two preorders drawn
below:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-030.jpg?height=266&width=404&top_left_y=276&top_left_x=855)

For bonus points, compute the upper set preorder on the result.

Example 1.58 (Opposite preorder). Given a preorder $(P, \leq$ ), we may define the opposite preorder $\left(P, \leq^{\mathrm{op}}\right)$ to have the same set of elements, but with $p \leq{ }^{\mathrm{op}} q$ if and only if $q \leq p$.

\subsection*{1.2.3 Monotone maps}

We have said that the categorical perspective emphasizes relationships between things. For example, a preorder is a setting-or world-in which we have one sort of relationship, $\leq$, and any two objects may be, or may not be, so-related. Jumping up a level, the categorical perspective emphasizes that preorders themselves-each a miniature world composed of many relationships-can be related to one another.

The most important sort of relationship between preorders is called a monotone map. These are functions that preserve preorder relations-in some sense mappings that respect $\leq$-and are hence considered the right notion of structure-preserving map for preorders.

Definition 1.59. A monotone map between preorders $\left(A, \leq_{A}\right)$ and $\left(B, \leq_{B}\right)$ is a function $f: A \rightarrow B$ such that, for all elements $x, y \in A$, if $x \leq_{A} y$ then $f(x) \leq_{B} f(y)$.

A monotone map $A \rightarrow B$ between two preorders associates to each element of preorder $A$ an element of the preorder $B$. We depict this by drawing a dotted arrow from each element $x \in A$ to its image $f(x) \in B$. Note that the order must be preserved in order to count as a valid monotone map, so if element $x$ is above element $y$ in the lefthand preorder $A$, then the image $f(x)$ will be above the image $f(y)$ in the righthand preorder.

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-030.jpg?height=407&width=851&top_left_y=1981&top_left_x=629)

Example 1.60. Let $\mathbb{B}$ and $\mathbb{N}$ be the preorders of booleans from Example 1.34 and $\mathbb{N}$ the preorder of natural numbers from Example 1.45. The map $\mathbb{B} \rightarrow \mathbb{N}$ sending false to 17 and true to 24 is a monotone map, because it preserves order.

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-031.jpg?height=187&width=1090&top_left_y=470&top_left_x=512)

Example 1.61 (The tree of life). Consider the set of all animal classifications, for example 'tiger', 'mammal', 'sapiens', 'carnivore', etc.. These are ordered by specificity: since 'tiger' is a type of 'mammal', we write tiger $\leq$ mammal. The result is a preorder, which in fact forms a tree, often called the tree of life. At the top of the following diagram we see a small part of it:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-031.jpg?height=737&width=1347&top_left_y=1041&top_left_x=389)

At the bottom we see the hierarchical structure as a preorder. The dashed arrows show a monotone map, call it $F$, from the classifications to the hierarchy. It is monotone because it preserves order: whenever there is a path $x \rightarrow y$ upstairs, there is a path $F(x) \rightarrow F(y)$ downstairs.

Example 1.62. Given a finite set $X$, recall the power set $P(X)$ and its natural order relation from Example 1.50. The map $|\cdot|: P(X) \rightarrow \mathbb{N}$ sending each subset $S$ to its number of elements $|S|$, also called its cardinality, is a monotone map.

Exercise 1.63. Let $X=\{0,1,2\}$.

1. Draw the Hasse diagram for $P(X)$.

2. Draw the Hasse diagram for the preorder $0 \leq 1 \leq 2 \leq 3$.

3. Draw the cardinality map $|\cdot|$ from Example 1.62 as dashed lines between them. $\diamond$

Example 1.64. Recall the notion of upper set from Example 1.54. Given a preorder $(P, \leq)$, the map $U(P) \rightarrow P(P)$ sending each upper set of $(P, \leq)$ to itself-considered as a subset of $P$-is a monotone map.

Exercise 1.65. Consider the preorder $\mathbb{B}$. The Hasse diagram for $U(\mathbb{B})$ was drawn in Example 1.54, and you drew the Hasse diagram for $P(\mathbb{B})$ in Exercise 1.51. Now draw the monotone map between them, as described in Example 1.64.

$\diamond$

Exercise 1.66. Let $(P, \leq)$ be a preorder, and recall the notion of opposite preorder from Example 1.58.

1. Show that the set $\uparrow p:=\left\{p^{\prime} \in P \mid p \leq p^{\prime}\right\}$ is an upper set, for any $p \in P$.

2. Show that this construction defines a monotone map $\uparrow: P^{\circ p} \rightarrow U(P)$.

3. Show that if $p \leq p^{\prime}$ in $P$ if and only if $\uparrow\left(p^{\prime}\right) \subseteq \uparrow(p)$.

4. Draw a picture of the map $\uparrow$ in the case where $P$ is the preorder $(b \geq a \leq c)$ from Example 1.56.

This is known as the Yoneda lemma for preorders. The if and only if condition proved in part 3 implies that, up to equivalence, to know an element is the same as knowing its upper set-that is, knowing its web of relationships with the other elements of the preorder. The general Yoneda lemma is a powerful tool in category theory, and a fascinating philosophical idea besides.

Exercise 1.67. As you yourself well know, a monotone map $f:\left(P, \leq_{P}\right) \rightarrow\left(Q, \leq_{Q}\right)$ consists of a function $f: P \rightarrow Q$ that satisfies a "monotonicity" property. Show that when $\left(P, \leq_{P}\right)$ is a discrete preorder, then every function $P \rightarrow Q$ satisfies the monotonicity property, regardless of the order $\leq_{Q}$.

Example 1.68. Recall from Example 1.52 that given a set $X$ we define $\operatorname{Prt}(X)$ to be the set of partitions on $X$, and that a partition may be defined using a surjective function $s: X \rightarrow P$ for some set $P$.

Any surjective function $f: X \rightarrow Y$ induces a monotone map $f^{*}: \operatorname{Prt}(Y) \rightarrow \operatorname{Prt}(X)$, going "backwards." It is defined by sending a partition $s: Y \rightarrow P$ to the composite $f ; s: X \rightarrow P .7$

Exercise 1.69. Choose two sets $X$ and $Y$ with at least three elements each and choose a surjective, non-identity function $f: X \rightarrow Y$ between them. Write down two different partitions $P$ and $Q$ of $Y$, and then find $f^{*}(P)$ and $f^{*}(Q)$.

The following proposition, Proposition 1.70, is straightforward to check. Recall the definition of the identity function from Example 1.23 and the definition of composition from Definition 1.28.
\footnotetext{
${ }^{7}$ We will later see that any function $f: X \rightarrow Y$, not necessarily surjective, induces a monotone map $f^{*}: \operatorname{Prt}(Y) \rightarrow \operatorname{Prt}(X)$, but it involves an extra step. See Section 1.4.2.
}

Proposition 1.70. For any preorder $\left(P, \leq_{P}\right)$, the identity function is monotone.

If $\left(Q, \leq_{Q}\right)$ and $\left(R, \leq_{R}\right)$ are preorders and $f: P \rightarrow Q$ and $g: Q \rightarrow R$ are monotone, then $(f ; g): P \rightarrow R$ is also monotone.

Exercise 1.71. Check the two claims made in Proposition 1.70.

Example 1.72. Recall again the definition of opposite preorder from Example 1.58. The identity function $\operatorname{id}_{P}: P \rightarrow P$ is a monotone map $(P, \leq) \rightarrow\left(P, \leq{ }^{\text {op }}\right)$ if and only if for all $p, q \in P$ we have $q \leq p$ whenever $p \leq q$. For historical reasons connected to linear algebra, when this is true, we call $(P, \leq)$ a dagger preorder.

But in fact, we have seen dagger preorders before in another guise. Indeed, if $(P, \leq)$ is a dagger preorder, then the relation $\leq$ is symmetric: $p \leq q$ if and only if $q \leq p$, and it is also reflexive and transitive by definition of preorder. So in fact $\leq$ is an equivalence relation (Definition 1.18).

Exercise 1.73. Recall the notion of skeletal preorders (Remark 1.35) and discrete preorders (Example 1.32). Show that a skeletal dagger preorder is just a discrete preorder, and hence can be identified with a set.

Remark 1.74. We say that an $A$ "can be identified with" a $B$ when any $A$ gives us a unique $B$ and any $B$ gives us a unique $A$, and both round-trips-from an $A$ to a $B$ and back to an $A$, or from a $B$ to an $A$ and back to a $B$-return us where we started. For example, any discrete preorder $(P, \leq)$ has an underlying set $P$, and any set $P$ can be made into a discrete preorder ( $p_{1} \leq p_{2}$ iff $p_{1}=p_{2}$ ), and the round-trips return us where we started. So what's the difference? It's like the notion of object-permanence from child development jargon: we can recognize "the same chair, just moved from one room to another." A chair in the room of sets can be moved to a chair in the room of preorders. The lighting is different but the chair is the same.

Eventually, we will be able to understand this notion in terms of equivalence of categories, which are related to isomorphisms, which we will explore next in Definition 1.75.

Definition 1.75. Let $\left(P, \leq_{P}\right)$ and $\left(Q, \leq_{Q}\right)$ be preorders. A monotone function $f: P \rightarrow Q$ is called an isomorphism if there exists a monotone function $g: Q \rightarrow P$ such that $f ; g=\operatorname{id}_{P}$ and $g ; f=\operatorname{id}_{Q}$. This means that for any $p \in P$ and $q \in Q$, we have

$$
p=g(f(p)) \quad \text { and } \quad q=f(g(q))
$$

We refer to $g$ as the inverse of $f$, and vice versa: $f$ is the inverse of $g$.

If there is an isomorphism $P \rightarrow Q$, we say that $P$ and $Q$ are isomorphic.

An isomorphism between preorders is basically just a relabeling of the elements.

Example 1.76. Here are the Hasse diagrams for three preorders $P, Q$, and $R$, all of which are isomorphic:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-034.jpg?height=420&width=1358&top_left_y=392&top_left_x=378)

The map $f: P \rightarrow Q$ given by $f(a)=v, f(b)=w, f(c)=x, f(d)=y$, and $f(e)=z$ has an inverse.

In fact $Q$ and $R$ are the same preorder. One may be confused by the fact that there is an arrow $x \rightarrow z$ in the Hasse diagram for $R$ and not one in $Q$, but in fact this arrow is superfluous. By the transitivity property of preorders (Definition 1.30), since $x \leq y$ and $y \leq z$, we must have $x \leq z$, whether it is drawn or not. Similarly, we could have drawn an arrow $v \rightarrow y$ in either $Q$ or $R$ and it would not have changed the preorder.

Recall the preorder $\mathbb{B}=\{$ false, true $\}$, where false $\leq$ true. As simple as this preorder is, it is also one of the most important.

Exercise 1.77. Show that the map $\Phi$ from Section 1.1.1, which was roughly given by 'Is - connected to $*$ ?' is a monotone map $\operatorname{Prt}(\{*, \bullet, \circ\}) \rightarrow \mathbb{B}$; see also Eq. (1.5).

Proposition 1.78. Let $P$ be a preorder. Monotone maps $P \rightarrow \mathbb{B}$ are in one-to-one correspondence with upper sets of $P$.

Proof. Let $f: P \rightarrow \mathbb{B}$ be a monotone map. We will show that the subset $f^{-1}$ (true) $\subseteq P$ is an upper set. Suppose $p \in f^{-1}$ (true) and $p \leq q$; then true $=f(p) \leq f(q)$. But in $\mathbb{B}$, if true $\leq f(q)$ then true $=f(q)$. This implies $q \in f^{-1}$ (true) and thus shows that $f^{-1}$ (true) is an upper set.

Conversely, if $U$ is an upper set in $P$, define $f_{U}: P \rightarrow \mathbb{B}$ such that $f_{U}(p)=$ true when $p \in U$, and $f_{U}(p)=$ false when $p \notin U$. This is a monotone map, because if $p \leq q$, then either $p \in U$, so $q \in U$ and $f(p)=$ true $=f(q)$, or $p \notin U$, so $f(p)=$ false $\leq f(q)$.

These two constructions are mutually inverse, and hence prove the proposition.

Exercise 1.79 (Pullback map). Let $P$ and $Q$ be preorders, and $f: P \rightarrow Q$ be a monotone map. Then we can define a monotone map $f^{*}: U(Q) \rightarrow U(P)$ sending an upper set $U \subseteq Q$ to the upper set $f^{-1}(U) \subseteq P$. We call this the pullback along $f$.

Viewing upper sets as a monotone maps to $\mathbb{B}$ as in Proposition 1.78, the pullback can be understood in terms of composition. Indeed, show that the $f^{*}$ is defined by taking $u: Q \rightarrow \mathbb{B}$ to $(f ; u): P \rightarrow \mathbb{B}$.

\subsection*{1.3 Meets and joins}

As we have said, a preorder is a set $P$ endowed with an order $\leq$ relating the elements. With respect to this order, certain elements of $P$ may have distinctive characterizations, either absolutely or in relation to other elements. We have discussed joins before, but we discuss them again now that we have built up some formalism.

\subsection*{1.3.1 Definition and basic examples}

Consider the preorder $(\mathbb{R}, \leq$ ) of real numbers ordered in the usual way. The subset $\mathbb{N} \subseteq \mathbb{R}$ has many lower bounds, namely -1.5 is a lower bound: every element of $\mathbb{N}$ is bigger than -1.5 . But within all lower bounds for $\mathbb{N} \subseteq \mathbb{R}$, one is distinctive: a greatest lower bound—also called a meet—namely 0 . It is a lower bound, and there is no lower bound for $\mathbb{N}$ that is above it. However, the set $\mathbb{N} \subseteq \mathbb{R}$ has no upper bound, and certainly no least upper bound-which would be called a join. On the other hand, the set

$$
\left\{\left.\frac{1}{n+1} \right\rvert\, n \in \mathbb{N}\right\}=\left\{1, \frac{1}{2}, \frac{1}{3}, \frac{1}{4}, \ldots\right\} \subseteq \mathbb{R}
$$

has both a greatest lower bound (meet), namely 0 , and a least upper bound (join), namely 1 .

These notions will have correlates in category theory, called limits and colimits, which we will discuss in Chapter 3. More generally, we say these distinctive characterizations are universal properties, since, for example, a greatest lower bound is greatest among all lower bounds. For now, however, we simply want to make the definition of greatest lower bounds and least upper bounds, called meets and joins, precise.

Exercise 1.80 .

1. Why is 0 a lower bound for $\left\{\left.\frac{1}{n+1} \right\rvert\, n \in \mathbb{N}\right\} \subseteq \mathbb{R}$ ?

2. Why is 0 a greatest lower bound (meet)?

Definition 1.81. Let $(P, \leq)$ be a preorder, and let $A \subseteq P$ be a subset. We say that an element $p \in P$ is a meet of $A$ if

(a) for all $a \in A$, we have $p \leq a$, and

(b) for all $q$ such that $q \leq a$ for all $a \in A$, we have that $q \leq p$.

We write $p=\bigwedge A, p=\bigwedge_{a \in A} a$, or, if the dummy variable $a$ is clear from context, just $p=\bigwedge_{A} a$. If $A$ just consists of two elements, say $A=\{a, b\}$, we can denote $\wedge A$ simply by $a \wedge b$.

Similarly, we say that $p$ is a join of $A$ if

(a) for all $a \in A$ we have $a \leq p$, and

(b) for all $q$ such that $a \leq q$ for all $a \in A$, we have that $p \leq q$.

We write $p=\bigvee A$ or $p=\bigvee_{a \in A} a$, or when $A=\{a, b\}$ we may simply write $p=a \vee b$.

Remark 1.82. In Definition 1.81, we committed a seemingly egregious abuse of notation. We will see next in Example 1.84 that there could be two different meets of $A \subseteq P$, say $p=\bigwedge A$ and $q=\bigwedge A$ with $p \neq q$, which does not make sense if $p \neq q$ !

But in fact, as we use the symbol $\wedge A$, this abuse won't matter because any two meets $p, q$ are automatically isomorphic: the very definition of meet forces both $p \leq q$ and $q \leq p$, and thus we have $p \cong q$. So for any $x \in P$, we have $p \leq x$ iff $q \leq x$ and $x \leq p$ iff $x \leq q$. Thus as long as we are only interested in elements of $P$ based on their relationships to other elements (and in category theory, this is the case: we should only care about things based on how they interact with other things, rather than on some sort of "internal essence"), the distinction between $p$ and $q$ will never matter.

This foreshadows a major theme of-as well as standard abuse of notation incategory theory, where any two things defined by the same universal property are automatically equivalent in a way known as 'unique up to unique isomorphism'; this means that we generally do not run into trouble if we pretend they are equal. We'll pick up this theme of 'the' vs 'a' again in Remark 3.85.

Example 1.83 (Meets or joins may not exist). Note that, in an arbitrary preorder $(P, \leq)$, a subset $A$ need not have a meet or a join. Consider the three element set $P=\{p, q, r\}$ with the discrete ordering. The set $A=\{p, q\}$ does not have a join in $P$ because if $x$ was a join, we would need $p \leq x$ and $q \leq x$, and there is no such element $x$.

Example 1.84 (Multiple meets or joins may exist). It may also be the case that a subset $A$ has more than one meet or join. Here is an example.

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-036.jpg?height=276&width=263&top_left_y=1586&top_left_x=926)

Let $A$ be the subset $\{a, b\}$ in the preorder specified by this Hasse diagram. Then both $c$ and $d$ are meets of $A$ : any element less than both $a$ and $b$ is also less than $c$, and also less than $d$. Note that, as in Remark 1.82, $c \leq d$ and $d \leq c$, so $c \cong d$. Such will always the case when there is more than one meet: any two meets of the same subset will be isomorphic.

Exercise 1.85. Let $(P, \leq)$ be a preorder and $p \in P$ an element. Consider the set $A=\{p\}$ with one element.

1. Show that $\bigwedge A \cong p$.

2. Show that if $P$ is in fact a partial order, then $\wedge A=p$.

3. Are the analogous facts true when $\wedge$ is replaced by $\bigvee$ ?

Example 1.86. In any partial order $P$, we have $p \vee p=p \wedge p=p$. The reason is that our notation says $p \vee p$ means $\bigvee\{p, p\}$. But $\{p, p\}=\{p\}$ (see Section 1.2.1), so $p \vee p=p$ by Exercise 1.85.

Example 1.87. In a power set $\mathrm{P}(X)$, the meet of a collection of subsets, say $A, B \subseteq X$ is their intersection $A \wedge B=A \cap B$, while the join is their union, $A \vee B=A \cup B$.

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-037.jpg?height=257&width=306&top_left_y=709&top_left_x=904)

Perhaps this justifies the terminology: the joining of two sets is their union, the meeting of two sets is their intersection.

Example 1.88. In the booleans $\mathbb{B}=\{$ false, true $\}$ (Example 1.34), the meet of any two elements is given by AND and the join of any two elements is given by OR (recall Exercise 1.7).

Example 1.89. In a total order, the meet of a set is its infimum, while the join of a set is its supremum. Note that $\mathbb{B}$ is a total order, and this generalizes Example 1.88.

Exercise 1.90. Recall the division ordering on $\mathbb{N}$ from Example 1.45: we write $n \mid m$ if $n$ divides perfectly into $m$. The meet of any two numbers in this preorder has a common name, that you may have learned when you were around 10 years old; what is it? Similarly the join of any two numbers has a common name; what is it?

Proposition 1.91. Suppose $(P, \leq)$ is a preorder and $A \subseteq B \subseteq P$ are subsets that have meets. Then $\wedge B \leq \wedge A$.

Similarly, if $A$ and $B$ have joins, then $\bigvee A \leq \bigvee B$.

Proof. Let $m=\wedge A$ and $n=\wedge B$. Then for any $a \in A$ we also have $a \in B$, so $n \leq a$ because $n$ is a lower bound for $B$. Thus $n$ is also a lower bound for $A$ and hence $n \leq m$, because $m$ is $A$ 's greatest lower bound. The second claim is proved similarly.

\subsection*{1.3.2 Back to observations and generative effects}

In the thesis [Ada17], Adam thinks of monotone maps as observations. A monotone map $\Phi: P \rightarrow Q$ is a phenomenon (we might say "feature") of $P$ as observed by $Q$. He defines the generative effect of such a map $\Phi$ to be its failure to preserve joins (or more generally, for categories, its failure to preserve colimits).

Definition 1.92. We say that a monotone map $f: P \rightarrow Q$ preserves meets if $f(a \wedge b) \cong$ $f(a) \wedge f(b)$ for all $a, b \in P$. We similarly say $f$ preserves joins if $f(a \vee b) \cong f(a) \vee f(b)$ for all $a, b \in P$.

Definition 1.93. We say that a monotone map $f: P \rightarrow Q$ has a generative effect if there exist elements $a, b \in P$ such that

$$
f(a) \vee f(b) \nsubseteq f(a \vee b)
$$

In Definition 1.93, if we think of $\Phi$ as a observation or measurement of the systems $a$ and $b$, then the left hand side $f(a) \vee f(b)$ may be interpreted as the combination of the observation of $a$ with the observation of $b$. On the other hand, the right hand side $f(a \vee b)$ is the observation of the combined system $a \vee b$. The inequality implies that we see something when we observe the combined system that we could not expect by merely combining our observations of the pieces. That is, that there are generative effects from the interconnection of the two systems.

Exercise 1.94. In Definition 1.93, we defined generativity of $f$ as the inequality $f(a \vee b) \neq$ $f(a) \vee f(b)$, but in the subsequent text we seemed to imply there would be not just a difference, but more stuff in $f(a \vee b)$ than in $f(a) \vee f(b)$.

Prove that for any monotone map $f: P \rightarrow Q$, if $a, b \in P$ have a join and $f(a), f(b) \in Q$ have a join, then indeed $f(a) \vee f(b) \leq f(a \vee b)$.

In his work on generative effects, Adam restricts his attention to generative maps that preserve meets (but do not preserve joins). The preservation of meets implies that the map $\Phi$ behaves well when restricting to subsystems, even though it can throw up surprises when joining systems.

This discussion naturally leads into Galois connections, which are pairs of monotone maps between preorders, one of which preserves all joins and the other of which preserves all meets.

\subsection*{1.4 Galois connections}

The preservation of meets and joins, and in particular issues concerning generative effects, is tightly related to the theory of Galois connections, which is a special case of a more general theory we will discuss later, namely that of adjunctions. We will use some adjunction terminology when describing Galois connections.

\subsection*{1.4.1 Definition and examples of Galois connections}

Galois connections between preorders were first considered by Évariste Galois-who didn't call them by that name-in the context of a connection he found between "field extensions" and "automorphism groups." We will not discuss this further, but the idea is that given two preorders $P$ and $Q$, a Galois connection is a pair of maps back and forth—-from $P$ to $Q$ and from $Q$ to $P$-with certain properties, which make it like a relaxed version of isomorphisms. To be a bit more precise, preorder isomorphisms are examples of Galois connections, but Galois connections need not be preorder isomorphisms.

Definition 1.95. A Galois connection between preorders $P$ and $Q$ is a pair of monotone maps $f: P \rightarrow Q$ and $g: Q \rightarrow P$ such that

$$
\begin{equation*}
f(p) \leq q \quad \text { if and only if } \quad p \leq g(q) \tag{1.96}
\end{equation*}
$$

We say that $f$ is the left adjoint and $g$ is the right adjoint of the Galois connection.

Example 1.97. Consider the map $(3 \times-): \mathbb{Z} \rightarrow \mathbb{R}$ which sends $x \in \mathbb{Z}$ to $3 x$, which we can consider as a real number $3 x \in \mathbb{Z} \subseteq \mathbb{R}$. Let's find a left adjoint for the map $(3 \times-)$.

Write $\lceil z\rceil$ for the smallest natural number above $z \in \mathbb{R}$, and write $\lfloor z\rfloor$ for the largest integer below $z \in \mathbb{R}$, e.g. $\lceil 3.14\rceil=4$ and $\lfloor 3.14\rfloor=3$. ${ }^{a}$ As the left adjoint $\mathbb{R} \rightarrow \mathbb{Z}$, let's see if $\lceil-/ 3\rceil$ works.

It is easily checked that

$$
\lceil x / 3\rceil \leq y \text { if and only if } x \leq 3 y
$$

Success! Thus we have a Galois connection between $\lceil-/ 3\rceil$ and $(3 \times-)$.
\footnotetext{
${ }^{a}$ By "above" and "below," we mean greater than or equal to or less than or equal to; the latter being a mouthful. Anyway, $\lfloor 3\rfloor=3=\lceil 3\rceil$.
}

Exercise 1.98. In Example 1.97 we found a left adjoint for the monotone map ( $3 \times-): \mathbb{Z} \rightarrow$ $\mathbb{R}$. Now find a right adjoint for the same map, and show it is correct.

Exercise 1.99. Consider the preorder $P=Q=\underline{3}$.

1. Let $f, g$ be the monotone maps shown below:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-039.jpg?height=225&width=664&top_left_y=2143&top_left_x=774)

Is it the case that $f$ is left adjoint to $g$ ? Check that for each $1 \leq p, q \leq 3$, one has $f(p) \leq q$ iff $p \leq g(q)$.

2. Let $f, g$ be the monotone maps shown below:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-040.jpg?height=222&width=680&top_left_y=342&top_left_x=755)

Is it the case that $f$ is left adjoint to $g$ ?

Remark 1.100. The pictures in Exercise 1.99 suggest the following idea. If $P$ and $Q$ are total orders and $f: P \rightarrow Q$ and $g: Q \rightarrow P$ are drawn with arrows bending counterclockwise, then $f$ is left adjoint to $g$ iff the arrows do not cross. With a little bit of thought, this can be formalised. We think this is a pretty neat way of visualizing Galois connections between total orders!

Exercise 1.101.

1. Does $\lceil-/ 3\rceil$ have a left adjoint $L: \mathbb{Z} \rightarrow \mathbb{R}$ ?

2. If not, why? If so, does its left adjoint have a left adjoint?

\subsection*{1.4.2 Back to partitions}

Recall from Example 1.52 that we can understand the set $\operatorname{Prt}(S)$ of partitions on a set $S$ in terms of surjective functions out of $S$.

Suppose we are given any function $g: S \rightarrow T$. We will show that this function $g$ induces a Galois connection $g_{!}: \operatorname{Prt}(S) \leftrightarrows \operatorname{Prt}(T): g^{*}$, between preorder of $S$-partitions and the preorder of $T$-partitions. The way you might explain it to a seasoned category theorist is:

The left adjoint is given by taking any surjection out of $S$ and pushing out along $g$ to get a surjection out of $T$. The right adjoint is given by taking any surjection out of $T$, composing with $g$ to get a function out of $S$, and then taking the epi-mono factorization to get a surjection out of $S$.
![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-040.jpg?height=226&width=874&top_left_y=1855&top_left_x=621)

By the end of this book, the reader will understand pushouts and epi-mono factorizations, so he or she will be able to make sense of the above statement. But for now we will explain the process in more down-to-earth terms.

Start with $g: S \rightarrow T$; we first want to understand $g!: \operatorname{Prt}(S) \rightarrow \operatorname{Prt}(T)$. So start with a partition $\sim_{S}$ of $S$. To begin the process of obtaining a partition $\sim_{T}$ on $T$, say that two elements $t_{1}, t_{2} \in T$ are in the same part, $t_{1} \sim_{T} t_{2}$, if there exist $s_{1}, s_{2} \in S$ with such that $s_{1} \sim s s_{2}$ and $g\left(s_{1}\right)=t_{1}$ and $g\left(s_{2}\right)=t_{2}$. However, the result of doing so will not
necessarily be transitive-you may get $t_{1} \sim_{T} t_{2}$ and $t_{2} \sim_{T} t_{3}$ without $t_{1} \sim_{T}^{?} t_{3}$-and partitions must be transitive. So complete the process by just adding in the missing pieces (take the transitive closure). The result is $g!(\sim S):=\sim T$.

Again starting with $g$, we want to get the right adjoint $g^{*}: \operatorname{Prt}(T) \rightarrow \operatorname{Prt}(S)$. So start with a partition $\sim_{T}$ of $T$. Get a partition $\sim_{S}$ on $S$ by saying that $s_{1} \sim_{S} s_{2}$ iff $g\left(s_{1}\right) \sim_{T} g\left(s_{2}\right)$. The result is $g^{*}\left(\sim_{T}\right):=\sim S$.

Example 1.102. Let $S=\{1,2,3,4\}, T=\{12,3,4\}$, and $g: S \rightarrow T$ by $g(1):=g(2):=12$, $g(3):=3$, and $g(4):=4$. The partition shown left below is translated by $g$ ! to the partition shown on the right.

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-041.jpg?height=369&width=935&top_left_y=813&top_left_x=584)

Exercise 1.103. There are 15 different partitions of a set with four elements. Choose 6 different ones and for each one, call it $c: S \rightarrow P$, find $g!(c)$, where $S, T$, and $g: S \rightarrow T$ are the same as they were in Example 1.102.

Example 1.104. Let $S, T$ be as below, and let $g: S \rightarrow T$ be the function shown in blue. Here is a picture of how $g^{*}$ takes a partition on $T$ and "pulls it back" to a partition on $S$ :
![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-041.jpg?height=350&width=1442&top_left_y=1586&top_left_x=336)

Exercise 1.105. There are five partitions possible on a set with three elements, say $T=\{12,3,4\}$. Using the same $S$ and $g: S \rightarrow T$ as in Example 1.102, determine the partition $g^{*}(c)$ on $S$ for each of the five partitions $c: T \rightarrow P$.

To check that for any function $g: S \rightarrow T$, the monotone map $g_{!}: \operatorname{Prt}(S) \rightarrow \operatorname{Prt}(T)$ really is left adjoint to $g^{*}: \operatorname{Prt}(T) \rightarrow \operatorname{Prt}(S)$ would take too much time for this sketch. But the following exercise gives some evidence.

Exercise 1.106. Let $S, T$, and $g: S \rightarrow T$ be as in Example 1.102.

1. Choose a nontrivial partition $c: S \rightarrow P$ and let $g!(c)$ be its push forward partition on $T$.

2. Choose any coarser partition $d: T \rightarrow P^{\prime}$, i.e. where $g_{!}(c) \leq d$.

3. Choose any non-coarser partition $e: T \rightarrow Q$, i.e. where $g$ ! (c) $\not e$. (If you can't do this, revise your answer for \#1.)

4. Find $g^{*}(d)$ and $g^{*}(e)$.

5. The adjunction formula Eq. (1.96) in this case says that since $g_{!}(c) \leq d$ and $g_{!}(c) \not \leq e$, we should have $c \leq g^{*}(d)$ and $c \not g^{*}(e)$. Show that this is true. $\diamond$

\subsection*{1.4.3 Basic theory of Galois connections}

Proposition 1.107. Suppose that $f: P \rightarrow Q$ and $g: Q \rightarrow P$ are monotone maps. The following are equivalent

(a) $f$ and $g$ form a Galois connection where $f$ is left adjoint to $g$,

(b) for every $p \in P$ and $q \in Q$ we have

$$
\begin{equation*}
p \leq g(f(p)) \quad \text { and } \quad f(g(q)) \leq q \tag{1.108}
\end{equation*}
$$

Proof. Suppose $f$ is left adjoint to $g$. Take any $p \in P$, and let $q:=f(p)$. By reflexivity, we have $f(p) \leq q$, so by the Definition 1.95 of Galois connection we have $p \leq g(q)$, but this means $p \leq g(f(p))$. The proof that $f(g(q)) \leq q$ is similar.

Now suppose that Eq. (1.108) holds for all $p \in P$ and $q \in Q$. We want show that $f(p) \leq q$ iff $p \leq g(q)$. Suppose $f(p) \leq q$; then since $g$ is monotonic, $g(f(p)) \leq g(q)$, but $p \leq g(f(p))$ so $p \leq g(q)$. The proof that $p \leq g(q)$ implies $f(p) \leq q$ is similar.

Exercise 1.109. Complete the proof of Proposition 1.107 by showing that

1. if $f$ is left adjoint to $g$ then for any $q \in Q$, we have $f(g(q)) \leq q$, and

2. if Eq. (1.108) holds, then holds $p \leq g(q)$ iff $f(p) \leq q$ holds, for all $p \in P$ and $q \in Q$.

If we replace $\leq$ with $=$ in Eq. (1.108), we get back the definition of isomorphism (Definition 1.75); this is why we said at the beginning of Section 1.4.1 that Galois connections are a kind of relaxed version of isomorphisms.

Exercise 1.110.

1. Show that if $f: P \rightarrow Q$ has a right adjoint $g$, then it is unique up to isomorphism. That means, for any other right adjoint $g^{\prime}$, we have $g(q) \cong g^{\prime}(q)$ for all $q \in Q$.

2. Is the same true for left adjoints? That is, if $h: P \rightarrow Q$ has a left adjoint, is it necessarily unique up to isomorphism?

Proposition 1.111 (Right adjoints preserve meets). Let $f: P \rightarrow Q$ be left adjoint to $g: Q \rightarrow P$. Suppose $A \subseteq Q$ any subset, and let $g(A):=\{g(a) \mid a \in A\}$ be its image. Then if $A$ has a meet $\bigwedge A \in Q$ then $g(A)$ has a meet $\bigwedge g(A)$ in $P$, and we have

$$
g(\bigwedge A) \cong \bigwedge g(A)
$$

That is, right adjoints preserve meets. Similarly, left adjoints preserve joins: if $A \subseteq P$ is any subset that has a join $\bigvee A \in P$, then $f(A)$ has a join $V f(A)$ in $Q$, and we have

$$
f(\bigvee A) \cong \bigvee f(A)
$$

Proof. Let $f: P \rightarrow Q$ and $g: Q \rightarrow P$ be adjoint monotone maps, with $g$ right adjoint to $f$. Let $A \subseteq Q$ be any subset and let $m:=\wedge A$ be its meet. Then since $g$ is monotone $g(m) \leq g(a)$ for all $a \in A$, so $g(m)$ is a lower bound for the set $g(A)$. We will be done if we can show $g(m)$ is a greatest lower bound.

So take any other lower bound $b$ for $g(A)$; that is suppose that for all $a \in A$, we have $b \leq g(a)$ and we want to show $b \leq g(m)$. Then by definition of $g$ being a right adjoint (Definition 1.95), we also have $f(b) \leq a$. This means that $f(b)$ is a lower bound for $A$ in $Q$. Since the meet $m$ is the greatest lower bound, we have $f(b) \leq m$. Once again using the Galois connection, $b \leq g(m)$, proving that $g(m)$ is indeed the greatest lower bound for $g(A)$, as desired.

The second claim is proved similarly; see Exercise 1.112.

Exercise 1.112. Complete the proof of Proposition 1.111 by showing that left adjoints preserve joins.

Since left adjoints preserve joins, we know that they cannot have generative effects. In fact, we will see in Theorem 1.115 that a monotone map does not have generative effects-i.e. it preserves joins-if and only if it is a left adjoint to some other monotone.

Example 1.113. Right adjoints need not preserve joins. Here is an example:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-043.jpg?height=380&width=919&top_left_y=1870&top_left_x=598)

Let $g$ be the map that preserves labels, and let $f$ be the map that preserves labels as far as possible but with $f(3.9):=4$. Both are $f$ and $g$ monotonic, and one can check that $g$
is right adjoint to $f$ (see Exercise 1.114). But $g$ does not preserve joins because $1 \vee 2=4$ holds in $Q$, whereas $g(1) \vee g(2)=1 \vee 2=3.9 \neq 4=g(4)$ in $P$.

Exercise 1.114. To be sure that $g$ really is right adjoint to $f$ in Example 1.113, there are twelve tiny things to check; do so. That is, for every $p \in P$ and $q \in Q$, check that $f(p) \leq q$ iff $p \leq g(q)$.

Theorem 1.115 (Adjoint functor theorem for preorders). Suppose $Q$ is a preorder that has all meets and let $P$ be any preorder. A monotone map $g: Q \rightarrow P$ preserves meets if and only if it is a right adjoint.

Similarly, if $P$ has all joins and $Q$ is any preorder, a monotone map $f: P \rightarrow Q$ preserves joins if and only if it is a left adjoint.

Proof. We will only prove the claim about meets; the claim about joins follows similarly.

We proved one direction in Proposition 1.111, namely that right adjoints preserve meets. For the other, suppose that $g$ is a monotone map that preserves meets; we shall construct a left adjoint $f$. We define our candidate $f: P \rightarrow Q$ on any $p \in P$ by

$$
\begin{equation*}
f(p):=\bigwedge\{q \in Q \mid p \leq g(q)\} \tag{1.116}
\end{equation*}
$$

this meet is well defined because $Q$ has all meets, but for $f$ to really be a candidate, we need to show it is monotone. So suppose that $p \leq p^{\prime}$. Then $\left\{q^{\prime} \in Q \mid p^{\prime} \leq g\left(q^{\prime}\right)\right\} \subseteq\{q \in$ $Q \mid p \leq g(q)\}$. By Proposition 1.91, this implies $f(p) \leq f\left(p^{\prime}\right)$. Thus $f$ is monotone.

By Proposition 1.111, it suffices to show that $p_{0} \leq g\left(f\left(p_{0}\right)\right)$ and that $f\left(g\left(q_{0}\right)\right) \leq q_{0}$ for all $p_{0} \in P$ and $q_{0} \in Q$. For the first, we have

$$
p_{0} \leq \bigwedge\left\{g(q) \in P \mid p_{0} \leq g(q)\right\} \cong g\left(\bigwedge\left\{q \in Q \mid p_{0} \leq g(q)\right\}\right)=g\left(f\left(p_{0}\right)\right)
$$

where the first inequality follows from the fact that if $p_{0}$ is below every element of a set, then it is below their meet, and the isomorphism is by definition of $g$ preserving meets. For the second, we have

$$
f\left(g\left(q_{0}\right)\right)=\bigwedge\left\{q \in Q \mid g\left(q_{0}\right) \leq g(q)\right\} \leq \bigwedge\left\{q_{0}\right\}=q_{0}
$$

where the first inequality follows from Proposition 1.91 since $\left\{q_{0}\right\} \subseteq\left\{q \in Q \mid g\left(q_{0}\right) \leq\right.$ $g(q)\}$, and the fact that $\bigwedge\left\{q_{0}\right\}=q_{0}$.

Example 1.117. Let $f: A \rightarrow B$ be a function between sets. We can imagine $A$ as a set of apples, $B$ as a set of buckets, and $f$ as putting each apple in a bucket.

Then we have the monotone map $f^{*}: P(Y) \rightarrow P(X)$ that category theorists call "pullback along $f$." This map takes a subset $B^{\prime} \subseteq B$ to its preimage $f^{-1}\left(B^{\prime}\right) \subseteq A$ : that is, it takes a collection $B^{\prime}$ of buckets, and tells you all the apples that they contain in total. This operation is monotonic (more buckets means more apples) and it has both a left and a right adjoint.

The left adjoint $f_{!}(A)$ is given by the direct image: it maps a subset $A^{\prime} \subseteq A$ to

$$
f_{!}\left(A^{\prime}\right):=\left\{b \in B \mid \text { there exists } a \in A^{\prime} \text { such that } f(a)=b\right\}
$$

This map takes a set $A^{\prime}$ of apples, and tells you all the buckets that contain at least one of those apples.

The right adjoint $f_{*}$ maps a subset $A^{\prime} \subseteq A$ to

$$
f_{*}\left(A^{\prime}\right):=\left\{b \in B \mid \text { for all } a \text { such that } f(a)=b, \text { we have } a \in A^{\prime}\right\}
$$

This map takes a set $A^{\prime}$ of apples, and tells you all the buckets $b$ that are all- $A^{\prime}$ : all the apples in $b$ are from the chosen subset $A^{\prime}$. Note that if a bucket doesn't contain any apples at all, then vacuously all its apples are from $A^{\prime}$, so empty buckets count as far as $f_{*}$ is concerned.

Notice that all three of these operations turn out to be interesting: start with a set $B^{\prime}$ of buckets and return all the apples in them, or start with a set $A^{\prime}$ of apples and either find the buckets that contain at least one apple from $A^{\prime}$, or the buckets whose only apples are from $A^{\prime}$. But we did not invent these mappings $f^{*}, f_{!}$, and $f_{*}$ : they were induced by the function $f$. They were automatic. It is one of the pleasures of category theory that adjoints so often turn out to have interesting semantic interpretations.

Exercise 1.118. Choose sets $X$ and $Y$ with between two and four elements each, and choose a function $f: X \rightarrow Y$.

1. Choose two different subsets $B_{1}, B_{2} \subseteq Y$ and find $f^{*}\left(B_{1}\right)$ and $f^{*}\left(B_{2}\right)$.

2. Choose two different subsets $A_{1}, A_{2} \subseteq X$ and find $f_{!}\left(A_{1}\right)$ and $f_{!}\left(A_{2}\right)$.

3. With the same $A_{1}, A_{2} \subseteq X$, find $f_{*}\left(A_{1}\right)$ and $f_{*}\left(A_{2}\right)$.

\subsection*{1.4.4 Closure operators}

Given a Galois connection with $f: P \rightarrow Q$ left adjoint to $g: Q \rightarrow P$, we may compose $f$ and $g$ to arrive at a monotone map $f ; g: P \rightarrow P$ from preorder $P$ to itself. This monotone map has the property that $p \leq(f ; g)(p)$, and that $(f ; g \circ f ; g)(p) \cong(f ; g)(p)$ for any $p \in P$. This is an example of a closure operator. ${ }^{8}$

Exercise 1.119. Suppose that $f$ is left adjoint to $g$. Use Proposition 1.107 to show the following.
1. $p \leq(f ; g)(p)$.
2. $(f ; g \circ f ; g)(p) \cong(f ; g)(p)$. To prove this, show inequalities in both directions, $\leq$ and $\geq$.

Definition 1.120. A closure operator $j: P \rightarrow P$ on a preorder $P$ is a monotone map such that for all $p \in P$ we have
\footnotetext{
${ }^{8}$ The other composite $g \curvearrowleft f$ satisfies the dual properties: $(g \curvearrowleft f)(q) \leq q$ and $(g \circ f \circ g \circ f)(q) \cong(g \cong f)(q)$ for all $q \in Q$. This is called an interior operator, though we will not discuss this concept further.
}
(a) $p \leq j(p)$;

(b) $j(j(p)) \cong j(p)$.

Example 1.121. Here is an example of closure operators from computation, very roughly presented. Imagine computation as a process of rewriting input expressions to output expressions. For example, a computer can rewrite the expression $7+2+3$ as the expression 12. The set of arithmetic expressions has a partial order according to whether one expression can be rewritten as another.

We might think of a computer program, then, as a method of taking an expression and reducing it to another expression. So it is a map $j: \exp \rightarrow \exp$. It furthermore is desirable to require that this computer program is a closure operator. Monotonicity means that if an expression $x$ can be rewritten into expression $y$, then the reduction $j(x)$ can be rewritten into $j(y)$. Moreover, the requirement $x \leq j(x)$ implies that $j$ can only turn one expression into another if doing so is a permissible rewrite. The requirement $j(j(x))=j(x)$ implies if you try to reduce an expression that has already been reduced, the computer program leaves it as is. These properties provide useful structure in the analysis of program semantics.

Example 1.122 (Adjunctions from closure operators). Just as every adjunction gives rise to a closure operator, from every closure operator we may construct an adjunction.

Let $P$ be a preorder and let $j: P \rightarrow P$ be a closure operator. We can define a preorder fix $_{j}$ to have elements the fixed points of $j$; that is,

$$
\operatorname{fix}_{j}:=\{p \in P \mid j(p) \cong p\}
$$

This is a subset of $P$, and inherits an order as a result; hence fix $j_{j}$ is a sub-preorder of $P$. Note that $j(p)$ is a fixed point for all $p \in P$, since $j(j(p)) \cong j(p)$.

We define an adjunction with left adjoint $j: P \rightarrow$ fix $_{j}$ sending $p$ to $j(p)$, and right adjoint $g$ : fix $j_{j} \rightarrow P$ simply the inclusion of the sub-preorder. To see it's really an adjunction, we need to see that for any $p \in P$ and $q \in$ fix $_{j}$, we have $j(p) \leq q$ if and only if $p \leq q$. Let's check it. Since $p \leq j(p)$, we have that $j(p) \leq q$ implies $p \leq q$ by transitivity. Conversely, since $q$ is a fixed point, $p \leq q$ implies $j(p) \leq j(q) \cong q$.

Example 1.123. Another example of closure operators comes from logic. This will be discussed in the final chapter of the book, in particular Section 7.4 .5 , but we will give a quick overview here. In essence, logic is the study of when one formal statement-or proposition-implies another. For example, if $n$ is prime then $n$ is not a multiple of 6 , or if it is raining then the ground is getting wetter. Here " $n$ is prime", " $n$ is not a multiple of 6", "it is raining", and "the ground is getting wetter" are propositions, and
we gave two implications.

Take the set of all propositions, and order them by $p \leq q$ iff $p$ implies $q$, denoted $p \Rightarrow q$. Since $p \Rightarrow p$ and since whenever $p \Rightarrow q$ and $q \Rightarrow r$, we also have $p \Rightarrow r$, this is indeed a preorder.

A closure operator on it is often called a modal operator. It is a function $j$ from propositions to propositions, for which $p \Rightarrow j(p)$ and $j(j(p))=j(p)$. An example of a $j$ is "assuming Bob is in San Diego...." Think of this as a proposition $B$; so "assuming Bob is in San Diego, $p^{\prime \prime}$ means $B \Rightarrow p$. Let's see why $B \Rightarrow-$ is a closure operator.

If ' $p$ ' is true then "assuming Bob is in San Diego, $p$ " is still true. Suppose that "assuming Bob is in San Diego it is the case that, assuming Bob is in San Diego, $p^{\prime}$ is true." It follows that "assuming Bob is in San Diego, $p$ " is true. So we have seen, at least informally, that "assuming Bob is in San Diego..." is a closure operator.

\subsection*{1.4.5 Level shifting}

The last thing we want to discuss in this chapter is a phenomenon that happens often in category theory, something we might informally call "level-shifting." It is easier to give an example of this than to explain it directly.

Given any set $S$, there is a set $\operatorname{Rel}(S)$ of binary relations on $S$. An element $R \in \operatorname{Rel}(S)$ is formally a subset $R \subseteq S \times S$. The set $\operatorname{Rel}(S)$ can be given an order via the subset relation, $R \subseteq R^{\prime}$, i.e. if whenever $R\left(s_{1}, s_{2}\right)$ holds then so does $R^{\prime}\left(s_{1}, s_{2}\right)$.

For example, the Hasse diagram for $\boldsymbol{\operatorname { R e }}(\{1\})$ is:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-047.jpg?height=71&width=241&top_left_y=1458&top_left_x=931)

Exercise 1.124. Draw the Hasse diagram for the preorder $\operatorname{Rel}(\{1,2\})$ of all binary relations on the set $\{1,2\}$.

For any set $S$, there is also a set $\operatorname{Pos}(S)$, consisting of all the preorder relations on $S$. In fact there is a preorder structure $\sqsubseteq$ on $\operatorname{Pos}(S)$, again given by inclusion: $\leq$ is below $\leq^{\prime}$ (we'll write $\leq \sqsubseteq \leq^{\prime}$ ) if $a \leq b$ implies $a \leq^{\prime} b$ for every $a, b \in S$. A preorder of preorder structures? That's what we mean by a level shift.

Every preorder relation is-in particular-a relation, so we have an inclusion $\operatorname{Pos}(S) \rightarrow \operatorname{Rel}(S)$. This is the right adjoint of a Galois connection. Its left adjoint is a monotone map $\mathrm{Cl}: \boldsymbol{\operatorname { R e l }}(S) \rightarrow \operatorname{Pos}(S)$ given by taking any relation $R$, writing it in infix notation using $\leq$, and taking the reflexive and transitive closure, i.e. adding $s \leq s$ for every $s$ and adding $s \leq u$ whenever $s \leq t$ and $t \leq u$.

Exercise 1.125. Let $S=\{1,2,3\}$. Let's try to understand the adjunction discussed above.

1. Come up with any preorder relation $\leq$ on $S$, and define $U(\leq)$ to be the subset $U(\leq):=\left\{\left(s_{1}, s_{2}\right) \mid s_{1} \leq s_{2}\right\} \subseteq S \times S$, i.e. $U(\leq)$ is the image of $\leq$ under the inclusion $\operatorname{Pos}(S) \rightarrow \operatorname{Rel}(S)$, the relation 'underlying' the preorder.

2. Come up with any two binary relations $Q \subseteq S \times S$ and $Q^{\prime} \subseteq S \times S$ such that $Q \subseteq U(\leq)$ but $Q^{\prime} \nsubseteq U(\leq)$. Note that your choice of $Q, Q^{\prime}$ do not have to come from preorders.

We now want to check that in this case, the closure operation $\mathrm{Cl}$ is really left adjoint to the 'underlying relation' map $U$.

3. Concretely (without using the assertion that there is some sort of adjunction), show that $\mathrm{CI}(Q) \sqsubseteq \leq$, where $\sqsubseteq$ is the order on $\operatorname{Pos}(S)$, defined immediately above this exercise.

4. Concretely show that $\mathrm{CI}\left(Q^{\prime}\right) \nsubseteq \leq$.

\subsection*{1.5 Summary and further reading}

In this first chapter, we set the stage for category theory by introducing one of the simplest interesting sorts of example: preorders. From this seemingly simple structure, a bunch of further structure emerges: monotone maps, meets, joins, and more. In terms of modeling real world phenomena, we thought of preorders as the states of a system, and monotone maps as describing a way to use one system to observe another. From this point of view, generative effects occur when observations of the whole cannot be deduced by combining observations of the parts.

In the final section we introduced Galois connections. A Galois connection, or adjunction, is a pair of maps that are like inverses, but allowed to be more "relaxed" by getting the orders involved. Perhaps surprisingly, it turns out adjunctions are closely related to joins and meets: if a preorder $P$ has all joins, then a monotone map out of $P$ is a left adjoint if and only if it preserves joins; similarly for meets and right adjoints.

The next two chapters build significantly on this material, but in two different directions. Chapter 2 adds a new operation on the underlying set: it introduces the idea of a monoidal structure on preorders. This allows us to construct an element $a \otimes b$ of a preorder $P$ from any elements $a, b \in P$, in a way that respects the order. On the other hand, Chapter 3 adds new structure on the order itself: it introduces the idea of a morphism, which describes not only whether $a \leq b$, but gives a name $f$ for how $a$ relates to $b$. This structure is known as a category. These generalizations are both fundamental to the story of compositionality, and in Chapter 4 we'll see them meet in the concept of a monoidal category. The lessons we have learned in this chapter will illuminate the more highly-structured generalizations in the chapters to come. Indeed, it is a useful principle in studying category theory to try to understand concepts first in the setting of preorders-where often much of the complexity is stripped away and one can develop some intuition-before considering the general case.

But perhaps you might be interested in exploring some ideas in this chapter in other directions. While we won't return to them in this book, we learned about generative effects from Elie Adam's thesis [Ada17], and a much richer treatment of generative
effect can be found there. In particular, he discusses abelian categories and cohomology, providing a way to detect generative effects in quite a general setting.

Another important application of preorders, monotone maps, and Galois connections is to the analysis of programming languages. In this setting, preorders describe the possible states of a computer, and monotone maps describe the action of programs, or relationships between different ways of modeling computation states. Galois connections are useful for showing how different models may be closely related, and for transporting program analysis from one framework to another. For more detail on this, see Chapter 4 of the textbook [NNH99].

\section*{Chapter 2}

\section*{Resource theories: \\ Monoidal preorders and enrichment}

\subsection*{2.1 Getting from $a$ to $b$}

You can't make an omelette without breaking an egg. To obtain the things we want requires resources, and the process of transforming what we have into what we want is often an intricate one. In this chapter, we will discuss how monoidal preorders can help us think about this matter.

Consider the following three questions you might ask yourself:
- Given what I have, is it possible to get what I want?
- Given what I have, what is the minimum cost to get what I want?
- Given what I have, what is the set of ways to get what I want?

These questions are about resources-those you have and those you want-but perhaps more importantly, they are about moving from have to want: possibility of, cost of, and ways to.

Such questions come up not only in our lives, but also in science and industry. In chemistry, one asks whether a certain set of compounds can be transformed into another set, how much energy such a reaction will require, or what methods exist for making it happen. In manufacturing, one asks similar questions.

From an external point of view, both a chemist and an industrial firm might be regarded as store-houses of information on the above subjects. The chemist knows which compounds she can make given other ones, and how to do so; the firm has stored knowledge of the same sort. The research work of the chemist and the firm is to use what they know in order to derive-or discover-new knowledge.

This is roughly the first goal of this chapter: to discuss a formalism for expressing recipes-methods for transforming one set of resources into another-and for deriving new recipes from old. The idea here is not complicated, neither in life nor in our mathematical formalism. The value added then is to simply see how it works, so we
can build on it within the book, and so others can build on it in their own work.

We briefly discuss the categorical approach to this idea-namely that of monoidal preorders-for building new recipes from old. The following wiring diagram shows, assuming one knows how to implement each of the interior boxes, how to implement the preparation of a lemon meringue pie:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-052.jpg?height=680&width=1367&top_left_y=557&top_left_x=368)

The wires show resources: we start with prepared crust, lemon, butter, sugar, and egg resources, and we end up with an unbaked pie resource. We could take this whole method and combine it with others, e.g. baking the pie:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-052.jpg?height=303&width=1241&top_left_y=1499&top_left_x=434)

In the above example we see that resources are not always consumed when they are used. For example, we use an oven to convert-or catalyze the transformation of-an unbaked pie into a baked pie, and we get the oven back after we are done. It's a nice feature of ovens! To use economic terms, the oven is a "means of production" for pies.

String diagrams are important mathematical objects that will come up repeatedly in this book. They were invented in the mathematical context-more specifically in the context of monoidal categories-by Joyal and Street [JS93], but they have been used less formally by engineers and scientists in various contexts for a long time.

As we said above, our first goal in this chapter is to use monoidal preorders, and the corresponding wiring diagrams, as a formal language for recipes from old. Our second goal is to discuss something called $\mathcal{V}$-categories for various monoidal preorders v.

A $\mathcal{V}$-category is a set of objects, which one may think of as points on a map, where $\mathcal{V}$ somehow "structures the question" of getting from point $a$ to point $b$. The examples of monoidal preorders $\mathcal{V}$ that we will be most interested in are called Bool and Cost. Roughly speaking, a Bool-category is a set of points where the question of getting from point $a$ to point $b$ has a true / false answer. A Cost-category is a set of points where the question of getting from $a$ to $b$ has an answer $d \in[0, \infty]$, a cost.

This story works in more generality than monoidal preorders. Indeed, in Chapter 4 we will discuss something called a monoidal category, a notion which generalizes monoidal preorders, and we will generalize the definition of $\mathcal{V}$-category accordingly. In this more general setting, $\mathcal{V}$-categories can also address our third question above, describing methods of getting between points. For example a Set-category is a set of points where the question of getting from point $a$ to point $b$ has a set of answers (elements of which might be called methods).

We will begin in Section 2.2 by defining symmetric monoidal preorders, giving a few preliminary examples, and discussing wiring diagrams. We then give many more examples of symmetric monoidal preorders, including both some real-world examples, in the form of resource theories, and some mathematical examples that will come up again throughout the book. In Section 2.3 we discuss enrichment and $\mathcal{V}$-categorieshow a monoidal preorder $\mathcal{V}$ can "structure the question" of getting from $a$ to $b$-and then give some important constructions on $\mathcal{V}$-categories (Section 2.4), and analyze them using a sort of matrix multiplication technique (Section 2.5).

\subsection*{2.2 Symmetric monoidal preorders}

In Section 1.2.2 we introduced preorders. The notation for a preorder, namely $(X, \leq)$, refers to two pieces of structure: a set called $X$ and a relation called $\leq$ that is reflexive and transitive.

We want to add to the concept of preorders a way of combining elements in $X$, an operation taking two elements and adding or multiplying them together. However, the operation does not have to literally be addition or multiplication; it only needs to satisfy some of the properties one expects from them.

\subsection*{2.2.1 Definition and first examples}

We begin with a formal definition of symmetric monoidal preorders.

Definition 2.2. A symmetric monoidal structure on a preorder $(X, \leq)$ consists of two constituents:

(i) an element $I \in X$, called the monoidal unit, and

(ii) a function $\otimes: X \times X \rightarrow X$, called the monoidal product

These constituents must satisfy the following properties, where we write $\otimes\left(x_{1}, x_{2}\right)=$ $x_{1} \otimes x_{2}$ :
(a) for all $x_{1}, x_{2}, y_{1}, y_{2} \in X$, if $x_{1} \leq y_{1}$ and $x_{2} \leq y_{2}$, then $x_{1} \otimes x_{2} \leq y_{1} \otimes y_{2}$,

(b) for all $x \in X$, the equations $I \otimes x=x$ and $x \otimes I=x$ hold,

(c) for all $x, y, z \in X$, the equation $(x \otimes y) \otimes z=x \otimes(y \otimes z)$ holds, and

(d) for all $x, y \in X$, the equation $x \otimes y=y \otimes x$ holds.

We call these conditions monotonicity, unitality, associativity, and symmetry respectively. A preorder equipped with a symmetric monoidal structure, $(X, \leq, I, \otimes)$, is called a symmetric monoidal preorder.

Anyone can propose a set $X$, an order $\leq$ on $X$, an element $I$ in $X$, and a binary operation $\otimes$ on $X$ and ask whether $(X, \leq, I, \otimes)$ is a symmetric monoidal preorder. And it will indeed be one, as long as it satisfies rules a, b, c, and d of Definition 2.2.

Remark 2.3. It is often useful to replace $=$ with $\cong$ throughout Definition 2.2. The result is a perfectly good notion, called a weak monoidal structure. The reason we chose equality is that it makes equations look simpler, which we hope aids first-time readers.

The notation for the monoidal unit and the monoidal product may vary: monoidal units we have seen include $I$ (as in the definition), 0,1 , true, false, $\{*\}$, and more. Monoidal products we have seen include $\otimes$ (as in the definition), $+, *, \wedge, \vee$, and $\times$. The preferred notation in a given setting is whatever best helps our brains remember what we're trying to do; the names $I$ and $\otimes$ are just defaults.

Example 2.4. There is a well-known preorder structure, denoted $\leq$, on the set $\mathbb{R}$ of real numbers; e.g. $-5 \leq \sqrt{2}$. We propose 0 as a monoidal unit and $+: \mathbb{R} \times \mathbb{R} \rightarrow \mathbb{R}$ as a monoidal product. Does $(\mathbb{R}, \leq, 0,+$ ) satisfy the conditions of Definition 2.2?

If $x_{1} \leq y_{1}$ and $x_{2} \leq y_{2}$, it is true that $x_{1}+x_{2} \leq y_{1}+y_{2}$. It is also true that $0+x=x$ and $x+0=x$, that $(x+y)+z=x+(y+z)$, and that $x+y=y+x$. Thus $(\mathbb{R}, \leq, 0,+)$ satisfies the conditions of being a symmetric monoidal preorder.

Exercise 2.5. Consider again the preorder $(\mathbb{R}, \leq)$ from Example 2.4. Someone proposes 1 as a monoidal unit and $*$ (usual multiplication) as a monoidal product. But an expert walks by and says "that won't work." Figure out why, or prove the expert wrong! $\diamond$

Example 2.6. A monoid consists of a set $M$, a function *: $M \times M \rightarrow M$ called the monoid multiplication, and an element $e \in M$ called the monoid unit, such that, when you write $*(m, n)$ as $m * n$, i.e. using infix notation, the equations

$$
\begin{equation*}
m * e=m, \quad e * m=m, \quad(m * n) * p=m *(n * p) \tag{2.7}
\end{equation*}
$$

hold for all $m, n, p \in M$. It is called commutative if also $m * n=n * m$.

Every set $S$ determines a discrete preorder Disc (where $m \leq n$ iff $m=n$; see Example 1.32), and it is easy to check that if $(M, e, *)$ is a commutative monoid then (Disc ${ }_{M},=, e, *$ ) is a symmetric monoidal preorder.

Exercise 2.8. We said it was easy to check that if $(M, *, e)$ is a commutative monoid then ( $\operatorname{Disc}_{M},=, *, e$ ) is a symmetric monoidal preorder. Are we telling the truth? $\diamond$

Example 2.9. Here is a non-example for people who know the game "standard poker." Let $H$ be the set of all poker hands, where a hand means a choice of five cards from the standard 52-card deck. As an order, put $h \leq h^{\prime}$ if $h^{\prime}$ beats or equals $h$ in poker.

One could propose a monoidal product $\otimes: H \times H \rightarrow H$ by assigning $h_{1} \otimes h_{2}$ to be "the best hand one can form out of the ten cards in $h_{1}$ and $h_{2}$." If some cards are in both $h_{1}$ and $h_{2}$, just throw the duplicates away. So for example $\{2 \odot, 3 \odot, 4 \ominus, 6 \boldsymbol{\oplus}, 7 \bullet\} \otimes$ $\{2 \odot, 50,60,6 \oplus, 7 \bullet\}=\{2 \odot, 3 \odot, 4 \ominus, 5 \odot, 6 \bullet\}$, because the latter is the best hand you can make with the former two.

This proposal for a monoidal structure will fail the condition (a) of Definition 2.2: it could be the case that $h_{1} \leq i_{1}$ and $h_{2} \leq i_{2}$, and yet not be the case that $h_{1} \otimes h_{2} \leq i_{1} \otimes i_{2}$. For example, consider this case:

$$
\begin{aligned}
& h_{1}:=\{2 \odot, 3 \odot, 10 \bullet, \mathrm{J} \uparrow, \mathrm{Q} \uparrow\} \quad i_{1}:=\{4 \star, 4 \oplus, 6 \diamond, 6 \diamond, 10 \diamond\} \\
& h_{2}:=\{2 \diamond, 3 \diamond, 4 \diamond, K \bullet, A \bullet\} \quad i_{2}:=\{5 \bullet, 5 \odot, 7 \diamond, J \diamond, Q \diamond\},
\end{aligned}
$$

Here, $h_{1} \leq i_{1}$ and $h_{2} \leq i_{2}$, but $h_{1} \otimes h_{2}=\{10 \bullet, \mathrm{J} \uparrow, \mathrm{Q} \bullet, \mathrm{K} \bullet, \mathrm{A} \bullet\}$ is the best possible hand and beats $i_{1} \otimes i_{2}=\{5 \oplus, 5 \odot, 6 \odot, 6 \diamond, Q \diamond\}$.

Subsections 2.2.3 and 2.2.4 are dedicated to examples of symmetric monoidal preorders. Some are aligned with the notion of resource theories, others come from pure math. When discussing the former, we will use wiring diagrams, so here is a quick primer.

\subsection*{2.2.2 Introducing wiring diagrams}

Wiring diagrams are visual representations for building new relationships from old. In a preorder without a monoidal structure, the only sort of relationship between objects is $\leq$, and the only way you build a new $\leq$ relationship from old ones is by chaining them together. We denote the relationship $x \leq y$ by

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-055.jpg?height=98&width=141&top_left_y=2068&top_left_x=989)

We can chain some number of these $\leq$-relationships-say $0,1,2$, or 3 of them-together in series as shown here

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-055.jpg?height=106&width=1323&top_left_y=2354&top_left_x=390)

If we add a symmetric monoidal structure, we can combine relationships not only in series but also in parallel. Here is an example:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-056.jpg?height=184&width=616&top_left_y=388&top_left_x=749)

Different styles of wiring diagrams In fact, we will see later that there are many styles of wiring diagrams. When we are dealing with preorders, the sort of wiring diagram we can draw is that with single-input, single-output boxes connected in series. When we are dealing with symmetric monoidal preorders, we can have more complex boxes and more complex wiring diagrams, including parallel composition. Later we will see that for other sorts of categorical structures, there are other styles of wiring diagrams:
![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-056.jpg?height=196&width=1330&top_left_y=994&top_left_x=388)

Wiring diagrams for symmetric monoidal preorders The style of wiring diagram that makes sense in any symmetric monoidal preorder is that shown in Eq. (2.12): boxes can have multiple inputs and outputs, and they may be arranged in series and parallel. Symmetric monoidal preorders and their wiring diagrams are tightly coupled with each other. How so?

The answer is that a monoidal preorder $(X, \leq, I, \otimes)$ has some notion of element $(x \in X)$, relationship $(\leq)$, and combination (using transitivity and $\otimes$ ), and so do wiring diagrams: the wires represent elements, the boxes represent relationships, and the wiring diagrams themselves show how relationships can be combined. We call boxes and wires icons; we will encounter several more icons in this chapter, and throughout the book.

To get a bit more rigorous about the connection, let's start with a monoidal preorder $(X, \leq, I, \otimes)$ as in Definition 2.2. Wiring diagrams have wires on the left and the right. Each element $x \in X$ can be made the label of a wire. Note that given two objects $x, y$, we can either draw two wires in parallel—one labeled $x$ and one labeled $y$-or we can draw one wire labeled $x \otimes y$.

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-056.jpg?height=131&width=412&top_left_y=2214&top_left_x=846)

We consider wires in parallel to represent the monoidal product of their labels, so we consider both cases above to represent the element $x \otimes y$. Note also that a wire labeled

I or an absence of wires:

$\bar{I}$ nothing

both represent the monoidal unit $I$; another way of thinking of this is that the unit is the empty monoidal product.

A wiring diagram runs between a set of parallel wires on the left and a set of parallel wires on the right. We say that a wiring diagram is valid if the monoidal product of the elements on the left is less than the monoidal product of those on the right. For example, if we have the inequality $x \leq y$, the the diagram that is a box with a wire labeled $x$ on the left and a wire labeled $y$ on the right is valid; see the first box below:
![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-057.jpg?height=170&width=1000&top_left_y=806&top_left_x=583)

The validity of the second box corresponds to the inequality $x_{1} \otimes x_{2} \leq y_{1} \otimes y_{2} \otimes y_{3}$. Before going on to the properties from Definition 2.2, let us pause for an example of what we've discussed so far.

Example 2.14. Recall the symmetric monoidal preorder $(\mathbb{R}, \leq, 0,+)$ from Example 2.4. The wiring diagrams for it allow wires labeled by real numbers. Drawing wires in parallel corresponds to adding their labels, and the wire labeled 0 is equivalent to no wires at all.

$$
\left.\overline{3.14} \quad \frac{}{-1} \quad \frac{3.14}{-1}\right\}=\frac{}{2.14} \quad=\text { nothing }
$$

And here we express a couple facts about $(\mathbb{R}, \leq, 0,+)$ in this language: $4 \leq 7$ and $2+5 \leq-1+5+3$.

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-057.jpg?height=157&width=583&top_left_y=1754&top_left_x=782)

We now return to how the properties of symmetric monoidal preorders correspond to properties of this sort of wiring diagram. Let's first talk about the order structure: conditions (a)—reflexivity—and (b)—transitivity—from Definition 1.30. Reflexivity says that $x \leq x$, this means the diagram just consisting of a wire

$x$

is always valid. Transitivity allows us to connect facts together: it says that if $x \leq y$ and $y \leq z$, then $x \leq z$. This means that if the diagrams

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-057.jpg?height=93&width=1079&top_left_y=2420&top_left_x=542)
are valid, we can put them together and obtain the valid diagram

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-058.jpg?height=86&width=697&top_left_y=342&top_left_x=733)

Next let's talk about the properties (a)-(d) from the definition of symmetric monoidal structure (Definition 2.2). Property (a) says that if $x_{1} \leq y_{1}$ and $x_{2} \leq y_{2}$ then $x_{1} \otimes x_{2} \leq$ $y_{1} \otimes y_{2}$. This corresponds to the idea that stacking any two valid boxes in parallel is still valid:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-058.jpg?height=225&width=637&top_left_y=652&top_left_x=733)

Condition (b), that $I \otimes x=x$ and $x \otimes I=x$, says we don't need to worry about $I$ or blank space; in particular diagrams such as the following are valid:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-058.jpg?height=124&width=417&top_left_y=1041&top_left_x=846)

Condition (c), that $(x \otimes y) \otimes z=x \otimes(y \otimes z)$ says that we don't have to worry about whether we build up diagrams from the top or from the bottom

$$
\left.\frac{\left.\frac{x}{y}\right\}}{z}\right\}=\left\{\begin{array}{c}
x \otimes y \\
\frac{y}{y \otimes z}=\left\{\frac{y}{z}\right.
\end{array}\right.
$$

But this looks much harder than it is: the associative property should be thought of as saying that there is no distinction between the stuff on the very left above and the stuff on the very right, i.e.

$\frac{x}{\frac{y}{z}}$

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-058.jpg?height=193&width=521&top_left_y=1801&top_left_x=1038)

and indeed a diagram that moves from one to the other is valid.

Finally, the symmetry condition (d), that $x \otimes y=y \otimes x$, says that a diagram is valid even if its wires cross:
![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-058.jpg?height=90&width=594&top_left_y=2248&top_left_x=756)

One may regard the pair of crossing wires as another icon in our iconography, in addition to the boxes and wires we already have.

Wiring diagrams as graphical proofs Given a monoidal preorder $X=(X, \leq, I, \otimes)$, a wiring diagram is a graphical proof of something about $X$. Each box in the diagram has a left side and a right side, say $x$ and $y$, and represents the assertion that $x \leq y$.

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-059.jpg?height=108&width=447&top_left_y=426&top_left_x=861)

A wiring diagram is a bunch of interior boxes connected together inside an exterior box. It represents a graphical proof that says: if all of the interior assertions are correct, then so is the exterior assertion.

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-059.jpg?height=276&width=908&top_left_y=759&top_left_x=606)

The inner boxes in Eq. (2.15) translate into the assertions:

$$
\begin{equation*}
t \leq v+w \quad w+u \leq x+z \quad v+x \leq y \tag{2.16}
\end{equation*}
$$

and the outer box translates into the assertion:

$$
\begin{equation*}
t+u \leq y+z \tag{2.17}
\end{equation*}
$$

The whole wiring diagram 2.15 says "if you know that the assertions in 2.16 are true, then I am a proof that the assertion in 2.17 is also true." What exactly is the proof that diagram 2.15 represents? It is the proof

$$
\begin{equation*}
t+u \leq v+w+u \leq v+x+z \leq y+z \tag{2.18}
\end{equation*}
$$

Indeed, each inequality here is a vertical slice of the diagram 2.15, and the transitivity of these inequalities is expressed by connecting these vertical slices together.

Example 2.19. Recall the lemon meringue pie wiring diagram from Eq. (2.1). It has five interior boxes, such as "separate egg" and "fill crust," and it has one exterior box called "prepare lemon meringue pie." Each box is the assertion that, given the collection of resources on the left, say an egg, you can transform it into the collection of resources on the right, say an egg white and an egg yolk. The whole string diagram is a proof that if each of the interior assertions is true-i.e. you really do know how to separate eggs, make lemon filling, make meringue, fill crust, and add meringue-then the exterior assertion is true: you can prepare a lemon meringue pie.

Exercise 2.20. The string of inequalities in Eq. (2.18) is not quite a proof, because technically there is no such thing as $v+w+u$, for example. Instead, there is $(v+w)+u$ and $v+(w+u)$, and so on.

1. Formally prove, using only the rules of symmetric monoidal preorders (Definition 2.2), that given the assertions in Eq. (2.16), the conclusion in Eq. (2.17) follows.

2. Reflexivity and transitivity should show up in your proof. Make sure you are explicit about where they do.

3. How can you look at the wiring diagram Eq. (2.12) and know that the symmetry axiom (Definition 2.2(d)) does not need to be invoked?

We next discuss some examples of symmetric monoidal preorders. We begin in Section 2.2.3 with some more concrete examples, from science, commerce, and informatics. Then in Section 2.2 .4 we discuss some examples arising from pure math, some of which will get a good deal of use later on, e.g. in Chapter 4.

\subsection*{2.2.3 Applied examples}

Resource theories are studies of how resources are exchanged in a given arena. For example, in social resource theory one studies a marketplace where combinations of goods can be traded for-as well as converted into-other combinations of goods.

Whereas marketplaces are very dynamic, and an apple might be tradable for an orange on Sunday but not on Monday, what we mean by resource theory in this chapter is a static notion: deciding "what buys what," once and for all. ${ }^{1}$ This sort of static notion of conversion might occur in chemistry: the chemical reactions that are possible one day will quite likely be possible on a different day as well. Manufacturing may be somewhere in between: the set of production techniques-whereby a company can convert one set of resources into another-do not change much from day to day.

We learned about resource theories from [CFS16; Fri17], who go much further than we will; see Section 2.6 for more information. In this section we will focus only on the main idea. While there are many beautiful mathematical examples of symmetric monoidal preorders, as we will see in Section 2.2.4, there are also ad hoc examples coming from life experience. In the next chapter, on databases, we will see the same theme: while there are some beautiful mathematical categories out there, database schemas are ad hoc organizational patterns of information. Describing something as "ad hoc" is often considered derogatory, but it just means "formed, arranged, or done for a particular purpose only." There is nothing wrong with doing things for a particular purpose; it's common outside of pure math and pure art. Let's get to it.

Chemistry In high school chemistry, we work with chemical equations, where material collections such as

$\mathrm{H}_{2} \mathrm{O}, \mathrm{NaCl}, 2 \mathrm{NaOH}, \quad \mathrm{CH}_{4}+3 \mathrm{O}_{2}$
\footnotetext{
${ }^{1}$ Using some sort of temporal theory, e.g. the one presented in Chapter 7, one could take the notion here and have it change in time.
}
are put together in the form of reaction equations, such as

$$
2 \mathrm{H}_{2} \mathrm{O}+2 \mathrm{Na} \rightarrow 2 \mathrm{NaOH}+\mathrm{H}_{2}
$$

The collection on the left, $2 \mathrm{H}_{2} \mathrm{O}+2 \mathrm{Na}$ is called the reactant, and the collection on the right, $2 \mathrm{NaOH}+\mathrm{H}_{2}$ is called the product.

We can consider reaction equations such as the one above as taking place inside a single symmetric monoidal preorder (Mat, $\rightarrow, 0,+$ ). Here Mat is the set of all collections of atoms and molecules, sometimes called materials. So we have $\mathrm{NaCl} \in$ Mat and $4 \mathrm{H}_{2} \mathrm{O}+6 \mathrm{Ne} \in$ Mat.

The set Mat has a preorder structure denoted by the $\rightarrow$ symbol, which is the preferred symbol in the setting of chemistry. To be clear, $\rightarrow$ is taking the place of the order relation $\leq$ from Definition 2.2. The + symbol is the preferred notation for the monoidal product in the chemistry setting, taking the place of $\otimes$. While it does not come up in practice, we use 0 to denote the monoidal unit.

Exercise 2.21. Here is an exercise for people familiar with reaction equations: check that conditions (a), (b), (c), and (d) of Definition 2.2 hold.

An important notion in chemistry is that of catalysis: one compound catalyzes a certain reaction. For example, one might have the following set of reactions:

$$
\begin{equation*}
y+k \rightarrow y^{\prime}+k^{\prime} \quad x+y^{\prime} \rightarrow z^{\prime} \quad z^{\prime}+k^{\prime} \rightarrow z+k \tag{2.22}
\end{equation*}
$$

Using the laws of monoidal preorders, we obtain the composed reaction

$$
\begin{equation*}
x+y+k \rightarrow x+y^{\prime}+k^{\prime} \rightarrow z^{\prime}+k^{\prime} \rightarrow z+k \tag{2.23}
\end{equation*}
$$

Here $k$ is the catalyst because it is found both in the reactant and the product of the reaction. It is said to catalyze the reaction $x+y \rightarrow z$. The idea is that the reaction $x+y \rightarrow z$ cannot take place given the reactions in Eq. (2.22). But if $k$ is present, meaning if we add $k$ to both sides, the resulting reaction can take place.

The wiring diagram for the reaction in Eq. (2.23) is shown in Eq. (2.24). The three interior boxes correspond to the three reactions given in Eq. (2.22), and the exterior box corresponds to the composite reaction $x+y+k \rightarrow z+k$.

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-061.jpg?height=260&width=705&top_left_y=1998&top_left_x=707)

Manufacturing Whether we are talking about baking pies, building smart phones, or following pharmaceutical recipes, manufacturing firms need to store basic recipes, and build new recipes by combining simpler recipes in schemes like the one shown in Eq. (2.1) or Eq. (2.24)

The basic idea in manufacturing is exactly the same as that for chemistry, except there is an important assumption we can make in manufacturing that does not hold for chemical reactions:

You can trash anything you want, and it disappears from view.

This simple assumption has caused the world some significant problems, but it is still in effect. In our meringue pie example, we can ask: "what happened to the egg shell, or the paper wrapping the stick of butter"? The answer is they were trashed, i.e. thrown in the garbage bin. It would certainly clutter our diagram and our thinking if we had to carry these resources through the diagram:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-062.jpg?height=683&width=1366&top_left_y=813&top_left_x=369)

Instead, in our daily lives and in manufacturing, we do not have to hold on to something if we don't need it; we can just discard it. In terms of wiring diagrams, this can be shown using a new icon $\longrightarrow$, as follows:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-062.jpg?height=276&width=656&top_left_y=1716&top_left_x=729)

To model this concept of waste using monoidal categories, one just adds an additional axiom to (a), (b), (c), and (d) from Definition 2.2:

(e) $x \leq I$ for all $x \in X$.

(discard axiom)

It says that every $x$ can be converted into the monoidal unit $I$. In the notation of the chemistry section, we would write instead $x \rightarrow 0$ : any $x$ yields nothing. But this is certainly not accepted in the chemistry setting. For example,

$$
\mathrm{H}_{2} \mathrm{O}+\mathrm{NaCl} \rightarrow ? \mathrm{H}_{2} \mathrm{O}
$$
is certainly not a legal chemical equation. It is easy to throw things away in manufacturing, because we assume that we have access to-the ability to grab onto and directly manipulate-each item produced. In chemistry, when you have $10^{23}$ of substance $A$ dissolved in something else, you cannot just simply discard $A$. So axiom (e) is valid in manufacturing but not in chemistry.

Recall that in Section 2.2.2 we said that there were many different styles of wiring diagrams. Now we're saying that adding the discard axiom changes the wiring diagram style, in that it adds this new discard icon that allows wires to terminate, as shown in Eq. (2.25). In informatics, we will change the wiring diagram style yet again.

Informatics A major difference between information and a physical object is that information can be copied. Whereas one cup of butter never becomes two, it is easy for a single email to be sent to two different people. It is much easier to copy a music file than it is to copy a CD. Here we do not mean "copy the information from one compact disc onto another"-of course that's easy-instead, we mean that it's quite difficult to copy the physical disc, thereby forming a second physical disc! In diagrams, the distinction is between the relation

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-063.jpg?height=189&width=740&top_left_y=1239&top_left_x=687)

and the relation

\begin{tabular}{|lc|c|}
\hline Beyoncé cd & \begin{tabular}{c} 
no, I mean \\
literally copy cd!
\end{tabular} & Beyoncé cd \\
\hline & & Beyoncé cd \\
\hline
\end{tabular}

The former is possible, the latter is magic.

Of course material objects can sometimes be copied; cell mitosis is a case in point. But this is a remarkable biological process, certainly not something that is expected for ordinary material objects. In the physical world, we would make mitosis a box transforming one cell into two. But in (classical, not quantum) information, everything can be copied, so we add a new icon to our repertoire.

Namely, in wiring diagram notation, copying information appears as a new icon, $\checkmark$, allowing us to split wires:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-063.jpg?height=180&width=543&top_left_y=2325&top_left_x=783)

Now with two copies of the email, we can send one to Alice and one to Bob.

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-064.jpg?height=273&width=981&top_left_y=335&top_left_x=564)

Information can also be discarded, at least in the conventional way of thinking, so in addition to axioms (a) to (d) from Definition 2.2, we can keep axiom (e) from page 50 and add a new copy axiom:

(f) $x \leq x+x$ for all $x \in X$.

(copy axiom)

allowing us to make mathematical sense of diagrams like Eq. (2.26).

Now that we have examples of monoidal preorders under our belts, let's discuss some nice mathematical examples.

\subsection*{2.2.4 Abstract examples}

In this section we discuss several mathematical examples of symmetric monoidal structures on preorders.

The Booleans The simplest nontrivial preorder is the booleans: $\mathbb{B}=$ \{true, false $\}$ with false $\leq$ true. There are two different symmetric monoidal structures on it.

Example 2.27 (Booleans with AND). We can define a monoidal structure on $\mathbb{B}$ by letting the monoidal unit be true and the monoidal product be $\wedge$ (AND). If one thinks of false $=0$ and true $=1$, then $\wedge$ corresponds to the usual multiplication operation $*$. That is, with this correspondence, the two tables below match up:

\begin{tabular}{|c|c|c|c|}
\hline$\Lambda$ & false true & * & \begin{tabular}{ll}
0 & 1
\end{tabular} \\
\hline false & false false & 0 & 0 \\
\hline true & false true & 1 & 0 \\
\hline
\end{tabular}

One can check that all the properties in Definition 2.2 hold, so we have a monoidal preorder which we denote Bool $:=(\mathbb{B}, \leq$, true,$\wedge)$.

Bool will be important when we get to the notion of enrichment. Enriching in a monoidal preorder $\mathcal{V}=(V, \leq, I, \otimes)$ means "letting $\mathcal{V}$ structure the question of getting from $a$ to $b$." All of the structures of a monoidal preorder-i.e. the set $V$, the ordering relation $\leq$, the monoidal unit $I$, and the monoidal product $\otimes$-play a role in how enrichment works.

For example, let's look at the case of Bool $=(\mathbb{B}, \leq$, true, $\wedge)$. The fact that its underlying set is $\mathbb{B}=\{$ false, true $\}$ will translate into saying that "getting from $a$ to $b$ is a true/false question." The fact that true is the monoidal unit will translate into saying "you can always get from $a$ to $a$." The fact that $\wedge$ is the monoidal product will translate into saying "if you can get from $a$ to $b$ AND you can get from $b$ to $c$ then you can get from $a$ to $c$." Finally, the "if-then" form of the previous sentence is coming from the order relation $\leq$. We will make this more precise in Section 2.3.

We will be able to play the same game with other monoidal preorders, as we will see after we define a monoidal preorder called Cost in Example 2.37.

Some other monoidal preorders It is a bit imprecise to call Bool "the" boolean monoidal preorder, because there is another monoidal structure on $(\mathbb{B}, \leq$ ), which we describe in Exercise 2.29. The first structure, however, seems to be more useful in practice than the second.

Exercise 2.29. Let $(\mathbb{B}, \leq)$ be as above, but now consider the monoidal product to be $\vee$ (OR).

\begin{tabular}{c|cc}
$V$ & false & true \\
\hline false & false & true \\
true & true & true
\end{tabular}

\begin{tabular}{c|ll}
$\max$ & 0 & 1 \\
\hline 0 & 0 & 1 \\
1 & 1 & 1
\end{tabular}

What must the monoidal unit be in order to satisfy the conditions of Definition 2.2?

Does it satisfy the rest of the conditions?

In Example 2.30 and Exercise 2.31 we give two different monoidal structures on the preorder $(\mathbb{N}, \leq$ ) of natural numbers, where $\leq$ is the usual ordering ( $0 \leq 1$ and $5 \leq 16$.

Example 2.30 (Natural numbers with addition). There is a monoidal structure on $(\mathbb{N}, \leq$ ) where the monoidal unit is 0 and the monoidal product is + , i.e. $6+4=10$. It is easy to check that $x_{1} \leq y_{1}$ and $x_{2} \leq y_{2}$ implies $x_{1}+x_{2} \leq y_{1}+y_{2}$, as well as all the other conditions of Definition 2.2.

Exercise 2.31. Show there is a monoidal structure on $(\mathbb{N}, \leq$ ) where the monoidal product is *, i.e. $6 * 4=24$. What should the monoidal unit be?

Example 2.32 (Divisibility and multiplication). Recall from Example 1.45 that there is a "divisibility" order on $\mathbb{N}$ : we write $m \mid n$ to mean that $m$ divides into $n$ without remainder. So $1 \mid m$ for all $m$ and $4 \mid 12$.

There is a monoidal structure on ( $\mathbb{N}, \mid$ ), where the monoidal unit is 1 and the monoidal product is *, i.e. $6 * 4=24$. Then if $x_{1} \mid y_{1}$ and $x_{2} \mid y_{2}$, then $\left(x_{1} * x_{2}\right) \mid\left(y_{1} * y_{2}\right)$. Indeed, if there is some $p_{1}, p_{2} \in \mathbb{N}$ such that $x_{1} * p_{1}=y_{1}$ and $x_{2} * p_{2}=y_{2}$, then $\left(p_{1} * p_{2}\right) *\left(x_{1} * x_{2}\right)=y_{1} * y_{2}$.

Exercise 2.33. Again taking the divisibility order $(\mathbb{N}, \mid)$. Someone proposes 0 as the monoidal unit and + as the monoidal product. Does that proposal satisfy the conditions of Definition 2.2? Why or why not?

Exercise 2.34. Consider the preorder $(P, \leq)$ with Hasse diagram no $\rightarrow$ maybe $\rightarrow$ yes . We propose a monoidal structure with yes as the monoidal unit and "min" as the monoidal product.

1. Make sense of "min" by filling in the multiplication table with elements of $P$.

\begin{tabular}{c|ccc} 
min & no & maybe & yes \\
\hline no & $?$ & $?$ & $?$ \\
maybe & $?$ & $?$ & $?$ \\
yes & $?$ & $?$ & $?$
\end{tabular}

2. Check the axioms of Definition 2.2 hold for NMY := $(P, \leq$, yes, min), given your definition of min. If not, change your definition of min.

Exercise 2.35. Let $S$ be a set and let $\mathrm{P}(S)$ be its power set, the set of all subsets of $S$, including the empty subset, $\varnothing \subseteq S$, and the "everything" subset, $S \subseteq S$. We can give $\mathrm{P}(S)$ an order: $A \leq B$ is given by the subset relation $A \subseteq B$, as discussed in Example 1.50. We propose a symmetric monoidal structure on $\mathrm{P}(S)$ with monoidal unit $S$ and monoidal product given by intersection $A \cap B$.

Does it satisfy the conditions of Definition 2.2?

Exercise 2.36. Let Prop ${ }^{\mathbb{N}}$ denote the set of all mathematical statements one can make about a natural number, where we consider two statements to be the same if one is true if and only if the other is true. For example " $n$ is prime" is an element of Prop ${ }^{\mathbb{N}}$, and so are " $n=2$ " and " $n \geq 11$." The statements " $n+2=5$ " and " $n$ is the least odd prime" are considered the same. Given $P, Q \in \operatorname{Prop}^{\mathbb{N}}$, we say $P \leq Q$ if for all $n \in \mathbb{N}$, whenever $P(n)$ is true, so is $Q(n)$.

Define a monoidal unit and a monoidal product on Prop ${ }^{\mathbb{N}}$ that satisfy the conditions of Definition 2.2.

The monoidal preorder Cost As we said above, when we enrich in monoidal preorders we see them as different ways to structure the question of "getting from here to there." We will explain this in more detail in Section 2.3. The following monoidal preorder will eventually structure a notion of distance or cost for getting from here to there.

Example 2.37 (Lawvere's monoidal preorder, Cost). Let $[0, \infty]$ denote the set of nonnegative real numbers-such as $0,1,15.33 \overline{3}$, and $2 \pi$-together with $\infty$. Consider the preorder $([0, \infty], \geq)$, with the usual notion of $\geq$, where of course $\infty \geq x$ for all $x \in[0, \infty]$.

There is a monoidal structure on this preorder, where the monoidal unit is 0 and the monoidal product is + . In particular, $x+\infty=\infty$ for any $x \in[0, \infty]$. Let's call this
monoidal preorder

$$
\text { Cost }:=([0, \infty], \geq, 0,+)
$$

because we can think of the elements of $[0, \infty]$ as costs. In terms of structuring "getting from here to there," Cost seems to say "getting from $a$ to $b$ is a question of cost." The monoidal unit being 0 will translate into saying that you can always get from $a$ to $a$ at no cost. The monoidal product being + will translate into saying that the cost of getting from $a$ to $c$ is at most the cost of getting from $a$ to $b$ plus the cost of getting from $b$ to $c$. Finally, the "at most" in the previous sentence is coming from the $\geq$.

The opposite of a monoidal preorder One can take the opposite of any preorder, just flip the order: $(X, \leq)^{\text {op }}:=(X, \geq)$; see Example 1.58. Proposition 2.38 says that if the preorder had a symmetric monoidal structure, so does its opposite.

Proposition 2.38. Suppose $X=(X, \leq)$ is a preorder and $X^{\circ p}=(X, \geq)$ is its opposite. If $(X, \leq, I, \otimes)$ is a symmetric monoidal preorder then so is its opposite, $(X, \geq, I, \otimes)$.

Proof. Let's first check monotonicity. Suppose $x_{1} \geq y_{1}$ and $x_{2} \geq y_{2}$ in $X^{\text {op }}$; we need to show that $x_{1} \otimes x_{2} \geq y_{1} \otimes y_{2}$. But by definition of opposite order, we have $y_{1} \leq x_{1}$ and $y_{2} \leq x_{2}$ in $X$, and thus $y_{1} \otimes y_{2} \leq x_{1} \otimes x_{2}$ in $X$. Thus indeed $x_{1} \otimes x_{2} \geq y_{1} \otimes y_{2}$ in $X^{\text {op }}$. The other three conditions are even easier; see Exercise 2.39.

Exercise 2.39. Complete the proof of Proposition 2.38 by proving that the three remaining conditions of Definition 2.2 are satisfied.

Exercise 2.40. Since Cost is a symmetric monoidal preorder, Proposition 2.38 says that Cost $^{\mathrm{op}}$ is too.

1. What is Cost $^{\text {op }}$ as a preorder?

2. What is its monoidal unit?

3. What is its monoidal product?

\subsection*{2.2.5 Monoidal monotone maps}

Recall from Example 1.49 that for any preorder $(X, \leq)$, there is an induced equivalence relation $\cong$ on $X$, where $x \cong x^{\prime}$ iff both $x \leq x^{\prime}$ and $x^{\prime} \leq x$.

Definition 2.41. Let $\mathcal{P}=\left(P, \leq_{P}, I_{P}, \otimes_{P}\right)$ and $Q=\left(Q, \leq_{Q}, I_{Q}, \otimes_{Q}\right)$ be monoidal preorders. A monoidal monotone from $\mathcal{P}$ to $Q$ is a monotone map $f:\left(P, \leq_{P}\right) \rightarrow\left(Q, \leq_{Q}\right)$, satisfying two conditions:

(a) $I_{Q} \leq_{Q} f\left(I_{P}\right)$, and

(b) $f\left(p_{1}\right) \otimes_{Q} f\left(p_{2}\right) \leq_{Q} f\left(p_{1} \otimes_{P} p_{2}\right)$

for all $p_{1}, p_{2} \in P$.

There are strengthenings of these conditions that are also important. If $f$ satisfies the following conditions, it is called a strong monoidal monotone:
(a') $I_{Q} \cong f\left(I_{P}\right)$, and

(b') $f\left(p_{1}\right) \otimes_{Q} f\left(p_{2}\right) \cong f\left(p_{1} \otimes_{P} p_{2}\right)$;

and if it satisfies the following conditions it is called a strict monoidal monotone:

(a") $I_{Q}=f\left(I_{P}\right)$, and

(b") $f\left(p_{1}\right) \otimes_{Q} f\left(p_{2}\right)=f\left(p_{1} \otimes_{P} p_{2}\right)$.

Monoidal monotones are examples of monoidal functors, which we will see various incarnations of throughout the book; see Definition 6.68. What we call monoidal monotones could also be called lax monoidal monotones, and there is a dual notion of oplax monoidal monotones, where the inequalities in (a) and (b) are reversed; we will not use oplaxity in this book.

Example 2.42. There is a monoidal monotone $i:(\mathbb{N}, \leq, 0,+) \rightarrow(\mathbb{R}, \leq, 0,+)$, where $i(n)=$ $n$ for all $n \in \mathbb{N}$. It is clearly monotonic, $m \leq n$ implies $i(m) \leq i(n)$. It is even strict monoidal because $i(0)=0$ and $i(m+n)=i(m)+i(n)$.

There is also a monoidal monotone $f:(\mathbb{R}, \leq, 0,+) \rightarrow(\mathbb{N}, \leq, 0,+)$ going the other way. Here $f(x):=\lfloor x\rfloor$ is the floor function, e.g. $f(3.14)=3$. It is monotonic because $x \leq y$ implies $f(x) \leq f(y)$. Also $f(0)=0$ and $f(x)+f(y) \leq f(x+y)$, so it is a monoidal monotone. But it is not strict or even strong because $f(0.5)+f(0.5) \neq f(0.5+0.5)$.

Recall Bool $=(\mathbb{B}, \leq$, true,$\wedge)$ from Example 2.27 and Cost $=([0, \infty], \geq, 0,+)$ from Example 2.37. There is a monoidal monotone $g:$ Bool $\rightarrow$ Cost, given by $g$ (false) $:=\infty$ and $g$ (true) $:=0$.

Exercise 2.43.

1. Check that the map $g:(\mathbb{B}, \leq$, true,$\wedge) \rightarrow([0, \infty], \geq, 0,+)$ presented above indeed
- is monotonic,
- satisfies condition (a) of Definition 2.41, and
- satisfies condition (b) of Definition 2.41.

2. Is $g$ strict?

Exercise 2.44. Let Bool and Cost be as above, and consider the following quasi-inverse functions $d, u:[0, \infty] \rightarrow \mathbb{B}$ defined as follows:

$$
d(x):=\left\{\begin{array}{ll}
\text { false } & \text { if } x>0 \\
\text { true } & \text { if } x=0
\end{array} \quad u(x):= \begin{cases}\text { false } & \text { if } x=\infty \\
\text { true } & \text { if } x<\infty\end{cases}\right.
$$

1. Is $d$ monotonic?

2. Does $d$ satisfy conditions (a) and (b) of Definition 2.41?

3. Is $d$ strict?

4. Is $u$ monotonic?

5. Does $u$ satisfy conditions (a) and (b) of Definition 2.41?

6. Is $u$ strict?

1. Is $(\mathbb{N}, \leq, 1, *)$ a monoidal preorder, where $*$ is the usual multiplication of natural numbers?

2. If not, why not? If so, does there exist a monoidal monotone $(\mathbb{N}, \leq, 0,+) \rightarrow(\mathbb{N}, \leq$ $, 1, *)$ ? If not; why not? If so, find it.

3. Is $(\mathbb{Z}, \leq, *, 1)$ a monoidal preorder?

$\diamond$

\subsection*{2.3 Enrichment}

In this section we will introduce $\mathcal{V}$-categories, where $\mathcal{V}$ is a symmetric monoidal preorder. We will see that Bool-categories are preorders, and that Cost-categories are a nice generalization of the notion of metric space.

\subsection*{2.3.1 $\mathcal{V}$-categories}

While $\mathcal{V}$-categories can be defined even when $\mathcal{V}$ is not symmetric, i.e. just obeys conditions (a)-(c) of Definition 2.2, certain things don't work quite right. For example, we will see later in Exercise 2.75 that the symmetry condition is necessary in order for products of $\mathcal{V}$-categories to exist. Anyway, here's the definition.

Definition 2.46. Let $\mathcal{V}=(V, \leq, I, \otimes)$ be a symmetric monoidal preorder. A $\mathcal{V}$-category $x$ consists of two constituents, satisfying two properties. To specify $x$,

(i) one specifies a set $\operatorname{Ob}(X)$, elements of which are called objects;

(ii) for every two objects $x, y$, one specifies an element $X(x, y) \in V$, called the homobject. ${ }^{2}$

The above constituents are required to satisfy two properties:

(a) for every object $x \in \operatorname{Ob}(X)$ we have $I \leq X(x, x)$, and

(b) for every three objects $x, y, z \in \operatorname{Ob}(X)$, we have $X(x, y) \otimes X(y, z) \leq X(x, z)$. We call $\mathcal{V}$ the base of the enrichment for $X$ or say that $\mathcal{X}$ is enriched in $\mathcal{V}$.

Example 2.47. As we shall see in the next subsection, from every preorder we can construct a Bool-category, and vice versa. So, to get a feel for $\mathcal{\nu}$-categories, let us
\footnotetext{
${ }^{2}$ The word "hom" is short for homomorphism and reflects the origins of this subject. A more descriptive name for $X(x, y)$ might be mapping object, but we use "hom" mainly because it is an important jargon word to know in the field.
}
consider the preorder generated by the Hasse diagram:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-070.jpg?height=379&width=463&top_left_y=328&top_left_x=820)

How does this correspond to a Bool-category $X$ ? Well, the objects of $\mathcal{X}$ are simply the elements of the preorder, i.e. $\operatorname{Ob}(\mathcal{X})=\{p, q, r, s, t\}$. Next, for every pair of objects $(x, y)$ we need an element of $\mathbb{B}=\{$ false, true $\}$ : simply take true if $x \leq y$, and false if otherwise. So for example, since $s \leq t$ and $t \not s$, we have $X(s, t)=$ true and $X(t, s)=$ false. Recalling from Example 2.27 that the monoidal unit $I$ of Bool is true, it's straightforward to check that this obeys both (a) and (b), so we have a Bool-category.

In general, it's sometimes convenient to represent a $\mathcal{V}$-category $\mathcal{X}$ with a square matrix. The rows and columns of the matrix correspond to the objects of $X$, and the $(x, y)$ th entry is simply the hom-object $X(x, y)$. So, for example, the above preorder in Eq. (2.48) can be represented by the matrix

\begin{tabular}{c|ccccc}
$\not$ & $p$ & $q$ & $r$ & $s$ & $t$ \\
\hline$p$ & true & true & true & true & true \\
$q$ & false & true & false & true & true \\
$r$ & false & false & true & true & true \\
$s$ & false & false & false & true & true \\
$t$ & false & false & false & false & true
\end{tabular}

\subsection*{2.3.2 Preorders as Bool-categories}

Our colleague Peter Gates has called category theory "a primordial ooze," because so much of it can be defined in terms of other parts of it. There is nowhere to rightly call the beginning, because that beginning can be defined in terms of something else. So be it; this is part of the fun.

Theorem 2.49. There is a one-to-one correspondence between preorders and Boolcategories.

Here we find ourselves in the ooze, because we are saying that preorders are the same as Bool-categories, whereas Bool is itself a preorder. "So then Bool is like... enriched in itself?" Yes, every preorder, including Bool, is enriched in Bool, as we will now see.

Proof of Theorem 2.49. Let's check that we can construct a preorder from any Boolcategory. Since $\mathbb{B}=\{$ false, true $\}$, Definition 2.46 says a Bool-category consists of two things:

(i) a set $\mathrm{Ob}(X)$, and

(ii) for every $x, y \in \operatorname{Ob}(X)$ an element $X(x, y) \in \mathbb{B}$, i.e. either $X(x, y)=$ true or $X(x, y)=$ false.

We will use these two things to begin forming a preorder whose elements are the objects of $X$. So let's call the preorder $(X, \leq)$, and let $X:=\operatorname{Ob}(X)$. For the $\leq$ relation, let's declare $x \leq y$ iff $X(x, y)=$ true. We have the makings of a preorder, but for it to work, the $\leq$ relation must be reflexive and transitive. Let's see if we get these from the properties guaranteed by Definition 2.46:

(a) for every element $x \in X$ we have true $\leq X(x, x)$,

(b) for every three elements $x, y, z \in X$ we have $X(x, y) \wedge X(y, z) \leq X(x, z)$.

For $b \in$ Bool, if true $\leq b$ then $b=$ true, so the first statement says $X(x, x)=$ true, which means $x \leq x$. For the second statement, one can consult Eq. (2.28). Since false $\leq b$ for all $b \in \mathbb{B}$, the only way statement (b) has any force is if $X(x, y)=$ true and $X(y, z)=$ true, in which case it forces $X(x, z)=$ true. This condition exactly translates as saying that $x \leq y$ and $y \leq z$ implies $x \leq z$. Thus we have obtained reflexivity and transitivity from the two axioms of Bool-categories.

In Example 2.47, we constructed a Bool-category from a preorder. We leave it to the reader to generalize this example and show that the two constructions are inverses; see Exercise 2.50.

Exercise 2.50 .

1. Start with a preorder $(P, \leq)$, and use it to define a Bool-category as we did in Example 2.47. In the proof of Theorem 2.49 we showed how to turn that Boolcategory back into a preorder. Show that doing so, you get the preorder you started with.

2. Similarly, show that if you turn a Bool-category into a preorder using the above proof, and then turn the preorder back into a Bool-category using your method, you get the Bool-category you started with.

$\diamond$

We now discuss a beautiful application of the notion of enriched categories: metric spaces.

\subsection*{2.3.3 Lawvere metric spaces}

Metric spaces offer a precise way to describe spaces of points, each pair of which is separated by some distance. Here is the usual definition:

Definition 2.51. A metric space $(X, d)$ consists of:

(i) a set $X$, elements of which are called points, and

(ii) a function $d: X \times X \rightarrow \mathbb{R}_{\geq 0}$, where $d(x, y)$ is called the distance between $x$ and $y$.

These constituents must satisfy four properties:

(a) for every $x \in X$, we have $d(x, x)=0$,

(b) for every $x, y \in X$, if $d(x, y)=0$ then $x=y$,

(c) for every $x, y \in X$, we have $d(x, y)=d(y, x)$, and

(d) for every $x, y, z \in X$, we have $d(x, y)+d(y, z) \geq d(x, z)$.

The fourth property is called the triangle inequality.

If we ask instead in (ii) for a function $d: X \times X \rightarrow[0, \infty]=\mathbb{R} \geq 0 \cup\{\infty\}$, we call $(X, d)$ an extended metric space.

The triangle inequality says that when plotting a route from $x$ to $z$, the distance is always at most what you get by choosing an intermediate point $y$ and going $x \rightarrow y \rightarrow z$.

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-072.jpg?height=268&width=423&top_left_y=869&top_left_x=843)

It can be invoked three different ways in the above picture: $3+5 \geq 7.2$, but also $5+7.2 \geq 3$ and $3+7.2 \geq 5$. Oh yeah, and $5+3 \geq 7.2,7.2+5 \geq 3$ and $7.2+3 \geq 5$.

The triangle inequality wonderfully captures something about distance, as does the fact that $d(x, x)=0$ for any $x$. However, the other two conditions are not quite as general as we would like. Indeed, there are many examples of things that "should" be metric spaces, but which do not satisfy conditions (b) or (c) of Definition 2.51.

For example, what if we take $X$ to be places in your neighborhood, but instead of measuring distance, you want $d(x, y)$ to measure effort to get from $x$ to $y$. Then if there are any hills, the symmetry axiom, $d(x, y)=? d(y, x)$, fails: it's easier to get from $x$ downhill to $y$ then to go from $y$ uphill to $x$.

Another way to find a model that breaks the symmetry axiom is to imagine that the elements of $X$ are not points, but whole regions such as the US, Spain, and Boston. Say that the distance from region $A$ to region $B$ is understood using the setup "I will put you in an arbitrary part of $A$ and you just have to get anywhere in $B$; what is the distance in the worst-case scenario?" So $d$ (US, Spain) is the distance from somewhere in the western US to the western tip of Spain: you just have to get into Spain, but you start in the worst possible part of the US for doing so.

Exercise 2.52. Which distance is bigger under the above description, $d$ (Spain,US) or $d($ US, Spain $)$ ?

This notion of distance, which is strongly related to something called Hausdorff distance, ${ }^{3}$ will again satisfy the triangle inequality, but it violates the symmetry condition. It also violates another condition, because $d$ (Boston,US) $=0$. No matter where you
\footnotetext{
${ }^{3}$ The Hausdorff distance gives a metric on the set of all subsets $U \subseteq X$ of a given metric space $(X, d)$.
}
are in Boston, the distance to the nearest point of the US is 0 . On the other hand, $d($ US, Boston $) \neq 0$.

Finally, one can imagine a use for distances that are not finite. In terms of my effort, the distance from here to Pluto is $\infty$, and it would not be any better if Pluto was still a planet. Similarly, in terms of Hausdorff distance, discussed above, the distance between two regions is often infinite, e.g. the distance between $\{r \in \mathbb{R} \mid r<0\}$ and $\{0\}$ as subsets of $(\mathbb{R}, d)$ is infinite.

When we drop conditions (b) and (c) and allow for infinite distances, we get the following relaxed notion of metric space, first proposed by Lawvere. Recall the symmetric monoidal preorder Cost $=([0, \infty], \geq, 0,+)$ from Example 2.37.

Definition 2.53. A Lawvere metric space is a Cost-category.

This is a very compact definition, but it packs a punch. Let's work out what it means, by relating it to the usual definition of metric space. By Definition 2.46, a Cost-category $X$ consists of:

(i) a set $\operatorname{Ob}(X)$,

(ii) for every $x, y \in \operatorname{Ob}(X)$ an element $\mathcal{X}(x, y) \in[0, \infty]$.

Here the set $\operatorname{Ob}(X)$ is playing the role of the set of points, and $X(x, y) \in[0, \infty]$ is playing the role of distance, so let's write a little translator:

$$
X:=\operatorname{Ob}(X) \quad d(x, y):=X(x, y)
$$

The properties of a category enriched in Cost are:

(a) $0 \geq d(x, x)$ for all $x \in X$, and

(b) $d(x, y)+d(y, z) \geq d(x, z)$ for all $x, y, z \in X$.

Since $d(x, x) \in[0, \infty]$, if $0 \geq d(x, x)$ then $d(x, x)=0$. So the first condition is equivalent to the first condition from Definition 2.51, namely $d(x, x)=0$. The second condition is the triangle inequality.

Example 2.54. The set $\mathbb{R}$ of real numbers can be given a metric space structure, and hence a Lawvere metric space structure. Namely $d(x, y):=|y-x|$, the absolute value of the difference. So $d(3,7)=4$.

Exercise 2.55. Consider the symmetric monoidal preorder $\left(\mathbb{R}_{\geq 0} \geq, 0,+\right)$, which is almost the same as Cost, except it does not include $\infty$. How would you characterize the difference between a Lawvere metric space and a $\left(\mathbb{R}_{\geq 0}, \geq, 0,+\right)$-category in the sense of Definition 2.46?

One first defines

$$
d_{L}(U, V):=\sup _{u \in U} \inf d(u, v)
$$

and this is exactly the formula we intend above; the result will be a Lawvere metric space. However, if one wants the Hausdorff distance to define a (symmetric) metric, as in Definition 2.51, one must take the above formula and symmetrize it: $d(U, V):=\max \left(d_{L}(U, V), d_{L}(V, U)\right)$. We happen to see the unsymmetrized notion as more interesting.

Presenting metric spaces with weighted graphs Just as one can convert a Hasse diagram into a preorder, one can convert any weighted graph—a graph whose edges are labeled with numbers $w \geq 0$-into a Lawvere metric space. In fact, we shall consider these as graphs labelled with elements of $[0, \infty]$, and more precisely call them Cost-weighted graphs. ${ }^{4}$

One might think of a Cost-weighted graph as describing a city with some one-way roads (a two-way road is modeled as two one-way roads), each having some effort-totraverse, which for simplicity we just call length. For example, consider the following weighted graphs:
![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-074.jpg?height=356&width=1024&top_left_y=782&top_left_x=543)

Given a weighted graph, one forms a metric $d_{X}$ on its set $X$ of vertices by setting $d(p, q)$ to be the length of the shortest path from $p$ to $q$. For example, here is the the table of distances for $Y$

$$
\begin{array}{c|ccc}
d(\nearrow) & x & y & z  \tag{2.57}\\
\hline x & 0 & 4 & 3 \\
y & 3 & 0 & 6 \\
z & 7 & 4 & 0
\end{array}
$$

Exercise 2.58. Fill out the following table of distances in the weighted graph $X$ from Eq. (2.56)

\begin{tabular}{c|cccc}
$d(\nearrow)$ & $A$ & $B$ & $C$ & $D$ \\
\hline$A$ & 0 & $?$ & $?$ & $?$ \\
$B$ & 2 & $?$ & 5 & $?$ \\
$C$ & $?$ & $?$ & $?$ & $?$ \\
$D$ & $?$ & $?$ & $?$ & $?$
\end{tabular}

Above we converted a weighted graph G, e.g. as shown in Eq. (2.56), into a table of distances, but this takes a bit of thinking. There is a more direct construction for taking $G$ and getting a square matrix $M_{G}$, whose rows and columns are indexed by the vertices of $G$. To do so, set $M_{G}$ to be 0 along the diagonal, to be $\infty$ wherever an edge is missing, and to be the edge weight if there is an edge.
\footnotetext{
${ }^{4}$ This generalizes Hasse diagrams, which we could call Bool-weighted graphs-the edges of a Hasse diagram are thought of as weighted with true; we simply ignore any edges that are weighted with false, and neglect to even draw them!
}

For example, the matrix associated to $Y$ in Eq. (2.56) would be

$$
M_{Y}:=\begin{array}{c|ccc}
\nearrow & x & y & z  \tag{2.59}\\
\hline x & 0 & 4 & 3 \\
y & 3 & 0 & \infty \\
z & \infty & 4 & 0
\end{array}
$$

As soon as you see how we did this, you'll understand that it takes no thinking to turn a weighted graph $G$ into a matrix $M_{G}$ in this way. We will see later in Section 2.5.3 that the more difficult "distance matrices" $d_{Y}$, such as Eq. (2.57), can be obtained from the easy graph matrices $M_{Y}$, such as Eq. (2.59), by repeating a certain sort of "matrix multiplication."

Exercise 2.60. Fill out the matrix $M_{X}$ associated to the graph $X$ in Eq. (2.56):

$M_{X}=$\begin{tabular}{c|cccc}
$\nearrow$ & $A$ & $B$ & $C$ & $D$ \\
\hline$A$ & 0 & $?$ & $?$ & $?$ \\
$B$ & 2 & 0 & $\infty$ & $?$ \\
$C$ & $?$ & $?$ & $?$ & $?$ \\
$D$ & $?$ & $?$ & $?$ & $?$
\end{tabular}

\subsection*{2.3.4 $\quad \mathcal{V}$-variations on preorders and metric spaces}

We have told the story of Bool and Cost. But in Section 2.2.4 we gave examples of many other monoidal preorders, and each one serves as the base of enrichment for a kind of enriched category. Which of them are useful? Something only becomes useful when someone finds a use for it. We will find uses for some and not others, though we encourage readers to think about what it would mean to enrich in the various monoidal categories discussed above; maybe they can find a use we have not explored.

Exercise 2.61. Recall the monoidal preorder NMY := $(P, \leq$, yes, min) from Exercise 2.34. Interpret what a NMY-category is.

In the next two exercises, we use $\mathcal{V}$-weighted graphs to construct $\mathcal{V}$-categories. This is possible because we will use preorders that, like Bool and Cost, have joins.

Exercise 2.62. Let $M$ be a set and let $\mathcal{M}:=(\mathrm{P}(M), \subseteq, M, \cap)$ be the monoidal preorder whose elements are subsets of $M$.

Someone gives the following interpretation, "for any set $M$, imagine it as the set of modes of transportation (e.g. car, boat, foot). Then an $\mathcal{M}$-category $\mathcal{X}$ tells you all the modes that will get you from $a$ all the way to $b$, for any two points $a, b \in \operatorname{Ob}(X)$."

1. Draw a graph with four vertices and four or five edges, each labeled with a subset of $M=$ \{car, boat, foot $\}$.

2. From this graph is it possible to construct an $\mathcal{M}$-category, where the hom-object from $x$ to $y$ is computed as follows: for each path $p$ from $x$ to $y$, take the intersection of the sets labelling the edges in $p$. Then, take the union of the these
sets over all paths $p$ from $x$ to $y$. Write out the corresponding four-by-four matrix of hom-objects, and convince yourself that this is indeed an $\mathcal{M}$-category.

3. Does the person's interpretation look right, or is it subtly mistaken somehow? $\diamond$

Exercise 2.63. Consider the monoidal preorder $\mathcal{W}:=(\mathbb{N} \cup\{\infty\}, \leq, \infty, \mathrm{min})$.

1. Draw a small graph labeled by elements of $\mathbb{N} \cup\{\infty\}$.

2. Write out the matrix whose rows and columns are indexed by the nodes in the graph, and whose $(x, y)$ th entry is given by the maximum over all paths $p$ from $x$ to $y$ of the minimum edge label in $p$.

3. Prove that this matrix is the matrix of hom-objects for a $\mathcal{W}$-category. This will give you a feel for how $\mathcal{W}$ works.

4. Make up an interpretation, like that in Exercise 2.62, for how to imagine enrichment in $\mathcal{W}$.

\subsection*{2.4 Constructions on $\mathcal{V}$-categories}

Now that we have a good intuition for what $\mathcal{V}$-categories are, we give three examples of what can be done with $\mathcal{V}$-categories. The first (Section 2.4.1) is known as change of base. This allows us to use a monoidal monotone $f: \mathcal{V} \rightarrow \mathcal{W}$ to construct $\mathcal{W}$-categories from $\mathcal{V}$-categories. The second construction (Section 2.4.2), that of $\mathcal{V}$-functors, allows us to complete the analogy: a preorder is to a Bool-category as a monotone map is to what? The third construction (Section 2.4.2) is known as a $\mathcal{V}$-product, and gives us a way of combining two $\mathcal{V}$-categories.

\subsection*{2.4.1 Changing the base of enrichment}

Any monoidal monotone $\mathcal{V} \rightarrow \mathcal{W}$ between symmetric monoidal preorders lets us convert $\mathcal{V}$-categories into $\mathcal{W}$-categories.

Construction 2.64. Let $f: \mathcal{V} \rightarrow \mathcal{W}$ be a monoidal monotone. Given a $\mathcal{V}$-category $\mathcal{C}$, one forms the associated $\mathcal{W}$-category, say $\mathcal{C}_{f}$ as follows.

(i) We take the same objects: $\mathrm{Ob}\left(\mathcal{C}_{f}\right):=\mathrm{Ob}(\mathcal{C})$.

(ii) For any $c, d \in \operatorname{Ob}(\mathcal{C})$, put $\mathcal{C}_{f}(c, d):=f(\mathcal{C}(c, d))$.

This construction $\mathcal{C}_{f}$ does indeed obey the definition of a $\mathcal{W}$-category, as can be seen by applying Definition 2.41 (of monoidal monotone) and Definition 2.46 (of $\mathcal{V}$-category):

(a) for every $c \in \mathcal{C}$, we have

$$
\begin{aligned}
I_{W} & \leq f\left(I_{V}\right) \\
& \leq f(\mathcal{C}(c, c)) \\
& =\mathcal{C}_{f}(c, c)
\end{aligned}
$$

( $f$ is monoidal monotone)

( $\mathcal{C}$ is $\mathcal{V}$-category)

(definition of $\mathcal{C}_{f}$ )
(b) for every $c, d, e \in \operatorname{Ob}(\mathcal{C})$ we have

$$
\begin{array}{rlr}
\mathcal{C}_{f}(c, d) \otimes_{W} \mathcal{C}_{f}(d, e) & =f(\mathcal{C}(c, d)) \otimes_{W} f(\mathcal{C}(d, e)) & \text { (definition of } \mathcal{C}_{f} \text { ) } \\
& \leq f\left(\mathcal{C}(c, d) \otimes_{V} \mathcal{C}(d, e)\right) & \text { ( } f \text { is monoidal monotone) } \\
& \leq f(\mathcal{C}(c, e)) & \text { ( } \mathcal{e} \text { is } \mathcal{V} \text {-category) } \\
& =\mathcal{C}_{f}(c, e) & \text { (definition of } \mathcal{C}_{f} \text { ) }
\end{array}
$$

Example 2.65. As an example, consider the function $f:[0, \infty] \rightarrow$ \{true, false $\}$ given by

$$
f(x):= \begin{cases}\text { true } & \text { if } x=0  \tag{2.66}\\ \text { false } & \text { if } x>0\end{cases}
$$

It is easy to check that $f$ is monotonic and that $f$ preserves the monoidal product and monoidal unit; that is, it's easy to show that $f$ is a monoidal monotone. (Recall Exercise 2.44.)

Thus $f$ lets us convert Lawvere metric spaces into preorders.

Exercise 2.67. Recall the "regions of the world" Lawvere metric space from Exercise 2.52 and the text above it. We just learned that, using the monoidal monotone $f$ in Eq. (2.66), we can convert it to a preorder. Draw the Hasse diagram for the preorder corresponding to the regions: US, Spain, and Boston. How could you interpret this preorder relation?

Exercise 2.68 .

1. Find another monoidal monotone $g:$ Cost $\rightarrow$ Bool different from the one defined in Eq. (2.66).

2. Using Construction 2.64, both your monoidal monotone $g$ and the monoidal monotone $f$ in Eq. (2.66) can be used to convert a Lawvere metric space into a preorder. Find a Lawvere metric space $X$ on which they give different answers, $x_{f} \neq x_{g}$

\subsection*{2.4.2 Enriched functors}

The notion of functor provides the most important type of relationship between categories.

Definition 2.69. Let $X$ and $y$ be $\mathcal{V}$-categories. A $\mathcal{V}$-functor from $X$ to $y$, denoted $F: X \rightarrow y$, consists of one constituent:

(i) a function $F: \mathrm{Ob}(X) \rightarrow \mathrm{Ob}(y)$

subject to one constraint

(a) for all $x_{1}, x_{2} \in \operatorname{Ob}(X)$, one has $X\left(x_{1}, x_{2}\right) \leq y\left(F\left(x_{1}\right), F\left(x_{2}\right)\right)$.

Example 2.70. For example, we have said several times-e.g. in Theorem 2.49-that preorders are Bool-categories, where $X\left(x_{1}, x_{2}\right)=$ true is denoted $x_{1} \leq x_{2}$. One would hope that monotone maps between preorders would correspond exactly to Bool-functors, and that's true. A monotone map $\left(X, \leq_{X}\right) \rightarrow\left(Y, \leq_{Y}\right)$ is a function $F: X \rightarrow Y$ such that for every $x_{1}, x_{2} \in X$, if $x_{1} \leq_{X} x_{2}$ then $F\left(x_{1}\right) \leq_{Y} F\left(x_{2}\right)$. In other words, we have

$$
X\left(x_{1}, x_{2}\right) \leq y\left(F\left(x_{1}\right), F\left(x_{2}\right)\right)
$$

where the above $\leq$ takes place in the enriching category $\mathcal{\nu}=$ Bool; this is exactly the condition from Definition 2.69.

Remark 2.71. In fact, we have what is called an equivalence of categories between the category of preorders and the category of Bool-categories. In the next chapter we will develop the ideas necessary to state what this means precisely (Remark 3.59).

Example 2.72. Lawvere metric spaces are Cost-categories. The definition of Cost-functor should hopefully return a nice notion-a "friend"-from the theory of metric spaces, and it does: it recovers the notion of Lipschitz function. A Lipschitz (or more precisely, 1-Lipschitz) function is one under which the distance between any pair of points does not increase. That is, given Lawvere metric spaces $\left(X, d_{X}\right)$ and $\left(Y, d_{Y}\right)$, a Costfunctor between them is a function $F: X \rightarrow Y$ such that for every $x_{1}, x_{2} \in X$ we have $d_{X}\left(x_{1}, x_{2}\right) \geq d_{Y}\left(F\left(x_{1}\right), F\left(x_{2}\right)\right)$.

Exercise 2.73. The concepts of opposite, dagger, and skeleton (see Examples 1.58 and 1.72 and Remark 1.35) extend from preorders to $\mathcal{V}$-categories. The opposite of a $\mathcal{V}$-category $X$ is denoted $X^{\text {op }}$ and is defined by

(i) $\operatorname{Ob}\left(x^{\circ p}\right):=\operatorname{Ob}(X)$, and

(ii) for all $x, y \in X$, we have $X^{\text {op }}(x, y):=X(y, x)$.

A $\mathcal{V}$-category $\mathcal{X}$ is a dagger $\mathcal{V}$-category if the identity function is a $\mathcal{V}$-functor $\dagger: \mathcal{X} \rightarrow X^{\text {op }}$. And a skeletal $\mathcal{V}$-category is one in which if $I \leq X(x, y)$ and $I \leq X(y, x)$, then $x=y$.

Recall that an extended metric space $(X, d)$ is a Lawvere metric space with two extra properties; see properties (b) and (c) in Definition 2.51.

1. Show that a skeletal dagger Cost-category is an extended metric space.

2. Use Exercise 1.73 to make sense of the following analogy: "preorders are to sets as Lawvere metric spaces are to extended metric spaces."

\subsection*{2.4.3 Product $\mathcal{V}$-categories}

If $\mathcal{V}=(V, \leq, I, \otimes)$ is a symmetric monoidal preorder and $\mathcal{X}$ and $y$ are $\mathcal{V}$-categories, then we can define their $\mathcal{V}$-product, which is a new $\mathcal{V}$-category.

Definition 2.74. Let $X$ and $y$ be $\mathcal{V}$-categories. Define their $\mathcal{V}$-product, or simply product, to be the $\mathcal{V}$-category $x \times y$ with

(i) $\mathrm{Ob}(x \times y):=\mathrm{Ob}(x) \times \mathrm{Ob}(y)$,

(ii) $(x \times y)\left((x, y),\left(x^{\prime}, y^{\prime}\right)\right):=x\left(x, x^{\prime}\right) \otimes y\left(y, y^{\prime}\right)$,

for two objects $(x, y)$ and $\left(x^{\prime}, y^{\prime}\right)$ in $\mathrm{Ob}(x \times y)$.

Product $\mathcal{V}$-categories are indeed $\mathcal{V}$-categories (Definition 2.46); see Exercise 2.75.

Exercise 2.75. Let $\mathcal{X} \times \mathrm{y}$ be the $\mathcal{V}$-product of $\mathcal{V}$-categories as in Definition 2.74.

1. Check that for every object $(x, y) \in \mathrm{Ob}(x \times y)$ we have $I \leq(x \times y)((x, y),(x, y))$.

2. Check that for every three objects $\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right)$, and $\left(x_{3}, y_{3}\right)$, we have

$$
(x \times y)\left(\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right)\right) \otimes(x \times y)\left(\left(x_{2}, y_{2}\right),\left(x_{3}, y_{3}\right)\right) \leq(x \times y)\left(\left(x_{1}, y_{1}\right),\left(x_{3}, y_{3}\right)\right)
$$

3. We said at the start of Section 2.3.1 that the symmetry of $\mathcal{V}$ (condition (d) of Definition 2.2) would be required here. Point out exactly where that condition is used.

When taking the product of two preorders $\left(P, \leq_{P}\right) \times\left(Q, \leq_{Q}\right)$, as first described in Example 1.56, we say that $\left(p_{1}, q_{1}\right) \leq\left(p_{2}, q_{2}\right)$ iff both $p_{1} \leq p_{2}$ AND $q_{1} \leq q_{2}$; the AND is the monoidal product $\otimes$ from of Bool. Thus the product of preorders is an example of a Bool-product.

Example 2.76. Let $x$ and $y$ be the Lawvere metric spaces (i.e. Cost-categories) defined by the following weighted graphs:
![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-079.jpg?height=306&width=808&top_left_y=1525&top_left_x=650)

Their product is defined by taking the product of their sets of objects, so there are six objects in $x \times y$. And the distance $d_{X \times Y}\left((x, y),\left(x^{\prime}, y^{\prime}\right)\right)$ between any two points is given by the sum $d_{X}\left(x, x^{\prime}\right)+d_{Y}\left(y, y^{\prime}\right)$.

Examine the following graph, and make sure you understand how easy it is to
derive from the weighted graphs for $X$ and $y$ in Eq. (2.77):

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-080.jpg?height=350&width=667&top_left_y=321&top_left_x=729)

Exercise 2.78. Consider $\mathbb{R}$ as a Lawvere metric space, i.e. as a Cost-category (see Example 2.54). Form the Cost-product $\mathbb{R} \times \mathbb{R}$. What is the distance from $(5,6)$ to $(-1,4)$ ? Hint: apply Definition 2.74 ; the answer is not $\sqrt{40}$.

In terms of matrices, $\mathcal{V}$-products are also quite straightforward. They generalize what is known as the Kronecker product of matrices. The matrices for $x$ and $y$ in Eq. (2.77) are shown below

\begin{tabular}{|c|c|c|c|c|}
\hline$x$ & $A \quad B$ & C & & \\
\hline$A$ & $0 \quad 2$ & 5 & $y$ & \\
\hline$B$ & $\infty \quad 0$ & 3 & p & 0 \\
\hline$C$ & $\infty \quad \infty$ & 0 & & \\
\hline
\end{tabular}

and their product is as follows:

\begin{tabular}{c||ccc|ccc}
$x \times y$ & $(A, p)$ & $(B, p)$ & $(C, p)$ & $(A, q)$ & $(B, q)$ & $(C, q)$ \\
\hline \hline$(A, p)$ & 0 & 2 & 5 & 5 & 7 & 10 \\
$(B, p)$ & $\infty$ & 0 & 3 & $\infty$ & 5 & 8 \\
$(C, p)$ & $\infty$ & $\infty$ & 0 & $\infty$ & $\infty$ & 5 \\
\hline$(A, q)$ & 8 & 10 & 13 & 0 & 2 & 5 \\
$(B, q)$ & $\infty$ & 8 & 11 & $\infty$ & 0 & 3 \\
$(C, q)$ & $\infty$ & $\infty$ & 8 & $\infty$ & $\infty$ & 0
\end{tabular}

We have drawn the product matrix as a block matrix, where there is one block-shaped like $x$-for every entry of $y$. Make sure you can see each block as the $x$-matrix shifted by an entry in $y$. This comes directly from the formula from Definition 2.74 and the fact that the monoidal product in Cost is + .

\subsection*{2.5 Computing presented $\mathcal{V}$-categories with matrix multiplication}

In Section 2.3.3 we promised a straightforward way to construct the matrix representation of a Cost-category from a Cost-weighted graph. To do this, we use a generalized matrix multiplication. We shall show that this works, not just for Cost, but also for Bool, and many other monoidal preorders. The property required of the preorder is
that of being a unital, commutative quantale. These are preorders with all joins, plus one additional ingredient, being monoidal closed, which we define next, in Section 2.5.1. The definition of a quantale will be given in Section 2.5.2.

\subsection*{2.5.1 Monoidal closed preorders}

The definition of $\mathcal{V}$-category makes sense for any symmetric monoidal preorder $\mathcal{V}$. But that does not mean that any base of enrichment $\mathcal{V}$ is as useful as any other. In this section we define closed monoidal categories, which in particular enrich themselves! "Before you can really enrich others, you should really enrich yourself."

Definition 2.79. A symmetric monoidal preorder $\mathcal{V}=(V, \leq, I, \otimes)$ is called symmetric monoidal closed (or just closed) if, for every two elements $v, w \in V$, there is an element $v \multimap w$ in $\mathcal{V}$, called the hom-element, with the property

$$
\begin{equation*}
(a \otimes v) \leq w \quad \text { iff } \quad a \leq(v \multimap w) \tag{2.80}
\end{equation*}
$$

for all $a, v, w \in V$.

Remark 2.81. The term 'closed' refers to the fact that a hom-element can be constructed for any two elements, so the preorder can be seen as closed under the operation of "taking homs." In later chapters we'll meet the closely-related concepts of compact closed categories (Definition 4.58) and cartesian closed categories (Section 7.2.1) that make this idea more precise. See especially Exercise 7.11.

One can consider the hom-element $v \multimap w$ as a kind of "single-use $v$-to- $w$ converter." So Eq. (2.80) says that $a$ and $v$ are enough to get $w$ if and only if $a$ is enough to get a single-use $v$-to- $w$ converter.

Exercise 2.82. Condition Eq. (2.80) says precisely that there is a Galois connection in the sense of Definition 1.95. Let's prove this fact. In particular, we'll prove that a monoidal preorder is monoidal closed iff, given any $v \in V$, the map $(-\otimes v): V \rightarrow V$ given by multiplying with $v$ has a right adjoint. We write this right adjoint $(v \multimap-): V \rightarrow V$.

1. Using Definition 2.2 , show that $(-\otimes v)$ is monotone.

2. Supposing that $\mathcal{V}$ is closed, show that for all $v, w \in V$ we have $((v \multimap w) \otimes v) \leq w$.

3. Using 2., show that $(v \multimap-)$ is monotone.

4. Conclude that a symmetric monoidal preorder is closed if and only if the monotone map $(-\otimes v)$ has a right adjoint.

Example 2.83. The monoidal preorder Cost $=([0, \infty], \geq, 0,+)$ is monoidal closed. Indeed, for any $x, y \in[0, \infty]$, define $x \multimap y:=\max (0, y-x)$. Then, for any $a, x, y \in[0, \infty]$, we have

$$
a+x \geq y \quad \text { iff } \quad a \geq y-x \quad \text { iff } \quad \max (0, a) \geq \max (0, y-x) \quad \text { iff } \quad a \geq(x \multimap y)
$$
so $\multimap$ satisfies the condition of Eq. (2.80).

Note that we have not considered subtraction in Cost before; we can in fact use monoidal closure to define subtraction in terms of the order and monoidal structure!

Exercise 2.84. Show that Bool $=(\mathbb{B}, \leq$, true,$\wedge)$ is monoidal closed.

Example 2.85. A non-example is $(\mathbb{B}, \leq$, false, $\vee$ ). Indeed, suppose we had a $\multimap$ operator as in Definition 2.79. Note that false $\leq p \multimap q$, for any $p, q$ no matter what $\multimap$ is, because false is less than everything. But using $a=$ false, $p=$ true, and $q=$ false, we then get a contradiction: $(a \vee p) \not \leq q$ and yet $a \leq(p \multimap q)$.

Example 2.86. We started this chapter talking about resource theories. What does the closed structure look like from that perspective? For example, in chemistry it would say that for every two material collections $c, d$ one can form a material collection $c \multimap d$ with the property that for any $a$, one has

$$
a+c \rightarrow d \quad \text { if and only if } \quad a \rightarrow(c \multimap d) .
$$

Or more down to earth, since we have the reaction $2 \mathrm{H}_{2} \mathrm{O}+2 \mathrm{Na} \rightarrow 2 \mathrm{NaOH}+\mathrm{H}_{2}$, we must also have

$$
2 \mathrm{H}_{2} \mathrm{O} \rightarrow\left(2 \mathrm{Na} \multimap\left(2 \mathrm{NaOH}+\mathrm{H}_{2}\right)\right)
$$

So from just two molecules of water, you can form a certain substance, and not many substances fit the bill-our preorder Mat of chemical materials is not closed.

But it is not so far-fetched: this hypothetical new substance $\left(2 \mathrm{Na} \multimap\left(2 \mathrm{NaOH}+\mathrm{H}_{2}\right)\right)$ is not really a substance, but a potential reaction: namely that of converting a sodium to sodium-hydroxide-plus-hydrogen. Two molecules of water unlock that potential.

Proposition 2.87. Suppose $\mathcal{V}=(V, \leq, I, \otimes, \multimap)$ is a symmetric monoidal preorder that is closed. Then

(a) For every $v \in V$, the monotone map $-\otimes v:(V, \leq) \rightarrow(V, \leq)$ is left adjoint to $v \rightarrow-:(V, \leq) \rightarrow(V, \leq)$.

(b) For any element $v \in V$ and set of elements $A \subseteq V$, if the join $\bigvee_{a \in A} a$ exists then so does $\bigvee_{a \in A} v \otimes a$ and we have

$$
\begin{equation*}
\left(v \otimes \bigvee_{a \in A} a\right) \cong \bigvee_{a \in A}(v \otimes a) \tag{2.88}
\end{equation*}
$$

(c) For any $v, w \in V$, we have $v \otimes(v \multimap w) \leq w$.

(d) For any $v \in V$, we have $v \cong(I \multimap v)$.

(e) For any $u, v, w \in V$, we have $(u \multimap v) \otimes(v \multimap w) \leq(u \multimap w)$.

Proof. We go through the claims in order.

(a) The definition of $(-\otimes v)$ being left adjoint to $(v \multimap-)$ is exactly the condition Eq. (2.80); see Definition 1.95 and Exercise 2.82.

(b) This follows from (a), using the fact that left adjoints preserve joins (Proposition 1.111).

(c) This follows from (a), using the equivalent characterisation of Galois connection in Proposition 1.107. More concretely, from reflexivity $(v \multimap w) \leq(v \multimap w)$, we obtain $(v \multimap w) \otimes v \leq w$ Eq. (2.80), and we are done by symmetry, which says $v \otimes(v \multimap w)=(v \multimap w) \otimes v$.

(d) Since $v \otimes I=v \leq v$, Eq. (2.80) says $v \leq(I \multimap v)$. For the other direction, we have $(I \multimap v)=I \otimes(I \multimap v) \leq v$ by $(c)$.

(e) To obtain this inequality, we just need $u \otimes(u \multimap v) \otimes(v \multimap w) \leq w$. But this follows by two applications of (c).

One might read (c) as saying "if I have a $v$ and a single-use $v$-to- $w$ converter, I can have a $w$." One might read (d) as saying "having a $v$ is the same as having a single-use nothing-to- $v$ converter." And one might read (e) as saying "if I have a single-use $u$-to-v converter and a single-use $v$-to- $w$ converter, I can get a single-use $u$-to- $w$ converter.

Remark 2.89. We can consider $\mathcal{V}$ to be enriched in itself. That is, for every $v, w \in \operatorname{Ob}(\mathcal{V})$, we can define $\mathcal{V}(v, w):=(v \multimap w) \in \mathcal{V}$. For this to really be an enrichment, we just need to check the two conditions of Definition 2.46. The first condition $I \leq \mathcal{X}(x, x)=(x \multimap x)$ is satisfied because $I \otimes x \leq x$. The second condition is satisfied by Proposition 2.87(e).

\subsection*{2.5.2 Quantales}

To perform matrix multiplication over a monoidal preorder, we need one more thing: joins. These were first defined in Definition 1.81.

Definition 2.90. A unital commutative quantale is a symmetric monoidal closed preorder $\mathcal{V}=(V, \leq, I, \otimes, \multimap)$ that has all joins: $\bigvee A$ exists for every $A \subseteq V$. In particular, we often denote the empty join by $0:=\bigvee \varnothing$.

Whenever we speak of quantales in this book, we mean unital commutative quantales. We will try to remind the reader of that. There are also very interesting applications of noncommutative quantales; see Section 2.6.

Example 2.91. In Example 2.83, we saw that Cost is monoidal closed. To check whether Cost is a quantale, we take an arbitrary set of elements $A \subseteq[0, \infty]$ and ask if it has a join $V A$. To be a join, it needs to satisfy two properties:
a. $a \geq \vee A$ for all $a \in A$, and

b. if $b \in[0, \infty]$ is any element such that $a \geq b$ for all $a \in A$, then $\bigvee A \geq b$.

In fact we can define such a join: it is typically called the infimum, or greatest lower
bound, of $A .{ }^{5}$ For example, if $A=\{2,3\}$ then $\bigvee A=2$. We have joins for infinite sets too: if $B=\{2.5,2.05,2.005, \ldots\}$, its infimum is 2 . Finally, in order to say that $([0, \infty], \geq)$ has all joins, we need a join to exist for the empty set $A=\varnothing$ too. The first condition becomes vacuous-there are no $a$ 's in $A$-but the second condition says that for any $b \in[0, \infty]$ we have $V \varnothing \geq b$; this means $\bigvee \varnothing=\infty$.

Thus indeed $([0, \infty], \geq)$ has all joins, so Cost is a quantale.

\section*{Exercise 2.92.}

1. What is $\bigvee \varnothing$, which we generally denote 0 , in the case
a. $\mathcal{V}=$ Bool $=(\mathbb{B}, \leq$, true,$\wedge)$ ?
b. $\mathcal{V}=$ Cost $=([0, \infty], \geq, 0,+)$ ?

2. What is the join $x \vee y$ in the case
a. $\mathcal{V}=$ Bool, and $x, y \in \mathbb{B}$ are booleans?
b. $\mathcal{V}=$ Cost, and $x, y \in[0, \infty]$ are distances?

$\diamond$

Exercise 2.93. Show that $\mathbf{B o o l}=(\mathbb{B}, \leq$, true,$\wedge)$ is a quantale.

$\diamond$

Exercise 2.94. Let $S$ be a set and recall the power set monoidal preorder ( $\mathrm{P}(S), \subseteq, S, \cap$ ) from Exercise 2.35. Is it a quantale?

Remark 2.95. One can personify the notion of unital, commutative quantale as a kind of navigator. A navigator is someone who understands "getting from one place to another." Different navigators may care about or understand different aspects-whether one can get from $A$ to $B$, how much time it will take, what modes of travel will work, etc.-but they certainly have some commonalities. Most importantly, a navigator needs to be able to read a map: given routes $A$ to $B$ and $B$ to $C$, they understand how to get a route $A$ to $C$. And they know how to search over the space of way-points to get from $A$ to $C$. These will correspond to the monoidal product and the join operations, respectively.

Proposition 2.96. Let $\mathcal{P}=(P, \leq)$ be a preorder. It has all joins iff it has all meets.

Proof. The joins (resp. meets) in $\mathcal{P}$ are the meets (resp. joins) in $\mathcal{P}^{\text {op }}$, so the two claims are dual: it suffices to show that if $\mathcal{P}$ has all joins then it has all meets.

Suppose $\mathcal{P}$ has all joins and suppose that $A \subseteq \mathcal{P}$ is a subset for which we want the meet. Consider the set $M_{A}:=\{p \in P \mid p \leq a$ for all $a \in A\}$ of elements below everything in $A$. Let $m_{A}:=\bigvee_{p \in M_{A}} p$ be their join. We claim that $m_{A}$ is a meet for $A$.

We first need to know that for any $a \in A$ we have $m_{A} \leq a$, but this is by definition of join: since all $p \in M_{A}$ satisfy $p \leq a$, so does their join $m_{A} \leq a$. We second need to know that for any $m^{\prime} \in P$ with $m^{\prime} \leq a$ for all $a \in A$, we have $m^{\prime} \leq m$. But every such $m^{\prime}$ is actually an element of $M_{A}$ and $m$ is their join, so $m^{\prime} \leq m$. This completes the proof.
\footnotetext{
${ }^{5}$ Here, by the infimum of a subset $A \subseteq[0, \infty]$, we mean infimum in the usual order on $[0, \infty]$ : the largest number that is $\leq$ everything in $A$. For example, the infimum of $\{3.1,3.01,3.001, \ldots\}$ is 3 . But note that this is the supremum in the reversed, $\geq$, order of Cost.
}

In particular, a quantale has all meets and all joins, even though we only define it to have all joins.

Remark 2.97. The notion of Hausdorff distance can be generalized, allowing the role of Cost to be taken by any quantale $\mathcal{V}$. If $\mathcal{X}$ is a $\mathcal{V}$-category with objects $X$, and $U \subseteq X$ and $V \subseteq X$, we can generalize the usual Hausdorff distance, on the left below, to the formula on the right:

$$
d(U, V):=\sup _{u \in U} \inf d(u, v) \quad X(U, V):=\bigwedge_{u \in V} \bigvee_{v \in V} x(u, v)
$$

For example, if $\mathcal{V}=$ Bool, the Hausdorff distance between sub-preorders $U$ and $V$ answers the question "can I get into $V$ from every $u \in U$," i.e. $\forall_{u \in U} \cdot \exists_{v \in V} \cdot u \leq v$. Or for another example, use $\mathcal{V}=\mathrm{P}(M)$ with its interpretation as modes of transportation, as in Exercise 2.62. Then the Hausdorff distance $d(U, V) \in \mathrm{P}(M)$ tells us those modes of transportation that will get us into $V$ from every point in $U$.

Proposition 2.98. Suppose $\mathcal{V}=(V, \leq, I, \otimes)$ is any symmetric monoidal preorder that has all joins. Then $\mathcal{V}$ is closed-i.e. it has a $\multimap$ operation and hence is a quantale-if and only if $\otimes$ distributes over joins; i.e. if Eq. (2.88) holds for all $v \in V$ and $A \subseteq V$.

Proof. We showed one direction in Proposition 2.87(b): if $\mathcal{V}$ is monoidal closed then Eq. (2.88) holds. We need to show that Eq. (2.88) holds then $-\otimes v: V \rightarrow V$ has a right adjoint $v \multimap-$. This is just the adjoint functor theorem, Theorem 1.115. It says we can define $v \multimap w$ to be

$$
v \multimap w:=\bigvee_{\{a \in V \mid a \otimes v \leq w\}} a
$$

\subsection*{2.5.3 Matrix multiplication in a quantale}

A quantale $\mathcal{V}=(V, \leq, I, \otimes, \multimap)$, as defined in Definition 2.79, provides what is necessary to perform matrix multiplication. ${ }^{6}$ The usual formula for matrix multiplication is:

$$
\begin{equation*}
(M * N)(i, k)=\sum_{j} M(i, j) * N(j, k) \tag{2.99}
\end{equation*}
$$

We will get a formula where joins stand in for the sum operation $\sum$, and $\otimes$ stands in for the product operation $*$. Recall our convention of writing $0:=\bigvee \varnothing$.

Definition 2.100. Let $\mathcal{V}=(V, \leq, \otimes, I)$ be a quantale. Given sets $X$ and $Y$, a matrix with entries in $\mathcal{V}$, or simply a $\mathcal{V}$-matrix, is a function $M: X \times Y \rightarrow V$. For any $x \in X$ and $y \in Y$, we call $M(x, y)$ the $(x, y)$-entry.
\footnotetext{
${ }^{6}$ This works for noncommutative quantales as well.
}

Here is how you multiply $\mathcal{V}$-matrices $M: X \times Y \rightarrow V$ and $N: Y \times Z \rightarrow V$. Their product is defined to be the matrix $(M * N): X \times Z \rightarrow V$, whose entries are given by the formula

$$
\begin{equation*}
(M * N)(x, z):=\bigvee_{y \in Y} M(x, y) \otimes N(y, z) \tag{2.101}
\end{equation*}
$$

Note how similar this is to Eq. (2.99).

Example 2.102. Let $\mathcal{V}=$ Bool. Here is an example of matrix multiplication $M * N$. Here $X=\{1,2,3\}, Y=\{1,2\}$, and $Z=\{1,2,3\}$, matrices $M: X \times Y \rightarrow \mathbb{B}$ and $N: Y \times Z \rightarrow \mathbb{B}$ are shown to the left below, and their product is shown to the right:

$$
\left(\begin{array}{cc}
\text { false } & \text { false } \\
\text { false } & \text { true } \\
\text { true } & \text { true }
\end{array}\right) *\left(\begin{array}{ccc}
\text { true } & \text { true } & \text { false } \\
\text { true } & \text { false } & \text { true }
\end{array}\right)=\left(\begin{array}{ccc}
\text { false } & \text { false } & \text { false } \\
\text { true } & \text { false } & \text { true } \\
\text { true } & \text { true } & \text { true }
\end{array}\right)
$$

The identity $V$-matrix on a set $X$ is $I_{X}: X \times X \rightarrow V$ given by

$$
I_{X}(x, y):= \begin{cases}I & \text { if } x=y \\ 0 & \text { if } x \neq y\end{cases}
$$

Exercise 2.103. Write down the $2 \times 2$-identity matrix for each of the quantales $(\mathbb{N}, \leq, 1, *)$, Bool $=(\mathbb{B}, \leq$, true,$\wedge)$, and Cost $=([0, \infty], \geq, 0,+)$.

Exercise 2.104. Let $\mathcal{V}=(V, \leq, I, \otimes, \multimap)$ be a quantale. Use Eq. (2.101) and Proposition 2.87 to prove the following.

1. Prove the identity law: for any sets $X$ and $Y$ and $V$-matrix $M: X \times Y \rightarrow V$, one has $I_{X} * M=M$.

2. Prove the associative law: for any matrices $M: W \times X \rightarrow V, N: X \times Y \rightarrow V$, and $P: Y \times Z \rightarrow V$, one has $(M * N) * P=M *(N * P)$.

Recall the weighted graph $Y$ from Eq. (2.56). One can read off the associated matrix $M_{Y}$, and one can calculate the associated metric $d_{Y}$ :

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-086.jpg?height=360&width=361&top_left_y=1972&top_left_x=489)

\begin{tabular}{c|ccc}
$M_{Y}$ & $x$ & $y$ & $z$ \\
\hline$x$ & 0 & 4 & 3 \\
$y$ & 3 & 0 & $\infty$ \\
$z$ & $\infty$ & 4 & 0
\end{tabular}

\begin{tabular}{c|ccc}
$d_{Y}$ & $x$ & $y$ & $z$ \\
\hline$x$ & 0 & 4 & 3 \\
$y$ & 3 & 0 & 6 \\
$z$ & 7 & 4 & 0
\end{tabular}

Here we fully explain how to compute $d_{Y}$ using only $M_{Y}$.

The matrix $M_{Y}$ can be thought of as recording the length of paths that traverse either 0 or 1 edges: the diagonals being 0 mean we can get from $x$ to $x$ without traversing any
edges. When we can get from $x$ to $y$ in one edge we record its length in $M_{Y}$, otherwise we use $\infty$.

When we multiply $M_{Y}$ by itself using the formula Eq. (2.101), the result $M_{Y}^{2}$ tells us the length of the shortest path traversing 2 edges or fewer. Similarly $M_{Y}^{3}$ tells us about the shortest path traversing 3 edges or fewer:

$$
M_{Y}^{2}=\begin{array}{l|lll}
\nearrow & x & y & z \\
\hline x & 0 & 4 & 3 \\
y & 3 & 0 & 6 \\
z & 7 & 4 & 0
\end{array}
$$

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-087.jpg?height=233&width=371&top_left_y=545&top_left_x=1170)

One sees that the powers stabilize: $M_{Y}^{2}=M_{Y}^{3}$; as soon as that happens one has the matrix of distances, $d_{Y}$. Indeed $M_{Y}^{n}$ records the lengths of the shortest path traverse $n$ edges or fewer, and the powers will always stabilize if the set of vertices is finite, since the shortest path from one vertex to another will never visit a given vertex more than once. $^{7}$

Exercise 2.105. Recall from Exercise 2.60 the matrix $M_{X}$, for $X$ as in Eq. (2.56). Calculate $M_{X}^{2}, M_{X}^{3}$, and $M_{X}^{4}$. Check that $M_{X}^{4}$ is what you got for the distance matrix in Exercise 2.58 .

This procedure gives an algorithm for computing the $\mathcal{V}$-category presented by any $\mathcal{V}$-weighted graph using matrix multiplication.

\subsection*{2.6 Summary and further reading}

In this chapter we thought of elements of preorders as describing resources, with the order detailing whether one resource could be obtained from another. This naturally led to the question of how to describe what could be built from a pair of resources, which led us to consider monoid structures on preorders. More abstractly, these monoidal preorders were seen to be examples of enriched categories, or $\mathcal{V}$-categories, over the symmetric monoidal preorder Bool. Changing Bool to the symmetric monoidal preorder Cost, we arrived upon Lawvere metric spaces, a slight generalization of the usual notion of metric space. In terms of resources, Cost-categories tell us the cost of obtaining one resource from another.

At this point, we sought to get a better feel for $\mathcal{V}$-categories in two ways. First, we introduced various important constructions: base change, functors, products. Second, we looked at how to present $\mathcal{V}$-categories using labelled graphs; here, perhaps surprisingly, we saw that matrix multiplication gives an algorithm to compute the hom-objects from a labelled graph.

Resource theories are discussed in much more detail in [CFS16; Fri17]. The authors provide many more examples of resource theories in science, including in thermody-
\footnotetext{
${ }^{7}$ The method works even in the infinite case: one takes the infimum of all powers $M_{Y}^{n}$. The result always defines a Lawvere metric space.
}
namics, Shannon's theory of communication channels, and quantum entanglement. They also discuss more of the numerical theory than we did, including calculating the asymptotic rate of conversion from one resource into another.

Enrichment is a fundamental notion in category theory, and we will we return to it in Chapter 4, generalizing the definition so that categories, rather than mere preorders, can serve as bases of enrichment. In this more general setting we can still perform the constructions we introduced in Section 2.4 -base change, functors, products-and many others; the authoratitive, but by no means easy, reference on this is the book by Kelly $[K e l 05]$.

While preorders were familiar before category theory came along, Lawvere metric spaces are a beautiful generalization of the previous notion of (symmetric) metric space, that is due to, well, Lawvere. A deeper exploration than the taste we gave here can be found in his classic paper [Law73], where he also discusses ideas like Cauchy completeness in category-theoretic terms, and which hence generalize to other categorical settings.

We observed that while any symmetric monoidal preorder can serve as a base for enrichment, certain preorders-quantales-are better than others. Quantales are well known for links to other parts of mathematics too. The word quantale is in fact a portmanteau of 'quantum locale', where quantum refers to quantum physics, and locale is a fundamental structure in topology. For a book-length introduction of quantales and their applications, one might check [Ros90]. The notion of cartesian closed categories, later generalized to monoidal closed categories, is due to Ronnie Brown [Bro61].

Note that while we have only considered commutative quantales, the noncommutative variety also arise naturally. For example, the power set of any monoid forms a quantale that is commutative iff the monoid is. Another example is the set of all binary relations on a set $X$, where multiplication is relational composition; this is non-commutative. Such noncommutative quantales have application to concurrency theory, and in particular process semantics and automata; see [AV93] for details.

\section*{Chapter 3}

\section*{Databases: \\ Categories, functors, and universal constructions}

\subsection*{3.1 What is a database?}

Integrating data from disparate sources is a major problem in industry today. A study in 2008 [BH08] showed that data integration accounts for $40 \%$ of IT (information technology) budgets, and that the market for data integration software was $\$ 2.5$ billion in 2007 and increasing at a rate of more than $8 \%$ per year. In other words, it is a major problem; but what is it?

A database is a system of interlocking tables. Data becomes information when it is stored in a given formation. That is, the numbers and letters don't mean anything until they are organized, often into a system of interlocking tables. An organized system of interlocking tables is called a database. Here is a favorite example:

\begin{tabular}{c|ccc} 
Employee & FName & WorksIn & Mngr \\
\hline 1 & Alan & 101 & 2 \\
2 & Ruth & 101 & 2 \\
3 & Kris & 102 & 3
\end{tabular}

\begin{tabular}{c|cc} 
Department & DName & Secr \\
\hline 101 & Sales & 1 \\
102 & IT & 3
\end{tabular}

These two tables interlock by use of a special left-hand column, demarcated by a vertical line; it is called the ID column. The ID column of the first table is called 'Employee,' and the ID column of the second table is called 'Department.' The entries in the ID column-e.g. 1, 2, 3 or 101, 102-are like row labels; they indicate a whole row of the table they're in. Thus each row label must be unique (no two rows in a table can have the same label), so that it can unambiguously specify its row.

Each table's ID column, and the set of unique identifiers found therein, is what allows for the interlocking mentioned above. Indeed, other entries in various tables can reference rows in a given table by use of its ID column. For example, each entry in the WorksIn column references a department for each employee; each entry in the Mngr (manager) column references an employee for each employee, and each entry in the Secr (secretary) column references an employee for each department. Managing all this cross-referencing is the purpose of databases.

Looking back at Eq. (3.1), one might notice that every non-ID column, found in either table, is a reference to a label of some sort. Some of these, namely WorksIn, Mngr, and Secr, are internal references, often called foreign keys; they refer to rows (keys) in the ID column of some (foreign) table. Others, namely FName and DName, are external references; they refer to strings or integers, which can also be thought of as labels, whose meaning is known more broadly. Internal reference labels can be changed as long as the change is consistent-1 could be replaced by 1001 everywhere without changing the meaning-whereas external reference labels certainly cannot! Changing Ruth to Bruce everywhere would change how people understood the data.

The reference structure for a given database-i.e. how tables interlock via foreign keys-tells us something about what information was intended to be stored in it. One may visualize the reference structure for Eq. (3.1) graphically as follows:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-090.jpg?height=312&width=1109&top_left_y=1313&top_left_x=497)

This is a kind of "Hasse diagram for a database," much like the Hasse diagrams for preorders in Remark 1.39. How should you read it?

The two tables from Eq. (3.1) are represented in the graph (3.2) by the two black nodes, which are given the same name as the ID columns: Employee and Department. There is another node-drawn white rather than black-which represents the external reference type of strings, like "Alan," "Alpha," and "Sales". The arrows in the diagram represent non-ID columns of the tables; they point in the direction of reference: WorksIn refers an employee to a department.

Exercise 3.3. Count the number of non-ID columns in Eq. (3.1). Count the number of arrows (foreign keys) in Eq. (3.2). They should be the same number in this case; is this a coincidence?

A Hasse-style diagram like the one in Eq. (3.2) can be called a database schema; it represents how the information is being organized, the formation in which the data is kept. One may add rules, sometimes called 'business rules' to the schema, in order to ensure the integrity of the data. If these rules are violated, one knows that data being
entered does not conform to the way the database designers intended. For example, the designers may enforce rules saying
- every department's secretary must work in that department;
- every employee's manager must work in the same department as the employee. Doing so changes the schema, say from 'easySchema' (3.2) to 'mySchema' below.

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-091.jpg?height=417&width=1090&top_left_y=545&top_left_x=512)

In other words, the difference is that easySchema plus constraints equals mySchema.

We will soon see that database schemas are categories $\mathcal{C}$, that the data itself is given by a 'set-valued' functor $\mathcal{C} \rightarrow$ Set, and that databases can be mapped to each other via functors $\mathcal{C} \rightarrow \mathcal{D}$. In other words, there is a relatively large overlap between database theory and category theory. This has been worked out in a number of papers; see Section 3.6. It has also been implemented in working software, called FQL, which stands for functorial query language. Here is example FQL code for the schema shown above:

```
schema mySchema = {
    nodes
        Employee, Department;
    attributes
        DName : Department -> string,
        FName : Employee -> string
    arrows
        Mngr : Employee -> Employee,
        WorksIn : Employee -> Department,
        Secr : Department -> Employee;
    equations
        Department.Secr.WorksIn = Department,
        Employee.Mngr.WorksIn = Employee.WorksIn;
}
```

Communication between databases. We have said that databases are designed to store information about something. But different people or organizations might view the same sort of thing in different ways. For example, one bank stores its financial records according to European standards and another does so according to Japanese standards. If these two banks merge into one, they will need to be able to share their data despite differences in the shape of their database schemas.

Such problems are huge and intricate in general, because databases often comprise hundreds or thousands of interlocking tables. Moreover, these problems occur more frequently than just when companies want to merge. It is quite common that a given company moves data between databases on a daily basis. The reason is that different ways of organizing information are convenient for different purposes. Just like we pack our clothes in a suitcase when traveling but use a closet at home, there is generally not one best way to organize anything.

Category theory provides a mathematical approach for translating between these different organizational forms. That is, it formalizes a sort of automated reorganizing process called data migration, which takes data that fits snugly in one schema and moves it into another.

Here is a simple case. Imagine an airline company has two different databases, perhaps created at different times, that hold roughly the same data.
![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-092.jpg?height=508&width=1230&top_left_y=968&top_left_x=402)

Schema $A$ has more detail than schema $B$-an airline seat may be in first class or economy-but they are roughly the same. We will see that they can be connected by a functor, and that data conforming to $A$ can be migrated through this functor to schema $B$ and vice versa.

The statistics at the beginning of this section show that this sort of problemwhen occurring at enterprise scale-continues to prove difficult and expensive. If one attempts to move data from a source schema to a target schema, the migrated data could fail to fit into the target schema or fail to satisfy some of its constraints. This happens surprisingly often in the world of business: a night may be spent moving data, and the next morning it is found to have arrived broken and unsuitable for further use. In fact, it is believed that over half of database migration projects fail.

In this chapter, we will discuss a category-theoretic method for migrating data. Using categories and functors, one can prove up front that a given data migration will not fail, i.e. that the result is guaranteed to fit into the target schema and satisfy all its constraints.

The material in this chapter gets to the heart of category theory: in particular, we discuss categories, functors, natural transformations, adjunctions, limits, and colimits. In fact, many of these ideas have been present in the discussion above:
- The schema pictures, e.g. Eq. (3.4) depict categories $\mathcal{C}$.
- The instances, e.g. Eq. (3.1) are functors from $\mathcal{C}$ to a certain category called Set.
- The implicit mapping in Eq. (3.5), which takes economy and first class seats in $A$ to airline seats in $B$, constitutes a functor $A \rightarrow B$.
- The notion of data migration for moving data between schemas is formalized by adjoint functors.

We begin in Section 3.2 with the definition of categories and a bunch of different sorts of examples. In Section 3.3 we bring back databases, in particular their instances and the maps between them, by discussing functors and natural transformations. In Section 3.4 we discuss data migration by way of adjunctions, which generalize the Galois connections we introduced in Section 1.4. Finally in Section 3.5 we give a bonus section on limits and colimits. ${ }^{1}$

\subsection*{3.2 Categories}

A category $\mathfrak{C}$ consists of four pieces of data-objects, morphisms, identities, and a composition rule-satisfying two properties.

Definition 3.6. To specify a category $\mathrm{C}$ :

(i) one specifies a collection ${ }^{2} \mathrm{Ob}(\mathcal{C})$, elements of which are called objects.

(ii) for every two objects $c, d$, one specifies a set $\mathcal{C}(c, d),{ }^{3}$ elements of which are called morphisms from $c$ to $d$.

(iii) for every object $c \in \operatorname{Ob}(\mathcal{C})$, one specifies a morphism $\operatorname{id}_{c} \in \mathcal{C}(c, c)$, called the identity morphism on $c$.

(iv) for every three objects $c, d, e \in \operatorname{Ob}(\mathcal{C})$ and morphisms $f \in \mathcal{C}(c, d)$ and $g \in \mathcal{C}(d, e)$, one specifies a morphism $f ; g \in \mathcal{C}(c, e)$, called the composite of $f$ and $g$.

We will sometimes write an object $c \in \mathcal{C}$, instead of $c \in \operatorname{Ob}(\mathcal{C})$. It will also be convenient to denote elements $f \in \mathcal{C}(c, d)$ as $f: c \rightarrow d$. Here, $c$ is called the domain of $f$, and $d$ is called the codomain of $f$.

These constituents are required to satisfy two conditions:

(a) unitality: for any morphism $f: c \rightarrow d$, composing with the identities at $c$ or $d$ does nothing: $\operatorname{id}_{c} \cap f=f$ and $f ; \operatorname{id}_{d}=f$.

(b) associativity: for any three morphisms $f: c_{0} \rightarrow c_{1}, g: c_{1} \rightarrow c_{2}$, and $h: c_{2} \rightarrow c_{3}$, the following are equal: $(f ; g) ; h=f ;(g ; h)$. We write this composite simply as $f ; g$ g .
\footnotetext{
${ }^{1}$ By "bonus," we mean that although not strictly essential to the understanding of this particular chapter, limits and colimits will show up throughout the book and throughout one's interaction with category theory, and we think the reader will especially benefit from this material in the long run.

${ }^{2}$ Here, a collection can be thought of as a bunch of things, just like a set, but that may be too large to formally be a set. An example is the collection of all sets, which would run afoul of Russell's paradox if it were itself a set.

${ }^{3}$ This set $\mathcal{C}(c, d)$ is often denoted $\operatorname{Hom}_{\mathcal{C}}(c, d)$, and called the "hom-set from $c$ to $d$." The word "hom" stands for homomorphism, of which the word "morphism" is a shortened version.
}

Our next goal is to give lots of examples of categories. Our first source of examples is that of free and finitely-presented categories, which generalize the notion of Hasse diagram from Remark 1.39.

\subsection*{3.2.1 Free categories}

Recall from Definition 1.36 that a graph consists of two types of thing: vertices and arrows. From there one can define paths, which are just head-to-tail sequences of arrows. Every path $p$ has a start vertex and an end vertex; if $p$ goes from $v$ to $w$, we write $p: v \rightarrow w$. To every vertex $v$, there is a trivial path, containing no arrows, starting and ending at $v$; we often denote it by $\mathrm{id}_{v}$ or simply by $v$. We may also concatenate paths: given $p: v \rightarrow w$ and $q: w \rightarrow x$, their concatenation is denoted $p ; q$, and it goes $v \rightarrow x$.

In Chapter 1, we used graphs to depict preorders $(V, \leq)$ : the vertices form the elements of the preorder, and we say that $v \leq w$ if there is a path from $v$ to $w$ in $G$. We will now use graphs in a very similar way to depict certain categories, known as free categories. Then we will explain a strong relationship between preorders and categories in Section 3.2.3.

Definition 3.7. For any graph $G=(V, A, s, t)$, we can define a category Free( $G)$, called the free category on $G$, whose objects are the vertices $V$ and whose morphisms from $c$ to $d$ are the paths from $c$ to $d$. The identity morphism on an object $c$ is simply the trivial path at $c$. Composition is given by concatenation of paths.

For example, we define $\mathbf{2}$ to be the free category generated by the graph shown below:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-094.jpg?height=125&width=494&top_left_y=1721&top_left_x=810)

It has two objects $v_{1}$ and $v_{2}$, and three morphisms: $\mathrm{id}_{v_{1}}: v_{1} \rightarrow v_{1}, f_{1}: v_{1} \rightarrow v_{2}$, and $\operatorname{id}_{v_{2}}: v_{2} \rightarrow v_{2}$. Here $\operatorname{id}_{v_{1}}$ is the path of length 0 starting and ending at $v_{1}, f_{1}$ is the path of length 1 consisting of just the arrow $f_{1}$, and $\operatorname{id}_{v_{2}}$ is the length 0 path at $v_{2}$. As our notation suggests, $\operatorname{id}_{v_{1}}$ is the identity morphism for the object $v_{1}$, and similarly id $v_{v_{2}}$ for $v_{2}$. As composition is given by concatenation, we have, for example $\operatorname{id}_{v_{1}} ๆ f_{1}=f_{1}$, $\operatorname{id}_{v_{2}} ̊ \mathrm{id}_{v_{2}}=\mathrm{id}_{v_{2}}$, and so on.

From now on, we may elide the difference between a graph and the corresponding free category Free( $G$ ), at least when the one we mean is clear enough from context.

Exercise 3.9. For Free( $G$ ) to really be a category, we must check that this data we specified obeys the unitality and associativity properties. Check that these are obeyed for any graph $G$.

Exercise 3.10. The free category on the graph shown here: ${ }^{4}$

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-095.jpg?height=123&width=619&top_left_y=340&top_left_x=731)

has three objects and six morphisms: the three vertices and six paths in the graph.

Create six names, one for each of the six morphisms in 3. Write down a six-by-six table, label the rows and columns by the six names you chose.

1. Fill out the table by writing the name of the composite in each cell, when there is a composite.

2. Where are the identities?

Exercise 3.12. Let's make some definitions, based on the pattern above:

1. What is the category $\mathbf{1}$ ? That is, what are its objects and morphisms?

2. What is the category $\mathbf{0}$ ?

3. What is the formula for the number of morphisms in $\mathbf{n}$ for arbitrary $n \in \mathbb{N}$ ? $\diamond$

Example 3.13 (Natural numbers as a free category). Consider the following graph:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-095.jpg?height=184&width=98&top_left_y=1263&top_left_x=1011)

It has only one vertex and one arrow, but it has infinitely many paths. Indeed, it has a unique path of length $n$ for every natural number $n \in \mathbb{N}$. That is, Path $=$ $\{z, s,(s ; s),(s ; s ; s), \ldots\}$, where we write $z$ for the length 0 path on $z$; it represents the morphism $\mathrm{id}_{z}$. There is a one-to-one correspondence between Path and the natural numbers, $\mathbb{N}=\{0,1,2,3, \ldots\}$.

This is an example of a category with one object. A category with one object is called a monoid, a notion we first discussed in Example 2.6. There we said that a monoid is a tuple $(M, *, e)$ where $*: M \times M \rightarrow M$ is a function and $e \in M$ is an element, and $m * 1=m=1 * m$ and $(m * n) * p=m *(n * p)$.

The two notions may superficially look different, but it is easy to describe the connection. Given a category $\mathcal{C}$ with one object, say $\bullet$, let $M:=\mathcal{C}(\bullet, \bullet)$, let $e=\operatorname{id} \bullet$, and let $*: \mathcal{C}(\bullet, \bullet) \times \mathcal{C}(\bullet, \bullet) \rightarrow \mathcal{C}(\bullet, \bullet)$ be the composition operation $*=\%$. The associativity and unitality requirements for the monoid will be satisfied because $\mathcal{C}$ is a category.

Exercise 3.15. In Example 3.13 we identified the paths of the loop graph (3.14) with numbers $n \in \mathbb{N}$. Paths can be concatenated. Given numbers $m, n \in \mathbb{N}$, what number corresponds to the concatenation of their associated paths?
\footnotetext{
${ }^{4}$ As mentioned above, we elide the difference between the graph and the corresponding free category.
}

\subsection*{3.2.2 Presenting categories via path equations}

So for any graph $G$, there is a free category on $G$. But we don't have to stop there: we can add equations between paths in the graph, and still get a category. We are only allowed to equate two paths $p$ and $q$ when they are parallel, meaning they have the same source vertex and the same target vertex.

A finite graph with path equations is called a finite presentation for a category, and the category that results is known as a finitely-presented category. Here are two examples:
![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-096.jpg?height=326&width=1264&top_left_y=674&top_left_x=426)

Both of these are presentations of categories: in the left-hand one, there are no equations so it presents a free category, as discussed in Section 3.2.1. The free square category has ten morphisms, because every path is a unique morphism.

Exercise 3.16.

1. Write down the ten paths in the free square category above.

2. Name two different paths that are parallel.

3. Name two different paths that are not parallel.

On the other hand, the category presented on the right has only nine morphisms, because $f: h$ and $g \because i$ are made equal. This category is called the "commutative square." Its morphisms are

$$
\{A, B, C, D, f, g, h, i, f ; h\}
$$

One might say "the missing one is $g \circ i$," but that is not quite right: $g \circ i$ is there too, because it is equal to $f ; h$. As usual, $A$ denotes id $A$, etc.

Exercise 3.17. Write down all the morphisms in the category presented by the following diagram:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-096.jpg?height=323&width=260&top_left_y=1893&top_left_x=930)

Example 3.18. We should also be aware that enforcing an equation between two morphisms often implies additional equations. Here are two more examples of presenta-
tions, in which this phenomenon occurs:
![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-097.jpg?height=248&width=824&top_left_y=320&top_left_x=648)

In $\mathcal{C}$ we have the equation $s ; s=z$. But this implies $s ; s ; s=z ; s=s$ ! And similarly we have $s ; s ; s ; s=z ; z=z$. The set of morphisms in $\mathcal{C}$ is in fact merely $\{z, s\}$, with composition described by $s ; s=z ; z=z$, and $z ; s=s ; z=s$. In group theory, one would speak of a group called $\mathbb{Z} / 2 \mathbb{Z}$.

Exercise 3.19. Write down all the morphisms in the category $\mathcal{D}$ from Example 3.18.

Remark 3.20. We can now see that the schemas in Section 3.1, e.g. Eqs. (3.2) and (3.4) are finite presentations of categories. We will come back to this idea in Section 3.3.

\subsection*{3.2.3 Preorders and free categories: two ends of a spectrum}

Now that we have used graphs to depict preorders in Chapter 1 and categories above, one may want to know the relationship between these two uses. The main idea we want to explain now is that

"A preorder is a category where every two parallel arrows are the same."

Thus any preorder can be regarded as a category, and any category can be somehow "crushed down" into a preorder. Let's discuss these ideas.

Preorders as categories. Suppose $(P, \leq)$ is a preorder. It specifies a category $\mathcal{P}$ as follows. The objects of $\mathcal{P}$ are precisely the elements of $P$; that is, $\mathrm{Ob}(\mathcal{P})=P$. As for morphisms, $\mathcal{P}$ has exactly one morphism $p \rightarrow q$ if $p \leq q$ and no morphisms $p \rightarrow q$ if $p \not q$. The fact that $\leq$ is reflexive ensures that every object has an identity, and the fact that $\leq$ is transitive ensures that morphisms can be composed. We call $\mathcal{P}$ the category corresponding to the preorder $(P, \leq)$.

In fact, a Hasse diagram for a preorder can be thought of a presentation of a category where, for all vertices $p$ and $q$, every two paths from $p \rightarrow q$ are declared equal. For example, in Eq. (1.5) we saw a Hasse diagram that was like the graph on the left:
![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-097.jpg?height=352&width=1414&top_left_y=2176&top_left_x=348)

The Hasse diagram (left) might look the most like the free category presentation (middle) which has no equations, but that is not correct. The free category has three morphisms (paths) from bottom object to top object, whereas preorders are categories with at most one morphism between two given objects. Instead, the diagram on the right, with these paths from bottom to top made equal, is the correct presentation for the preorder on the left.

Exercise 3.21. What equations would you need to add to the graphs below in order to present the associated preorders?

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-098.jpg?height=162&width=306&top_left_y=765&top_left_x=351)
![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-098.jpg?height=238&width=1042&top_left_y=726&top_left_x=739)

The preorder reflection of a category. Given any category $\mathcal{C}$, one can obtain a preorder $(C, \leq)$ from it by destroying the distinction between any two parallel morphisms. That is, let $C:=\operatorname{Ob}(\mathcal{C})$, and put $c_{1} \leq c_{2}$ iff $\mathcal{C}\left(c_{1}, c_{2}\right) \neq \varnothing$. If there is one, or two, or fifty, or infinitely many morphisms $c_{1} \rightarrow c_{2}$ in $\mathcal{C}$, the preorder reflection does not see the difference. But it does see the difference between some morphisms and no morphisms.

Exercise 3.22. What is the preorder reflection of the category $\mathbb{N}$ from Example 3.13?

We have only discussed adjoint functors between preorders, but soon we will discuss adjoints in general. Here is a statement you might not understand exactly, but it's true; you can ask a category theory expert about it and they should be able to explain it to you:

Considering a preorder as a category is right adjoint to turning a category into a preorder by preorder reflection.

Remark 3.23 (Ends of a spectrum). The main point of this subsection is that both preorders and free categories are specified by a graph without path equations, but they denote opposite ends of a spectrum. In both cases, the vertices of the graph become the objects of a category and the paths become morphisms. But in the case of free categories, there are no equations so each path becomes a different morphism. In the case of preorders, all parallel paths become the same morphism. Every category presentation, i.e. graph with some equations, lies somewhere in between the free category (no equations) and its preorder reflection (all possible equations).

\subsection*{3.2.4 Important categories in mathematics}

We have been talking about category presentations, but there are categories that are best understood directly, not by way of presentations. Recall the definition of category
from Definition 3.6. The most important category in mathematics is the category of sets.

Definition 3.24. The category of sets, denoted Set, is defined as follows.

(i) $\mathrm{Ob}($ Set) is the collection of all sets.

(ii) If $S$ and $T$ are sets, then $\operatorname{Set}(S, T)=\{f: S \rightarrow T \mid f$ is a function $\}$.

(iii) For each set $S$, the identity morphism is the function $\operatorname{id}_{S}: S \rightarrow S$ given by $\operatorname{id}_{S}(s):=s$ for each $s \in S$.

(iv) Given $f: S \rightarrow T$ and $g: T \rightarrow U$, their composite is the function $f ; g: S \rightarrow U$ given by $(f ; g)(s):=g(f(s))$.

These definitions satisfy the unitality and associativity conditions, so Set is indeed a category.

Closely related is the category FinSet. This is the category whose objects are finite sets and whose morphisms are functions between them.

Exercise 3.25. Let $\underline{2}=\{1,2\}$ and $\underline{3}=\{1,2,3\}$. These are objects in the category Set discussed in Definition 3.24. Write down all the elements of the set Set( $2, \underline{3})$; there should be nine.

Remark 3.26. You may have wondered what categories have to do with $\mathcal{V}$-categories (Definition 2.46); perhaps you think the definitions hardly look alike. Despite the term 'enriched category', $\mathcal{V}$-categories are not categories with extra structure. While some sorts of $\mathcal{V}$-categories, such as Bool-categories, i.e. preorders, can naturally be seen as categories, other sorts, such as Cost-categories, cannot.

The reason for the importance of Set is that, if we generalize the definition of enriched category (Definition 2.46), we find that categories in the sense of Definition 3.6 are exactly Set-categories-so categories are $\mathcal{V}$-categories for a very special choice of $\mathcal{V}$. We'll come back to this in Section 4.4.4. For now, we simply remark that just like a deep understanding of the category Cost-for example, knowing that it is a quantale-yields insight into Lawvere metric spaces, so the study of Set yields insights into categories.

There are many other categories that mathematicians care about:
- Top: the category of topological spaces (neighborhood)
- Grph: the category of graphs (connection)
- Meas: the category of measure spaces (amount)
- Mon: the category of monoids (action)
- Grp: the category of groups (reversible action, symmetry)
- Cat: the category of categories (action in context, structure)

But in fact, this does not at all do justice to the diversity of categories mathematicians think about. They work with whatever category they find fits their purpose at the time, like 'the category of connected Riemannian manifolds of dimension at most 4'.

Here is one more source of examples: take any category you already have and reverse all its morphisms; the result is again a category.

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-100.jpg?height=52&width=1440&top_left_y=272&top_left_x=337)
the same objects, $\mathrm{Ob}\left(\mathcal{C}^{\circ \mathrm{p}}\right):=\mathrm{Ob}(\mathcal{C})$, and for any two objects $c, d \in \mathrm{Ob}(\mathcal{C})$, one has $\mathcal{C}^{\text {op }}(c, d):=\mathcal{C}(d, c)$. Identities and composition are as in $\mathcal{C}$.

\subsection*{3.2.5 Isomorphisms in a category}

The previous sections have all been about examples of categories: free categories, presented categories, and important categories in math. In this section, we briefly switch gears and talk about an important concept in category theory, namely the concept of isomorphism.

In a category, there is often the idea that two objects are interchangeable. For example, in the category Set, one can exchange the set $\{\boldsymbol{\square}, \square\}$ for the set $\{0,1\}$ and everything will be the same, other than the names for the elements. Similarly, if one has a preorder with elements $a, b$, such that $a \leq b$ and $b \leq a$, i.e. $a \cong b$, then $a$ and $b$ are essentially the same. How so? Well they act the same, in that for any other object $c$, we know that $c \leq a$ iff $c \leq b$, and $c \geq a$ iff $c \geq b$. The notion of isomorphism formalizes this notion of interchangeability.

Definition 3.28. An isomorphism is a morphism $f: A \rightarrow B$ such that there exists a morphism $g: B \rightarrow A$ satisfying $f ; g=\operatorname{id}_{A}$ and $g \circ f=\operatorname{id}_{B}$. In this case we call $f$ and $g$ inverses, and we often write $g=f^{-1}$, or equivalently $f=g^{-1}$. We also say that $A$ and $B$ are isomorphic objects.

Example 3.29. The set $A:=\{a, b, c\}$ and the set $\underline{3}=\{1,2,3\}$ are isomorphic; that is, there exists an isomorphism $f: A \rightarrow \underline{3}$ given by $f(a)=2, f(b)=1, f(c)=3$. The isomorphisms in the category Set are the bijections.

Recall that the cardinality of a finite set is the number of elements in it. This can be understood in terms of isomorphisms in FinSet. Namely, for any finite set $A \in$ FinSet, its cardinality is the number $n \in \mathbb{N}$ such that there exists an isomorphism $A \cong \underline{n}$. Georg Cantor defined the cardinality of any set $X$ to be its isomorphism class, meaning the equivalence class consisting of all sets that are isomorphic to $X$.

Exercise 3.30.

1. What is the inverse $f^{-1}: \underline{3} \rightarrow A$ of the function $f$ given in Example 3.29?

2. How many distinct isomorphisms are there $A \rightarrow \underline{3}$ ?

Exercise 3.31. Show that in any given category $\mathcal{C}$, for any given object $c \in \mathcal{C}$, the identity $\mathrm{id}_{c}$ is an isomorphism.

Exercise 3.32. Recall Examples 3.13 and 3.18. A monoid in which every morphism is an isomorphism is known as a group.

1. Is the monoid in Example 3.13 a group?

2. What about the monoid $\mathcal{C}$ in Example 3.18?

Exercise 3.33. Let $G$ be a graph, and let Free( $G$ ) be the corresponding free category. Somebody tells you that the only isomorphisms in Free $(G)$ are the identity morphisms. Is that person correct? Why or why not?

Example 3.34. In this example, we will see that it is possible for $g$ and $f$ to be almost—but not quite-inverses, in a certain sense.

Consider the functions $f: \underline{2} \rightarrow \underline{3}$ and $g: \underline{3} \rightarrow \underline{2}$ drawn below:
![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-101.jpg?height=314&width=724&top_left_y=732&top_left_x=690)

Then the reader should be able to instantly check that $f ; g=\operatorname{id}_{\underline{2}}$ but $g \because f \neq \operatorname{id}_{\underline{3}}$. Thus $f$ and $g$ are not inverses and hence not isomorphisms. We won't need this terminology, but category theorists would say that $f$ and $g$ form a retraction.

\subsection*{3.3 Functors, natural transformations, and databases}

In Section 3.1 we showed some database schemas: graphs with path equations. Then in Section 3.2.2 we said that graphs with path equations correspond to finitely-presented categories. Now we want to explain what the data in a database is, as a way to introduce functors. To do so, we begin by noticing that sets and functions-the objects and morphisms in the category Set-can be captured by particularly simple databases.

\subsection*{3.3.1 Sets and functions as databases}

The first observation is that any set can be understood as a table with only one column: the ID column.

\begin{tabular}{c|c|c|} 
Planet of Sol & Prime number \\
\cline { 1 - 1 } Mercury & 2 \\
Venus & 3 \\
Earth & 5 \\
Mars & 7 \\
Jupiter & 11 \\
Saturn & 13 \\
Uranus & 17 \\
Neptune & $\vdots$
\end{tabular}

Rather than put the elements of the set between braces, e.g. $\{2,3,5,7,11, \ldots\}$, we write them down as rows in a table.

In databases, single-column tables are often called controlled vocabularies, or master data. Now to be honest, we can only write out every single entry in a table when its set of rows is finite. A database practitioner might find the idea of our prime number table a bit unrealistic. But we're mathematicians, so since the idea makes perfect sense abstractly, we will continue to think of sets as one-column tables.

The above databases have schemas consisting of just one vertex:
![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-102.jpg?height=124&width=1190&top_left_y=716&top_left_x=457)

Obviously, there's really not much difference between these schemas, other than the label of the unique vertex. So we could say "sets are databases whose schema consists of a single vertex." Let's move on to functions.

A function $f: A \rightarrow B$ can almost be depicted as a two-column table

\begin{tabular}{c|c} 
Beatle & Played \\
\hline George & Lead guitar \\
John & Rhythm guitar \\
Paul & Bass guitar \\
Ringo & Drums
\end{tabular}

except it is unclear whether the elements of the right-hand column exhaust all of $B$. What if there are rock-and-roll instruments out there that none of the Beatles played? So a function $f: A \rightarrow B$ requires two tables, one for $A$ and its $f$ column, and one for $B$ :

\begin{tabular}{c|c} 
Beatle & Played \\
\hline George & Lead guitar \\
John & Rhythm guitar \\
Paul & Bass guitar \\
Ringo & Drums
\end{tabular}

\begin{tabular}{c|} 
Rock-and-roll instrument \\
\hline Bass guitar \\
Drums \\
Keyboard \\
Lead guitar \\
Rhythm guitar
\end{tabular}

Thus the database schema for any function is just a labeled version of $\mathbf{2}$ :

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-102.jpg?height=144&width=547&top_left_y=2072&top_left_x=781)

The lesson is that an instance of a database takes a presentation of a category, and turns every vertex into a set, and every arrow into a function. As such, it describes a map from the presented category to the category Set. In Section 2.4.2 we saw that maps of $\mathcal{V}$-categories are known as $\mathcal{V}$-functors. Similarly, we call maps of plain old categories, functors.

\subsection*{3.3.2 Functors}

A functor is a mapping between categories. It sends objects to objects and morphisms to morphisms, all while preserving identities and composition. Here is the formal definition.

Definition 3.35. Let $\mathcal{C}$ and $\mathcal{D}$ be categories. To specify a functor from $\mathcal{C}$ to $\mathcal{D}$, denoted $F: \mathcal{C} \rightarrow \mathcal{D}$,

(i) for every object $c \in \mathrm{Ob}(\mathcal{C})$, one specifies an object $F(c) \in \mathrm{Ob}(\mathcal{D})$;

(ii) for every morphism $f: c_{1} \rightarrow c_{2}$ in $\mathcal{C}$, one specifies a morphism $F(f): F\left(c_{1}\right) \rightarrow$ $F\left(c_{2}\right)$ in $\mathcal{D}$.

The above constituents must satisfy two properties:

(a) for every object $c \in \mathrm{Ob}(\mathcal{C})$, we have $F\left(\mathrm{id}_{c}\right)=\mathrm{id}_{F(c)}$.

(b) for every three objects $c_{1}, c_{2}, c_{3} \in \operatorname{Ob}(\mathcal{C})$ and two morphisms $f \in \mathcal{C}\left(c_{1}, c_{2}\right), g \in$ $\mathcal{C}\left(c_{2}, c_{3}\right)$, the equation $F(f ; g)=F(f) \circ F(g)$ holds in $\mathcal{D}$.

Example 3.36. For example, here we draw three functors $F: \mathbf{2} \rightarrow \mathbf{3}$ :
![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-103.jpg?height=360&width=1352&top_left_y=1180&top_left_x=381)

In each case, the dotted arrows show what the functor $F$ does to the vertices in $\mathbf{2}$; once that information is specified, it turns out-in this special case-that what $F$ does to the three paths in $\mathbf{2}$ is completely determined. In the left-hand diagram, $F$ sends every path to the trivial path, i.e. the identity on $n_{0}$. In the middle diagram $F\left(m_{0}\right)=n_{0}$, $F\left(f_{1}\right)=g_{1}$, and $F\left(m_{1}\right)=n_{1}$. In the right-hand diagram, $F\left(m_{0}\right)=n_{0}, F\left(m_{1}\right)=n_{2}$, and $F\left(f_{1}\right)=g_{1} \circ g_{2}$.

Exercise 3.37. Above we wrote down three functors $\mathbf{2} \rightarrow \mathbf{3}$. Find and write down all the remaining functors $2 \rightarrow 3$.

Example 3.38. Recall the categories presented by Free_square and Comm_square in Section 3.2.2. Here they are again, with ' added to the labels in Free_square to help
distinguish them:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-104.jpg?height=344&width=1198&top_left_y=316&top_left_x=455)

There are lots of functors from the free square category (let's call it $\mathcal{F}$ ) to the commutative square category (let's call it $\mathcal{C}$ ).

However, there is exactly one functor $F: \mathcal{F} \rightarrow \mathcal{C}$ that sends $A^{\prime}$ to $A, B^{\prime}$ to $B, C^{\prime}$ to $C$, and $D^{\prime}$ to $D$. That is, once we have made this decision about how $F$ acts on objects, each of the ten paths in $\mathcal{F}$ is forced to go to a certain path in $\mathcal{C}$ : the one with the right source and target.

Exercise 3.39. Say where each of the ten morphisms in $\mathcal{F}$ is sent under the functor $F$ from Example 3.38.

All of our example functors so far have been completely determined by what they do on objects, but this is usually not the case.

Exercise 3.40. Consider the free categories $\mathcal{C}=\bullet \rightarrow \bullet$ and $\mathcal{D}=\bullet \rightrightarrows \bullet$. Give two functors $F, G: \mathcal{C} \rightarrow \mathcal{D}$ that act the same on objects but differently on morphisms. $\diamond$

Example 3.41. There are also lots of functors from the commutative square category $\mathcal{C}$ to the free square category $\mathcal{F}$, but none that sends $A$ to $A^{\prime}, B$ to $B^{\prime}, C$ to $C^{\prime}$, and $D$ to $D^{\prime}$. The reason is that if $F$ were such a functor, then since $f ; h=g ; i$ in $\mathcal{C}$, we would have $F(f ; h)=F(g ; i)$, but then the rules of functors would let us reason as follows:

$$
f^{\prime} ; h^{\prime}=F(f) ; F(h)=F(f ; h)=F(g ; i)=F(g) \circ F(i)=g^{\prime} ; i^{\prime}
$$

The resulting equation, $f^{\prime} ; h^{\prime}=g^{\prime} \circ i^{\prime}$ does not hold in $\mathcal{F}$ because it is a free category (there are "no equations"): every two paths are considered different morphisms. Thus our proposed $F$ is not a functor.

Example 3.42 (Functors between preorders are monotone maps). Recall from Section 3.2.3 that preorders are categories with at most one morphism between any two objects. A functor between preorders is exactly a monotone map.

For example, consider the preorder $(\mathbb{N}, \leq)$ considered as a category $\mathcal{N}$ with objects $\operatorname{Ob}(\mathcal{N})=\mathbb{N}$ and a unique morphism $m \rightarrow n$ iff $m \leq n$. A functor $F: \mathcal{N} \rightarrow \mathcal{N}$ sends each object $n \in \mathbb{N}$ to an object $F(n) \in \mathbb{N}$. It must send morphisms in $\mathcal{N}$ to morphisms in $\mathbb{N}$. This means if there is a morphism $m \rightarrow n$ then there had better be a morphism $F(m) \rightarrow F(n)$. In other words, if $m \leq n$, then we had better have $F(m) \leq F(n)$. But as
long as $m \leq n$ implies $F(m) \leq F(n)$, we have a functor.

Thus a functor $F: \mathcal{N} \rightarrow \mathcal{N}$ and a monotone map $\mathbb{N} \rightarrow \mathbb{N}$ are the same thing.

Exercise 3.43 (The category of categories). Back in the primordial ooze, there is a category Cat in which the objects are themselves categories. Your task here is to construct this category.

1. Given any category $\mathcal{C}$, show that there exists a functor id $\mathcal{C}: \mathcal{C} \rightarrow \mathcal{C}$, known as the identity functor on $\mathcal{C}$, that maps each object to itself and each morphism to itself.

Note that a functor $\mathcal{C} \rightarrow \mathcal{D}$ consists of a function from $\mathrm{Ob}(\mathcal{C})$ to $\mathrm{Ob}(\mathcal{D})$ and for each pair of objects $c_{1}, c_{2} \in \mathcal{C}$ a function from $\mathcal{C}\left(c_{1}, c_{2}\right)$ to $\mathcal{D}\left(F\left(c_{1}\right), F\left(c_{2}\right)\right)$.

2. Show that given $F: \mathcal{C} \rightarrow \mathcal{D}$ and $G: \mathcal{D} \rightarrow \mathcal{E}$, we can define a new functor $(F ;$ $G): \mathcal{C} \rightarrow \mathcal{E}$ just by composing functions.

3. Show that there is a category, call it Cat, where the objects are categories, morphisms are functors, and identities and composition are given as above. $\diamond$

\subsection*{3.3.3 Database instances as Set-valued functors}

Let $\mathcal{C}$ be a category, and recall the category Set from Definition 3.24. A functor $F: \mathcal{C} \rightarrow$ Set is known as a set-valued functor on $\mathcal{C}$. Much of database theory (not how to make them fast, but what they are and what you do with them) can be cast in this light.

Indeed, we already saw in Remark 3.20 that any database schema can be regarded as (presenting) a category $\mathcal{C}$. The next thing to notice is that the data itself-any instance of the database-is given by a set-valued functor $I: \mathcal{C} \rightarrow$ Set. The only additional detail is that for any white node, such as $c=\underset{0}{\text { string }}$, we want to force $I$ to map to the set of strings. We suppress this detail in the following definition.

Definition 3.44. Let $\mathcal{C}$ be a schema, i.e. a finitely-presented category. A $\mathcal{C}$-instance is a functor $I: \mathcal{C} \rightarrow$ Set. $^{5}$

Exercise 3.45. Let $\mathbf{1}$ denote the category with one object, called 1, one identity morphism $\mathrm{id}_{1}$, and no other morphisms. For any functor $F: \mathbf{1} \rightarrow$ Set one can extract a set $F(1)$. Show that for any set $S$, there is a functor $F_{S}: \mathbf{1} \rightarrow$ Set such that $F_{S}(1)=S$.

The above exercise reaffirms that the set of planets, the set of prime numbers, and the set of flying pigs are all set-valued functors-instances-on the schema 1. Similarly, set-valued functors on the category $\mathbf{2}$ are functions. All our examples so far are for the situation where the schema is a free category (no equations). Let's try an example of a category that is not free.
\footnotetext{
${ }^{5}$ Warning: a $\mathcal{C}$-instance is a state of the database "at an instant in time." The term "instance" should not be confused with its usage in object oriented programming, which would correspond more to what we call a row $r \in I(c)$.
}

Example 3.46. Consider the following category:

$$
\mathcal{C}:=\begin{gather*}
s^{s}  \tag{3.47}\\
\bullet \\
z \\
s ; s=s \\
\hline
\end{gather*}
$$

What is a set-valued functor $F: \mathcal{C} \rightarrow$ Set? It will consist of a set $Z:=F(z)$ and a function $S:=F(s): Z \rightarrow Z$, subject to the requirement that $S ; S=S$. Here are some examples
- $Z$ is the set of US citizens, and $S$ sends each citizen to her or his president. The president's president is her- or him-self.
- $Z=\mathbb{N}$ is the set of natural numbers and $S$ sends each number to 0 . In particular, 0 goes to itself.
- $Z$ is the set of all well-formed arithmetic expressions, such as $13+(2 * 4)$ or -5 , that one can write using integers and the symbols $+,-, *,($,$) . The function S$ evaluates the expression to return an integer, which is itself a well-formed expression. The evaluation of an integer is itself.
- $Z=\mathbb{N}_{\geq 2}$, and $S$ sends $n$ to its smallest prime factor. The smallest prime factor of a prime is itself.

\begin{tabular}{c|c}
$\mathbb{N}_{\geq 2}$ & smallest prime factor \\
\hline 2 & 2 \\
3 & 3 \\
4 & 2 \\
$\vdots$ & $\vdots$ \\
49 & 7 \\
50 & 2 \\
51 & 3 \\
$\vdots$ & $\vdots$
\end{tabular}

Exercise 3.48. Above, we thought of the sort of data that would make sense for the schema (3.47). Give an example of the sort of data that would make sense for the

following schemas:
![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-106.jpg?height=232&width=820&top_left_y=1962&top_left_x=746)

$\diamond$

The main idea is this: a database schema is a category, and an instance on that schema-the data itself-is a set-valued functor. All the constraints, or business rules, are ensured by the rules of functors, namely that functors preserve composition. ${ }^{6}$
\footnotetext{
${ }^{6}$ One can put more complex constraints, called embedded dependencies, on a database; these correspond category theoretically to what are called "lifting problems" in category theory. See [Spi14b] for more on this.
}

\subsection*{3.3.4 Natural transformations}

If $\mathcal{C}$ is a schema-i.e. a finitely-presented category-then there are many database instances on it, which we can organize into a category. But this is part of a larger story, namely that of natural transformations. An abstract picture to have in mind is this:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-107.jpg?height=160&width=257&top_left_y=519&top_left_x=934)

Definition 3.49. Let $\mathcal{C}$ and $\mathcal{D}$ be categories, and let $F, G: \mathcal{C} \rightarrow \mathcal{D}$ be functors. To specify a natural transformation $\alpha: F \Rightarrow G$,

(i) for each object $c \in \mathcal{C}$, one specifies a morphism $\alpha_{c}: F(c) \rightarrow G(c)$ in $\mathcal{D}$, called the c-component of $\alpha$.

These components must satisfy the following, called the naturality condition:

(a) for every morphism $f: c \rightarrow d$ in $\mathcal{C}$, the following equation must hold:

$$
F(f) ; \alpha_{d}=\alpha_{c} ; G(f)
$$

A natural transformation $\alpha: F \rightarrow G$ is called a natural isomorphism if each component $\alpha_{c}$ is an isomorphism in $\mathcal{D}$.

The naturality condition can also be written as a so-called commutative diagram. A diagram in a category is drawn as a graph whose vertices and arrows are labeled by objects and morphisms in the category. For example, here is a diagram that's relevant to the naturality condition in Definition 3.49 :

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-107.jpg?height=214&width=350&top_left_y=1628&top_left_x=882)

Definition 3.51. A diagram $D$ in $\mathcal{C}$ is a functor $D: \mathcal{J} \rightarrow \mathcal{C}$ from any category $\mathcal{J}$, called the indexing category of the diagram $D$. We say that $D$ commutes if $D(f)=D\left(f^{\prime}\right)$ holds for every parallel pair of morphisms $f, f^{\prime}: a \rightarrow b$ in $\mathcal{J} .{ }^{7}$

In terms of Eq. (3.50), the only case of two parallel morphisms is that of $F(c) \rightrightarrows G(d)$,

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-107.jpg?height=63&width=1442&top_left_y=2191&top_left_x=336)
the naturality condition from Definition 3.49.
\footnotetext{
${ }^{7}$ We could package this formally by saying that $D$ commutes iff it factors through the preorder reflection of $\mathcal{J}$.
}

Example 3.52. A representative picture is as follows:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-108.jpg?height=306&width=1211&top_left_y=362&top_left_x=457)

We have depicted, in blue and red respectively, two functors $F, G: \mathcal{C} \rightarrow \mathcal{D}$. A natural transformation $\alpha: F \Rightarrow G$ is given by choosing components $\alpha_{1}: v \rightarrow x$ and $\alpha_{2}: w \rightarrow y$. We have highlighted the only choice for each in green; namely, $\alpha_{1}=c$ and $\alpha_{2}=g$.

The key point is that the functors $F$ and $G$ are ways of viewing the category $\mathcal{C}$ as lying inside the category $\mathcal{D}$. The natural transformation $\alpha$, then, is a way of relating these two views using the morphisms in $\mathcal{D}$. Does this help you to see and appreciate the notation $\mathcal{C} \xrightarrow[G]{a \Downarrow l} \mathcal{D}$ ?

Example 3.53. We said in Exercise 3.45 that a functor $\mathbf{1} \rightarrow$ Set can be identified with a set. So suppose $A$ and $B$ are sets considered as functors $A, B: \mathbf{1} \rightarrow$ Set. A natural transformation between these functors is just a function between the sets.

Definition 3.54. Let $\mathcal{C}$ and $\mathcal{D}$ be categories. We denote by $\mathcal{D}^{\mathcal{C}}$ the category whose objects are functors $F: \mathcal{C} \rightarrow \mathcal{D}$ and whose morphisms $\mathcal{D}^{\mathfrak{e}}(F, G)$ are the natural transformations $\alpha: F \rightarrow G$. This category $\mathcal{D}^{\mathfrak{e}}$ is called the functor category, or the category of functors from $\mathcal{C}$ to $\mathcal{D}$.

Exercise 3.55. Let's look more deeply at how $\mathcal{D}^{\mathfrak{e}}$ is a category.

1. Figure out how to compose natural transformations. (Hint: an expert tells you "for each object $c \in \mathcal{C}$, compose the $c$-components.")

2. Propose an identity natural transformation on any object $F \in \mathcal{D}^{\mathcal{e}}$, and check that it is unital (i.e. that it obeys condition (a) of Definition 3.6).

Example 3.56. In our new language, Example 3.53 says that Set $^{1}$ is equivalent to Set.

Example 3.57. Let $\mathcal{N}$ denote the category associated to the preorder $(\mathbb{N}, \leq$ ), and recall from Example 3.42 that we can identify a functor $F: \mathcal{N} \rightarrow \mathcal{N}$ with a non-decreasing sequence $\left(F_{0}, F_{1}, F_{2}, \ldots\right)$ of natural numbers, i.e. $F_{0} \leq F_{1} \leq F_{2} \leq \cdots$. If $G$ is another functor, considered as a non-decreasing sequence, then what is a natural transformation
$\alpha: F \rightarrow G$ ?

Since there is at most one morphism between two objects in a preorder, each component $\alpha_{n}: F_{n} \rightarrow G_{n}$ has no data, it just tells us a fact: that $F_{n} \leq G_{n}$. And the naturality condition is vacuous: every square in a preorder commutes. So a natural transformation between $F$ and $G$ exists iff $F_{n} \leq G_{n}$ for each $n$, and any two natural transformations $F \Rightarrow G$ are the same. In other words, the category $\mathcal{N}^{\mathcal{N}}$ is itself a preorder; namely the preorder of monotone maps $\mathbb{N} \rightarrow \mathbb{N}$.

Exercise 3.58. Let $\mathcal{C}$ be an arbitrary category and let $\mathcal{P}$ be a preorder, thought of as a category. Consider the following statements:

1. For any two functors $F, G: \mathcal{C} \rightarrow \mathcal{P}$, there is at most one natural transformation $F \rightarrow G$.

2. For any two functors $F, G: \mathcal{P} \rightarrow \mathcal{C}$, there is at most one natural transformation $F \rightarrow G$.

For each, if it is true, say why; if it is false, give a counterexample.

Remark 3.59. Recall that in Remark 2.71 we said the category of preorders is equivalent to the category of Bool-categories. We can now state the precise meaning of this sentence. First, there exists a category PrO in which the objects are preorders and the morphisms are monotone maps. Second, there exists a category Bool-Cat in which the objects are Bool-categories and the morphisms are Bool-functors. We call these two categories equivalent because there exist functors $F: \operatorname{PrO} \rightarrow$ Bool-Cat and $G:$ Bool-Cat $\rightarrow \mathrm{PrO}$ such that there exist natural isomorphisms $F ; G \cong \operatorname{id}_{\text {PrO }}$ and $G \because F \cong \mathrm{id}_{\text {Bool-Cat }}$ in the sense of Definition 3.49.

\subsection*{3.3.5 The category of instances on a schema}

Definition 3.60. Suppose that $\mathcal{C}$ is a database schema and $I, J: \mathcal{C} \rightarrow$ Set are database instances. An instance homomorphism between them is a natural transformation $\alpha: I \rightarrow$ $J$. Write $\mathcal{C}$-Inst $:=$ Set $^{\mathfrak{C}}$ to denote the functor category as defined in Definition 3.54.

We saw in Example 3.53 that 1-Inst is equivalent to the category Set. In this subsection, we will show that there is a schema whose instances are graphs and whose instance homomorphisms are graph homomorphisms.

Extended example: the category of graphs as a functor category. You may find yourself back in the primordial ooze (first discussed in Section 2.3.2), because while previously we have been using graphs to present categories, now we obtain graphs themselves as database instances on a specific schema (which is itself a graph):

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-109.jpg?height=212&width=545&top_left_y=2317&top_left_x=779)

Here's an example $\mathbf{G r}$-instance, i.e. set-valued functor $I: \mathbf{G r} \rightarrow \mathbf{S e t}$, in table form:

\begin{tabular}{c|ccc} 
Arrow & source & target & Vertex \\
\hline$a$ & 1 & 2 & 1 \\
$b$ & 1 & 3 & 2 \\
$c$ & 1 & 3 & 3 \\
$d$ & 2 & 2 & 4 \\
$e$ & 2 & 3 &
\end{tabular}

Here $I$ (Arrow $)=\{a, b, c, d, e\}$, and $I($ Vertex $)=\{1,2,3,4\}$. One can draw the instance $I$ as a graph:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-110.jpg?height=271&width=480&top_left_y=786&top_left_x=817)

Every row in the Vertex table is drawn as a vertex, and every row in the Arrow table is drawn as an arrow, connecting its specified source and target. Every possible graph can be written as a database instance on the schema $\mathbf{G r}$, and every possible $\mathbf{G r}$-instance can be represented as a graph.

Exercise 3.62. In Eq. (3.2), a graph is shown (forget the distinction between white and black nodes). Write down the corresponding Gr-instance, as in Eq. (3.61). (Do not be concerned that you are in the primordial ooze.)

Thus the objects in the category Gr-Inst are graphs. The morphisms in Gr-Inst are called graph homomorphisms. Let's unwind this. Suppose that $G, H: \mathbf{G r} \rightarrow$ Set are functors (i.e. $\mathrm{Gr}$-instances); that is, they are objects $G, H \in \mathbf{G r}$-Inst. A morphism $G \rightarrow H$ is a natural transformation $\alpha: G \rightarrow H$ between them; what does that entail?

By Definition 3.49, since Gr has two objects, $\alpha$ consists of two components,

$$
\alpha_{\text {Vertex }}: G(\text { Vertex }) \rightarrow H(\text { Vertex }) \quad \text { and } \quad \alpha_{\text {Arrow }}: G(\text { Arrow }) \rightarrow H \text { (Arrow) }
$$

both of which are morphisms in Set. In other words, $\alpha$ consists of a function from vertices of $G$ to vertices of $H$ and a function from arrows of $G$ to arrows of $H$. For these functions to constitute a graph homomorphism, they must "respect source and target" in the precise sense that the naturality condition, Eq. (3.50) holds. That is, for every morphism in Gr, namely source and target, the following diagrams must commute:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-110.jpg?height=214&width=1417&top_left_y=2170&top_left_x=343)

These may look complicated, but they say exactly what we want. We want the functions $\alpha_{\text {Vertex }}$ and $\alpha_{\text {Arrow }}$ to respect source and targets in $G$ and $H$. The left diagram says "start
with an arrow in $G$. You can either apply $\alpha$ to the arrow and then take its source in $H$, or you can take its source in $G$ and then apply $\alpha$ to that vertex; either way you get the same answer." The right-hand diagram says the same thing about targets.

Example 3.63. Consider the graphs $G$ and $H$ shown below

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-111.jpg?height=129&width=1029&top_left_y=586&top_left_x=537)

Here they are, written as database instances-i.e. set-valued functors-on Gr:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-111.jpg?height=447&width=1390&top_left_y=828&top_left_x=362)

The top row is $G$ and the bottom row is $H$. They are offset so you can more easily complete the following exercise.

Exercise 3.64. We claim that-with $G, H$ as in Example 3.63-there is exactly one graph homomorphism $\alpha: G \rightarrow H$ such that $\alpha_{\text {Arrow }}(a)=d$.

1. What is the other value of $\alpha_{\text {Arrow }}$, and what are the three values of $\alpha_{\text {Vertex }}$ ?

2. In your own copy of the tables of Example 3.63, draw $\alpha_{\text {Arrow }}$ as two lines connecting the cells in the ID column of $G$ (Arrow) to those in the ID column of $H$ (Arrow). Similarly, draw $\alpha_{\text {Vertex }}$ as connecting lines.

3. Check the source column and target column and make sure that the matches are natural, i.e. that "alpha-then-source equals source-then-alpha" and similarly for "target."

\subsection*{3.4 Adjunctions and data migration}

We have talked about how set-valued functors on a schema can be understood as filling that schema with data. But there are also functors between schemas. When the two sorts of functors are composed, data is migrated. This is the simplest form of data migration; more complex ways to migrate data come from using adjoints. All of the above is the subject of this section.

\subsection*{3.4.1 Pulling back data along a functor}

To begin, we will migrate data between the graph-indexing schema $\mathrm{Gr}$ and the loop schema, which we call DDS, shown below
![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-112.jpg?height=250&width=1114&top_left_y=460&top_left_x=496)

We begin by writing down a sample instance $I:$ DDS $\rightarrow$ Set on this schema:

\begin{tabular}{c|c} 
State & next \\
\hline 1 & 4 \\
2 & 4 \\
3 & 5 \\
4 & 5 \\
5 & 5 \\
6 & 7 \\
7 & 6
\end{tabular}

We call the schema DDS to stand for discrete dynamical system. Indeed, we may think of the data in the DDS-instance of Eq. (3.65) as listing the states and movements of a deterministic machine: at every point in time the machine is in one of the listed states, and given the machine in one of the states, in the next instant it moves to a uniquely determined next state.

Our goal is to migrate the data in Eq. (3.65) to data on Gr; this will give us the data of a graph and so allow us to visualise our machine.

We will use a functor connecting these schemas in order to move data between them. The reader can create any functor she likes, but we will use a specific functor $F: \mathrm{Gr} \rightarrow$ DDS to migrate data in a way that makes sense to us, the authors. Here we draw $F$, using colors to hopefully aid understanding:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-112.jpg?height=344&width=564&top_left_y=1945&top_left_x=778)

The functor $F$ sends both objects of $\mathrm{Gr}$ to the 'State' object of DDS (as it must). On morphisms, it sends the 'source' morphism to the identity morphism on 'State', and the 'target' morphism to the morphism 'next'.

A sample database instance on DDS was given in Eq. (3.65); recall this is a functor $I:$ DDS $\rightarrow$ Set. So now we have two functors as follows:

$$
\begin{equation*}
\mathrm{Gr} \xrightarrow{F} \text { DDS } \xrightarrow{I} \text { Set. } \tag{3.66}
\end{equation*}
$$

Objects in Gr are sent by $F$ to objects in DDS, which are sent by $I$ to objects in Set, which are sets. Morphisms in $\mathrm{Gr}$ are sent by $F$ to morphisms in DDS, which are sent by $I$ to morphisms in Set, which are functions. This defines a composite functor $F ; I: \operatorname{Gr} \rightarrow$ Set. Both $F$ and $I$ respect identities and composition, so $F 9 I$ does too. Thus we have obtained an instance on $\mathrm{Gr}$, i.e. we have converted our discrete dynamical system from Eq. (3.65) into a graph! What graph is it?

For an instance on $\mathrm{Gr}$, we need to fill an Arrow table and a Vertex table. Both of these are sent by $F$ to State, so let's fill both with the rows of State in Eq. (3.65). Similarly, since $F$ sends 'source' to the identity and sends 'target' to 'next', we obtain the following tables:

\begin{tabular}{c|ccc} 
Arrow & source & target & Vertex \\
\hline 1 & 1 & 4 & 1 \\
2 & 2 & 4 & 2 \\
3 & 3 & 5 & 3 \\
4 & 4 & 5 & 4 \\
5 & 5 & 5 & 5 \\
6 & 6 & 7 & 6 \\
7 & 7 & 6 & 7
\end{tabular}

Now that we have a graph, we can draw it.

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-113.jpg?height=388&width=781&top_left_y=1584&top_left_x=669)

Each arrow is labeled by its source vertex, as if to say, "What I do next is determined by what I am now."

Exercise 3.67. Consider the functor $G: \mathbf{G r} \rightarrow$ DDS given by sending 'source' to 'next' and sending 'target' to the identity on 'State'. Migrate the same data, called $I$ in Eq. (3.65), using the functor $G$. Write down the tables and draw the corresponding graph.

$\diamond$

We refer to the above procedure-basically just composing functors as in Eq. (3.66)as "pulling back data along a functor." We just now pulled back data I along functor F.

Definition 3.68. Let $\mathcal{C}$ and $\mathcal{D}$ be categories and let $F: \mathcal{C} \rightarrow \mathcal{D}$ be a functor. For any set-valued functor $I: \mathcal{D} \rightarrow$ Set, we refer to the composite functor $F ; I: \mathcal{C} \rightarrow$ Set as the pullback of I along $F$.

Given a natural transformation $\alpha: I \Rightarrow J$, there is a natural transformation $\alpha_{F}: F \circ I \Rightarrow$ $F \because J$, whose component $(F ; I)(c) \rightarrow(F \because J)(c)$ for any $c \in \operatorname{Ob}(\mathcal{C})$ is given by $\left(\alpha_{F}\right)_{c}:=\alpha_{F c}$.

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-114.jpg?height=176&width=1011&top_left_y=557&top_left_x=557)

This uses the data of $F$ to define a functor $\Delta_{F}: \mathcal{D}$-Inst $\rightarrow \mathcal{C}$-Inst.

Note that the term pullback is also used for a certain sort of limit, for more details see Remark 3.100.

\subsection*{3.4.2 Adjunctions}

In Section 1.4 we discussed Galois connections, which are adjunctions between preorders. Now that we've defined categories and functors, we can discuss adjunctions in general. The relevance to databases is that the data migration functor $\Delta$ from Definition 3.68 always has two adjoints of its own: a left adjoint which we denote $\Sigma$ and a right adjoint which we denote $\Pi$.

Recall that an adjunction between preorders $P$ and $Q$ is a pair of monotone maps $f: P \rightarrow Q$ and $g: Q \rightarrow P$ that are almost inverses: we have

$$
\begin{equation*}
f(p) \leq q \text { if and only if } p \leq g(q) \tag{3.69}
\end{equation*}
$$

Recall from Section 3.2.3 that in a preorder $P$, a hom-set $P(a, b)$ has one element when $a \leq b$, and no elements otherwise. We can thus rephrase Eq. (3.69) as an isomorphism of sets $Q(f(p), q) \cong P(p, g(q))$ : either both are one-element sets or both are 0 -element sets. This suggests how to define adjunctions in the general case.

Definition 3.70. Let $\mathcal{C}$ and $\mathcal{D}$ be categories, and $L: \mathcal{C} \rightarrow \mathcal{D}$ and $R: \mathcal{D} \rightarrow \mathcal{C}$ be functors. We say that $L$ is left adjoint to $R$ (and that $R$ is right adjoint to $L$ ) if, for any $c \in \mathcal{C}$ and $d \in \mathcal{D}$, there is an isomorphism of hom-sets

$$
\alpha_{c, d}: \mathcal{C}(c, R(d)) \xrightarrow{\cong} \mathcal{D}(L(c), d)
$$

that is natural in $c$ and $d .{ }^{8}$

Given a morphism $f: c \rightarrow R(d)$ in $\mathcal{C}$, its image $g:=\alpha_{c, d}(f)$ is called its mate. Similarly, the mate of $g: L(c) \rightarrow d$ is $f$.

To denote an adjunction we write $L \dashv R$, or in diagrams,

$$
\mathcal{C} \underset{R}{\underset{\sim}{\rightleftarrows}} \mathcal{D}
$$

with the $\Rightarrow$ in the direction of the left adjoint.

Example 3.71. Recall that every preorder $\mathcal{P}$ can be regarded as a category. Galois connections between preorders and adjunctions between the corresponding categories are exactly the same thing.

Example 3.72. Let $B \in \mathrm{Ob}(\mathrm{Set})$ be any set. There is an adjunction called 'currying $B$,' after the logician Haskell Curry:

$$
\text { Set } \underset{(-)^{B}}{\stackrel{-\times B}{\Longrightarrow}} \text { Set } \quad \operatorname{Set}(A \times B, C) \cong \operatorname{Set}\left(A, C^{B}\right)
$$

Abstractly we write it as on the left, but what this means is that for any sets $A, C$, there is a natural isomorphism as on the right.

To explain this, we need to talk about exponential objects in Set. Suppose that $B$ and $C$ are sets. Then the set of functions $B \rightarrow C$ is also a set; let's denote it $C^{B}$. It's written this way because if $C$ has 10 elements and $B$ has 3 elements then $C^{B}$ has $10^{3}$ elements, and more generally for any two finite sets $\left|C^{B}\right|=|C|^{|B|}$.

The idea of currying is that given sets $A, B$, and $C$, there is a one-to-one correspondence between functions $(A \times B) \rightarrow C$ and functions $A \rightarrow C^{B}$. Intuitively, if $\mathrm{I}$ have a function $f$ of two variables $a, b$, I can "put off" entering the second variable: if you give me just $a$, I'll return a function $B \rightarrow C$ that's waiting for the $B$ input. This is the curried version of $f$. As one might guess, there is a formal connection between exponential objects and what we called hom-elements $b \multimap c$ in Definition 2.79.

Exercise 3.73. In Example 3.72, we discussed an adjunction between functors $-\times B$ and $(-)^{B}$. But we only said how these functors worked on objects: for an arbitrary set $X$, they return sets $X \times B$ and $X^{B}$ respectively.
\footnotetext{
${ }^{8}$ This naturality is between functors $\mathcal{C}^{\circ \mathrm{p}} \times \mathcal{D} \rightarrow$ Set. It says that for any morphisms $f: c^{\prime} \rightarrow c$ in $\mathcal{C}$ and $g: d \rightarrow d^{\prime}$ in $\mathcal{D}$, the following diagram commutes:
}

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-115.jpg?height=217&width=501&top_left_y=2252&top_left_x=801)

1. Given a morphism $f: X \rightarrow Y$, what morphism should $-\times B: X \times B \rightarrow Y \times B$ return?

2. Given a morphism $f: X \rightarrow Y$, what morphism should $(-)^{B}: X^{B} \rightarrow Y^{B}$ return?

3. Consider the function $+: \mathbb{N} \times \mathbb{N} \rightarrow \mathbb{N}$, which sends $(a, b) \mapsto a+b$. Currying + , we get a certain function $p: \mathbb{N} \rightarrow \mathbb{N}^{\mathbb{N}}$. What is $p(3)$ ?

Example 3.74. If you know some abstract algebra or topology, here are some other examples of adjunctions.

1. Free constructions: given any set you get a free group, free monoid, free ring, free vector space, etc.; each of these is a left adjoint. The corresponding right adjoint takes a group, a monoid, a ring, a vector space etc. and forgets the algebraic structure to return the underlying set.

2. Similarly, given a graph you get a free preorder or a free category, as we discussed in Section 3.2.3; each is a left adjoint. The corresponding right adjoint is the underlying graph of a preorder or of a category.

3. Discrete things: given any set you get a discrete preorder, discrete graph, discrete metric space, discrete category, discrete topological space; each of these is a left adjoint. The corresponding right adjoint is again underlying set.

4. Codiscrete things: given any set you get a codiscrete preorder, complete graph, codiscrete category, codiscrete topological space; each of these is a right adjoint. The corresponding left adjoint is the underlying set.

5. Given a group, you can quotient by its commutator subgroup to get an abelian group; this is a left adjoint. The right adjoint is the inclusion of abelian groups into groups.

\subsection*{3.4.3 Left and right pushforward functors, $\Sigma$ and $\Pi$}

Given $F: \mathcal{C} \rightarrow \mathcal{D}$, the data migration functor $\Delta_{F}$ turns $\mathcal{D}$-instances into $\mathcal{C}$-instances. This functor has both a left and a right adjoint:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-116.jpg?height=184&width=518&top_left_y=2334&top_left_x=801)

Using the names $\Sigma$ and $\Pi$ in this context is fairly standard in category theory. In the case of databases, they have the following helpful mnemonic:

\begin{tabular}{c|lll} 
Migration Functor & Pronounced & Reminiscent of & Database idea \\
\hline$\Delta$ & Delta & \begin{tabular}{l} 
Duplicate \\
or destroy \\
Sum
\end{tabular} & \begin{tabular}{l} 
Duplicate or destroy \\
tables or columns
\end{tabular} \\
$\Sigma$ & Sigma & Union (sum up) data
\end{tabular}

Just like we used $\Delta_{F}$ to pull back any discrete dynamical system along $F: \mathbf{G r} \rightarrow$ DDS and get a graph, the migration functors $\Sigma_{F}$ and $\Pi_{F}$ can be used to turn any graph into a discrete dynamical system. That is, given an instance $J: \mathbf{G r} \rightarrow$ Set, we can get instances $\Sigma_{F}(J)$ and $\Pi_{F}(J)$ on DDS. This, however, is quite technical, and we leave it to the adventurous reader to compute an example, with help perhaps from [Spi14a], which explores the definitions of $\Sigma$ and $\Pi$ in detail. A less technical shortcut is simply to code up the computation in the open-source FQL software.

To get the basic idea across without getting mired in technical details, here we shall instead discuss a very simple example. Recall the schemas from Eq. (3.5). We can set up a functor between them, the one sending black dots to black dots and white dots to white dots:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-117.jpg?height=412&width=1312&top_left_y=1380&top_left_x=404)

With this functor $F$ in hand, we can transform any $B$-instance into an $A$-instance using $\Delta_{F}$. Whereas $\Delta$ was interesting in the case of turning discrete dynamical systems into graphs in Section 3.4.1, it is not very interesting in this case. Indeed, it will just copy- $\Delta$ for duplicate-the rows in Airline seat into both Economy and First Class.

$\Delta_{F}$ has two adjoints, $\Sigma_{F}$ and $\Pi_{F}$, both of which transform any $A$-instance $I$ into a $B$-instance. The functor $\Sigma_{F}$ does what one would most expect from reading the names on each object: it will put into Airline Seat the union of Economy and First Class:

$$
\left.\Sigma_{F}(I)(\text { Airline Seat })=I(\text { Economy }) \sqcup I \text { (First Class }\right) \text {. }
$$

The functor $\Pi_{F}$ puts into Airline Seat the set of those pairs $(e, f)$ where $e$ is an Economy seat, $f$ is a First Class seat, and $e$ and $f$ have the same price and position.
\footnotetext{
${ }^{9}$ This is more commonly called "join" by database programmers.
}

In this particular example, one imagines that there should be no such seats in a valid instance $I$, in which case $\Pi_{F}(I)$ (Airline Seat) would be empty. But in other uses of these same schemas, $\Pi_{F}$ can be a useful operation. For example, in the schema $A$ replace the label 'Economy' by 'Rewards Program', and in $B$ replace 'Airline Seat' by 'First Class Seats'. Then the operation $\Pi_{F}$ finds those first class seats that are also rewards program seats. This operation is a kind of database query; querying is the operation that databases are built for.

The moral is that complex data migrations can be specified by constructing functors $F$ between schemas and using the "induced" functors $\Delta_{F}, \Sigma_{F}$, and $\Pi_{F}$. Indeed, in practice essentially all useful migrations can be built up from these. Hence the language of categories provides a framework for specifying and reasoning about data migrations.

\subsection*{3.4.4 Single set summaries of databases}

To give a stronger idea of the flavor of $\Sigma$ and $\Pi$, we consider another special case, namely where the target category $\mathcal{D}$ is equal to $\mathbf{1}$; see Exercise 3.12. In this case, there is exactly one functor $\mathcal{C} \rightarrow \mathbf{1}$ for any $\mathcal{C}$; let's denote it

$$
\begin{equation*}
!: \mathcal{C} \rightarrow \mathbf{1} \tag{3.75}
\end{equation*}
$$

Exercise 3.76. Describe the functor $!: \mathcal{C} \rightarrow \mathbf{1}$ from Eq. (3.75). Where does it send each object? What about each morphism?

$\diamond$

We want to consider the data migration functors $\Sigma_{!}: \mathcal{C}$-Inst $\rightarrow$ 1-Inst and $\Pi_{!}: \mathcal{C}$-Inst $\rightarrow$ 1-Inst. In Example 3.53, we saw that an instance on 1 is the same thing as a set. So let's identify 1-Inst with Set, and hence discuss

$$
\Sigma_{!}: \mathcal{C} \text {-Inst } \rightarrow \text { Set } \quad \text { and } \quad \Pi_{!}: \mathcal{C} \text {-Inst } \rightarrow \text { Set. }
$$

Given any schema $\mathcal{C}$ and instance $I: \mathcal{C} \rightarrow$ Set, we will get sets $\Sigma_{!}(I)$ and $\Pi_{!}(I)$. Thinking of these sets as database instances, each corresponds to a single one-column table-a controlled vocabulary-summarizing an entire database instance on the schema $\mathcal{C}$.

Consider the following schema

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-118.jpg?height=206&width=566&top_left_y=1849&top_left_x=774)

\begin{tabular}{|c|c|c|c|}
\hline Email & sent_by & received_by & Address \\
\hline Em_1 & Bob & Grace & Bob \\
\hline Em_2 & Grace & Pat & Doug \\
\hline Em_3 & Bob & Emmy & Emmy \\
\hline Em_4 & Sue & Doug & Grace \\
\hline Em_5 & Doug & Sue & Pat \\
\hline Em_6 & Bob & Bob & Sue \\
\hline
\end{tabular}

Here's a sample instance $I: \mathcal{G} \rightarrow$ Set:

Exercise 3.78. Note that $\mathcal{G}$ from Eq. (3.77) is isomorphic to the schema Gr. In Section 3.3.5 we saw that instances on $\mathrm{Gr}$ are graphs. Draw the above instance $I$ as a graph.

Now we have a unique functor !: $\mathcal{G} \rightarrow \mathbf{1}$, and we want to say what $\Sigma_{!}(I)$ and $\Pi_{!}(I)$ give us as single-set summaries. First, $\Sigma_{!}(I)$ tells us all the emailing groups-the "connected components"-in $I$ :

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-119.jpg?height=179&width=431&top_left_y=539&top_left_x=844)

This form of summary, involving identifying entries into common groups, or quotients, is typical of $\Sigma$-operations.

The functor $\Pi_{!}(I)$ lists the emails from $I$ which were sent from a person to her- or him-self.

$$
\frac{\mathbf{1}}{\text { Em_6 }}
$$

This is again a sort of query, selecting the entries that fit the criterion of self-to-self emails. Again, this is typical of $\Pi$-operations.

Where do these facts-that $\Pi_{!}$and $\Sigma_{!}$act the way we said-come from? Everything follows from the definition of adjoint functors (3.70): indeed we hope this, together with the examples given in Example 3.74, give the reader some idea of how general and useful adjunctions are, both in mathematics and in database theory.

One more point: while we will not spell out the details, we note that these operations are also examples of constructions known as colimits and limits in Set. We end this chapter with bonus material, exploring these key category theoretic constructions. The reader should keep in mind that, in general and not just for functors to $1, \Sigma$-operations are built from colimits in Set, and $\Pi$-operations are built from limits in Set.

\subsection*{3.5 Bonus: An introduction to limits and colimits}

What do products of sets, the results of $\prod_{!}$-operations on database instances, and meets in a preorder all have in common? The answer, as we shall see, is that they are all examples of limits. Similarly, disjoint unions of sets, the results of $\Sigma_{!}$-operations on database instances, and joins in a preorder are all colimits. Let's begin with limits.

Recall that $\Pi_{!}$takes a database instance $I: \mathcal{C} \rightarrow$ Set and turns it into a set $\Pi_{!}(I)$. More generally, a limit turns a functor $F: \mathcal{C} \rightarrow \mathcal{D}$ into an object of $\mathcal{D}$.

\subsection*{3.5.1 Terminal objects and products}

Terminal objects and products are each a sort of limit. Let's discuss them in turn.

Terminal objects. The most basic limit is a terminal object.

Definition 3.79. Let $\mathcal{C}$ be a category. Then an object $Z$ in $\mathcal{C}$ is a terminal object if, for each object $C$ of $\mathcal{C}$, there exists a unique morphism !: $C \rightarrow Z$.

Since this unique morphism exists for all objects in $\mathcal{C}$, we say that terminal objects have a universal property.

Example 3.80. In Set, any set with exactly one element is a terminal object. Why? Consider some such set $\{\bullet\}$. Then for any other set $C$ we need to check that there is exactly one function $!: C \rightarrow\{\bullet\}$. This unique function is the one that does the only thing that can be done: it maps each element $c \in C$ to the element $\bullet \in\{\bullet\}$.

Exercise 3.81. Let $(P, \leq)$ be a preorder, let $z \in P$ be an element, and let $\mathcal{P}$ be the corresponding category (see Section 3.2.3). Show that $z$ is a terminal object in $\mathcal{P}$ if and only if it is a top element in $P$ : that is, if and only if for all $c \in P$ we have $c \leq z$.

Exercise 3.82. Name a terminal object in the category Cat. (Hint: recall Exercise 3.76.)

Exercise 3.83. Not every category has a terminal object. Find one that doesn't. $\diamond$

Proposition 3.84. All terminal objects in a category $\mathcal{C}$ are isomorphic.

Proof. This is a simple, but powerful standard argument. Suppose $Z$ and $Z^{\prime}$ are both terminal objects in some category $\mathcal{C}$. Then there exist (unique) maps $a: Z \rightarrow Z^{\prime}$ and $b: Z^{\prime} \rightarrow Z$. Composing these, we get a map $a \circ b: Z \rightarrow Z$. Now since $Z$ is terminal, this map $Z \rightarrow Z$ must be unique. But id $Z$ is also such a map. So we must have $a ; b=\operatorname{id}_{Z}$. Similarly, we find that $b ; a=\operatorname{id}_{Z^{\prime}}$. Thus $a$ is an isomorphism, with inverse $b$.

Remark 3.85 ("The limit" vs. "a limit"). Not only are all terminal objects isomorphic, there is a unique isomorphism between any two. We hence say "terminal objects are unique up to unique isomorphism." To a category theorist, this is very nearly the same thing as saying "all terminal objects are equal." Thus we often abuse terminology and talk of 'the' terminal object, rather than "a" terminal object. We will do the same for any sort of limit or colimit, e.g. speak of "the product" of two sets, rather than "a product." We saw a similar phenomenon in Definition 1.81.

Products. Products are slightly more complicated to formalize than terminal objects, but they are familiar in practice.

Definition 3.86. Let $\mathcal{C}$ be a category, and let $X, Y$ be objects in $\mathcal{C}$. A product of $X$ and $Y$ is an object, denoted $X \times Y$, together with morphisms $p_{X}: X \times Y \rightarrow X$ and $p_{Y}: X \times Y \rightarrow Y$ such that for all objects $C$ together with morphisms $f: C \rightarrow X$ and $g: C \rightarrow Y$, there exists a unique morphism $C \rightarrow X \times Y$, denoted $\langle f, g\rangle$, for which the following diagram
commutes:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-121.jpg?height=238&width=437&top_left_y=285&top_left_x=844)

We will try to bring this down to earth in Example 3.87. Before we do, note that $X \times Y$ is an object equipped with morphisms to $X$ and $Y$. Roughly speaking, it is like "the best object-equipped-with-morphisms-to- $X$-and $-Y$ " in all of $\mathcal{C}$, in the sense that any other object-equipped-with-morphisms-to- $X$-and- $Y$ maps to it uniquely. This is called a universal property. It's customary to use a dotted line to indicate the unique morphism that exists because of some universal property.

Example 3.87. In Set, a product of two sets $X$ and $Y$ is their usual cartesian product

$$
X \times Y:=\{(x, y) \mid x \in X, y \in Y\}
$$

which comes with two projections $p_{X}: X \times Y \rightarrow X$ and $p_{Y}: X \times Y \rightarrow Y$, given by $p_{X}(x, y):=x$ and $p_{Y}(x, y):=y$.

Given any set $C$ with functions $f: C \rightarrow X$ and $g: C \rightarrow Y$, the unique map from $C$ to $X \times Y$ such that the required diagram commutes is given by $\langle f, g\rangle(c):=(f(c), g(c))$.

Here is a picture of the product $\underline{6} \times \underline{4}$ of sets $\underline{6}$ and $\underline{4}$.

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-121.jpg?height=727&width=976&top_left_y=1428&top_left_x=561)

Exercise 3.88. Let $(P, \leq)$ be a preorder, let $x, y \in P$ be elements, and let $\mathcal{P}$ be the corresponding category. Show that the product $x \times y$ in $\mathcal{P}$ agrees with their meet $x \wedge y$ in $P$.

Example 3.89. Given two categories $\mathcal{C}$ and $\mathcal{D}$, their product $\mathcal{C} \times \mathcal{D}$ may be given as follows. The objects of this category are pairs $(c, d)$, where $c$ is an object of $\mathcal{C}$ and $d$ is an object of $\mathcal{D}$. Similarly, morphisms $(c, d) \rightarrow\left(c^{\prime}, d^{\prime}\right)$ are pairs $(f, g)$ where $f: c \rightarrow c^{\prime}$ is a morphism in $\mathcal{C}$ and $g: d \rightarrow d^{\prime}$ is a morphism in $\mathcal{D}$. Composition of morphisms is simply given by composing each entry in the pair separately, so $(f, g) \circ\left(f^{\prime}, g^{\prime}\right)=\left(f ; f^{\prime}, g ; g^{\prime}\right)$.

Exercise 3.90 .

1. What are the identity morphisms in a product category $\mathcal{C} \times \mathcal{D}$ ?

2. Why is composition in a product category associative?

3. What is the product category $1 \times 2$ ?

4. What is the product category $\mathcal{P} \times \mathcal{Q}$ when $P$ and $Q$ are preorders and $\mathcal{P}$ and $Q$ are the corresponding categories?

These two constructions, terminal objects and products, are subsumed by the notion of limit.

\subsection*{3.5.2 Limits}

We'll get a little abstract. Consider the definition of product. This says that given any pair of maps $X \stackrel{f}{\leftarrow} C \xrightarrow{g} Y$, there exists a unique map $C \rightarrow X \times Y$ such that certain diagrams commute. This has the flavor of being terminal-there is a unique map to $X \times Y$-but it seems a bit more complicated. How are the two ideas related?

It turns out that products are terminal objects, but of a different category, which we'll call Cone $(X, Y)$, the category of cones over $X$ and $Y$ in $\mathcal{C}$. We will see in Exercise 3.91 that $X \stackrel{p_{X}}{\leftarrow} X \times Y \xrightarrow{p_{Y}} Y$ is a terminal object in Cone $(X, Y)$.

An object of Cone $(X, Y)$ is simply a pair of maps $X \stackrel{f}{\leftarrow} C \xrightarrow{g} Y$. A morphism from $X \stackrel{f}{\leftarrow} C \xrightarrow{g} Y$ to $X \stackrel{f^{\prime}}{\leftarrow} C^{\prime} \xrightarrow{g^{\prime}} Y$ in Cone $(X, Y)$ is a morphism $a: C \rightarrow C^{\prime}$ in $\mathcal{C}$ such that the following diagram commutes:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-122.jpg?height=242&width=358&top_left_y=1825&top_left_x=881)

Exercise 3.91. Check that a product $X \stackrel{p_{X}}{\longleftrightarrow} X \times Y \xrightarrow{p_{Y}} Y$ is exactly the same as a terminal object in $\operatorname{Cone}(X, Y)$.

We're now ready for the abstract definition. Don't worry if the details are unclear; the main point is that it is possible to unify terminal objects, maximal elements, and meets, products of sets, preorders, and categories, and many other familiar friends under the scope of a single definition. In fact, they're all just terminal objects in different categories.

Recall from Definition 3.51 that formally speaking, a diagram in $\mathcal{C}$ is just a functor $D: \mathcal{J} \rightarrow \mathcal{C}$. Here $\mathcal{J}$ is called the indexing category of the diagram $D$.

Definition 3.92. Let $D: \mathcal{J} \rightarrow \mathcal{C}$ be a diagram. A cone $\left(C, c_{*}\right)$ over $D$ consists of

(i) an object $C \in \mathcal{C}$;

(ii) for each object $j \in \mathcal{J}$, a morphism $c_{j}: C \rightarrow D(j)$.

To be a cone, these must satisfy the following property:

(a) for each $f: j \rightarrow k$ in $\mathcal{J}$, we have $c_{k}=c_{j} \circ D(f)$.

A morphism of cones $\left(C, c_{*}\right) \rightarrow\left(C^{\prime}, c_{*}^{\prime}\right)$ is a morphism $a: C \rightarrow C^{\prime}$ in $\mathcal{C}$ such that for all $j \in \mathcal{J}$ we have $c_{j}=a ; c_{j}^{\prime}$. Cones over $D$, and their morphisms, form a category Cone $(D)$.

The limit of $D$, denoted $\lim (D)$, is the terminal object in the category Cone $(D)$. Say it is the cone $\lim (D)=\left(C, c_{*}\right)$; we refer to $C$ as the limit object and the map $c_{j}$ for any $j \in \mathcal{J}$ as the $j^{\text {th }}$ projection map.

For visualization purposes, if $\mathcal{J}$ is the free category on the graph

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-123.jpg?height=157&width=455&top_left_y=1098&top_left_x=824)

with five objects and five non-identity morphisms, then we may draw a diagram $D: \mathcal{J} \rightarrow \mathcal{C}$ inside $\mathcal{C}$ as on the left below, and a cone on it as on the right:
![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-123.jpg?height=328&width=1418&top_left_y=1430&top_left_x=340)

Here, any two parallel paths that start at $C$ are considered the same. Note that both these diagrams depict a collection of objects and morphisms inside the category $\mathcal{C}$.

Example 3.93. Terminal objects are limits where the indexing category is empty, $\mathcal{J}=\varnothing$.

Example 3.94. Products are limits where the indexing category consists of two objects

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-123.jpg?height=73&width=537&top_left_y=2164&top_left_x=328)

\subsection*{3.5.3 Finite limits in Set}

Recall that this discussion was inspired by wanting to understand $\Pi$-operations, and in particular $\Pi_{!}$. We can now see that a database instance $I: \mathcal{C} \rightarrow$ Set is a diagram in

Set. The functor $\Pi_{!}$takes the limit of this diagram. In this subsection we give a formula describing the result. This captures all finite limits in Set.

In database theory, we work with categories $\mathcal{C}$ that are presented by a finite graph plus equations. We won't explain the details, but it's in fact enough just to work with the graph part: as far as limits are concerned, the equations in $\mathcal{C}$ don't matter. For consistency with the rest of this section, let's denote the database schema by $\mathcal{J}$ instead of $\mathcal{C}$.

Theorem 3.95. Let $\mathcal{J}$ be a category presented by the finite graph $(V, A, s, t)$ together with some equations, and let $D: \mathcal{J} \rightarrow$ Set be a set-valued functor. Write $V=\left\{v_{1}, \ldots, v_{n}\right\}$. The set

$$
\begin{aligned}
\lim _{\gamma} D:=\left\{\left(d_{1}, \ldots, d_{n}\right) \mid d_{i} \in D\left(v_{i}\right) \text { for all } 1 \leq i \leq n\right. \text { and } \\
\text { for all } \left.a: v_{i} \rightarrow v_{j} \in A, \text { we have } D(a)\left(d_{i}\right)=d_{j}\right\} .
\end{aligned}
$$

together with the projection maps $p_{i}:\left(\lim _{\mathcal{J}} D\right) \rightarrow D\left(v_{i}\right)$ given by $p_{i}\left(d_{1}, \ldots, d_{n}\right):=d_{i}$, is a limit of $D$.

Example 3.96. If $J$ is the empty graph $\square$, then $n=0$ : there are no vertices. There is exactly one empty tuple ( ), which vacuously satisfies the properties, so we've constructed the limit as the singleton set $\{()\}$ consisting of just the empty tuple. Thus the limit of the empty diagram, i.e. the terminal object in Set is the singleton set. See Remark 3.85.

Exercise 3.97. Show that the limit formula in Theorem 3.95 works for products. See Example 3.94.

Exercise 3.98. If $D: \mathbf{1} \rightarrow$ Set is a functor, what is the limit of $D$ ? Compute it using Theorem 3.95, and check your answer against Definition 3.92.

Pullbacks. In particular, the condition that the limit of $D: \mathcal{J} \rightarrow$ Set selects tuples $\left(d_{1}, \ldots, d_{n}\right)$ such that $D(a)\left(d_{i}\right)=d_{j}$ for each morphism $a: i \rightarrow j$ in $\mathcal{J}$ allows us to use limits to select data that satisfies certain equations or constraints. This is what allows us to express queries in terms of limits. Here is an example.

Example 3.99. If $J$ is presented by the cospan graph $\xrightarrow{x} \xrightarrow{f} \stackrel{a}{\longleftrightarrow} y$, then its limit is known as a pullback. Given the diagram $X \stackrel{f}{\rightarrow} A \stackrel{g}{\leftarrow} Y$, the pullback is the cone shown
on the left below:
![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-125.jpg?height=226&width=806&top_left_y=320&top_left_x=648)

The fact that the diagram commutes means that the diagonal arrow $c_{a}$ is in some sense superfluous, so one generally denotes pullbacks by dropping the diagonal arrow, naming the cone point $X \times_{A} Y$, and adding the $\lrcorner$ symbol, as shown to the right above.

Here is a picture to help us unpack the definition in Set. We take $X=\underline{6}, Y=\underline{4}$, and $A$ to be the set of colors \{red, blue, black\}.

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-125.jpg?height=531&width=805&top_left_y=865&top_left_x=649)

The functions $f: \underline{6} \rightarrow A$ and $g: \underline{4} \rightarrow A$ are expressed in the coloring of the dots: for example, $g(2)=g(4)=$ red, while $f(5)=$ black. The pullback selects pairs $(i, j) \in \underline{6} \times \underline{4}$ such that $f(i)$ and $g(j)$ have the same color.

Remark 3.100. As mentioned following Definition 3.68, this definition of pullback is not to be confused with the pullback of a set-valued functor along a functor; they are for now best thought of as different concepts which accidentally have the same name. Due to the power of the primordial ooze, however, the pullback along a functor is a special case of pullback as the limit of a cospan: it can be understood as the pullback of a certain cospan in Cat. To unpack this, however, requires the notions of category of elements and discrete opfibration; ask your friendly neighborhood category theorist.

\subsection*{3.5.4 A brief note on colimits}

Just like upper bounds have a dual concept-namely that of lower bounds-so limits have a dual concept: colimits. To expose the reader to this concept, we provide a succinct definition of these using opposite categories and opposite functors. The point, however, is just exposure; we will return to explore colimits in detail in Chapter 6.

Exercise 3.101. Recall from Example 3.27 that every category $\mathcal{C}$ has an opposite $\mathcal{C}^{\text {oop }}$. Let $F: \mathcal{C} \rightarrow \mathcal{D}$ be a functor. How should we define its opposite, $F^{\mathrm{op}}: \mathcal{C}^{\mathrm{op}} \rightarrow \mathcal{D}^{\mathrm{op}}$ ? That
is, how should $F^{\text {op }}$ act on objects, and how should it act on morphisms?

Definition 3.102. Given a category $\mathcal{C}$ we say that a cocone in $\mathcal{C}$ is a cone in $\mathcal{C}^{o p}$.

Given a diagram $D: \mathcal{J} \rightarrow \mathcal{C}$, we may take the limit of the functor $D^{\text {op }}: \mathcal{J}^{\circ \mathrm{p}} \rightarrow \mathcal{C}^{\text {op }}$. This is a cone in $\mathcal{C}^{\circ p}$, and so by definition a cocone in $\mathcal{C}$. The colimit of $D$ is this cocone.

Definition 3.102 is like a compressed file: useful for transmitting quickly, but completely useless for working with, unless you can successfully unpack it. We will unpack it later in Chapter 6 when we discuss electric circuits.

\subsection*{3.6 Summary and further reading}

Congratulations on making it through one of the longest chapters in the book! We apologize for the length, but this chapter had a lot of work to do. Namely it introduced the "big three" of category theory-categories, functors, and natural transformationsas well as discussed adjunctions, limits, and very briefly colimits.

That's really quite a bit of material. For more on all these subjects, one can consult any standard book on category theory, of which there are many. The bible (old, important, seminal, and requires a priest to explain it) is [Mac98]; another thorough introduction is [Bor94]; a logical perspective is given in [Awo10]; a computer science perspective is given in [BW90] and [Pie91] and [Wa192]; math students should probably read [Lei14] or [Rie17] or [Gra18]; a general audience might start with [Spi14a].

We presented categories from a database perspective, because data is pretty ubiquitous in our world. A database schema-i.e. a system of interlocking tables-can be captured by a category $\mathcal{C}$, and filling it with data corresponds to a functor $\mathcal{C} \rightarrow$ Set. Here Set is the category of sets, perhaps the most important category to mathematicians.

The perspective of using category theory to model databases has been rediscovered several times. It seems to have first been discussed by various authors around the mid-90's [IP94; CD95; PS95; TG96]. Bob Rosebrugh and collaborators took it much further in a series of papers including [FGR03; JR02; RW92]. Most of these authors tend to focus on sketches, which are more expressive categories. Spivak rediscovered the idea again quite a bit later, but focused on categories rather than sketches, so as to have all three data migration functors $\Delta, \Sigma, \Pi$; see [Spi12; SW15b]. The version of this story presented in the chapter, including the white and black nodes in schemas, is part of a larger theory of algebraic databases, where a programming language such as Java or Haskell is attached to a database. The technical details are worked out in [Sch+17], and its use in database integration projects can be found in [SW15a; Wis+15].

Before we leave this chapter, we want to emphasize two things: coherence conditions and universal constructions.

Coherence conditions. In the definitions of category, functor, and natural transformations, we have data (indexed by (i)) that is required to satisfy certain properties (indexed
by (a)). Indeed, for categories it was about associativity and unitality of composition, for functors it was about respecting composition and identities, and for natural transformations it was the naturality condition. These conditions are often called coherence conditions: we want the various structures to cohere, to work well together, rather than to flop around unattached.

Understanding why these particular structures and coherence conditions are "the right ones" is more science than mathematics: we empirically observe that certain combinations result in ideas that are both widely applicable and also strongly compositional. That is, we become satisfied with coherence conditions when they result in beautiful mathematics down the road.

Universal constructions. Universal constructions are one of the most important themes of category theory. Roughly speaking, one gives some specified shape in a category and says "find me the best solution!" And category theory comes back and says "do you want me to approximate from the left or the right (colimit or limit)?" You respond, and either there is a best solution or there is not. If there is, it's called the (co)limit; if there's not we say "the (co)limit does not exist."

Even data migration fits this form. We say "find me the closest thing in $\mathcal{D}$ that matches my $\mathcal{C}$-instance using my functor $F: \mathcal{C} \rightarrow \mathcal{D}$." In fact this approach—known as Kan extensions-subsumes the others. One of the two founders of category theory, Saunders Mac Lane, has a section in his book [Mac98] called "All concepts are Kan extensions," a big statement, no?

\section*{Chapter 4}

\section*{Collaborative design: Profunctors, categorification, and monoidal categories}

\subsection*{4.1 Can we build it?}

When designing a large-scale system, many different fields of expertise are joined to work on a single project. Thus the whole project team is divided into multiple sub-teams, each of which is working on a sub-project. And we recurse downward: the sub-project is again factored into sub-sub-projects, each with their own team. One could refer to this sort of hierarchical design process as collaborative design, or co-design. In this chapter, we discuss a mathematical theory of co-design, due to Andrea Censi [Cen15].

Consider just one level of this hierarchy: a project and a set of teams working on it. Each team is supposed to provide resources-sometimes called "functionalities"-to the project, but the team also requires resources in order to do so. Different design teams must be allowed to plan and work independently from one another in order for progress to be made. Yet the design decisions made by one group affect the design decisions others can make: if $A$ wants more space in order to provide a better radio speaker, then $B$ must use less space. So these teams-though ostensibly working independently-are dependent on each other after all.

The combination of dependence and independence is crucial for progress to be made, and yet it can cause major problems. When a team requires more resources than it originally expected to require, or if it cannot provide the resources that it originally claimed it could provide, the usual response is for the team to issue a design-change notice. But these affect neighboring teams: if team $A$ now requires more than originally claimed, team $B$ may have to change their design, which can in turn affect team $C$. Thus these design-change notices can ripple through the system through feedback loops and
can cause whole projects to fail [S+15].

As an example, consider the design problem of creating a robot to carry some load at some velocity. The top-level planner breaks the problem into three design teams: chassis team, motor team, and battery team. Each of these teams could break up into multiple parts and the process repeated, but let's remain at the top level and consider the resources produced and the resources required by each of our three teams.

The chassis in some sense provides all the functionality-it carries the load at the velocity-but it requires some things in order to do so. It requires money to build, of course, but more to the point it requires a source of torque and speed. These are supplied by the motor, which in turn needs voltage and current from the battery. Both the motor and the battery cost money, but more importantly they need to be carried by the chassis: they become part of the load. A feedback loop is created: the chassis must carry all the weight, even that of the parts that power the chassis. A heavier battery might provide more energy to power the chassis, but is the extra power worth the heavier load?

In the following picture, each part-chassis, motor, battery, and robot-is shown as a box with ports on the left and right. The functionalities, or resources produced by the part are shown as ports on the left of the box, and the resources required by the part are shown as ports on its right.

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-130.jpg?height=469&width=1356&top_left_y=1300&top_left_x=379)

The boxes marked $\Sigma$ correspond to summing inputs. These boxes are not to be designed, but we will see later that they fit easily into the same conceptual framework. Note also the $\leq$ 's on each wire; they indicate that if box $A$ requires a resource that box $B$ produces, then $A$ 's requirement must be less-than-or-equal-to $B$ 's production. The chassis requires torque, and the motor must produce at least that much torque.

To formalize this a bit more, let's call diagrams like the one above co-design diagrams. Each of the wires in a co-design diagram represents a preorder of resources. For example, in Eq. (4.1) every wire corresponds to a resource type-weights, velocities, torques, speeds, costs, voltages, and currents-where resources of each type can be ordered from less useful to more useful. In general, these preorders do not have to be linear orders, though in the above cases each will likely correspond to a linear order: $\$ 10 \leq \$ 20,5 \mathrm{~W} \leq 6 \mathrm{~W}$, and so on.

Each of the boxes in a co-design diagram corresponds to what we call a feasibility
relation. A feasibility relation matches resource production with requirements. For every pair $(p, r) \in P \times R$, where $P$ is the preorder of resources to be produced and $R$ is the preorder of resources to be required, the box says "true" or "false"-feasible or infeasible-for that pair. In other words, "yes I can provide $p$ given $r$ " or "no, I cannot provide $p$ given $r . "$

Feasibility relations hence define a function $\Phi: P \times R \rightarrow$ Bool. For a function $\Phi: P \times R \rightarrow$ Bool to make sense as a feasibility relation, however, there are two conditions:

(a) If $\Phi(p, r)=$ true and $p^{\prime} \leq p$, then $\Phi\left(p^{\prime}, r\right)=$ true.

(b) If $\Phi(p, r)=$ true and $r \leq r^{\prime}$ then $\Phi\left(p, r^{\prime}\right)=$ true.

These conditions, which we will see again in Definition 4.2, say that if you can produce $p$ given resources $r$, you can (a) also produce less $p^{\prime} \leq p$ with the same resources $r$, and (b) also produce $p$ given more resources $r^{\prime} \geq r$. We will see that these two conditions are formalized by requiring $\Phi$ to be a monotone map $P^{\text {op }} \times R \rightarrow$ Bool.

A co-design problem, represented by a co-design diagram, asks us to find the composite of some feasibility relations. It asks, for example, given these capabilities of the chassis, motor, and battery teams, can we build a robot together? Indeed, a co-design diagram factors a problem-for example, that of designing a robot-into interconnected subproblems, as in Eq. (4.1). Once the feasibility relation is worked out for each of the subproblems, i.e. the inner boxes in the diagram, the mathematics provides an algorithm producing the feasibility relation of the whole outer box. This process can be recursed downward, from the largest problem to tiny subproblems.

In this chapter, we will understand co-design problems in terms of enriched profunctors, in particular Bool-profunctors. A Bool-profunctor is like a bridge connecting one preorder to another. We will show how the co-design framework gives rise to a structure known as a compact closed category, and that any compact closed category can interpret the sorts of wiring diagrams we see in Eq. (4.1).

\subsection*{4.2 Enriched profunctors}

In this section we will understand how co-design problems form a category. Along the way we will develop some abstract machinery that will allow us to replace preorder design spaces with other enriched categories.

\subsection*{4.2.1 Feasibility relationships as Bool-profunctors}

The theory of co-design is based on preorders: each resource-e.g. velocity, torque, or $\$$-is structured as a preorder. The order $x \leq y$ represents the availability of $x$ given $y$, i.e. that whenever you have $y$, you also have $x$. For example, in our preorder of wattage, if $5 \mathrm{~W} \leq 10 \mathrm{~W}$, it means that whenever we are provided $10 \mathrm{~W}$, we implicitly also have $5 \mathrm{~W}$. Above we referred to this as an order from less useful to more useful: if $x$ is always available given $y$, then $x$ is less useful than $y$.

We know from Section 2.3.2 that a preorder $\mathcal{X}$ can be conceived of as a Bool-category. Given $x, y \in X$, we have $X(x, y) \in \mathbb{B}$; this value responds to the assertion " $x$ is available given $y$," marking it either true or false.

Our goal is to see feasibility relations as Bool-profunctors, which are a special case of something called enriched profunctors. Indeed, we hope that this chapter will give you some intuition for profunctors, arising from the table

\section*{Bool-category $\mid$ preorder \\ Bool-functor monotone map \\ Bool-profunctor feasibility relation}

Because enriched profunctors are a bit abstract, we first concretely discuss Boolprofunctors as feasibility relations. Recall that if $X=(X, \leq)$ is a preorder, then its opposite $X^{\circ \mathrm{op}}=(X, \geq)$ has $x \geq y$ iff $y \leq x$.

Definition 4.2. Let $X=\left(X, \leq_{X}\right)$ and $y=\left(Y, \leq_{Y}\right)$ be preorders. A feasibility relation for $X$ given $y$ is a monotone map

$$
\begin{equation*}
\Phi: X^{\mathrm{op}} \times y \rightarrow \text { Bool. } \tag{4.3}
\end{equation*}
$$

We denote this by $\Phi: x \rightarrow y$.

Given $x \in X$ and $y \in Y$, if $\Phi(x, y)=$ true we say $x$ can be obtained given $y$.

As mentioned in the introduction, the requirement that $\Phi$ is monotone says that if $x^{\prime} \leq_{X} x$ and $y \leq_{Y} y^{\prime}$ then $\Phi(x, y) \leq_{\text {Bool }} \Phi\left(x^{\prime}, y^{\prime}\right)$. In other words, if $x$ can be obtained given $y$, and if $x^{\prime}$ is available given $x$, then $x^{\prime}$ can be obtained given $y$. And if furthermore $y$ is available given $y^{\prime}$, then $x^{\prime}$ can also be obtained given $y^{\prime}$.

Exercise 4.4. Suppose we have the preorders

$$
x:=\begin{array}{|r|}
\hline \text { monoid } \\
\text { category } \\
\text { preorder }
\end{array} \quad y:=\begin{aligned}
& \text { this book } \\
& \text { nothing } \\
& \hline
\end{aligned}
$$

1. Draw the Hasse diagram for the preorder $x^{\mathrm{op}} \times y$.

2. Write down a profunctor $\Lambda: x \rightarrow y$ and, reading $\Lambda(x, y)=$ true as "my aunt can explain an $x$ given $y$, ," give an interpretation of the fact that the preimage of true forms an upper set in $X^{\text {op }} \times y$.

To generalize the notion of feasibility relation, we must notice that the symmetric monoidal preorder Bool has more structure than just that of a symmetric monoidal preorder: as mentioned in Exercise 2.93, Bool is a quantale. That means it has all joins $\vee$, and a closure operation, which we'll write $\Rightarrow: \mathbb{B} \times \mathbb{B} \rightarrow \mathbb{B}$. By definition, this operation satisfies the property that for all $b, c, d \in \mathbb{B}$ one has

$$
\begin{equation*}
b \wedge c \leq d \quad \text { iff } \quad b \leq(c \Rightarrow d) \tag{4.5}
\end{equation*}
$$

The operation $\Rightarrow$ is given by the following table:

\begin{tabular}{cc|c}
$c$ & $d$ & $c \Rightarrow d$ \\
\hline true & true & true \\
true & false & false \\
false & true & true \\
false & false & true
\end{tabular}

Exercise 4.7. Show that $\Rightarrow$ as defined in Eq. (4.6) indeed satisfies Eq. (4.5).

On an abstract level, it is the fact that Bool is a quantale which makes everything in this chapter work; any other (unital commutative) quantale also defines a way to interpret co-design diagrams. For example, we could use the quantale Cost, which would describe not whether $x$ is available given $y$ but the cost of obtaining $x$ given $y$; see Example 2.37 and Definition 2.46.

\subsection*{4.2.2 $\mathcal{V}$-profunctors}

We are now ready to recast Eq. (4.3) in abstract terms. Recall the notions of enriched product (Definition 2.74), enriched functor (Definition 2.69), and quantale (Definition 2.79).

Definition 4.8. Let $\mathcal{V}=(V, \leq, I, \otimes)$ be a (unital commutative) quantale, ${ }^{1}$ and let $\mathcal{X}$ and $y$ be $\mathcal{V}$-categories. A $\mathcal{V}$-profunctor from $\mathcal{X}$ to $y$, denoted $\Phi: X \rightarrow y$, is a $\mathcal{V}$-functor

$$
\Phi: X^{\mathrm{op}} \times y \rightarrow \mathcal{V}
$$

Note that a $\mathcal{V}$-functor must have $\mathcal{V}$-categories for domain and codomain, so here we are considering $\mathcal{V}$ as enriched in itself; see Remark 2.89.

Exercise 4.9. Show that a $\mathcal{\nu}$-profunctor (Definition 4.8) is the same as a function $\Phi: \operatorname{Ob}(\mathcal{X}) \times \mathrm{Ob}(y) \rightarrow V$ such that for any $x, x^{\prime} \in X$ and $y, y^{\prime} \in y$ the following inequality holds in $\mathcal{V}$ :

$$
x\left(x^{\prime}, x\right) \otimes \Phi(x, y) \otimes y\left(y, y^{\prime}\right) \leq \Phi\left(x^{\prime}, y^{\prime}\right)
$$

Exercise 4.10. Is it true that a Bool-profunctor, as in Definition 4.8, is exactly the same as a feasibility relation, as in Definition 4.2, once you peel back all the jargon? Or is there some subtle difference?

We know that Definition 4.8 is quite abstract. But have no fear, we will take you through it in pictures.
\footnotetext{
${ }^{1}$ From here on, as in Chapter 2, whenever we speak of quantales we mean unital commutative quantales.
}

Example 4.11 (Bool-profunctors and their interpretation as bridges). Let's discuss Definition 4.8 in the case $\mathcal{V}=$ Bool. One way to imagine a Bool-profunctor $\Phi: X \rightarrow Y$ is in terms of building bridges between two cities. Recall that a preorder (a Bool-category) can be drawn using a Hasse diagram. We'll think of the preorder as a city, and each vertex in it as some point of interest. An arrow $A \rightarrow B$ in the Hasse diagram means that there exists a way to get from point $A$ to point $B$ in the city. So what's a profunctor?

A profunctor is just a bunch of bridges connecting points in one city to points in another. Let's see a specific example. Here is a picture of a Bool-profunctor $\Phi: X \rightarrow Y$ :

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-134.jpg?height=523&width=1052&top_left_y=741&top_left_x=534)

Both $X$ and $Y$ are preorders, e.g. with $W \leq N$ and $b \leq a$. With bridges coming from the profunctor in blue, one can now use both paths within the cities and the bridges to get from points in city $X$ to points in city $Y$. For example, since there is a path from $N$ to $e$ and $E$ to $a$, we have $\Phi(N, e)=$ true and $\Phi(E, a)=$ true. On the other hand, since there is no path from $W$ to $d$, we have $\Phi(W, d)=$ false.

In fact, one could put a box around this entire picture and see a new preorder with $W \leq N \leq c \leq a$, etc. This is called the collage of $\Phi$; we'll explore this in more detail later; see Definition 4.42.

Exercise 4.12. We can express $\Phi$ as a matrix where the $(m, n)$ th entry is the value of $\Phi(m, n) \in \mathbb{B}$. Fill out the Bool-matrix:

\begin{tabular}{c|ccccc}
$\Phi$ & $a$ & $b$ & $c$ & $d$ & $e$ \\
\hline$N$ & $?$ & $?$ & $?$ & $?$ & true \\
$E$ & true & $?$ & $?$ & $?$ & $?$ \\
$W$ & $?$ & $?$ & $?$ & false & $?$ \\
$S$ & $?$ & $?$ & $?$ & $?$ & $?$
\end{tabular}

We'll call this the feasibility matrix of $\Phi$.

Example 4.13 (Cost-profunctors and their interpretation as bridges). Let's now consider Cost-profunctors. Again we can view these as bridges, but this time our bridges are
labelled by their length. Recall from Definition 2.53 and Eq. (2.56) that Cost-categories are Lawvere metric spaces, and can be depicted using weighted graphs. We'll think of such a weighted graph as a chart of distances between points in a city, and generate a Cost-profunctor by building a few bridges between the cities.

Here is a depiction of a Cost-profunctor $\Phi: X \rightarrow Y$ :

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-135.jpg?height=388&width=1041&top_left_y=543&top_left_x=539)

The distance from a point $x$ in city $X$ to a point $y$ in city $Y$ is given by the shortest path that runs from $x$ through $X$, then across one of the bridges, and then through $Y$ to the destination $y$. So for example

$$
\Phi(B, x)=11, \quad \Phi(A, z)=20, \quad \Phi(C, y)=17
$$

Exercise 4.15. Fill out the Cost-matrix:

\begin{tabular}{c|ccc}
$\Phi$ & $x$ & $y$ & $z$ \\
\hline$A$ & $?$ & $?$ & 20 \\
$B$ & 11 & $?$ & $?$ \\
$C$ & $?$ & 17 & $?$ \\
$D$ & $?$ & $?$ & $?$
\end{tabular}

Remark 4.16 (Computing profunctors via matrix multiplication). We can give an algorithm for computing the above distance matrix using matrix multiplication. First, just like in Eq. (2.59), we can begin with the labelled graphs in Eq. (4.14) and read off the matrices of arrow labels for $X, Y$, and $\Phi$ :

\begin{tabular}{c|cccc}
$M_{X}$ & $A$ & $B$ & $C$ & $D$ \\
\hline$A$ & 0 & $\infty$ & 3 & $\infty$ \\
$B$ & 2 & 0 & $\infty$ & 5 \\
$C$ & $\infty$ & 3 & 0 & $\infty$ \\
$D$ & $\infty$ & $\infty$ & 4 & 0
\end{tabular}

\begin{tabular}{c|ccc}
$M_{\Phi}$ & $x$ & $y$ & $z$ \\
\hline$A$ & $\infty$ & $\infty$ & $\infty$ \\
$B$ & 11 & $\infty$ & $\infty$ \\
$C$ & $\infty$ & $\infty$ & $\infty$ \\
$D$ & $\infty$ & 9 & $\infty$
\end{tabular}

\begin{tabular}{c|ccc}
$M_{Y}$ & $x$ & $y$ & $z$ \\
\hline$x$ & 0 & 4 & 3 \\
$y$ & 3 & 0 & $\infty$ \\
$z$ & $\infty$ & 4 & 0
\end{tabular}

Recall from Section 2.5.3 that the matrix of distances $d_{Y}$ for Cost-category $X$ can be obtained by taking the matrix power of $M_{X}$ with smallest entries, and similarly for $Y$. The matrix of distances for the profunctor $\Phi$ will be equal to $d_{X} * M_{\Phi} * d_{Y}$. In fact, since $X$ has four elements and $Y$ has three, we also know that $\Phi=M_{X}^{3} * M_{\Phi} * M_{Y}^{2}$.

Exercise 4.17. Calculate $M_{X}^{3} * M_{\Phi} * M_{Y}^{2}$, remembering to do matrix multiplication according to the (min, + )-formula for matrix multiplication in the quantale Cost; see Eq. (2.101).

Your answer should agree with what you got in Exercise 4.15; does it?

\subsection*{4.2.3 Back to co-design diagrams}

Each box in a co-design diagram has a left-hand and a right-hand side, which in turn consist of a collection of ports, which in turn are labeled by preorders. For example, consider the chassis box below:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-136.jpg?height=241&width=434&top_left_y=595&top_left_x=843)

Its left side consists of two ports-one for load and one for velocity-and these are the functionality that the chassis produces. Its right side consists of three portsone for torque, one for speed, and one for $\$$-and these are the resources that the chassis requires. Each of these resources is to be taken as a preorder. For example, load might be the preorder $([0, \infty], \leq)$, where an element $x \in[0, \infty]$ represents the idea "I can handle any load up to $x$.," while $\$$ might be the two-element preorder \{up_to_ $\$ 100$, more_than_ $\$ 100$ \}, where the first element of this set is less than the second.

We then multiply—i.e. we take the product preorder-of all preorders on the left, and similarly for those on the right. The box then represents a feasibility relation between the results. For example, the chassis box above represents a feasibility relation

$$
\text { Chassis: }(\text { load } \times \text { velocity }) \rightarrow(\text { torque } \times \text { speed } \times \$)
$$

Let's walk through this a bit more concretely. Consider the design problem of filming a movie, where you must pit the tone and entertainment value against the cost. A feasibility relation describing this situation details what tone and entertainment value can be obtained at each cost; as such, it is described by a feasibility relation $\Phi:(T \times E) \nrightarrow \$$. We represent this by the box

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-136.jpg?height=192&width=309&top_left_y=1842&top_left_x=903)

where $T, E$, and $\$$ are the preorders drawn below:
![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-136.jpg?height=382&width=996&top_left_y=2129&top_left_x=560)

A possible feasibility relation is then described by the profunctor

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-137.jpg?height=442&width=1000&top_left_y=359&top_left_x=557)

This says, for example, that a good-natured but boring movie costs $\$ 500 \mathrm{~K}$ to produce (of course, the producers would also be happy to get $\$ 1 \mathrm{M}$ ).

To elaborate, each arrow in the above diagram is to be interpreted as saying, "I can provide the source given the target". For example, there are arrows witnessing each of "I can provide $\$ 500 \mathrm{~K}$ given $\$ 1 \mathrm{M}$ ", "I can provide a good-natured but boring movie given $\$ 500 \mathrm{~K}$ ", and "I can provide a mean and boring movie given a good-natured but boring movie". Moreover, this relationship is transitive, so the path from (mean, boring) to $\$ 1 \mathrm{M}$ indicates also that "I can provide a mean and boring movie given $\$ 1 \mathrm{M}$ ".

Note the similarity and difference with the bridge interpretation of profunctors in Example 4.11: the arrows still indicate the possibility of moving between source and target, but in this co-design driven interpretation we understand them as indicating that it is possible to get to the source from the target.

Exercise 4.18. In the above diagram, the node ( $\mathrm{g} / \mathrm{n}$, funny) has no dashed blue arrow emerging from it. Is this valid? If so, what does it mean?

\subsection*{4.3 Categories of profunctors}

There is a category Feas whose objects are preorders and whose morphisms are feasibility relations. In order to describe it, we must give the composition formula and the identities, and prove that they satisfy the properties of being a category: unitality and associativity.

\subsection*{4.3.1 Composing profunctors}

If feasibility relations are to be morphisms, we need to give a formula for composing two of them in series. Imagine you have cities $\mathcal{P}, \mathcal{Q}$, and $\mathcal{R}$ and you have bridges-and
hence feasibility matrices-connecting these cities, say $\Phi: \mathcal{P} \rightarrow Q$ and $\Psi: Q \rightarrow \mathcal{R}$.

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-138.jpg?height=464&width=1222&top_left_y=337&top_left_x=403)

The feasibility matrices for $\Phi$ (in blue) and $\Psi$ (in red) are:

\begin{tabular}{c|ccccc}
$\Phi$ & $a$ & $b$ & $c$ & $d$ & $e$ \\
\hline$N$ & true & false & true & false & false \\
$E$ & true & false & true & false & true \\
$W$ & true & true & true & true & false \\
$S$ & true & true & true & true & true
\end{tabular}

\begin{tabular}{c|cc}
$\Psi$ & $x$ & $y$ \\
\hline$a$ & false & true \\
$b$ & true & true \\
$c$ & false & true \\
$d$ & true & true \\
$e$ & false & false
\end{tabular}

As in Remark 2.95, we personify a quantale as a navigator. So imagine a navigator is trying to give a feasibility matrix $\Phi ; \Psi$ for getting from $\mathcal{P}$ to $\mathcal{R}$. How should this be done? Basically, for every pair $p \in \mathcal{P}$ and $r \in \mathcal{R}$, the navigator searches through $\mathcal{Q}$ for a way-point $q$, somewhere both to which we can get from $p$ AND from which we can get to $r$. It is true that we can navigate from $p$ to $r$ iff there is a way-point $q$ through which to travel; this is a big OR over all possible $q$. The composition formula is thus:

$$
(\Phi \rightrightarrows \Psi)(p, r):=\bigvee_{q \in Q} \Phi(p, q) \wedge \Psi(q, r)
$$

But as we said in Eq. (2.101), this can be thought of as matrix multiplication. In our example, the result is

\begin{tabular}{c|cc}
$\Phi ; \Psi$ & $x$ & $y$ \\
\hline$N$ & false & true \\
$E$ & false & true \\
$W$ & true & true \\
$S$ & true & true
\end{tabular}

and one can check that this answers the question, "can you get from here to there" in Eq. (4.19): you can't get from $N$ to $x$ but you can get from $N$ to $y$.

The formula (4.20) is written in terms of the quantale Bool, but it works for arbitrary (unital commutative) quantales. We give the following definition.

Definition 4.21. Let $\mathcal{v}$ be a quantale, let $x, y$, and $z$ be $\mathcal{V}$-categories, and let $\Phi: x \rightarrow y$ and $\Psi: y \rightarrow z$ be $\mathcal{V}$-profunctors. We define their composite, denoted $\Phi ; \Psi: X \rightarrow z$ by the formula

$$
(\Phi ; \Psi)(p, r)=\bigvee_{q \in Q}(\Phi(p, q) \otimes \Psi(q, r))
$$

Exercise 4.22. Consider the Cost-profunctors $\Phi: x \rightarrow y$ and $\Psi: y \rightarrow z$ shown below:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-139.jpg?height=360&width=1133&top_left_y=682&top_left_x=491)

Fill in the matrix for the composite profunctor:

\begin{tabular}{c|cccc}
$\Phi \circ \Psi$ & $p$ & $q$ & $r$ & $s$ \\
\hline$A$ & $?$ & 24 & $?$ & $?$ \\
$B$ & $?$ & $?$ & $?$ & $?$ \\
$C$ & $?$ & $?$ & $?$ & $?$ \\
$D$ & $?$ & $?$ & 9 & $?$
\end{tabular}

\subsection*{4.3.2 The categories $\mathcal{V}$-Prof and Feas}

A composition rule suggests a category, and there is indeed a category where the objects are Bool-categories and the morphisms are Bool-profunctors. To make this work more generally, however, we need to add one technical condition.

Recall from Remark 1.35 that a preorder is a skeletal preorder if whenever $x \leq y$ and $y \leq x$, we have $x=y$. Skeletal preorders are also known as posets. We say a quantale is skeletal if its underlying preorder is skeletal; Bool and Cost are skeletal quantales.

Theorem 4.23. For any skeletal quantale $\mathcal{V}$, there is a category Prof $\mathcal{V}$ whose objects are $\mathcal{V}$-categories $X$, whose morphisms are $\mathcal{V}$-profunctors $X \rightarrow y$, and with composition defined as in Definition 4.21 .

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-139.jpg?height=60&width=753&top_left_y=2228&top_left_x=328)

At this point perhaps you have two questions in mind. What are the identity morphisms? And why did we need to specialize to skeletal quantales? It turns out these two questions are closely related.

Define the unit profunctor $\mathrm{U}_{x}: X \rightarrow X$ on a $\mathcal{V}$-category $X$ by the formula

$$
\begin{equation*}
\mathrm{U}_{x}(x, y):=X(x, y) . \tag{4.25}
\end{equation*}
$$

How do we interpret this? Recall that, by Definition 2.46, $x$ already assigns to each pair of elements $x, y \in X$ an hom-object $X(x, y) \in \mathcal{V}$. The unit profunctor $\mathrm{U}_{x}$ just assigns each pair $(x, y)$ that same object.

In the Bool case the unit profunctor on some preorder $x$ can be drawn like this:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-140.jpg?height=385&width=1350&top_left_y=656&top_left_x=385)

Obviously, composing a feasibility relation with with the unit leaves it unchanged; this is the content of Lemma 4.27.

Exercise 4.26. Choose a not-too-simple Cost-category $X$. Give a bridge-style diagram for the unit profunctor $U_{x}: X \rightarrow X$.

Lemma 4.27. Composing any profunctor $\Phi: \mathcal{P} \rightarrow \mathcal{Q}$ with either unit profunctor, $\mathrm{U}_{\mathcal{P}}$ or $\mathrm{U}_{2}$, returns $\Phi$ :

$$
\mathrm{U}_{\mathcal{P}} \circ \Phi=\Phi=\Phi \circ \mathrm{U}_{2}
$$

Proof. We show that $\mathrm{U}_{\mathcal{P}} ; \Phi=\Phi$ holds; proving $\Phi=\Phi ; \mathrm{U}_{Q}$ is similar. Fix $p \in P$ and $q \in Q$. Since $\mathcal{V}$ is skeletal, to prove the equality it's enough to show $\Phi \leq \mathrm{U}_{\mathcal{P}} \circ \Phi$ and $\mathrm{U}_{\mathcal{P}} \circ \Phi \leq \Phi$. We have one direction:

$$
\begin{equation*}
\Phi(p, q)=I \otimes \Phi(p, q) \leq \mathcal{P}(p, p) \otimes \Phi(p, q) \leq \bigvee_{p_{1} \in P}\left(\mathcal{P}\left(p, p_{1}\right) \otimes \Phi\left(p_{1}, q\right)\right)=\left(\mathrm{U}_{\mathcal{P}} ; \Phi\right)(p, q) \tag{4.28}
\end{equation*}
$$

For the other direction, we must show $\bigvee_{p_{1} \in P}\left(\mathcal{P}\left(p, p_{1}\right) \otimes \Phi\left(p_{1}, q\right)\right) \leq \Phi(p, q)$. But by definition of join, this holds iff $\mathcal{P}\left(p, p_{1}\right) \otimes \Phi\left(p_{1}, q\right) \leq \Phi(p, q)$ is true for each $p_{1} \in \mathcal{P}$. This follows from Definitions 2.46 and 4.8:

$$
\begin{equation*}
\mathcal{P}\left(p, p_{1}\right) \otimes \Phi\left(p_{1}, q\right)=\mathcal{P}\left(p, p_{1}\right) \otimes \Phi\left(p_{1}, q\right) \otimes I \leq \mathcal{P}\left(p, p_{1}\right) \otimes \Phi\left(p_{1}, q\right) \otimes \mathcal{Q}(q, q) \leq \Phi(p, q) \tag{4.29}
\end{equation*}
$$

Exercise 4.30 .

1. Justify each of the four steps ( $=, \leq, \leq,=$ ) in Eq. (4.28).

2. In the case $\mathcal{V}=$ Bool, we can directly show each of the four steps in Eq. (4.28) is actually an equality. How?

3. Justify each of the three steps $(=, \leq, \leq)$ in Eq. (4.29).

Composition of profunctors is also associative; we leave the proof to you.

Lemma 4.31. Serial composition of profunctors is associative. That is, given profunctors $\Phi: \mathcal{P} \rightarrow Q, \Psi: \mathcal{R} \rightarrow \mathcal{R}$, and $\Upsilon: \mathcal{R} \rightarrow \mathcal{S}$, we have

$$
(\Phi ; \Psi) ; \Upsilon=\Phi ;(\Psi ; \Upsilon)
$$

Exercise 4.32. Prove Lemma 4.31. (Hint: remember to use the fact that $\mathcal{V}$ is skeletal.)

So, feasibility relations form a category. Since this is the case, we can describe feasibility relations using wiring diagrams for categories (see also Section 4.4.2), which are very simple. Indeed, each box can only have one input and one output, and they're connected in a line:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-141.jpg?height=152&width=935&top_left_y=1000&top_left_x=584)

On the other hand, we have seen that feasibility relations are the building blocks of co-design problems, and we know that co-design problems can be depicted with a much richer wiring diagram, for example:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-141.jpg?height=469&width=1352&top_left_y=1370&top_left_x=381)

This hints that the category Feas has more structure. We've seen wiring diagrams where boxes can have multiple inputs and outputs before, in Chapter 2; there they depicted morphisms in a monoidal preorder. On other hand the boxes in the wiring diagrams of Chapter 2 could not have distinct labels, like the boxes in a co-design problem: all boxes in a wiring diagram for monoidal preorders indicate the order $\leq$, while above we see boxes labelled by "Chassis", "Motor", and so on. Similarly, we know that Feas is a proper category, not just a preorder. To understand these diagrams then, we must introduce a new structure, called a monoidal category. A monoidal category is a categorified monoidal preorder.

Remark 4.33. While we have chosen to define Prof $\mathcal{v}$ only for skeletal quantales in Theorem 4.23, it is not too hard to work with non-skeletal ones. There are two straightforward ways to do this. First, we might let the morphisms of Prof $\mathcal{v}$ be isomorphism
classes of $\mathcal{V}$-profunctors. This is analogous to the trick we will use when defining the category Cospan $_{\mathcal{C}}$ in Definition 6.45. Second, we might relax what we mean by category, only requiring composition to be unital and associative 'up to isomorphism'. This is also a type of categorification, known as bicategory theory.

In the next section we'll discuss categorification and introduce monoidal categories. First though, we finish this section by discussing why profunctors are called profunctors, and by formally introducing something called the collage of a profunctor.

\subsection*{4.3.3 Fun profunctor facts: companions, conjoints, collages}

Companions and conjoints. Recall that a preorder is a Bool-category and a monotone map is a Bool-functor. We said above that a profunctor is a generalization of a functor; how so?

In fact, every $\mathcal{V}$-functor gives rise to two $\mathcal{V}$-profunctors, called the companion and the conjoint.

Definition 4.34. Let $F: \mathcal{P} \rightarrow Q$ be a $\mathcal{V}$-functor. The companion of $F$, denoted $\widehat{F}: \mathcal{P} \rightarrow Q$ and the conjoint of $F$, denoted $\check{F}: Q \rightarrow \mathcal{P}$ are defined to be the following $\mathcal{V}$-profunctors:

$$
\widehat{F}(p, q):=Q(F(p), q) \quad \text { and } \quad \check{F}(q, p):=\mathcal{Q}(q, F(p))
$$

Let's consider the Bool case again. One can think of a monotone map $F: \mathcal{P} \rightarrow Q$ as a bunch of arrows, one coming out of each vertex $p \in P$ and landing at some vertex $F(p) \in Q$.

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-142.jpg?height=420&width=1171&top_left_y=1590&top_left_x=474)

This looks like the pictures of bridges connecting cities, and if one regards the above picture in that light, they are seeing the companion $\widehat{F}$. But now mentally reverse every dotted arrow, and the result would be bridges $Q$ to $\mathcal{P}$. This is a profunctor $Q \rightarrow \mathcal{P}$ ! We call it $\check{F}$.

Example 4.35. For any preorder $\mathcal{P}$, there is an identity functor id: $\mathcal{P} \rightarrow \mathcal{P}$. Its companion and conjoint agree id $=\overline{i d}: \mathcal{P} \rightarrow \mathcal{P}$. The resulting profunctor is in fact the unit profunctor, $\mathrm{U}_{\mathcal{P}}$, as defined in Eq. (4.25).

Exercise 4.36. Check that the companion id of id: $\mathcal{P} \rightarrow \mathcal{P}$ really has the unit profunctor formula given in Eq. (4.25).

Example 4.37. Consider the function $+: \mathbb{R} \times \mathbb{R} \times \mathbb{R} \rightarrow \mathbb{R}$, sending a triple $(a, b, c)$ of real numbers to $a+b+c \in \mathbb{R}$. This function is monotonic, because if $(a, b, c) \leq\left(a^{\prime}, b^{\prime}, c^{\prime}\right)-$ i.e. if $a \leq a^{\prime}$ and $b \leq b^{\prime}$, and $c \leq c^{\prime}$-then obviously $a+b+c \leq a^{\prime}+b^{\prime}+c^{\prime}$. Thus it has a companion and a conjoint.

Its companion $\widehat{\mp}:(\mathbb{R} \times \mathbb{R} \times \mathbb{R}) \rightarrow \mathbb{R}$ is the function that sends $(a, b, c, d)$ to true if $a+b+c \leq d$ and to false otherwise.

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-143.jpg?height=65&width=1385&top_left_y=778&top_left_x=327)

Remark 4.39 ( $\mathcal{V}$-Adjoints). Recall from Definition 1.95 the definition of Galois connection between preorders $\mathcal{P}$ and $\mathcal{Q}$. The definition of adjoint can be extended from the Bool-enriched setting (of preorders and monotone maps) to the $\mathcal{V}$-enriched setting for arbitrary monoidal preorders $\mathcal{V}$. In that case, the definition of a $\mathcal{V}$-adjunction is a pair of $\mathcal{V}$-functors $F: \mathcal{P} \rightarrow Q$ and $G: Q \rightarrow \mathcal{P}$ such that the following holds for all $p \in P$ and $q \in Q$.

$$
\begin{equation*}
\mathcal{P}(p, G(q)) \cong \mathcal{Q}(F(p), q) \tag{4.40}
\end{equation*}
$$

Exercise 4.41. Let $\mathcal{V}$ be a skeletal quantale, let $\mathcal{P}$ and $\mathcal{Q}$ be $\mathcal{V}$-categories, and let $F: \mathcal{P} \rightarrow \mathcal{Q}$ and $G: \mathcal{Q} \rightarrow \mathcal{P}$ be $\mathcal{V}$-functors.

1. Show that $F$ and $G$ are $\mathcal{V}$-adjoints (as in Eq. (4.40)) if and only if the companion of the former equals the conjoint of the latter: $\widehat{F}=\breve{G}$.

2. Use this to prove that $\widehat{\mathrm{id}}=\mathrm{id}$, as was stated in Example 4.35.

Collage of a profunctor. We have been drawing profunctors as bridges connecting cities. One may get an inkling that given a $\mathcal{V}$-profunctor $\Phi: X \rightarrow Y$ between $\mathcal{V}$-categories $X$ and $y$, we have turned $\Phi$ into a some sort of new $\mathcal{V}$-category that has $X$ on the left and $y$ on the right. This works for any $\mathcal{V}$ and profunctor $\Phi$, and is called the collage construction.

Definition 4.42. Let $\mathcal{V}$ be a quantale, let $X$ and $y$ be $\mathcal{V}$-categories, and let $\Phi: X \rightarrow y$ be a $\mathcal{V}$-profunctor. The collage of $\Phi$, denoted $\operatorname{Col}(\Phi)$ is the $\mathcal{V}$-category defined as follows:

(i) $\mathrm{Ob}(\operatorname{Col}(\Phi)):=\operatorname{Ob}(X) \sqcup \mathrm{Ob}(y)$;

(ii) For any $a, b \in \operatorname{Ob}(\operatorname{Col}(\Phi))$, define $\operatorname{Col}(\Phi)(a, b) \in \mathcal{V}$ to be

$$
\operatorname{Col}(\Phi)(a, b):= \begin{cases}X(a, b) & \text { if } a, b \in X \\ \Phi(a, b) & \text { if } a \in X, b \in y \\ \varnothing & \text { if } a \in y, b \in X \\ y(a, b) & \text { if } a, b \in y\end{cases}
$$

There are obvious functors $i_{x}: x \rightarrow \operatorname{Col}(\Phi)$ and $i_{y}: y \rightarrow \operatorname{Col}(\Phi)$, sending each object
and morphism to "itself," called collage inclusions.

Some pictures will help clarify this.

Example 4.43. Consider the following picture of a Cost-profunctor $\Phi: X \rightarrow y$ :

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-144.jpg?height=336&width=788&top_left_y=510&top_left_x=663)

It corresponds to the following matrices

$$
\begin{array}{c|cc}
x & A & B \\
\hline A & 0 & 2 \\
B & \infty & 0
\end{array} \quad \begin{array}{c|cc}
\Phi & x & y \\
\hline A & 5 & 8 \\
B & \infty & \infty
\end{array} \quad \begin{array}{c|cc}
y & x & y \\
\hline x & 0 & 3 \\
y & 4 & 0
\end{array}
$$

A generalized Hasse diagram of the collage can be obtained by simply taking the union of the Hasse diagrams for $X$ and $y$, and adding in the bridges as arrows. Given the above profunctor $\Phi$, we draw the Hasse diagram for $\operatorname{Col}(\Phi)$ below left, and the Cost-matrix representation of the resulting Cost-category on the right:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-144.jpg?height=329&width=1161&top_left_y=1402&top_left_x=490)

Exercise 4.44. Draw a Hasse diagram for the collage of the profunctor shown here:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-144.jpg?height=404&width=1044&top_left_y=1842&top_left_x=535)

\subsection*{4.4 Categorification}

Here we switch gears, to discuss a general concept called categorification. We will begin again with the basics, categorifying several of the notions we've encountered
already. The goal is to define compact closed categories and their feedback-style wiring diagrams. At that point we will return to the story of co-design, and $\mathcal{V}$-profunctors in general, and show that they do in fact form a compact closed category, and thus interpret the diagrams we've been drawing since Eq. (4.1).

\subsection*{4.4.1 The basic idea of categorification}

The general idea of categorification is that we take a thing we know and add structure to it, so that what were formerly properties become structures. We do this in such a way that we can recover the thing we categorified by forgetting this new structure. This is rather vague; let's give an example.

Basic arithmetic concerns properties of the natural numbers $\mathbb{N}$, such as the fact that $5+3=8$. One way to categorify $\mathbb{N}$ is to use the category FinSet of finite sets and functions. To obtain a categorification, we replace the brute 5,3 , and 8 with sets of that many elements, say $\overline{5}=$ \{apple,banana, cherry,dragonfruit, elephant\}, $\overline{3}=\{$ apple, tomato, cantaloupe $\}$, and $\overline{8}=$ \{Ali, Bob, Carl, Deb, Eli, Fritz, Gem, Helen\} respectively. We also replace + with disjoint union of sets $\sqcup$, and the brute property of equality with the structure of an isomorphism. What makes this a good categorification is that, having made these replacements, the analogue of $5+3=8$ is still true: $\overline{5} \sqcup \overline{3} \cong \overline{8}$.

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-145.jpg?height=685&width=963&top_left_y=1311&top_left_x=581)

In this categorified world, we have more structure available to talk about the relationships between objects, so we can be more precise about how they relate to each other. Thus it's not the case that $\overline{5} \sqcup \overline{3}$ is equal to our chosen eight-element set $\overline{8}$, but more precisely that there exists an invertible function comparing the two, showing that they are isomorphic in the category FinSet.

Note that in the above construction we made a number of choices; here we must beware. Choosing a good categorification-like designing a good algebraic structure such as that of preorders or quantales-is part of the art of mathematics. There is
no prescribed way to categorify, and the success of a chosen categorification is often empirical: its richer structure should allow us more insights into the subject we want to model.

As another example, an empirically pleasing way to categorify preorders is to categorify them as, well, categories. In this case, rather than the brute property "there exists a morphism $a \rightarrow b$," denoted $a \leq b$ or $\mathcal{P}(a, b)=$ true, we instead say "here is a set of morphisms $a \rightarrow b$." We get a hom-set rather than a hom-Boolean. In fact-to state this in a way straight out of the primordial ooze-just as preorders are Bool-categories, ordinary categories are actually Set-categories.

\subsection*{4.4.2 A reflection on wiring diagrams}

Suppose we have a preorder. We introduced a very simple sort of wiring diagram in Section 2.2.2. These allowed us to draw a box

$$
x_{0}^{x_{0}} \leq \leq^{x_{1}}
$$

whenever $x_{0} \leq x_{1}$. Chaining these together, we could prove facts in our preorder. For example

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-146.jpg?height=122&width=409&top_left_y=1218&top_left_x=853)

provides a proof that $x_{0} \leq x_{3}$ (the exterior box) using three facts (the interior boxes), $x_{0} \leq x_{1}, x_{1} \leq x_{2}$, and $x_{2} \leq x_{3}$.

As categorified preorders, categories have basically the same sort of wiring diagram as preorders-namely sequences of boxes inside a box. But since we have replaced the fact that $x_{0} \leq x_{1}$ with the structure of a set of morphisms, we need to be able to label our boxes with morphism names:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-146.jpg?height=149&width=417&top_left_y=1714&top_left_x=846)

Suppose given additional morphisms $g: B \rightarrow C$, and $h: C \rightarrow D$. Representing these each as boxes like we did for $f$, we might be tempted to stick them together to form a new box:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-146.jpg?height=122&width=390&top_left_y=2042&top_left_x=865)

Ideally this would also be a morphism in our category: after all, we have said that we can represent morphisms with boxes with one input and one output. But wait, you say! We don't know which morphism it is. Is it $f ;(g ; h)$ ? Or $(f ; g) ; h$ ? It's good that you are so careful. Luckily, we are saved by the properties that a category must have. Associativity says $f \circ(g \circ h)=(f ; g) \circ h$, so it doesn't matter which way we chose to try to decode the box.

Similarly, the identity morphism on an object $x$ is drawn as on the left below, but we will see that it is not harmful to draw $\mathrm{id}_{x}$ in any of the following three ways:

$x \square x$

$x \stackrel{\cdots \cdots}{: \ldots \ldots} x$

$x-x$

By Definition 3.6 the morphisms in a category satisfy two properties, called the unitality property and the associativity property. The unitality says that $\operatorname{id}_{x} \Re f=f=f ; \mathrm{id}_{y}$ for any $f: x \rightarrow y$. In terms of diagrams this would say

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-147.jpg?height=114&width=832&top_left_y=694&top_left_x=641)

This means you can insert or discard any identity morphism you see in a wiring diagram. From this perspective, the coherence laws of a category-that is, the associativity law and the unitality law-are precisely what are needed to ensure we can lengthen and shorten wires without ambiguity.

In Section 2.2.2, we also saw wiring diagrams for monoidal preorders. Here we were allowed to draw boxes which can have multiple typed inputs and outputs, but with no choice of label (always $\leq$ ):

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-147.jpg?height=225&width=415&top_left_y=1264&top_left_x=844)

If we combine these ideas, we will obtain a categorification of symmetric monoidal preorders: symmetric monoidal categories. A symmetric monoidal category is an algebraic structure in which we have labelled boxes with multiple typed inputs and outputs:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-147.jpg?height=223&width=415&top_left_y=1748&top_left_x=844)

Furthermore, a symmetric monoidal category has a composition rule and a monoidal product, which permit us to combine these boxes to interpret diagrams like this:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-147.jpg?height=358&width=810&top_left_y=2144&top_left_x=652)

Finally, this structure must obey coherence laws, analogous to associativity and unitality in categories, that allow such diagrams to be unambiguously interpreted. In the next section we will be a bit more formal, but it is useful to keep in mind that, when we say our data must be "well behaved," this is all we mean.

\subsection*{4.4.3 Monoidal categories}

We defined $\mathcal{V}$-categories, for a symmetric monoidal preorder $\mathcal{V}$ in Definition 2.46. Just like preorders turned out to be special kinds of categories (see Section 3.2.3), monoidal preorders are special kinds of monoidal categories. And just like we can consider $\mathcal{V}$-categories for a monoidal preorder, we can also consider $\mathcal{V}$-categories when $\mathcal{V}$ is a monoidal category. This is another sort of categorification.

We will soon meet the monoidal category (Set, $\{1\}, \times$ ). The monoidal product will take two sets, $S$ and $T$, and return the set $S \times T=\{(s, t) \mid s \in S, t \in T\}$. But whereas for monoidal preorders we had the brute associative property $(p \otimes q) \otimes r=p \otimes(q \otimes r)$, the corresponding idea in Set is not quite true:

$$
\begin{aligned}
& S \times(T \times U):=\{(s,(t, u)) \mid s \in S, t \in T, u \in U\} \\
=? & (S \times T) \times U:=\{((s, t), u) \mid s \in S, t \in T, u \in U\} .
\end{aligned}
$$

They are slightly different sets: the first contains pairs consisting of an elements in $S$ and an element in $T \times U$, while the second contains pairs consisting of an element in $S \times T$ and an element in $U$. The sets are not equal, but they are clearly isomorphic, i.e. the difference between them is "just a matter of bookkeeping." We thus need a structure-a bookkeeping isomorphism-to keep track of the associativity:

$$
\alpha_{s, t, u}:\{(s,(t, u)) \mid s \in S, t \in T, u \in U\} \xrightarrow{\cong}\{((s, t), u) \mid s \in S, t \in T, u \in U\} .
$$

There are a couple things to mention before we dive into these ideas. First, just because you replace brute things and properties with structures, it does not mean that you no longer have brute things and properties: new ones emerge! Not only that, but second, the new brute stuff tends to be more complex than what you started with. For example, above we replaced the associativity equation with an isomorphism $\alpha_{s, t, u}$, but now we need a more complex property to ensure that all these $\alpha^{\prime}$ s behave reasonably! The only way out of this morass is to add infinitely much structure, which leads one to " $\infty$-categories," but we will not discuss that here.

Instead, we will continue with our categorification of monoidal preorders, starting with a rough definition of symmetric monoidal categories. It's rough in the sense that we suppress the technical bookkeeping, hiding it under the name "well behaved."

Rough Definition 4.45. Let $\mathcal{C}$ be a category. A symmetric monoidal structure on $\mathcal{C}$ consists of the following constituents:

(i) an object $I \in \mathrm{Ob}(\mathcal{C})$ called the monoidal unit, and
(ii) a functor $\otimes: \mathcal{e} \times \mathcal{C} \rightarrow \mathcal{C}$, called the monoidal product subject to well-behaved, natural isomorphisms

(a) $\lambda_{c}: I \otimes c \cong c$ for every $c \in \operatorname{Ob}(\mathcal{C})$,

(b) $\rho_{c}: c \otimes I \cong c$ for every $c \in \mathrm{Ob}(\mathcal{C})$,

(c) $\alpha_{c, d, e}:(c \otimes d) \otimes e \cong c \otimes(d \otimes e)$ for every $c, d, e \in \operatorname{Ob}(\mathcal{e})$, and

(d) $\sigma_{c, d}: c \otimes d \cong d \otimes c$ for every $c, d \in \mathrm{Ob}(\mathcal{C})$, called the swap map, such that $\sigma \circ \sigma=\mathrm{id}$. A category equipped with a symmetric monoidal structure is called a symmetric monoidal category.

Remark 4.46. If the isomorphisms in (a), (b), and (c)—but not (d)—are replaced by equalities, then we say that the monoidal structure is strict, and this is a complete (non-rough) definition of symmetric strict monoidal category. In fact, symmetric strict monoidal categories are almost the same thing as symmetric monoidal categories, via a result known as Mac Lane's coherence theorem. An upshot of this theorem is that we can, when useful to us, pretend that our monoidal categories are strict: for example, we implicitly do this when we draw wiring diagrams. Ask your friendly neighborhood category theorist to explain how!

Remark 4.47. For those yet to find a friendly expert category theorist, we make the following remark. A complete (non-rough) definition of symmetric monoidal category is that a symmetric monoidal category is a category equipped with an equivalence to (the underlying category of) a symmetric strict monoidal category. This can be unpacked, using Remark 4.46 and our comment about equivalence of categories in Remark 3.59, but we don't expect you to do so. Instead, we hope this gives you more incentive to ask a friendly expert category theorist!

Exercise 4.48. Check that monoidal categories indeed generalize monoidal preorders: a monoidal preorder is a monoidal category $(\mathcal{P}, I, \otimes)$ where, for every $p, q \in \mathcal{P}$, the set $\mathcal{P}(p, q)$ has at most one element.

Example 4.49. As we said above, there is a monoidal structure on Set where the monoidal unit is some choice of singleton set, say $I:=\{1\}$, and the monoidal product is $\otimes:=\times$. What it means that $\times$ is a functor is that:
- For any pair of objects, i.e. sets, $(S, T) \in \mathrm{Ob}($ Set $\times$ Set), one obtains a set $(S \times T) \in$ $\mathrm{Ob}($ Set). We know what it is: the set of pairs $\{(s, t) \mid s \in S, t \in T\}$.
- For any pair of morphisms, i.e. functions, $f: S \rightarrow S^{\prime}$ and $g: T \rightarrow T^{\prime}$, one obtains a function $(f \times g):(S \times T) \rightarrow\left(S^{\prime} \times T^{\prime}\right)$. It works pointwise: $(f \times g)(s, t):=(f(s), g(t))$.
- These should preserve identities: $\mathrm{id}_{S} \times \mathrm{id}_{T}=\mathrm{id}_{S \times T}$ for any sets $S, T$.
- These should preserve composition: for any functions $S \xrightarrow{f} S^{\prime} \xrightarrow{f^{\prime}} S^{\prime \prime}$ and $T \xrightarrow{g}$ $T^{\prime} \xrightarrow{g^{\prime}} T^{\prime \prime}$, one has

$$
(f \times g) \nsubseteq\left(f^{\prime} \times g^{\prime}\right)=(f ; g) \times\left(f^{\prime} ; g^{\prime}\right)
$$

The four conditions, (a), (b), (c), and (d) give isomorphisms $\{1\} \times S \cong S$, etc. These maps are obvious in the case of Set, e.g. the function $\{(1, s) \mid s \in S\} \rightarrow S$ sending $(1, s)$ to $s$. We have been calling such things bookkeeping.

Exercise 4.50. Consider the monoidal category $($ Set, $1, \times$ ), together with the diagram

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-150.jpg?height=366&width=821&top_left_y=519&top_left_x=649)

Suppose that $A=B=C=D=F=G=\mathbb{Z}$ and $E=\mathbb{B}=\{$ true, false $\}$, and suppose that $f_{C}(a)=|a|, f_{D}(a)=a * 5, g_{E}(d, b)=" d \leq b, " g_{F}(d, b)=d-b$, and $h(c, e)=$ if $e$ then $c$ else $1-c$.

1. What are $g_{E}(5,3)$ and $g_{F}(5,3)$ ?

2. What are $g_{E}(3,5)$ and $g_{F}(3,5)$ ?

3. What is $h(5$, true)?

4. What is $h(-5$, true)?

5. What is $h(-5$, false $)$ ?

The whole diagram now defines a function $A \times B \rightarrow G \times F$; call it $q$.

6. What are $q_{G}(-2,3)$ and $q_{F}(-2,3)$ ?

7. What are $q_{G}(2,3)$ and $q_{F}(2,3)$ ?

We will see more monoidal categories throughout the remainder of this book.

\subsection*{4.4.4 Categories enriched in a symmetric monoidal category}

We will not need this again, but we once promised to explain why $\mathcal{V}$-categories, where $\mathcal{V}$ is a symmetric monoidal preorder, deserve to be seen as types of categories. The reason, as we have hinted, is that categories should really be called Set-categories. But wait, Set is not a preorder! We'll have to generalize-categorify— $\mathcal{V}$-categories.

We now give a rough definition of categories enriched in a symmetric monoidal category $\mathcal{V}$. As in Definition 4.45, we suppress some technical parts in this sketch, hiding them under the name "usual associative and unital laws."

Rough Definition 4.51. Let $\mathcal{V}$ be a symmetric monoidal category, as in Definition 4.45. To specify a category enriched in $\mathcal{V}$, or a $\mathcal{V}$-category, denoted $\mathcal{X}$,

(i) one specifies a collection $\operatorname{Ob}(X)$, elements of which are called objects;

(ii) for every pair $x, y \in \operatorname{Ob}(X)$, one specifies an object $X(x, y) \in \mathcal{V}$, called the homobject for $x, y$;

(iii) for every $x \in \operatorname{Ob}(\mathcal{X})$, one specifies a morphism $\operatorname{id}_{x}: I \rightarrow \mathcal{X}(x, x)$ in $\mathcal{V}$, called the
identity element;

(iv) for each $x, y, z \in \operatorname{Ob}(X)$, one specifies a morphism $9: X(x, y) \otimes X(y, z) \rightarrow X(x, z)$, called the composition morphism.

These constituents are required to satisfy the usual associative and unital laws.

The precise, non-rough, definition can be found in other sources, e.g. [nLa18], [Wik18], [Kel05].

Exercise 4.52. Recall from Example 4.49 that $\mathcal{V}=($ Set, $\{1\}, \times)$ is a symmetric monoidal category. This means we can apply Definition 4.51. Does the (rough) definition roughly agree with the definition of category given in Definition 3.6? Or is there a subtle difference?

Remark 4.53. We first defined $\mathcal{V}$-categories in Definition 2.46, where $\mathcal{V}$ was required to be a monoidal preorder. To check we're not abusing our terms, it's a good idea to make sure that $\mathcal{V}$-categories as per Definition 2.46 are still $\mathcal{V}$-categories as per Definition 4.51.

The first thing to observe is that every symmetric monoidal preorder is a symmetric monoidal category (Exercise 4.48). So given a symmetric monoidal preorder $\mathcal{V}$, we can apply Definition 4.51. The required data (i) and (ii) then get us off to a good start: both definitions of $\mathcal{V}$-category require objects and hom-objects, and they are specified in the same way. On the other hand, Definition 4.51 requires two additional pieces of data: (iii) identity elements and (iv) composition morphisms. Where do these come from?

In the case of preorders, there is at most one morphism between any two objects, so we do not need to choose an identity element and a composition morphism. Instead, we just need to make sure that an identity element and a composition morphism exist. This is exactly what properties (a) and (b) of Definition 2.46 say.

For example, the requirement (iii) that a $\mathcal{V}$-category $X$ has a chosen identity element $\operatorname{id}_{x}: I \rightarrow X(x, x)$ for the object $x$ simply becomes the requirement (a) that $I \leq X(x, x)$ is true in $\mathcal{V}$. This is typical of the story of categorification: what were mere properties in Definition 2.46 have become structures in Definition 4.51.

Exercise 4.54. What are identity elements in Lawvere metric spaces (that is, Costcategories)? How do we interpret this in terms of distances?

\subsection*{4.5 Profunctors form a compact closed category}

In this section we will define compact closed categories and show that Feas, and more generally $\mathcal{\nu}$-profunctors, form such a thing. Compact-closed categories are monoidal
categories whose wiring diagrams allow feedback. The wiring diagrams look like this:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-152.jpg?height=377&width=1000&top_left_y=329&top_left_x=557)

It's been a while since we thought about co-design, but these were the kinds of wiring diagrams we drew, e.g. connecting the chassis, the motor, and the battery in Eq. (4.1). Compact closed categories are symmetric monoidal categories, with a bit more structure that allow us to formally interpret the sorts of feedback that occur in co-design problems. This same structure shows up in many other fields, including quantum mechanics and dynamical systems.

In Eq. (2.13) and Section 2.2.3 we discussed various flavors of wiring diagrams, including those with icons for splitting and terminating wires. For compact-closed categories, our additional icons allow us to bend outputs into inputs, and vice versa. To keep track of this, however, we draw arrows on our wire, which can either point forwards or backwards. For example, we can draw this

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-152.jpg?height=176&width=838&top_left_y=1359&top_left_x=638)

We then add icons-called a cap and a cup-allowing any wire to reverse direction from forwards to backwards and from backwards to forwards.
![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-152.jpg?height=190&width=408&top_left_y=1705&top_left_x=858)

Thus we can draw the following

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-152.jpg?height=231&width=832&top_left_y=2015&top_left_x=641)

and its meaning is equivalent to that of Eq. (4.56).

We will begin by giving the axioms for a compact closed category. Then we will look again at feasibility relations in co-design-and more generally at enriched profunctors-and show that they indeed form a compact closed category.

\subsection*{4.5.1 Compact closed categories}

As we said, compact closed categories are symmetric monoidal categories (see Definition 4.45) with extra structure.

Definition 4.58. Let $(\mathcal{C}, I, \otimes)$ be a symmetric monoidal category, and $c \in \mathrm{Ob}(\mathcal{C})$ an object. A dual for $c$ consists of three constituents

(i) an object $c^{*} \in \operatorname{Ob}(\mathcal{C})$, called the dual of $c$,

(ii) a morphism $\eta_{c}: I \rightarrow c^{*} \otimes c$, called the unit for $c$,

(iii) a morphism $\epsilon_{c}: c \otimes c^{*} \rightarrow I$, called the counit for $c$.

These are required to satisfy two equations for every $c \in \operatorname{Ob}(\mathcal{C})$, which we draw as commutative diagrams:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-153.jpg?height=311&width=1263&top_left_y=880&top_left_x=382)

These equations are sometimes called the snake equations.

If for every object $c \in \operatorname{Ob}(\mathcal{C})$ there exists a dual $c^{*}$ for $c$, then we say that $(\mathcal{C}, I, \otimes)$ is compact closed.

In a compact closed category, each wire is equipped with a direction. For any object $c$, a forward-pointing wire labeled $c$ is considered equivalent to a backward-pointing wire labeled $c^{*}$, i.e. $\xrightarrow{c}$ is the same as $\stackrel{c^{*}}{\leftarrow}$. The cup and cap discussed above are in fact the unit and counit morphisms; they are drawn as follows.
![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-153.jpg?height=188&width=702&top_left_y=1759&top_left_x=706)

In wiring diagrams, the snake equations (4.59) are then drawn as follows:
![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-153.jpg?height=336&width=1248&top_left_y=2078&top_left_x=431)

Note that the pictures in Eq. (4.57) correspond to $\epsilon_{\text {sound }}$ and $\eta_{\text {sound }}$.

Recall the notion of monoidal closed preorder; a monoidal category can also be monoidal closed. This means that for every pair of objects $c, d \in \mathrm{Ob}(\mathcal{C})$ there is an object $c \multimap d$ and an isomorphism $\mathcal{C}(b \otimes c, d) \cong \mathcal{C}(b, c \multimap d)$, natural in $b$. While we will not provide a full proof here, compact closed categories are so-named because they are a special type of monoidal closed category.

Proposition 4.60. If $\mathcal{C}$ is a compact closed category, then
1. $\mathcal{C}$ is monoidal closed;

and for any object $c \in \mathrm{Ob}(\mathcal{C})$,

2. if $c^{*}$ and $c^{\prime}$ are both duals to $c$ then there is an isomorphism $c^{*} \cong c^{\prime}$; and

3. there is an isomorphism between $c$ and its double-dual, $c \cong c^{* *}$.

To prove 1., the key idea is that for any $c$ and $d$, the object $c \multimap d$ is given by $c^{*} \otimes d$, and the natural isomorphism $\mathcal{C}(b \otimes c, d) \cong \mathcal{C}(b, c \multimap d)$ is given by precomposing with $\operatorname{id}_{b} \otimes \eta_{c}$.

Before returning to co-design, we give another example of a compact closed category, called Corel, which we'll see again in the chapters to come.

Example 4.61. Recall, from Definition 1.18, that an equivalence relation on a set $A$ is a reflexive, symmetric, and transitive binary relation on $A$. Given two finite sets, $A$ and $B$, a corelation $A \rightarrow B$ is an equivalence relation on $A \sqcup B$.

So, for example, here is a corelation from a set $A$ having five elements to a set $B$ having six elements; two elements are equivalent if they are encircled by the same dashed line.

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-154.jpg?height=309&width=504&top_left_y=1493&top_left_x=800)

There exists a category, denoted Corel, where the objects are finite sets, and where a morphism from $A \rightarrow B$ is a corelation $A \rightarrow B$. The composition rule is simpler to look at than to write down formally. ${ }^{2}$ If in addition to the corelation $\alpha: A \rightarrow B$ above we have another corelation $\beta: B \rightarrow C$

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-154.jpg?height=314&width=501&top_left_y=2063&top_left_x=801)

Then the composite $\beta \circ \alpha$ of our two corelations is given by

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-155.jpg?height=380&width=1225&top_left_y=222&top_left_x=447)

That is, two elements are equivalent in the composite corelation if we may travel from one to the other staying within equivalence classes of either $\alpha$ or $\beta$.

The category Corel may be equipped with the symmetric monoidal structure $(\varnothing, \sqcup)$. This monoidal category is compact closed, with every finite set its own dual. Indeed, note that for any finite set $A$ there is an equivalence relation on $A \sqcup A:=\{(a, 1),(a, 2) \mid a \in A\}$ where each part simply consists of the two elements $(a, 1)$ and $(a, 2)$ for each $a \in A$. The unit on a finite set $A$ is the corelation $\eta_{A}: \varnothing \rightarrow A \sqcup A$ specified by this equivalence relation; similarly the counit on $A$ is the corelation $\epsilon_{A}: A \sqcup A \rightarrow \varnothing$ specifed by this same equivalence relation.

Exercise 4.62. Consider the set $\underline{3}=\{1,2,3\}$.

1. Draw a picture of the unit corelation $\varnothing \rightarrow \underline{3} \sqcup \underline{3}$.

2. Draw a picture of the counit corelation $\underline{3} \sqcup \underline{3} \rightarrow \varnothing$.

3. Check that the snake equations (4.59) hold. (Since every object is its own dual, you only need to check one of them.)

\subsection*{4.5.2 Feas as a compact closed category}

We close the chapter by returning to co-design and showing that Feas has a compact closed structure. This is what allows us to draw the kinds of wiring diagrams we saw in Eqs. (4.1), (4.55), and (4.56): it is what puts actual mathematics behind these pictures.

Instead of just detailing this compact closed structure for Feas $=$ Prof $_{\text {Bool }}$, it's no extra work to prove that for any skeletal (unital, commutative) quantale $(\mathcal{V}, I, \otimes)$ the profunctor category $\operatorname{Prof} \mathcal{V}$ of Theorem 4.23 is compact closed, so we'll discuss this general fact.
\footnotetext{
${ }^{2}$ To compose corelations $\alpha: A \rightarrow B$ and $\beta: B \rightarrow C$, we need to construct an equivalence relation $\alpha ; \beta$ on $A \sqcup C$. To do so requires three steps: (i) consider $\alpha$ and $\beta$ as relations on $A \sqcup B \sqcup C$, (ii) take the transitive closure of their union, and then (iii) restrict to an equivalence relation on $A \sqcup C$. Here is the formal description. Note that as binary relations, we have $\alpha \subseteq(A \sqcup B) \times(A \sqcup B)$, and $\beta \subseteq(B \sqcup C) \times(B \sqcup C)$. We also have three inclusions: $\iota_{A} \sqcup B: A \sqcup B \rightarrow A \sqcup B \sqcup C, \iota_{B} \sqcup C: B \sqcup C \rightarrow A \sqcup B \sqcup C$, and $\iota_{A} \sqcup C: A \sqcup C \rightarrow$ $A \sqcup B \sqcup C$. Recalling our notation from Section 1.4, we define
}

$$
\alpha \Re \beta:=\iota_{A \sqcup C}^{*}\left(\left(\iota_{A \sqcup B}\right)!(\alpha) \vee\left(\iota_{B} \sqcup C\right)!(\beta)\right) .
$$

Theorem 4.63. Let $\mathcal{V}$ be a skeletal quantale. The category Prof $\mathcal{v}$ can be given the structure of a compact closed category, with monoidal product given by the product of $\mathcal{V}$-categories.

Indeed, all we need to do is construct the monoidal structure and duals for objects. Let's sketch how this goes.

Monoidal products in Prof $\mathcal{v}$ are just product categories. In terms of wiring diagrams, the monoidal structure looks like stacking wires or boxes on top of one another, with no new interaction.

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-156.jpg?height=347&width=347&top_left_y=800&top_left_x=865)

We take our monoidal product on Prof $\mathcal{V}$ to be that given by the product of $\mathcal{V}$-categories; the definition was given in Definition 2.74, and we worked out several examples there. To recall, the formula for the hom-sets in $x \times y$ is given by

$$
(x \times y)\left((x, y),\left(x^{\prime}, y^{\prime}\right)\right):=X\left(x, x^{\prime}\right) \otimes y\left(y, y^{\prime}\right)
$$

But monoidal products need to be given on morphisms also, and the morphisms in Prof $\mathcal{V}$ are $\mathcal{V}$-profunctors. So given $\mathcal{V}$-profunctors $\Phi: x_{1} \rightarrow x_{2}$ and $\Psi: y_{1} \rightarrow y_{2}$, one defines a $\mathcal{V}$-profunctor $(\Phi \times \Psi): X_{1} \times y_{1} \rightarrow X_{2} \times y_{2}$ by

$$
(\Phi \times \Psi)\left(\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right)\right):=\Phi\left(x_{1}, x_{2}\right) \otimes \Psi\left(y_{1}, y_{2}\right)
$$

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-156.jpg?height=60&width=1453&top_left_y=1778&top_left_x=325)
preorders represent resources ordered by availability ( $x \leq x^{\prime}$ means that $x$ is available given $x^{\prime}$ ) and a profunctor is a feasibility relation. Explain why $x \times y$ makes sense as the monoidal product of resource preorders $X$ and $y$ and why $\Phi \times \Psi$ makes sense as the monoidal product of feasibility relations $\Phi$ and $\Psi$.

The monoidal unit in Prof $\mathcal{v}_{\mathcal{V}}$ is 1. To define a monoidal structure on $\operatorname{Prof}_{v}$, we need not only a monoidal product—as defined above-but also a monoidal unit. Recall the $\mathcal{V}$-category 1 ; it has one object, say 1 , and $(1,1)=I$ is the monoidal unit of $\mathcal{V}$. We take 1 to be the monoidal unit of $\operatorname{Prof} v$.

Exercise 4.65. In order for $\mathbf{1}$ to be a monoidal unit, there are supposed to be isomorphisms $X \times 1 \rightarrow X$ and $1 \times X \rightarrow X$ in Prof $\mathcal{v}$, for any $\mathcal{V}$-category $X$. What are they?

Duals in Prof $\mathcal{v}$ are just opposite categories. In order to regard Prof $\mathcal{v}$ as a compact closed category (Definition 4.58), it remains to specify duals and the corresponding cup and cap.

Duals are easy: for every $\mathcal{V}$-category $\mathcal{X}$, its dual is its opposite category $\mathcal{X}^{\text {op }}$ (see Exercise 2.73). The unit and counit then look like identities. To elaborate, the unit is a $\mathcal{V}$-profunctor $\eta_{X}: \mathbf{1} \rightarrow X^{\text {op }} \times \mathcal{X}$. By definition, this is a $\mathcal{V}$-functor

$$
\eta_{x}: \mathbf{1} \times X^{\mathrm{op}} \times X \rightarrow \mathcal{v}
$$

we define it by $\eta_{x}\left(1, x, x^{\prime}\right):=X\left(x, x^{\prime}\right)$. Similarly, the counit is the profunctor $\epsilon_{X}:(X \times$ $\left.X^{\text {op }}\right) \rightarrow \mathbf{1}$, defined by $\epsilon x\left(x, x^{\prime}, 1\right):=X\left(x, x^{\prime}\right)$.

Exercise 4.66. Check these proposed units and counits do indeed obey the snake equations Eq. (4.59).

\subsection*{4.6 Summary and further reading}

This chapter introduced three important ideas in category theory: profunctors, categorification, and monoidal categories. Let's talk about them in turn.

Profunctors generalize binary relations. In particular, we saw that the idea of profunctor over a monoidal preorder gave us the additional power necessary to formalize the idea of a feasibility relation between resource preorders. The idea of a feasibility relation is due to Andrea Censi; he called them monotone codesign problems. The basic idea is explained in [Cen15], where he also gives a programming language to specify and solve codesign problems. In [Cen17], Censi further discusses how to use estimation to make solving codesign problems computationally efficient.

We also saw profunctors over the preorder Cost, and how to think of these as bridges between Lawvere metric space. We referred earlier to Lawvere's paper [Law73]; plenty more on Cost-profunctors can be found there.

Profunctors, however are vastly more general than the two examples we have discussed; $\mathcal{V}$-profunctors can be defined not only when $\mathcal{V}$ is a preorder, but for any symmetric monoidal category. A delightful, detailed exposition of profunctors and related concepts such as equipments, companions and conjoints, symmetric monoidal bicategories can be found in [Shu08; Shu10].

We have not defined symmetric monoidal bicategories, but you would be correct if you guessed this is a sort of categorification of symmetric monoidal categories. Baez and Dolan tell the subtle story of categorifying categories to get ever higher categories in [BD98]. Crane and Yetter give a number of examples of categorification in [CY96].

Finally, we talked about monoidal categories and compact closed categories. Monoidal categories are a classic, central topic in category theory, and a quick introduction can be found in [Mac98]. Wiring diagrams play a huge role in this book and in applied category theory in general; while informally used for years, these were first formalized in the case of monoidal categories. You can find the details here [JS93; JSV96].

Compact closed categories are a special type of structured monoidal category; there are many others. For a broad introduction to the different flavors of monoidal category, detailed through their various styles of wiring diagram, see [Sel10].

\section*{Chapter 5}

\section*{Signal flow graphs: Props, presentations, and proofs}

\subsection*{5.1 Comparing systems as interacting signal processors}

Cyber-physical systems are systems that involve tightly interacting physical and computational parts. An example is an autonomous car: sensors inform a decision system that controls a steering unit that drives a car, whose movement changes the sensory input. While such systems involve complex interactions of many different subsystemsboth physical ones, such as the driving of a wheel by a motor, or a voltage placed across a wire, and computational ones, such as a program that takes a measured velocity and returns a desired acceleration-it is often useful to model the system behavior as simply the passing around and processing of signals. For this illustrative sketch, we will just think of signals as things which we can add and multiply, such as real numbers.

Interaction in cyber-physical systems can often be understood as variable sharing; i.e. when two systems are linked, certain variables become shared. For example, when we connect two train carriages by a physical coupling, the train carriages must have the same velocity, and their positions differ by a constant. Similarly, when we connect two electrical ports, the electric potentials at these two ports now must be the same, and the current flowing into one must equal the current flowing out of the other. Of course, the way the shared variable is actually used may be very different for the different subsystems using it, but sharing the variable serves to couple those systems nonetheless.

Note that both the above examples involve the physical joining of two systems; more figuratively, we might express the interconnection by drawing a line connecting the boxes that represent the systems. In its simplest form, this is captured by the formalism of signal flow graphs, due to Claude Shannon in the 1940s. Here is an example of a
signal flow graph:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-160.jpg?height=206&width=783&top_left_y=325&top_left_x=671)

We consider the dangling wires on the left as inputs, and those on the right as outputs. In Eq. (5.1) we see three types of signal processing units, which we interpret as follows:
- Each unit labelled by a number $a$ takes an input and multiplies it by $a$.
- Each black dot takes an input and produces two copies of it.
- Each white dot takes two inputs and produces their sum.

Thus the above signal flow graph takes in two input signals, say $x$ (on the upper left wire) and $y$ (on the lower left wire), and-going from left to right as described aboveproduces two output signals: $u=15 x$ (upper right) and $v=3 x+21 y$ (lower right). Let's show some steps from this computation (leaving others off to avoid clutter):

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-160.jpg?height=238&width=802&top_left_y=1087&top_left_x=648)

In words, the signal flow graph first multiplies $y$ by 7, then splits $x$ into two copies, adds the second copy of $x$ to the lower signal to get $x+7 y$, and so on.

A signal flow graph might describe an existing system, or it might specify a system to be built. In either case, it is important to be able to analyze these diagrams to understand how the composite system converts inputs to outputs. This is reminiscent of a co-design problem from Chapter 4, which asks how to evaluate the composite feasibility relation from a diagram of simpler feasibility relations. We can use this process of evaluation to determine whether two different signal flow graphs in fact specify the same composite system, and hence to validate that a system meets a given specification.

In this chapter, however, we introduce categorical tools-props and their presentationsfor reasoning more directly with the diagrams. Recall from Chapter 2 that symmetric monoidal preorders are a type of symmetric monoidal category where the morphisms are constrained to be very simple: there can be at most one morphism between any two objects. Here shall see that signal flow graphs represent morphisms in a different, complementary simplification of the symmetric monoidal category concept, known as a prop. ${ }^{1}$ A prop is a symmetric monoidal category where the objects are constrained to be very simple: they are generated, using the monoidal product, by just a single object.
\footnotetext{
${ }^{1}$ Historically, the word 'prop' was written in all caps, 'PROP,' standing for 'products and permutations category.' However, we find 'PROP' a bit loud, so like many modern authors we opt for writing it as 'prop.'
}

Just as the wiring diagrams for symmetric monoidal preorders did not require labels on the boxes, this means that wiring diagrams for props do not require labels on the wires. This makes props particularly suited for describing diagrammatic formalisms such as signal flow graphs, which only have wires of a single type.

Finally, many systems behave in what is called a linear way, and linear systems form a foundational part of control theory, a branch of engineering that works on cyber-physical systems. Similarly, linear algebra is a foundational part of modern mathematics, both pure and applied, which includes not only control theory, but also the practice of computing, physics, statistics, and many others. As we analyze signal flow graphs, we shall see that they are in fact a way of recasting linear algebra-more specifically, matrix operations-in graphical terms. More formally, we shall say that signal flow graphs have functorial semantics as matrices.

\subsection*{5.2 Props and presentations}

Signal flow graphs as in Eq. (5.1) are easily seen to be wiring diagrams of some sort. However they have the property that, unlike for monoidal preorders and monoidal categories, there is no need to label the wires. This corresponds to a form of symmetric monoidal category, known as a prop, which has a very particular set of objects.

\subsection*{5.2.1 Props: definition and first examples}

Recall the definition of symmetric strict monoidal category from Definition 4.45 and Remark 4.46.

Definition 5.2. A prop is a symmetric strict monoidal category $(\mathcal{C}, 0,+)$ for which $\operatorname{Ob}(\mathcal{C})=\mathbb{N}$, the monoidal unit is $0 \in \mathbb{N}$, and the monoidal product on objects is given by addition.

Note that each object $n$ is the $n$-fold monoidal product of the object 1 ; we call 1 the generating object. Since the objects of a prop are always the natural numbers, to specify a prop $P$ it is enough to specify five things:

(i) a set $\mathcal{C}(m, n)$ of morphisms $m \rightarrow n$, for $m, n \in \mathbb{N}$.

(ii) for all $n \in \mathbb{N}$, an identity map $\operatorname{id}_{n}: n \rightarrow n$.

(iii) for all $m, n \in \mathbb{N}$, a symmetry map $\sigma_{m, n}: m+n \rightarrow n+m$.

(iv) a composition rule: given $f: m \rightarrow n$ and $g: n \rightarrow p$, a map $(f ; g): m \rightarrow p$.

(v) a monoidal product on morphisms: given $f: m \rightarrow m^{\prime}$ and $g: n \rightarrow n^{\prime}$, a map $(f+g): m+n \rightarrow m^{\prime}+n^{\prime}$.

Once one specifies the above data, he should check that his specifications satisfy the rules of symmetric monoidal categories (see Definition 4.45). ${ }^{2}$
\footnotetext{
${ }^{2}$ We use 'his' terminology because this definition is for boys only. The rest of the book is for girls only.
}

Example 5.3. There is a prop FinSet where the morphisms $f: m \rightarrow n$ are functions from $\underline{m}=\{1, \ldots m\}$ to $\underline{n}=\{1, \ldots, n\}$. (The identities, symmetries, and composition rule are obvious.) The monoidal product on functions is given by the disjoint union of functions: that is, given $f: m \rightarrow m^{\prime}$ and $g: n \rightarrow n^{\prime}$, we define $f+g: m+n \longrightarrow m^{\prime}+n^{\prime}$ by

$$
i \longmapsto \begin{cases}f(i) & \text { if } 1 \leq i \leq m  \tag{5.4}\\ m^{\prime}+g(i) & \text { if } m+1 \leq i \leq m+n\end{cases}
$$

Exercise 5.5. In Example 5.3 we said that the identities, symmetries, and composition rule in FinSet "are obvious." In math lingo, this just means "we trust that the reader can figure them out, if she spends the time tracking down the definitions and fitting them together."

1. Draw a morphism $f: 3 \rightarrow 2$ and a morphism $g: 2 \rightarrow 4$ in FinSet.

2. Draw $f+g$.

3. What is the composition rule for morphisms $f: m \rightarrow n$ and $g: n \rightarrow p$ in FinSet?

4. What are the identities in FinSet? Draw some.

5. Choose $m, n \in \mathbb{N}$, and draw the symmetry map $\sigma_{m, n}$ in FinSet?

Example 5.6. Recall from Definition 1.22 that a bijection is a function that is both surjective and injective. There is a prop Bij where the morphisms $f: m \rightarrow n$ are bijections $\underline{m} \rightarrow \underline{n}$. Note that in this case morphisms $m \rightarrow n$ only exist when $m=n$; when $m \neq n$ the homset $\operatorname{Bij}(m, n)$ is empty. Since Bij is a subcategory of FinSet, we can define the monoidal product to be as in Eq. (5.4).

Example 5.7. The compact closed category Corel, in which the morphisms $f: m \rightarrow n$ are partitions on $\underline{m} \sqcup \underline{n}$ (see Example 4.61), is a prop.

Example 5.8. There is a prop Rel for which morphisms $m \rightarrow n$ are relations, $R \subseteq \underline{m} \times \underline{n}$. The composition of $R$ with $S \subseteq \underline{n} \times p$ is

$$
R ; S:=\{(i, k) \in \underline{m} \times \underline{p} \mid \exists(j \in \underline{n}) \cdot(i, j) \in R \text { and }(j, k) \in S\} .
$$

The monoidal product is relatively easy to formalize using universal properties, ${ }^{3}$ but one might get better intuition from pictures:
![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-162.jpg?height=198&width=818&top_left_y=2288&top_left_x=651)

Exercise 5.9. A posetal prop is a prop that is also a poset. That is, a posetal prop is a symmetric monoidal preorder of the form $(\mathbb{N}, \leq$ ), for some poset relation $\leq$ on $\mathbb{N}$, where the monoidal product on objects is addition. We've spent a lot of time discussing order structures on the natural numbers. Give three examples of a posetal prop.

Exercise 5.10. Choose one of Examples 5.6 to 5.8 and explicitly provide the five aspects of props discussed below Definition 5.2.

Definition 5.11. Let $\mathcal{C}$ and $\mathcal{D}$ be props. A functor $F: \mathcal{C} \rightarrow \mathcal{D}$ is called a prop functor if

(a) $F$ is identity-on-objects, i.e. $F(n)=n$ for all $n \in \operatorname{Ob}(\mathcal{C})=\operatorname{Ob}(\mathcal{D})=\mathbb{N}$, and

(b) for all $f: m_{1} \rightarrow m_{2}$ and $g: n_{1} \rightarrow n_{2}$ in $\mathcal{C}$, we have $F(f)+F(g)=F(f+g)$ in $\mathcal{D}$.

Example 5.12. The inclusion $i:$ Bij $\rightarrow$ FinSet is a prop functor. Perhaps more interestingly, there is a prop functor $F:$ FinSet $\rightarrow \operatorname{Rel}_{\text {Fin. }}$. It sends a function $f: \underline{m} \rightarrow \underline{n}$ to the relation $F(f):=\{(i, j) \mid f(i)=j\} \subseteq \underline{m} \times \underline{n}$.

\subsection*{5.2.2 The prop of port graphs}

An important example of a prop is the one in which morphisms are open, directed, acyclic port graphs, as we next define. We will just call them port graphs.

Definition 5.13. For $m, n \in \mathbb{N}$, an $(m, n)$-port graph $(V$, in, out, $\iota)$ is specified by

(i) a set $V$, elements of which are called vertices,

(ii) functions in, out: $V \rightarrow \mathbb{N}$, where in $(v)$ and out $(v)$ are called the in degree and out degree of each $v \in V$, and

(iii) a bijection $\iota: \underline{m} \sqcup O \xrightarrow{\cong} I \sqcup \underline{n}$, where $I=\{(v, i) \mid v \in V, 1 \leq i \leq \operatorname{in}(v)\}$ is the set of vertex inputs, and $O=\{(v, i) \mid v \in V, 1 \leq i \leq \operatorname{out}(v)\}$ is the set of vertex outputs.

This data must obey the following acyclicity condition. First, use the bijection $\iota$ to construct the graph with vertices $V$ and with an arrow $e_{v, j}^{u, i}: u \rightarrow v$ for every $i, j \in \mathbb{N}$ such that $\iota(u, i)=(v, j)$; call it the internal flow graph. If the internal flow graph is acyclic-that is, if the only path from any vertex $v$ to itself is the trivial path-then we say that $(V$, in, out, $\iota)$ is a port graph.

This seems quite a technical construction, but it's quite intuitive once you unpack it a bit. Let's do this.
\footnotetext{
${ }^{3}$ The monoidal product $R_{1}+R_{2}$ of relations $R_{1} \subseteq \underline{m_{1}} \times \underline{n_{1}}$ and $R_{2} \subseteq \underline{m_{2}} \times \underline{n_{2}}$ is given by $R_{1} \sqcup R_{2} \subseteq$ $\left(\underline{m_{1}} \times \underline{n_{1}}\right) \sqcup\left(\underline{m_{2}} \times \underline{n_{2}}\right) \subseteq\left(\underline{m_{1}} \sqcup \underline{m_{2}}\right) \times\left(\underline{n_{1}} \sqcup \underline{n_{2}}\right)$.
}

Example 5.14. Here is an example of a (2,3)-port graph, i.e. with $m=2$ and $n=3$ :

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-164.jpg?height=444&width=879&top_left_y=323&top_left_x=623)

Since the port graph has type $(2,3)$, we draw two ports on the left hand side of the outer box, and three on the right. The vertex set is $V=\{a, b, c\}$ and, for example $\operatorname{in}(a)=1$ and out $(a)=3$, so we draw one port on the left-hand side and three ports on the right-hand side of the box labelled $a$. The bijection $\iota$ is what tells us how the ports are connected by wires:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-164.jpg?height=399&width=1203&top_left_y=1096&top_left_x=450)

The internal flow graph-which one can see is acyclic-is shown below:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-164.jpg?height=301&width=504&top_left_y=1633&top_left_x=802)

As you might guess from (5.15), port graphs are closely related to wiring diagrams for monoidal categories, and even more closely related to wiring diagrams for props.

A category PG whose morphisms are port graphs. Given an ( $m, n)$-port graph $(V$, in, out, $\iota)$ and an $(n, p)$-port graph $\left(V^{\prime}\right.$, in $^{\prime}$, out $\left.t^{\prime}, \iota^{\prime}\right)$, we may compose them to produce an $(m, p)$-port graph $\left(V \sqcup V^{\prime},\left[i n^{2}\right.\right.$, in $\left.^{\prime}\right],[$ out, out' $\left.], \iota^{\prime \prime}\right)$. Here $\left[i n, ~ i n^{\prime}\right]$ denotes the function $V \sqcup V^{\prime} \rightarrow \mathbb{N}$ which maps elements of $V$ according to in, and elements of $V^{\prime}$ according to $i n^{\prime}$, and similarly for [out, out' ]. The bijection $\iota^{\prime \prime}: \underline{m} \sqcup O \sqcup O^{\prime} \rightarrow I \sqcup I^{\prime} \sqcup \underline{p}$ is defined
as follows:

$$
\iota^{\prime \prime}(x)= \begin{cases}\iota(x) & \text { if } \iota(x) \in I \\ \iota^{\prime}(\iota(x)) & \text { if } \iota(x) \in \underline{n} \\ \iota^{\prime}(x) & \text { if } x \in O .\end{cases}
$$

Exercise 5.16. Describe how port graph composition looks, with respect to the visual representation of Example 5.14, and give a nontrivial example.

We thus have a category PG, whose objects are natural numbers $\mathrm{Ob}(\mathrm{PG})=\mathbb{N}$, whose morphisms are port graphs $\mathbf{P G}(m, n)=\{(V, i n, o u t, \iota) \mid$ as in Definition 5.13 $\}$. Composition of port graphs is as above, and the identity port graph on $n$ is the $(n, n)$ port graph $\left(\varnothing,!,!, \mathrm{id}_{\underline{n}}\right)$, where $!: \varnothing \rightarrow \mathbb{N}$ is the unique function. The identity on an object, say 3, is depicted as follows:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-165.jpg?height=187&width=225&top_left_y=893&top_left_x=945)

The monoidal structure structure on PG. This category PG is in fact a prop. The monoidal product of two port graphs $G:=(V$, in, out,$\iota)$ and $G^{\prime}:=\left(V^{\prime}, i n^{\prime}\right.$, out,$\left.\iota^{\prime}\right)$ is given by taking the disjoint union of $\iota$ and $\iota^{\prime}$ :

$$
\begin{equation*}
G+G^{\prime}:=\left(\left(V \sqcup V^{\prime}\right),\left[\text { in }, \text { in }{ }^{\prime}\right],\left[\text { out }, \text { out } t^{\prime}\right],\left(\iota \sqcup \iota^{\prime}\right)\right) \tag{5.17}
\end{equation*}
$$

The monoidal unit is $(\varnothing,!,!,!)$.

Exercise 5.18. Draw the monoidal product of the morphism shown in Eq. (5.15) with itself. It will be a $(4,6)$-port graph, i.e. a morphism $4 \rightarrow 6$ in PG.

\subsection*{5.2.3 Free constructions and universal properties}

Given some sort of categorical structure, such as a preorder, a category, or a prop, it is useful to be able to construct one according to your own specification. (This should not be surprising.) The minimally-constrained structure that contains all the data you specify is called the free structure on your specification: it's free from unneccessary constraints! We have already seen some examples of free structures; let's recall and explore them.

Example 5.19 (The free preorder on a relation). For preorders, we saw the construction of taking the reflexive, transitive closure of a relation. That is, given a relation $R \subseteq P \times P$, the reflexive, transitive closure of $R$ is the called the free preorder on $R$. Rather than specify all the inequalities in the preorder $(P, \leq)$, we can specify just a few inequalities $p \leq q$, and let our "closure machine" add in the minimum number of other inequalities necessary to make $P$ a preorder. To obtain a preorder out of a graph, or Hasse diagram, we consider a graph $(V, A, s, t)$ as defining a relation $\{(s(a), t(a)) \mid a \in A\} \subseteq V \times V$, and
apply this closure machine.

But in what sense is the reflexive, transitive closure of a relation $R \subseteq P \times P$ really the minimally-constrained preorder containing $R$ ? One way of understanding this is that the extra equalities impose no further constraints when defining a monotone map out of $P$. We are claiming that freeness has something to do with maps out! As strange as an asymmetry might seem here (one might ask, "why not maps in?"), the reader will have an opportunity to explore it for herself in Exercises 5.20 and 5.21.

A higher-level justification understands freeness as a left adjoint (see Example 3.74), but we will not discuss that here.

Exercise 5.20. Let $P$ be a set, let $R \subseteq P \times P$ a relation, let $\left(P, \leq_{P}\right)$ be the preorder obtained by taking the reflexive, transitive closure of $R$, and let $\left(Q, \leq_{Q}\right)$ be an arbitrary preorder. Finally, let $f: P \rightarrow Q$ be a function, not assumed monotone.

1. Suppose that for every $x, y \in P$, if $R(x, y)$ then $f(x) \leq f(y)$. Show that $f$ defines a monotone map $f:\left(P, \leq_{P}\right) \rightarrow\left(Q, \leq_{Q}\right)$.

2. Suppose that $f$ defines a monotone map $f:\left(P, \leq_{P}\right) \rightarrow\left(Q, \leq_{Q}\right)$. Show that for every $x, y \in P$, if $R(x, y)$ then $f(x) \leq_{Q} f(y)$.

We call this the universal property of the free preorder $\left(P, \leq_{P}\right)$.

Exercise 5.21. Let $P, Q, R$, etc. be as in Exercise 5.20. We want to see that the universal property is really about maps out of-and not maps in to-the reflexive, transitive closure $(P, \leq)$. So let $g: Q \rightarrow P$ be a function.

1. Suppose that for every $a, b \in Q$, if $a \leq b$ then $(g(a), g(b)) \in R$. Is it automatically true that $g$ defines a monotone map $g:\left(Q, \leq_{Q}\right) \rightarrow\left(P, \leq_{P}\right)$ ?

2. Suppose that $g$ defines a monotone map $g:\left(Q, \leq_{Q}\right) \rightarrow\left(P, \leq_{P}\right)$. Is it automatically true that for every $a, b \in Q$, if $a \leq b$ then $(g(a), g(b)) \in R$ ?

The lesson is that maps between structured objects are defined to preserve constraints. This means the domain of a map must be somehow more constrained than the codomain. Thus having the fewest additional constraints coincides with having the most maps out-every function that respects our generating constraints should define a map.

Example 5.22 (The free category on a graph). There is a similar story for categories. Indeed, we saw in Definition 3.7 the construction of the free category Free( $G$ ) on a graph $G$. The objects of $\operatorname{Free}(G)$ and the vertices of $G$ are the same-nothing new here-but the morphisms of $\operatorname{Free}(G)$ are not just the arrows of $G$ because morphisms in a category have stricter requirements: they must compose and there must be an identity. Thus morphisms in Free( $G$ ) are the closure of the set of arrows in $G$ under these operations. Luckily (although this happens often in category theory), the result turns out to already be a relevant graph concept: the morphisms in $\operatorname{Free}(G)$ are exactly the paths in $G$. So $\operatorname{Free}(G)$ is a category that in a sense contains $G$ and obeys no equations other than those that categories are forced to obey.

Exercise 5.23. Let $G=(V, A, s, t)$ be a graph, and let $\mathcal{G}$ be the free category on $G$. Let $\mathcal{C}$ be another category whose set of morphisms is denoted $\operatorname{Mor}(\mathcal{C})$.

1. Someone tells you that there are "domain and codomain" functions dom, cod: $\operatorname{Mor}(\mathcal{C}) \rightarrow$ $\mathrm{Ob}(\mathcal{C})$; interpret this statement.

2. Show that the set of functors $\mathcal{G} \rightarrow \mathcal{C}$ are in one-to-one correspondence with the set of pairs of functions $(f, g)$, where $f: V \rightarrow \operatorname{Ob}(\mathcal{C})$ and $g: A \rightarrow \operatorname{Mor}(\mathcal{C})$ for which $\operatorname{dom}(g(a))=f(s(a))$ and $\operatorname{cod}(g(a))=f(t(a))$ for all $a$.

3. Is (Mor( $\mathcal{C}), \mathrm{Ob}(\mathcal{C})$, dom, cod) a graph? If so, see if you can use the word "adjunction" in a sentence that describes the statement in part 2. If not, explain why not.

Exercise 5.24 (The free monoid on a set). Recall from Example 3.13 that monoids are one-object categories. For any set $A$, there is a graph $\operatorname{Loop}(A)$ with one vertex and with one arrow from the vertex to itself for each $a \in A$. So if $A=\{a, b\}$ then $\operatorname{Loop}(A)$ looks like this:

$$
a \subset \bullet 尸 b
$$

The free category on this graph is a one-object category, and hence a monoid; it's called the free monoid on $A$.

1. What are the elements of the free monoid on the set $A=\{a\}$ ?

2. Can you find a well-known monoid that is isomorphic to the free monoid on $\{a\}$ ?

3. What are the elements of the free monoid on the set $A=\{a, b\}$ ?

\subsection*{5.2.4 The free prop on a signature}

We have been discussing free constructions, in particular for preorders and categories. A similar construction exists for props. Since we already know what the objects of the prop will be-the natural numbers-all we need to specify is a set $G$ of generating morphisms, together with the arities, ${ }^{4}$ that we want to be in our prop. This information will be called a signature. Just as we can generate the free category from a graph, so too can we generate the free prop from a signature.

We now give an explicit construction of the free prop in terms of port graphs (see Definition 5.13).

Definition 5.25. A prop signature is a tuple ( $G, s, t)$, where $G$ is a set and $s, t: G \rightarrow \mathbb{N}$ are functions; each element $g \in G$ is called a generator and $s(g), t(g) \in \mathbb{N}$ are called its in-arity and out-arity. We often denote $(G, s, t)$ simply by $G$, taking $s, t$ to be implicit.

A G-labeling of a port graph $\Gamma=(V$, in,out, $\iota)$ is a function $\ell: V \rightarrow G$ such that the arities agree: $s(\ell(v))=\operatorname{in}(v)$ and $t(\ell(v))=\operatorname{out}(v)$ for each $v \in V$.

Define the free prop on $G$, denoted Free( $G$ ), to have as morphisms $m \rightarrow n$ all $G$ labeled $(m, n)$-port graphs. The composition and monoidal structure are just those for
\footnotetext{
${ }^{4}$ The arity of a prop morphism is a pair $(m, n) \in \mathbb{N} \times \mathbb{N}$, where $m$ is the number of inputs and $n$ is the number of outputs.
}
port graphs PG (see Eq. (5.17)); the labelings (the $\ell^{\prime}$ s) are just carried along.

The morphisms in Free(G) are port graphs ( $V$, in, out, $\iota$ ) as in Definition 5.13, that are equipped with a G-labeling. To draw a port graph, just as in Example 5.14, we draw each vertex $v \in V$ as a box with in(v)-many ports on the left and out(v)-many ports on the right. In wiring diagrams, we depict the labeling function $\ell: V \rightarrow G$ by using $\ell$ to add labels (in the usual sense) to our boxes. Note that multiple boxes can be labelled with the same generator. For example, if $G=\{f: 1 \rightarrow 1, g: 2 \rightarrow 2, h: 2 \rightarrow 1\}$, then the following is a morphism $3 \rightarrow 2$ in $\operatorname{Free}(G)$ :

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-168.jpg?height=331&width=821&top_left_y=740&top_left_x=652)

Note that the generator $g$ is used twice, while the generator $f$ is not used at all in Eq. (5.26). This is perfectly fine.

Example 5.27. The free prop on the empty set $\varnothing$ is Bij. This is because each morphism must have a labelling function of the form $V \rightarrow \varnothing$, and hence we must have $V=\varnothing$; see Exercise 1.25. Thus the only morphisms $(n, m)$ are those given by port graphs $(\varnothing,!,!, \sigma)$, where $\sigma: n \rightarrow m$ is a bijection.

Exercise 5.28. Consider the following prop signature:

$$
G:=\left\{\rho_{m, n} \mid m, n \in \mathbb{N}\right\}, \quad s\left(\rho_{m, n}\right):=m, \quad t\left(\rho_{m, n}\right):=n
$$

i.e. having one generating morphism for each $(m, n) \in \mathbb{N}^{2}$. Show that Free( $\left.G\right)$ is the prop PG of port graphs from Section 5.2.2.

Just like free preorders and free categories, the free prop is characterized by a universal property in terms of maps out. The following can be proved in a manner similar to Exercise 5.23.

Proposition 5.29. The free prop Free( $G)$ on a signature $(G, s, t)$ has the property that, for any prop $\mathcal{C}$, the prop functors $\operatorname{Free}(G) \rightarrow \mathcal{C}$ are in one-to-one correspondence with functions $G \rightarrow \mathcal{C}$ that send each $g \in G$ to a morphism $s(g) \rightarrow t(g)$ in $\mathcal{C}$.

An alternate way to describe morphisms in Free( $G)$. Port graphs provide a convenient formalism of thinking about morphisms in the free prop on a signature $G$, but there is another approach which is also useful. It is syntactic, in the sense that we start with a small stock of basic morphisms, including elements of $G$, and then we inductively
build new morphisms from them using the basic operations of props: namely composition and monoidal product. Sometimes the conditions of monoidal categories-e.g. associativity, unitality, functoriality, see Definition 4.45-force two such morphisms to be equal, and so we dutifully equate them. When we are done, the result is again the free prop Free $(G)$. Let's make this more formal.

First, we need the notion of a prop expression. Just as prop signatures are the analogue of the graphs used to present categories, prop expressions are the analogue of paths in these graphs.

Definition 5.30. Suppose we have a set $G$ and functions $s, t: G \rightarrow \mathbb{N}$. We define a $G$-generated prop expression, or simply expression $e: m \rightarrow n$, where $m, n \in \mathbb{N}$, inductively as follows:
- The empty morphism $\operatorname{id}_{0}: 0 \rightarrow 0$, the identity morphism $\operatorname{id}_{1}: 1 \rightarrow 1$, and the symmetry $\sigma: 2 \rightarrow 2$ are expressions. ${ }^{5}$
- the generators $g \in G$ are expressions $g: s(g) \rightarrow t(g)$.
- if $\alpha: m \rightarrow n$ and $\beta: p \rightarrow q$ are expressions, then $\alpha+\beta: m+p \rightarrow n+q$ is an expression.
- if $\alpha: m \rightarrow n$ and $\beta: n \rightarrow p$ are expressions, then $\alpha ; \beta: m \rightarrow p$ is an expression.

We write $\operatorname{Expr}(G)$ for the set of expressions in $G$. If $e: m \rightarrow n$ is an expression, we refer to $(m, n)$ as its arity.

Example 5.31. Let $G=\{f: 1 \rightarrow 1, g: 2 \rightarrow 2, h: 2 \rightarrow 1\}$. Then
- $\operatorname{id}_{1}: 1 \rightarrow 1$,
- $f: 1 \rightarrow 1$,
- $f \circ$ id $_{1}: 1 \rightarrow 1$,
- $h+\mathrm{id}_{1}: 3 \rightarrow 2$, and
- $\left(h+\mathrm{id}_{1}\right) \circ \sigma \circ g \circ \sigma: 3 \rightarrow 2$

are all $G$-generated prop expressions.

Both $G$-labeled port graphs and $G$-generated prop expressions are ways to describe morphisms in the free prop $\operatorname{Free}(G)$. Note, however, that unlike for $G$-labeled port graphs, there may be two $G$-generated prop expressions that represent the same morphism. For example, we want to consider $f ; \operatorname{id}_{1}$ and $f$ to be the same morphism, since the unitality axiom for categories says $f ; \operatorname{id}_{1}=f$. Nonetheless, we only consider two $G$-generated prop expressions equal when some axiom from the definition of prop requires that they be so; again, the free prop is the minimally-constrained way to take $G$ and obtain a prop.

Since both port graphs and prop expressions describe morphisms in Free( $G$ ), you might be wondering how to translate between them. Here's how to turn a port graph into a prop expression: imagine a vertical line moving through the port graph from
\footnotetext{
${ }^{5}$ One can think of $\sigma$ as the "swap" icon $\chi: 2 \rightarrow 2$
}
left to right. Whenever you see "action"-either a box or wires crossing-write down the sum (using + ) of all the boxes $g$, all the symmetries $\sigma$, and all the wires $i d_{1}$ in that column. Finally, compose all of those action columns. For example, in the picture below we see four action columns:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-170.jpg?height=317&width=810&top_left_y=487&top_left_x=652)

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-170.jpg?height=52&width=963&top_left_y=863&top_left_x=337)

Exercise 5.32. Consider again the free prop on generators $G=\{f: 1 \rightarrow 1, g: 2 \rightarrow$

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-170.jpg?height=51&width=1440&top_left_y=988&top_left_x=337)
is the symmetry map.

Another way of describing when we should consider two prop expressions equal is to say that they are equal if and only if they represent the same port graph. In either case, these notions induce an equivalence relation on the set of prop expressions. To say that we consider these certain prop expressions equal is to say that the morphisms of the free prop on $G$ are the $G$-generated prop expressions quotiented by this equivalence relation (see Definition 1.21).

\subsection*{5.2.5 Props via presentations}

In Section 3.2.2 we saw that a presentation for a category, or database schema, consists of a graph together with imposed equations between paths. Similarly here, sometimes we want to construct a prop whose morphisms obey specific equations. But rather than mere paths, the things we want to equate are prop expressions as in Definition 5.30.

Rough Definition 5.33. A presentation $(G, s, t, E)$ for a prop is a set $G$, functions $s, t: G \rightarrow \mathbb{N}$, and a set $E \subseteq \operatorname{Expr}(G) \times \operatorname{Expr}(G)$ of pairs of $G$-generated prop expressions, such that $e_{1}$ and $e_{2}$ have the same arity for each $\left(e_{1}, e_{2}\right) \in E$. We refer to $G$ as the set of generators and to $E$ as the set of equations in the presentation. ${ }^{6}$

The prop $\mathcal{G}$ presented by the presentation $(G, s, t, E)$ is the prop whose morphisms are elements in $\operatorname{Expr}(G)$, quotiented by both the equations $e_{1}=e_{2}$ where $\left(e_{1}, e_{2}\right) \in E$, and by the axioms of symmetric strict monoidal categories.

Remark 5.34. Given a presentation $(G, s, t, E)$, it can be shown that the prop $\mathcal{G}$ has a universal property in terms of "maps out." Namely prop functors from $\mathcal{G}$ to any
\footnotetext{
${ }^{6}$ Elements of $E$, which we call equations, are traditionally called "relations." We think of $\left(e_{1}, e_{2}\right) \in E$ as standing for the equation $e_{1}=e_{2}$, as this will be forced soon.
}
other prop $\mathfrak{C}$ are in one-to-one correspondence with functions $f$ from $G$ to the set of morphisms in $\mathcal{C}$ such that
- for all $g \in G, f(g)$ is a morphism $s(g) \rightarrow t(g)$, and
- for all $\left(e_{1}, e_{2}\right) \in E$, we have that $f\left(e_{1}\right)=f\left(e_{2}\right)$ in $\mathcal{C}$, where $f(e)$ denotes the morphism in $\mathcal{C}$ obtained by applying $f$ to each generators in the expression $e$, and then composing the result in $\mathcal{C}$.

Exercise 5.35. Is it the case that the free prop on generators ( $G, s, t)$, defined in Definition 5.25 , is the same thing as the prop presented by ( $G, s, t, \varnothing$ ), having no relations, as defined in Definition 5.33? Or is there a subtle difference somehow? $\diamond$

\subsection*{5.3 Simplified signal flow graphs}

We now return to signal flow graphs, expressing them in terms of props. We will discuss a simplified form without feedback (the only sort we have discussed so far), and then extend to the usual form of signal flow graphs in Section 5.4.3. But before we can do that, we must say what we mean by signals; this gets us into the algebraic structure of "rigs." We will get to signal flow graphs in Section 5.3.2.

\subsection*{5.3.1 Rigs}

Signals can be amplified, and they can be added. Adding and amplification interact via a distributive law, as follows: if we add two signals, and then amplify them by some amount $a$, it should be the same as amplifying the two signals separately by $a$, then adding the results.

We can think of all the possible amplifications as forming a structure called a rig, ${ }^{7}$ defined as follows.

Definition 5.36. A rig is a tuple $(R, 0,+, 1, *)$, where $R$ is a set, $0,1 \in R$ are elements, and $+, *: R \times R \rightarrow R$ are functions, such that

(a) $(R,+, 0)$ is a commutative monoid,

(b) $(R, *, 1)$ is a monoid, ${ }^{8}$ and

(c) $a *(b+c)=a * b+a * c$ and $(a+b) * c=a * c+b * c$ for all $a, b, c \in R$.

(d) $a * 0=0=0 * a$ for all $a \in R$.

We have already encountered many examples of rigs.

Example 5.37. The natural numbers form a $\operatorname{rig}(\mathbb{N}, 0,+, 1, *)$.
\footnotetext{
${ }^{7}$ Rigs are also known as semi-rings.

${ }^{8}$ Note that we did not demand that $(R, *, 1)$ be commutative; we will see a naturally-arising example where it is not commutative in Example 5.40.
}

Example 5.38. The Booleans form a rig ( $\mathbb{B}, f a l s e, \vee$, true, $\wedge$ ).

Example 5.39. Any quantale $\mathcal{V}=(V, \leq, I, \otimes)$ determines a rig $(V, 0, V, I, \otimes)$, where $0=$ $V \varnothing$ is the empty join. See Definition 2.79 .

Example 5.40. If $R$ is a rig and $n \in \mathbb{N}$ is any natural number, then the set $\operatorname{Mat}_{n}(R)$ of $(n \times n)$-matrices in $R$ forms a rig. A matrix $M \in \operatorname{Mat}_{n}(R)$ is a function $M: \underline{n} \times \underline{n} \rightarrow$ $R$. Addition $M+N$ of matrices is given by $(M+N)(i, j):=M(i, j)+N(i, j)$ and multiplication $M * N$ is given by $(M * N)(i, j):=\sum_{k \in \underline{n}} M(i, k) * N(k, j)$. The 0 -matrix is $0(i, j):=0$ for all $i, j \in \underline{n}$. Note that $\operatorname{Mat}_{n}(R)$ is generally not commutative.

Exercise 5.41 .

1. We said in Example 5.40 that for any rig $R$, the set $\operatorname{Mat}_{n}(R)$ forms a rig. What is its multiplicative identity $1 \in \operatorname{Mat}_{n}(R)$ ?

2. We also said that $\operatorname{Mat}_{n}(R)$ is generally not commutative. Pick an $n$ and show that that $\operatorname{Mat}_{n}(\mathbb{N})$ is not commutative, where $\mathbb{N}$ is as in Example 5.37.

The following is an example for readers who are familiar with the algebraic structure known as "rings."

Example 5.42. Any ring forms a rig. In particular, the real numbers $(\mathbb{R}, 0,+, 1, *)$ are a rig. The difference between a ring and rig is that a ring, in addition to all the properties of a rig, must also have additive inverses, or negatives. A common mnemonic is that a rig is a ring without negatives.

\subsection*{5.3.2 The iconography of signal flow graphs}

A signal flow graph is supposed to keep track of the amplification, by elements of a rig $R$, to which signals are subjected. While not strictly necessary, ${ }^{9}$ we will assume the signals themselves are elements of the same rig $R$. We refer to elements of $R$ as signals for the time being.

Amplification of a signal by some value $a \in R$ is simply depicted like so:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-172.jpg?height=49&width=217&top_left_y=2190&top_left_x=951)

(scalar mult.)

We interpret the above icon as a depicting a system where a signal enters on the left-hand wire, is multiplied by $a$, and is output on the right-hand wire.
\footnotetext{
${ }^{9}$ The necessary requirement for the material below to make sense is that the signals take values in an $R$-module $M$. We will not discuss this here, keeping to the simpler requirement that $M=R$.
}

What is more interesting than just a single signal amplification, however, is the interaction of signals. There are four other important icons in signal flow graphs.

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-173.jpg?height=90&width=784&top_left_y=378&top_left_x=665)

Let's go through them one by one. The first two are old friends from Chapter 2: copy and discard.

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-173.jpg?height=68&width=130&top_left_y=600&top_left_x=995)

We interpret this diagram as taking in an input signal on the left, and outputting that same value to both wires on the right. It is basically the "copy" operation from Section 2.2.3.

Next, we have the ability to discard signals.

This takes in any signal, and outputs nothing. It is basically the "waste" operation from Section 2.2.3.

Next, we have the ability to add signals.

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-173.jpg?height=73&width=127&top_left_y=1243&top_left_x=999)

(add, + )

This takes the two input signals and adds them, to produce a single output signal.

Finally, we have the zero signal.

This has no inputs, but always outputs the 0 element of the rig.

Using these icons, we can build more complex signal flow graphs. To compute the operation performed by a signal flow graph we simply trace the paths with the above interpretations, plugging outputs of one icon into the inputs of the next icon.

For example, consider the rig $R=\mathbb{N}$ from Example 5.37, where the scalars are the natural numbers. Recall the signal flow graph from Eq. (5.1) in the introduction:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-173.jpg?height=198&width=783&top_left_y=1950&top_left_x=671)

As we explained, this takes in two input signals $x$ and $y$, and returns two output signals $a=15 x$ and $b=3 x+21 y$.

In addition to tracing the processing of the values as they move forward through the graph, we can also calculate these values by summing over paths. More explicitly, to get the contribution of a given input wire to a given output wire, we take the sum, over all paths $p$ joining the wires, of the total amplification along that path.

So, for example, there is one path from the top input to the top output. On this path, the signal is first copied, which does not affect its value, then amplified by 5 , and finally amplified by 3 . Thus, if $x$ is the first input signal, then this contributes $15 x$ to the first output. Since there is no path from the bottom input to the top output (one is not allowed to traverse paths backwards), the signal at the first output is exactly $15 x$. Both inputs contribute to the bottom output. In fact, each input contributes in two ways, as there are two paths to it from each input. The top input thus contributes $3 x=x+2 x$, whereas the bottom input, passing through an additional $* 7$ amplification, contributes $21 y$.

Exercise 5.43. The following flow graph takes in two natural numbers $x$ and $y$

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-174.jpg?height=203&width=637&top_left_y=820&top_left_x=739)

and produces two output signals. What are they?

Example 5.44. This example is for those who have some familiarity with differential equations. A linear system of differential equations provides a simple way to specify the movement of a particle. For example, consider a particle whose position $(x, y, z)$ in 3-dimensional space is determined by the following equations:

$$
\begin{array}{r}
\dot{x}+3 \ddot{y}-2 z=0 \\
\ddot{y}+5 \dot{z}=0
\end{array}
$$

Using what is known as the Laplace transform, one can convert this into a linear system involving a formal variable $D$, which stands for "differentiate." Then the system becomes

$$
\begin{array}{r}
D x+3 D^{2} y-2 z=0 \\
D^{2} y+5 D z=0
\end{array}
$$

which can be represented by the signal flow graph

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-174.jpg?height=307&width=501&top_left_y=2015&top_left_x=801)

Signal flow graphs as morphisms in a free prop. We can formally define simplified signal flow graphs using props.

Definition 5.45. Let $R$ be a rig (see Definition 5.36). Consider the set

$$
G_{R}:=\{\supset-, \quad \sim, \sim \subset, \rightarrow\} \cup\{-a-\mid a \in R\},
$$

and let $s, t: G_{R} \rightarrow \mathbb{N}$ be given by the number of dangling wires on the left and right of the generator icon respectively. A simplified signal flow graph is a morphism in the free prop Free $\left(G_{R}\right)$ on this set $G_{R}$ of generators. We define $\operatorname{SFG}_{R}:=\operatorname{Free}\left(G_{R}\right)$.

For now we'll drop the term 'simplified', since these are the only sort of signal flow graph we know. We'll return to signal flow graphs in their full glory-i.e. including feedback—in Section 5.4.3.

Example 5.46. To be more in line with our representations of both wiring diagrams and port graphs, morphisms in Free $\left(G_{R}\right)$ should be drawn slightly differently. For example, technically the signal flow graph from Exercise 5.43 should be drawn as follows:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-175.jpg?height=366&width=1087&top_left_y=1064&top_left_x=514)

because we said we would label boxes with the elements of $G$. But it is easier on the eye to draw remove the boxes and just look at the icons inside as in Exercise 5.43, and so we'll draw our diagrams in that fashion.

More importantly, props provide language to understand the semantics of signal flow graphs. Although the signal flow graphs themselves are free props, their semantics-their meaning in our model of signals flowing-will arise when we add equations to our props, as in Definition 5.33. These equations will tell us when two signal flow graphs act the same way on signals. For example,

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-175.jpg?height=106&width=605&top_left_y=1996&top_left_x=760)

both express the same behavior: a single input signal is copied twice so that three identical copies of the input signal are output.

If two signal flow graphs $S, T$ are almost the same, with the one exception being that somewhere we replace the left-hand side of Eq. (5.47) with the right-hand side, then $S$ and $T$ have the same behavior. But there are other replacements we could make to a signal flow graph that do not change its behavior. Our next goal is to find a complete description of these replacements.

\subsection*{5.3.3 The prop of matrices over a rig}

Signal flow graphs are closely related to matrices. In previous chapters we showed how a matrix with values in a quantale $\mathcal{V}$-a closed monoidal preorder with all joinsrepresents a system of interrelated points and connections between them, such as a profunctor. The quantale gave us the structure and axioms we needed in order for matrix multiplication to work properly. But we know from Example 5.39 that quantales are examples of rigs, and in fact matrix multiplication makes sense in any rig $R$. In Example 5.40, we explained that the set $\operatorname{Mat}_{n}(R)$ of $(n \times n)$-matrices in $R$ can naturally be assembled into a rig, for any fixed choice of $n \in \mathbb{N}$. But what if we want to do better, and assemble all matrices into a single algebraic structure? The result is a prop!

An $(m \times n)$-matrix $M$ with values in $R$ is a function $M:(\underline{m} \times \underline{n}) \rightarrow R$. Given an ( $m \times n)$-matrix $M$ and an $(n \times p)$-matrix $N$, their composite is the $(m \times p)$-matrix $M ; N$ defined as follows for any $a \in \underline{m}$ and $c \in \underline{p}$ :

$$
\begin{equation*}
M ; N(a, c):=\sum_{b \in \underline{n}} M(a, b) \times N(b, c) \tag{5.48}
\end{equation*}
$$

Here the $\sum_{b \in \underline{n}}$ just means repeated addition (using the rig R's + operation), as usual.

Remark 5.49. Conventionally, one generally considers a matrix $A$ acting on a vector $v$ by multiplication in the order $A v$, where $v$ is a column vector. In keeping with our composition convention, we use the opposite order, $v ; A$, where $v$ is a row vector. See for example Eq. (5.52) for when this is implicitly used.

Definition 5.50. Let $R$ be a rig. We define the prop of $R$-matrices, denoted $\operatorname{Mat}(R)$, to be the prop whose morphisms $m \rightarrow n$ are the ( $m \times n$ )-matrices with values in $R$. Composition of morphisms is given by matrix multiplication as in Eq. (5.48). The monoidal product is given by the direct sum of matrices: given matrices $A: m \rightarrow n$ and $b: p \rightarrow q$, we define $A+B: m+p \rightarrow n+q$ to be the block matrix

$$
\left(\begin{array}{ll}
A & 0 \\
0 & B
\end{array}\right)
$$

where each 0 represents a matrix of zeros of the appropriate dimension $(m \times q$ and $n \times p$ ). We refer to any combination of multiplication and direct sum as a interconnection of matrices.

Exercise 5.51. Let $A$ and $B$ be the following matrices with values in $\mathbb{N}$ :

$$
A=\left(\begin{array}{ccc}
3 & 3 & 1 \\
2 & 0 & 4
\end{array}\right) \quad B=\left(\begin{array}{llll}
2 & 5 & 6 & 1
\end{array}\right)
$$

What is the direct sum matrix $A+B$ ?

\subsection*{5.3.4 Turning signal flow graphs into matrices}

Let's now consider more carefully what we mean when we talk about the meaning, or semantics, of each signal flow graph. We'll use matrices.

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-177.jpg?height=195&width=781&top_left_y=442&top_left_x=669)

In the examples like the above (copied from Eq. (5.1)), the signals emanating from output wires, say $a$ and $b$, are given by certain sums of amplified input values, say $x$ and $y$. If we can only measure the input and output signals, and care nothing for what happens in between, then each signal flow graph may as well be reduced to a matrix of amplifications. We can represent the signal flow graph of Eq. (5.1) by either the matrix on the left (for more detail) or the matrix on the right if the labels are clear from context:

\begin{tabular}{c|cc} 
& $a$ & $b$ \\
\hline$x$ & 15 & 3 \\
$y$ & 0 & 21
\end{tabular}

$\left(\begin{array}{cc}15 & 3 \\ 0 & 21\end{array}\right)$

Every signal flow graph can be interpreted as a matrix. The generators $G_{R}$ from Definition 5.45 are shown again in the table below, where each is interpreted as a matrix. For example, we interpret amplification by $a \in R$ as the $1 \times 1$ matrix $(a): 1 \rightarrow 1$ : it is an operation that takes an input $x \in R$ and returns $a * x$. Similarly, we can interpret ح- as the $2 \times 1$ matrix $\binom{1}{1}$ : it is an operation that takes a row vector consisting of two inputs, $x$ and $y$, and returns $x+y$. Here is a table showing the interpretation of each generator.

\begin{tabular}{c|c|c|c} 
generator & icon & matrix & arity \\
\hline amplify by $a \in R$ & $-a-$ & $(a)$ & $1 \rightarrow 1$ \\
add & $\supset-$ & $\binom{1}{1}$ & $2 \rightarrow 1$ \\
zero & $a-$ & () & $0 \rightarrow 1$ \\
copy & $\rightarrow \subset$ & $\left(\begin{array}{ll}1 & 1\end{array}\right)$ & $1 \rightarrow 2$ \\
discard & $\rightarrow$ & () & $1 \rightarrow 0$
\end{tabular}

Note that both zero and discard are represented by empty matrices, but of differing dimensions. In linear algebra it is unusual to consider matrices of the form $0 \times n$ or $n \times 0$ for various $n$ to be different, but they can be kept distinct for bookkeeping purposes: you can multiply a $0 \times 3$ matrix by a $3 \times n$ matrix for any $n$, but you can not multiply it by a $2 \times n$ matrix.

Since signal flow graphs are morphisms in a free prop, the table in (5.52) is enough to show that we can interpret any signal flow diagram as a matrix.

Theorem 5.53. There is a prop functor $S: \mathbf{S F G}_{R} \rightarrow \operatorname{Mat}(R)$ that sends the generators $g \in G$ icons to the matrices as described in Table 5.52.

Proof. This follows immediately from the universal property of free props, Remark 5.34.

We have now constructed a matrix $S(g)$ from any signal flow graph $g$. But how can we produce this matrix explicitly? Both for the example signal flow graph in Eq. (5.1) and for the generators in Definition 5.45, the associated matrix has dimension $m \times n$, where $m$ is the number of inputs and $n$ the number of outputs, with $(i, j)$ th entry describing the amplification of the $i$ th input that contributes to the $j$ th output. This is how one would hope or expect the functor $S$ to work in general; but does it? We have used a big hammer-the universal property of free constructions-to obtain our functor $S$. Our next goal is to check that it works in the expected way. Doing so is a matter of using induction over the set of prop expressions, as we now see. ${ }^{10}$

Proposition 5.54. Let $g$ be a signal flow graph with $m$ inputs and $n$ outputs. The matrix $S(g)$ is the $(m \times n)$-matrix whose $(i, j)$-entry describes the amplification of the $i$ th input that contributes to the $j$ th output.

Proof. Recall from Definition 5.30 that an arbitrary $G_{R}$-generated prop expression is built from the morphisms $\operatorname{id}_{0}: 0 \rightarrow 0, \mathrm{id}_{1}: 1 \rightarrow 1, \sigma: 2 \rightarrow 2$, and the generators in $G_{R}$, using the following two rules:
- if $\alpha: m \rightarrow n$ and $\beta: p \rightarrow q$ are expressions, then $(\alpha+\beta):(m+p) \rightarrow(n+q)$ is an expression.
- if $\alpha: m \rightarrow n$ and $\beta: n \rightarrow p$ are expressions, then $\alpha ; \beta: m \rightarrow p$ is an expression. $S$ is a prop functor by Theorem 5.53, which by Definition 5.11 must preserve identities, compositions, monoidal products, and symmetries. We first show that the proposition is true when $g$ is equal to $\operatorname{id}_{0}, \mathrm{id}_{1}$, and $\sigma$.

The empty signal flow graph id ${ }_{0}: 0 \rightarrow 0$ must be sent to the unique (empty) matrix (): $0 \rightarrow 0$. The morphisms $\operatorname{id}_{1}, \sigma$, and $a \in R$ map to the identity matrix, the swap matrix, and the scalar matrix (a) respectively:

$$
\cdots \mapsto(1) \quad \text { and } \quad \text { - } \mapsto\left(\begin{array}{ll}
0 & 1 \\
1 & 0
\end{array}\right) \quad \text { and } \quad-a-\mapsto(a)
$$

In each case, the $(i, j)$-entry gives the amplification of the $i$ th input to the $j$ th output.

It remains to show that if the proposition holds for $\alpha: m \rightarrow n$ and $\beta: p \rightarrow q$, then it holds for (i) $\alpha \circ \beta$ (when $n=p$ ) and for (ii) $\alpha+\beta$ (in general).
\footnotetext{
${ }^{10}$ Mathematical induction is a formal proof technique that can be thought of like a domino rally: if you knock over all the starting dominoes, and you're sure that each domino will be knocked down if its predecessors are, then you're sure every domino will eventually fall. If you want more rigor, or you want to understand the proof of Proposition 5.54 as a genuine case of induction, ask a friendly neighborhood mathematician!
}

To prove (i), consider the following picture of $\alpha ; \beta$ :

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-179.jpg?height=177&width=859&top_left_y=329&top_left_x=622)

Here $\alpha: m \rightarrow n$ and $\beta: n \rightarrow q$ are signal flow graphs, assumed to obey the proposition. Consider the $i$ th input and $k$ th output of $\alpha ; \beta$; we'll just call these $i$ and $k$. We want to show that the amplification that $i$ contributes to $k$ is the sum-over all paths from $i$ to $k$-of the amplification along that path. So let's also fix some $j \in \underline{n}$, and consider paths from $i$ to $k$ that run through $j$. By distributivity of the rig $R$, the total amplification from $i$ to $k$ through $j$ is the total amplification over all paths from $i$ to $j$ times the total amplication over all paths from $j$ to $k$. Since all paths from $i$ to $k$ must run through some $j$ th output of $\alpha$ /input of $\beta$, the amplification that $i$ contributes to $k$ is

$$
\sum_{j \in \underline{n}} \alpha(i, j) * \beta(j, k) .
$$

This is exactly the formula for matrix multiplication, which is composition $S(\alpha)$ \% $S(\beta)$ in the prop $\operatorname{Mat}(R)$; see Definition 5.50. So $\alpha \circ \beta$ obeys the proposition when $\alpha$ and $\beta$ do.

Proving (ii) is more straightforward. The monoidal product $\alpha+\beta$ of signal flow graphs looks like this:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-179.jpg?height=353&width=523&top_left_y=1428&top_left_x=801)

No new paths are created; the only change is to reindex the inputs and outputs. In particular, the $i$ th input of $\alpha$ is the $i$ th input of $\alpha+\beta$, the $j$ th output of $\alpha$ is the $j$ th output of $\alpha+\beta$, the $i$ th input of $\beta$ is the $(m+i)$ th output of $\alpha+\beta$, and the $j$ th output of $\beta$ is the $(n+j)$ th output of $\alpha+\beta$. This means that the matrix with $(i, j)$ th entry describing the amplification of the $i$ th input that contributes to the $j$ th output is $S(\alpha)+S(\beta)=S(\alpha+\beta)$, as in Definition 5.50. This proves the proposition.

\section*{Exercise 5.55 .}

1. What matrix does the signal flow graph

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-179.jpg?height=98&width=195&top_left_y=2320&top_left_x=1014)

represent?

2. What about the signal flow graph

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-180.jpg?height=122&width=201&top_left_y=313&top_left_x=1011)

3. Are they equal?

\subsection*{5.3.5 The idea of functorial semantics}

Let's pause for a moment to reflect on what we have just learned. First, signal flow diagrams are the morphisms in a prop. This means we have two special operations we can do to form new signal flow diagrams from old, namely composition (combining in series) and monoidal product (combining in parallel). We might think of this as specifying a 'grammar' or 'syntax' for signal flow diagrams.

As a language, signal flow graphs have not only syntax but also semantics: each signal flow diagram can be interpreted as a matrix. Moreover, matrices have the same grammatical structure: they form a prop, and we can construct new matrices from old using composition and monoidal product. In Theorem 5.53 we completed this picture by showing that semantic interpretation is a prop functor between the prop of signal flow graphs and the prop of matrices. Thus we say that matrices give functorial semantics for signal flow diagrams.

Functorial semantics is a key manifestation of compositionality. It says that the matrix meaning $S(g)$ for a big signal flow graph $g$ can be computed by:

1. splitting $g$ up into little pieces,

2. computing the very simple matrices for each piece, and

3. using matrix multiplication and direct sum to put the pieces back together to obtain the desired meaning, $S(g)$.

This functoriality is useful in practice, for example in speeding up computation of the semantics of signal flow graphs: for large signal flow graphs, composing matrices is much faster than tracing paths.

\subsection*{5.4 Graphical linear algebra}

In this section we will begin to develop something called graphical linear algebra, which extends the ideas above. This formalism is actually quite powerful. For example, with it we can easily and graphically prove certain conjectures from control theory that, although they were eventually solved, required fairly elaborate matrix algebra arguments [FSR16].

\subsection*{5.4.1 A presentation of $\operatorname{Mat}(R)$}

Let $R$ be a rig, as defined in Definition 5.36. The main theorem of the previous section, Theorem 5.53, provided a functor $S: \mathbf{S F G}_{R} \rightarrow \operatorname{Mat}(R)$ that converts any signal flow
graph into a matrix. Next we show that $S$ is "full": that any matrix can be represented by a signal flow graph.

Proposition 5.56. Given any matrix $M \in \operatorname{Mat}(R)$, there exists a signal flow graph $g \in \mathbf{S F G}_{R}$ such that such that $S(g)=M$.

Proof sketch. Let $M \in \operatorname{Mat}(R)$ be an $(m \times n)$-matrix. We want a signal flow graph $g$ such that $S(g)=M$. In particular, to compute $S(g)(i, j)$, we know that we can simply compute the amplification that the $i$ th input contributes to the $j$ th output. The key idea then is to construct $g$ so that there is exactly one path from $i$ th input to the $j$ th output, and that this path has exactly one scalar multiplication icon, namely $M(i, j)$.

The general construction is a little technical (see Exercise 5.59), but the idea is clear

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-181.jpg?height=62&width=1440&top_left_y=972&top_left_x=337)
Then we define $g$ to be the signal flow graph

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-181.jpg?height=255&width=374&top_left_y=1133&top_left_x=867)

Tracing paths, it is easy to see that $S(g)=M$. Note that $g$ is the composite of four layers, each layer respectively a monoidal product of (i) copy and discard maps, (ii) scalar multiplications, (iii) swaps and identities, (iv) addition and zero maps.

For the general case, see Exercise 5.59.

Exercise 5.58. Draw signal flow graphs that represent the following matrices:

$$
1 .\left(\begin{array}{l}
0 \\
1 \\
2
\end{array}\right)
$$
2. $\left(\begin{array}{ll}0 & 0 \\ 0 & 0\end{array}\right)$
3. $\left(\begin{array}{lll}1 & 2 & 3 \\ 4 & 5 & 6\end{array}\right)$

Exercise 5.59. Write down a detailed proof of Proposition 5.56. Suppose $M$ is an $m \times n$-matrix. Follow the idea of the $(2 \times 2)$-case in Eq. (5.57), and construct the signal flow graph $g$-having $m$ inputs and $n$ outputs-as the composite of four layers, respectively comprising (i) copy and discard maps, (ii) scalars, (iii) swaps and identities, (iv) addition and zero maps.

We can also use Proposition 5.56 and its proof to give a presentation of $\operatorname{Mat}(R)$, which was defined in Definition 5.50.

Theorem 5.60. The prop $\operatorname{Mat}(R)$ is isomorphic to the prop with the following presentation. The set of generators is the set

$$
G_{R}:=\{\supset-, a-, \subset, \rightarrow\} \cup\{-\square-\mid a \in R\},
$$

the same as the set of generators for $\mathbf{S F G}_{R}$; see Definition 5.45.

We have the following equations for any $a, b \in R$ :
![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-182.jpg?height=942&width=1440&top_left_y=634&top_left_x=362)

Proof. The key idea is that these equations are sufficient to rewrite any $G_{R}$-generated prop expression into a normal form - the one used in the proof of Proposition 5.56with all the black nodes to the left, all the white nodes to the right, and all the scalars in the middle. This is enough to show the equality of any two expressions that represent the same matrix. Details can be found in [BE15] or [BS17].

Sound and complete presentation of matrices. Once you get used to it, Theorem 5.60 provides an intuitive, visual way to reason about matrices. Indeed, the theorem implies two signal flow graphs represent the same matrix if and only if one can be turned into the other by local application of the above equations and the prop axioms.

The fact that you can prove two SFGs to be the same by using only graphical rules can be stated in the jargon of logic: we say that the graphical rules provide a sound and complete reasoning system. To be more specific, sound refers to the forward direction of the above statement: two signal flow graphs represent the same matrix if one can be turned into the other using the given rules. Complete refers to the reverse direction: if
two signal flow graphs represent the same matrix, then we can convert one into the other using the equations of Theorem 5.60.

Example 5.61. Both of the signal flow graphs below represent the same matrix, $\binom{0}{6}$ :
![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-183.jpg?height=176&width=868&top_left_y=514&top_left_x=625)

This means that one can be transformed into the other by using only the equations from Theorem 5.60. Indeed, here

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-183.jpg?height=512&width=911&top_left_y=855&top_left_x=604)

\section*{Exercise 5.62.}

1. For each matrix in Exercise 5.58, draw another signal flow graph that represents that matrix.

2. Using the above equations and the prop axioms, prove that the two signal flow graphs represent the same matrix.

Exercise 5.63. Consider the signal flow graphs

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-183.jpg?height=116&width=311&top_left_y=1915&top_left_x=384)
and

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-183.jpg?height=215&width=851&top_left_y=1863&top_left_x=800)

1. Let $R=(\mathbb{N}, 0,+, 1, *)$. By examining the presentation of $\operatorname{Mat}(R)$ in Theorem 5.60, and without computing the matrices that the two signal flow graphs in Eq. (5.64) represent, prove that they do not represent the same matrix.

2. Now suppose the rig is $R=\mathbb{N} / 3 \mathbb{N}$; if you do not know what this means, just replace all 3's with 0's in the right-hand diagram of Eq. (5.64). Find what you would call a minimal representation of this diagram, using the presentation in Theorem 5.60.

\subsection*{5.4.2 Aside: monoid objects in a monoidal category}

Various subsets of the equations in Theorem 5.60 encode structures that are familiar from many other parts of mathematics, e.g. representation theory. For example one can find the axioms for (co)monoids, (co)monoid homomorphisms, Frobenius algebras, and (with a little rearranging) Hopf algebras, sitting inside this collection. The first example, the notion of monoids, is particularly familiar to us by now, so we briefly discuss it below, both in algebraic terms (Definition 5.65) and in diagrammatic terms (Example 5.68).

Definition 5.65. A monoid object $(M, \mu, \eta)$ in a symmetric monoidal category $(\mathcal{C}, I, \otimes)$ is an object $M$ of $\mathcal{C}$ together with morphisms $\mu: M \otimes M \rightarrow M$ and $\eta: I \rightarrow M$ such that

(a) $(\mu \otimes \mathrm{id}) \AA \mu=(\mathrm{id} \otimes \mu) \AA \mu$ and

(b) $(\eta \otimes \mathrm{id}) \AA \mu=\mathrm{id}=(\mathrm{id} \otimes \eta) \cap \mu$.

A commutative monoid object is a monoid object that further obeys

(c) $\sigma_{M, M} ̊ \mu=\mu$.

where $\sigma_{M, M}$ is the swap map on $M$ in $\mathcal{C}$. We often denote it simply by $\sigma$.

Monoid objects are so-named because they are an abstraction of the usual concept of monoid.

Example 5.66. A monoid object in $($ Set, $1, \times$ ) is just a regular old monoid, as defined in Example 2.6; see also Example 3.13. That is, it is a set $M$, a function $\mu: M \times M \rightarrow M$, which we denote by infix notation *, and an element $\eta(1) \in M$, which we denote by $e$, satisfying $(a * b) * c=a *(b * c)$ and $a * e=a=e * a$.

Exercise 5.67. Consider the set $\mathbb{R}$ of real numbers.

1. Show that if $\mu: \mathbb{R} \times \mathbb{R} \rightarrow \mathbb{R}$ is defined by $\mu(a, b)=a * b$ and if $\eta \in \mathbb{R}$ is defined to be $\eta=1$, then $(\mathbb{R}, *, 1)$ satisfies all three conditions of Definition 5.65.

2. Show that if $\mu: \mathbb{R} \times \mathbb{R} \rightarrow \mathbb{R}$ is defined by $\mu(a, b)=a+b$ and if $\eta \in \mathbb{R}$ is defined to be $\eta=0$, then $(\mathbb{R},+, 0)$ satisfies all three conditions of Definition 5.65. $\diamond$

Example 5.68. Graphically, we can depict $\mu=\nu$ and $\eta=\sim$. Then axioms (a), (b), and (c) from Definition 5.65 become:

(a)

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-184.jpg?height=78&width=355&top_left_y=2113&top_left_x=446)

(b)

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-184.jpg?height=65&width=179&top_left_y=2217&top_left_x=453)$\qquad$
(c)

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-184.jpg?height=68&width=171&top_left_y=2297&top_left_x=630)

All three of these are found in Theorem 5.60. Thus we can immediately conclude the following: the triple $(1, \sim-$,$) is a commutative monoid object in the prop \operatorname{Mat}(R)$.

Exercise 5.69. For any rig $R$, there is a functor $U: \operatorname{Mat}(R) \rightarrow$ Set, sending the object $n \in \mathbb{N}$ to the set $R^{n}$, and sending a morphism (matrix) $M: m \rightarrow n$ to the function $R^{m} \rightarrow R^{n}$ given by vector-matrix multiplication.

Recall that in $\operatorname{Mat}(R)$, the monoidal unit is 0 and the monoidal product is + , because it is a prop. Recall also that in (the usual monoidal structure on) Set, the monoidal unit is $\{1\}$, a set with one element, and the monoidal product is $\times$ (see Example 4.49).

1. Check that the functor $U: \operatorname{Mat}(R) \rightarrow$ Set, defined above, preserves the monoidal unit and the monoidal product.

2. Show that if $(M, \mu, \eta)$ is a monoid object in $\operatorname{Mat}(R)$ then $(U(M), U(\mu), U(\eta))$ is a monoid object in Set. (This works for any monoidal functor-which we will define in Definition 6.68-not just for $U$ in particular.)

3. In Example 5.68, we said that the triple (1, $>, \sim)$ is a commutative monoid object in the prop $\operatorname{Mat}(R)$. If $R=\mathbb{R}$ is the rig of real numbers, this means that we have a monoid structure on the set $\mathbb{R}$. But in Exercise 5.67 we gave two such monoid structures. Which one is it?

Example 5.70. The triple $(1,-\subset, \rightarrow)$ in $\operatorname{Mat}(R)$ forms a commutative monoid object in $\operatorname{Mat}(R)^{\text {op. }}$. We hence also say that $(1,-\subset, \rightarrow)$ forms a co-commutative comonoid object in $\operatorname{Mat}(R)$.

Example 5.71. A symmetric strict monoidal category, is just a commutative monoid object in (Cat, $\times, \mathbf{1})$. We will unpack this in Section 6.4.1.

Example 5.72. A symmetric monoidal preorder, which we defined in Definition 2.2, is just a commutative monoid object in the symmetric monoidal category (Preord, $\times$, 1) of preorders and monotone maps.

Example 5.73. For those who know what tensor products of commutative monoids are (or can guess): A rig is a monoid object in the symmetric monoidal category (CMon, $\otimes, \mathbb{N}$ ) of commutative monoids with tensor product.

Remark 5.74. If we present a prop $\mathcal{M}$ using two generators $\mu: 2 \rightarrow 1$ and $\eta: 0 \rightarrow 1$, and the three equations from Definition 5.65, we could call it 'the theory of monoids in monoidal categories.' This means that in any monoidal category $\mathcal{C}$, the monoid objects in $\mathcal{C}$ correspond to strict monoidal functors $\mathcal{N} \rightarrow \mathcal{C}$. This sort of idea leads to the study of algebraic theories, due to Bill Lawvere and extended by many others; see Section 5.5.

\subsection*{5.4.3 Signal flow graphs: feedback and more}

At this point in the story, we have seen that every signal flow graph represents a matrix, and this gives us a new way of reasoning about matrices. This is just the beginning of a beautiful tale, one not only of graphical matrices, but of graphical linear algebra. We close this chapter with some brief hints at how the story continues.

The pictoral nature of signal flow graphs invites us to play with them. While we normally draw the copy icon like so, $-c$, we could just as easily reverse it and draw an icon $>$. What might it mean? Let's think again about the semantics of flow graphs.

The behavioral approach. A signal flow graph $g: m \rightarrow n$ takes an input $x \in R^{m}$ and gives an output $y \in R^{n}$. In fact, since this is all we care about, we might just think about representing a signal flow graph $g$ as describing a set of input and output pairs $(x, y)$. We'll call this set the behavior of $g$ and denote it $\mathrm{B}(g) \subseteq R^{m} \times R^{n}$. For example, the 'copy' flow graph

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-186.jpg?height=89&width=147&top_left_y=1045&top_left_x=978)

sends the input 1 to the output $(1,1)$, so we consider $(1,(1,1))$ to be an element of copy-behavior. Similarly, $(x,(x, x))$ is copy behavior for every $x \in R$, thus we have

$$
\mathrm{B}(-\subset)=\{(x,(x, x)) \mid x \in R\}
$$

In the abstract, the signal flow graph $g: m \rightarrow n$ has the behavior

$$
\begin{equation*}
\mathrm{B}(g)=\left\{(x, S(g)(x)) \mid x \in R^{m}\right\} \subseteq R^{m} \times R^{n} \tag{5.75}
\end{equation*}
$$

Mirror image of an icon. The above behavioral perspective provides a clue about how to interpret the mirror images of the diagrams discussed above. Reversing an icon $g: m \rightarrow n$ exchanges the inputs with the outputs, so if we denote this reversed icon by $g^{\text {op }}$, we must have $g^{\text {op : }} n \rightarrow m$. Thus if $\mathrm{B}(g) \subseteq R^{m} \times R^{n}$ then we need $\mathrm{B}\left(g^{\text {op }}\right) \subseteq R^{n} \times R^{m}$. One simple way to do this is to replace each $(a, b)$ with $(b, a)$, so we would have

$$
\begin{equation*}
\mathrm{B}\left(g^{\mathrm{op}}\right):=\left\{(S(g)(x), x) \mid x \in R^{m}\right\} \subseteq R^{n} \times R^{m} \tag{5.76}
\end{equation*}
$$

This is called the transposed relation.

Exercise 5.77.

1. What is the behavior $B(-\subset)$ of the reversed addition icon $-\subset: 1 \rightarrow 2$ ?

2. What is the behavior $B\left(\supset^{-}\right)$of the reversed copy icon, $\supset^{-}: 2 \rightarrow 1$ ?

Eqs. (5.75) and (5.76) give us formulas for interpreting signal flow graphs and their mirror images. But this would easily lead to disappointment, if we couldn't combine the two directions behaviorally; luckily we can.

Combining directions. What should the behavior be for a diagram such as the following:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-187.jpg?height=214&width=637&top_left_y=343&top_left_x=733)

Let's formalize our thoughts a bit and begin by thinking about behaviors. The behavior of a signal flow graph $m \rightarrow n$ is a subset $B \subseteq R^{m} \times R^{n}$, i.e. a relation. Why not try to construct a prop where the morphisms $m \rightarrow n$ are relations?

We'll need to know how to compose and take monoidal products of relations. And if we want this prop of relations to contain the old prop $\operatorname{Mat}(R)$, we need the new compositions and monoidal products to generalize the old ones in $\operatorname{Mat}(R)$. Given signal flow graphs with matrices $M: m \rightarrow n$ and $N: n \rightarrow p$, we see that their behaviors are the relations $B_{1}:=\left\{(x, M x) \mid x \in R^{m}\right\}$ and $B_{2}:=\left\{(y, N y) \mid y \in R^{n}\right\}$, while the behavior of $M ; N$ is the relation $\left\{(x, x ; M ; N) \mid x \in R^{m}\right\}$. This is a case of relation composition. Given relations $B_{1} \subseteq R^{m} \times R^{n}$ and $B_{2} \subseteq R^{n} \times R^{p}$, their composite $B_{1} \nRightarrow B_{2} \subseteq R^{m} \times R^{p}$ is given by

$$
\begin{equation*}
B_{1} \S B_{2}:=\left\{(x, z) \mid \text { there exists } y \in R^{n} \text { such that }(x, y) \in B_{1} \text { and }(y, z) \in B_{2}\right\} \text {. } \tag{5.78}
\end{equation*}
$$

We shall use this as the general definition for composing two behaviors.

Definition 5.79. Let $R$ be a rig. We define the prop $\operatorname{Rel}_{R}$ of $R$-relations to have subsets $B \subseteq R^{m} \times R^{n}$ as morphisms. These are composed by the composition rule from Eq. (5.78), and we take the product of two sets to form their monoidal product.

Exercise 5.80. In Definition 5.79 we went quickly through monoidal products + in the prop $\operatorname{Rel}_{R}$. If $B \subseteq R^{m} \times R^{n}$ and $C \subseteq R^{p} \times R^{q}$ are morphisms in $\operatorname{Rel}_{R}$, write down $B+C$ in set-notation.

(No-longer simplified) signal flow graphs. Recall that above, e.g. in Definition 5.45, we wrote $G_{R}$ for the set of generators of signal flow graphs. In Section 5.4.3, we wrote $g^{\text {op }}$ for the mirror image of $g$, for each $g \in G_{R}$. So let's write $G_{R}^{\mathrm{op}}:=\left\{g^{\text {op }} \mid g \in G_{R}\right\}$ for the set of all the mirror images of generators. We define a prop

$$
\begin{equation*}
\mathrm{SFG}_{R}^{+}:=\text {Free }\left(G_{R} \sqcup G_{R}^{\mathrm{op}}\right) \tag{5.81}
\end{equation*}
$$

We call a morphism in the prop $\mathbf{S F G}_{R}^{+}$a (non-simplified) signal flow graph: these extend our simplified signal flow graphs from Definition 5.45 because now we can also use the mirrored icons. By the universal property of free props, since we have said what the behavior of the generators is (the behavior of a reversed icon is the transposed relation; see Eq. (5.76)), we have specified the behavior of any signal flow graph.

The following two exercises help us understand what this behavior is.

Exercise 5.82. Let $g: m \rightarrow n, h: \ell \rightarrow n$ be signal flow graphs. Note that $h^{\mathrm{op}}: n \rightarrow \ell$ is a signal flow graph, and we can form the composite $g \circ\left(h^{\text {op }}\right)$ :

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-188.jpg?height=160&width=631&top_left_y=381&top_left_x=736)

Show that the behavior of $g \circ\left(h^{\circ \mathrm{P}}\right) \subseteq R^{m} \times R^{\ell}$ is equal to

$$
\mathrm{B}\left(g ;\left(h^{\mathrm{op}}\right)\right)=\{(x, y) \mid S(g)(x)=S(h)(y)\}
$$

Exercise 5.83. Let $g: m \rightarrow n, h: m \rightarrow p$ be signal flow graphs. Note that ( $\left.g^{\circ}\right): n \rightarrow m$ is a signal flow graph, and we can form the composite $g^{\text {op } ;} h$

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-188.jpg?height=168&width=631&top_left_y=905&top_left_x=736)

Show that the behavior of $g^{\text {op } ;} h$ is equal to

$$
\mathrm{B}\left(\left(g^{\circ \mathrm{op}}\right) ; h\right)=\left\{(S(g)(x), S(h)(x)) \mid x \in R^{m}\right\}
$$

Linear algebra via signal flow graphs. In Eq. (5.75) we see that every matrix, or linear map, can be represented as the behavior of a signal flow graph, and in Exercise 5.82 we see that solution sets of linear equations can also be represented. This includes central concepts in linear algebra, like kernels and images.

Exercise 5.84. Here is an exercise for those that know linear algebra, in particular kernels and cokernels. Let $R$ be a field, let $g: m \rightarrow n$ be a signal flow graph, and let $S(g) \in \operatorname{Mat}(R)$ be the associated $(m \times n)$-matrix (see Theorem 5.53).

1. Show that the composite of $g$ with 0 -reverses, shown here

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-188.jpg?height=154&width=379&top_left_y=1796&top_left_x=922)

is equal to the kernel of the matrix $S(g)$.

2. Show that the composite of discard-reverses with $g$, shown here

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-188.jpg?height=157&width=344&top_left_y=2139&top_left_x=934)

is equal to the image of the matrix $S(g)$.

3. Show that for any signal flow graph $g$, the subset $\mathrm{B}(g) \subseteq R^{m} \times R^{n}$ is a linear subspace. That is, if $b_{1}, b_{2} \in \mathrm{B}(g)$ then so are $b_{1}+b_{2}$ and $r * b_{1}$, for any $r \in R$. $\diamond$

We have thus seen that signal flow graphs provide a uniform, compositional language to talk about many concepts in linear algebra. Moreover, in Exercise 5.84 we showed that the behavior of a signal flow graph is a linear relation, i.e. a relation whose elements can be added and multiplied by scalars $r \in R$. In fact the converse is true too: any linear relation $B \subseteq R^{m} \times R^{n}$ can be represented by a signal flow graph.

Exercise 5.85. One might want to show that linear relations on $R$ form a prop, $\operatorname{LinRel}_{R}$. That is, one might want to show that there is a sub-prop of the prop $\operatorname{Rel}_{R}$ from Definition 5.79, where the morphisms $m \rightarrow n$ are the subsets $B \subseteq R^{m} \times R^{n}$ such that $B$ is linear. In other words, where for any $(x, y) \in B$ and $r \in R$, the element $(r * x, r * y) \in R^{m} \times R^{n}$ is in $B$, and for any $\left(x^{\prime}, y^{\prime}\right) \in B$, the element $\left(x+x^{\prime}, y+y^{\prime}\right)$ is in $B$.

This is certainly doable, but for this exercise, we only ask that you prove that the composite of two linear relations is linear.

Just like we gave a sound and complete presentation for the prop of matrices in Theorem 5.60, it is possible to give a sound and complete presentation for linear relations on $R$. Moreover, it is possible to give such a presentation whose generating set is $G_{R} \sqcup G_{R}^{\mathrm{op}}$ as in Eq. (5.81) and whose equations include those from Theorem 5.60, plus a few more. This presentation gives a graphical method for doing linear algebra: an equation between linear subspaces is true if and only if it can be proved using the equations from the presentation.

Although not difficult, we leave the full presentation to further reading (Section 5.5). Instead, we'll conclude our exploration of the prop of linear relations by noting that some of these 'few more' equations state that relations-just like co-design problems in Chapter 4-form a compact closed category.

Compact closed structure. Using the icons available to us for signal flow graphs, we can build morphisms that look like the 'cup' and 'cap' from Definition 4.58:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-189.jpg?height=106&width=621&top_left_y=1687&top_left_x=752)

The behaviors of these graphs are respectively

$$
\{(0,(x, x)) \mid x \in R\} \subseteq R^{0} \times R^{2} \quad \text { and } \quad\{((x, x), 0) \mid x \in R\} \subseteq R^{2} \times R^{0}
$$

In fact, these show the object 1 in the prop $\operatorname{Rel}_{R}$ is dual to itself: the morphisms from Eq. (5.86) serve as the $\eta_{1}$ and $\epsilon_{1}$ from Definition 4.58. Using monoidal products of these morphisms, one can show that any object in $\operatorname{Rel}_{R}$ is dual to itself.

Graphically, this means that the three signal flow graphs
![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-189.jpg?height=170&width=780&top_left_y=2202&top_left_x=478)

all represent the same relation.

Using these relations, it is straightforward to check the following result.

Theorem 5.87. The prop $\operatorname{Rel}_{R}$ is a compact closed category in which every object $n \in \mathbb{N}$ is dual to itself, $n=n^{*}$.

To make our signal flow graphs simpler, we define new icons cup and cap by the equations

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-190.jpg?height=108&width=922&top_left_y=510&top_left_x=599)

Back to control theory. Let's close by thinking about how to represent a simple control theory problem in this setting. Suppose we want to design a system to maintain the speed of a car at a desired speed $u$. We'll work in signal flow diagrams over the rig $\mathbb{R}\left[s, s^{-1}\right]$ of polynomials in $s$ and $s^{-1}$ with coefficients in $\mathbb{R}$ and where $s s^{-1}=s^{-1} s=1$. This is standard in control theory: we think of $s$ as integration, and $s^{-1}$ as differentiation.

There are three factors that contribute to the actual speed $v$. First, there is the actual speed $v$. Second, there are external forces $F$. Third, we have our control system: this will take some linear combination $a * u+b * v$ of the desired speed and actual speed, amplify it by some factor $p$ to give a (possibly negative) acceleration. We can represent this system as follows, where $m$ is the mass of the car.

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-190.jpg?height=295&width=870&top_left_y=1224&top_left_x=622)

This can be read as the following equation, where one notes that $v$ occurs twice:

$$
v=\int \frac{1}{m} F(t) d t+u(t)+p \int a u(t)+b v(t) d t .
$$

Our control problem then asks: how do we choose $a$ and $b$ to make the behavior of this signal flow graph close to the relation $\{(F, u, v) \mid u=v\}$ ? By phrasing problems in this way, we can use extensions of the logic we have discussed above to reason about such complex, real-world problems.

\subsection*{5.5 Summary and further reading}

The goal of this chapter was to explain how props formalize signal flow graphs, and provide a new perspective on linear algebra. To do this, we examined the idea of free and presented structures in terms of universal properties. This allowed us to build props that exactly suited our needs.

Paweł Sobociński's Graphical Linear Algebra blog is an accessible and fun exploration of the key themes of this chapter, which goes on to describe how concepts such as determinants, eigenvectors, and division by zero can be expressed using signal flow
graphs [Sob]. For the technical details, one could start with Baez and Erbele [BE15], or Zanasi's thesis [Zan15] and its related series of papers [BSZ14; BSZ15; BS17]. For details about applications to control theory, see [FSR16]. From the control theoretic perspective, the ideas and philosophy of this chapter are heavily influenced by Willems' behavioral approach [Wil07].

For the reader that has not studied abstract algebra, we mention that rings, monoids, and matrices are standard fare in abstract algebra, and can be found in any standard introduction, such as [Fra67]. Rigs, also known as semirings, are a bit less well known, but no less interesting; a comprehensive survey of the literature can be found in [Gla13].

Perhaps the most significant idea in this chapter is the separation of structure into syntax and semantics, related by a functor. This is not only present in the running theme of studying signal flow graphs, but in our aside Section 5.4.2, where we talk, for example, about monoid objects in monoidal categories. The idea of functorial semantics is yet another due to Lawvere, first appearing in his thesis [Law04].

\section*{Chapter 6}

\section*{Electric circuits: Hypergraph categories and operads}

\subsection*{6.1 The ubiquity of network languages}

Electric circuits, chemical reaction networks, finite state automata, Markov processes: these are all models of physical or computational systems that are commonly described using network diagrams. Here, for example, we draw a diagram that models a flip-flop, an electric circuit-important in computer memory-that can store a bit of information:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-193.jpg?height=599&width=1005&top_left_y=1430&top_left_x=560)

Network diagrams have time-tested utility. In this chapter, we are interested in understanding the common mathematical structure that they share, for the purposes of translating between and unifying them; for example certain types of Markov processes can be simulated and hence solved using circuits of resisters. When we understand the underlying structures that are shared by network diagram languages, we can make comparisons between the corresponding mathematical models easily.

At first glance network diagrams appear quite different from the wiring diagrams we have seen so far. For example, the wires are undirected in the case above, whereas in a
category-including monoidal categories seen in resource theories or co-design-every morphism has a domain and codomain, giving it a sense of direction. Nonetheless, we shall see how to use categorical constructions such as universal properties to create categorical models that precisely capture the above type of "network" compositionality, i.e. allowing us to effectively drop directedness when convenient.

In particular we'll return to the idea of a colimit, which we sketched for you at the end of Chapter 3, and show how to use colimits in the category of sets to formalize ideas of connection. Here's the key idea.

Connections via colimits. Let's say we want to install some lights: we want to create a circuit so that when we flick a switch, a light turns on or off. To start, we have a bunch of circuit components: a power source, a switch, and a lamp connected to a resistor:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-194.jpg?height=178&width=786&top_left_y=987&top_left_x=667)

We want to connect them together, but there are many ways to do so. How should we describe the particular way that will form a light switch?

First, we claim that circuits should really be thought of as open circuits: each carries the additional structure of an 'interface' exposing it to the rest of the electrical world. Here by interface we mean a certain set of locations, or ports, at which we are able to connect them with other components. ${ }^{1}$ As is so common in category theory, we begin by making this more-or-less obvious fact explicit. Let's depict the available ports using a bold $\bullet$. If we say that in the each of the three drawings above, the ports are simply the dangling end points of the wires, they would be redrawn as follows:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-194.jpg?height=168&width=830&top_left_y=1762&top_left_x=642)

Next, we have to describe which ports should be connected. We'll do this by drawing empty circles o connected by arrows to two ports $\bullet$. Each will be a witness-toconnection, saying 'connect these two!'

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-194.jpg?height=165&width=829&top_left_y=2205&top_left_x=645)
\footnotetext{
${ }^{1}$ If your circuit has no such ports, it still falls within our purview, by taking its interface to be the empty set.
}

Looking at this picture, it is clear what we need to do: just identify-i.e. merge or make equal—the ports as indicated, to get the following circuit:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-195.jpg?height=163&width=610&top_left_y=379&top_left_x=752)

But mathematics doesn't have a visual cortex with which to generate the intuitions we can count on with a human reader such as yourself. ${ }^{2}$ Thus we need to specify formally what 'identifying ports as indicated' means mathematically. As it turns out, we can do this using finite colimits in a given category $\mathcal{C}$.

Colimits are diagrams with certain universal properties, which is kind of an epiphenomenon of the category $\mathcal{C}$. Our goal is to obtain $\mathcal{C}$ 's colimits more directly, as a kind of operation in some context, so that we can think of them as telling us how to connect circuit parts together. To that end, we produce a certain monoidal category-namely that of cospans in $\mathcal{C}$, denoted Cospan ${ }_{\mathcal{C}}$-that can conveniently package $\mathcal{C}^{\prime}$ 's colimits in terms of its own basic operations: composition and monoidal product.

In summary, the first part of this chapter is devoted to the slogan 'colimits model interconnection'. In addition to universal constructions such as colimits, however, another way to describe interconnection is to use wiring diagrams. We go full circle when we find that these wiring diagrams are strongly connected to cospans, and hence colimits.

Composition operations and wiring diagrams. In this book we have seen the utility of defining syntactic or algebraic structures that describe the sort of composition operations that make sense and can be performed in a given application area. Examples include monoidal preorders with discarding, props, and compact closed categories. Each of these has an associated sort of wiring diagram style, so that any wiring diagram of that style represents a composition operation that makes sense in the given area: the first makes sense in manufacturing, the second in signal flow, and the third in collaborative design. So our second goal is to answer the question, "how do we describe the compositional structure of network-style wiring diagrams?"

Network-type interconnection can be described using something called a hypergraph category. Roughly speaking, these are categories whose wiring diagrams are those of symmetric monoidal categories together with, for each pair of natural numbers $(m, n)$, an icon $s_{m, n}: m \rightarrow n$. These icons, known as spiders, ${ }^{3}$ are drawn as follows:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-195.jpg?height=98&width=130&top_left_y=2171&top_left_x=995)

Two spiders can share a leg, and when they do, we can fuse them into one spider. The intuition is that spiders are connection points for a number of wires, and when two
\footnotetext{
${ }^{2}$ Unless the future has arrived since the writing of this book.

${ }^{3}$ Our spiders have any number of legs.
}
connection points are connected, they fuse to form an even more 'connect-y' connection point. Here is an example:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-196.jpg?height=131&width=477&top_left_y=379&top_left_x=824)

A hypergraph category may have many species of spiders with the rule that spiders of different species cannot share a leg-and hence not fuse-but two spiders of the same species can share legs and fuse. We add spider diagrams to the iconography of hypergraph categories.

As we shall see, the ideas of describing network interconnection using colimits and hypergraph categories come together in the notion of a theory. We first introduced the idea of a theory in Section 5.4.2, but here we explore it more thoroughly, starting with the idea that, approximately speaking, cospans in the category FinSet form the theory of hypergraph categories.

We can assemble all cospans in FinSet into something called an 'operad'. Throughout this book we have talked about using free structures and presentations to create instances of algebraic structures such as preorders, categories, and props, tailored to the needs of a particular situation. Operads can be used to tailor the algebraic structures themselves to the needs of a particular situation. We will discuss how this works, in particular how operads encode various sorts of wiring diagram languages and corresponding algebraic structures, at the end of the chapter.

\subsection*{6.2 Colimits and connection}

Universal constructions are central to category theory. They allow us to define objects, at least up to isomorphism, by describing their relationship with other objects. So far we have seen this theme in a number of different forms: meets and joins (Section 1.3), Galois connections and adjunctions (Sections 1.4 and 3.4), limits (Section 3.5), and free and presented structures (Section 5.2.3-5.2.5). Here we turn our attention to colimits.

In this section, our main task is to have a concrete understanding of colimits in the category FinSet of finite sets and functions. The idea will be to take a bunch of setssay two or fifteen or zero-use functions between them to designate that elements in one set 'should be considered the same' as elements in another set, and then merge the sets together accordingly.

\subsection*{6.2.1 Initial objects}

Just as the simplest limit is a terminal object (see Section 3.5.1), the simplest colimit is an initial object. This is the case where you start with no objects and you merge them together.

Definition 6.1. Let $\mathcal{C}$ be a category. An initial object in $\mathcal{C}$ is an object $\varnothing \in \mathcal{C}$ such that for each object $T$ in $\mathcal{C}$ there exists a unique morphism $!_{T}: \varnothing \rightarrow T$.

The symbol $\varnothing$ is just a default name, a notation, intended to evoke the right idea; see Example 6.4 for the reason why we use the notation $\varnothing$, and Exercise 6.7 for a case when the default name $\varnothing$ would probably not be used.

Again, the hallmark of universality is the existence of a unique map to any other comparable object.

Example 6.2. An initial object of a preorder is a bottom element-that is, an element that is less than every other element. For example 0 is the initial object in ( $\mathbb{N}, \leq$ ), whereas $(\mathbb{R}, \leq)$ has no initial object.

Exercise 6.3. Consider the set $A=\{a, b\}$. Find a preorder relation $\leq$ on $A$ such that
1. $(A, \leq)$ has no initial object.
2. $(A, \leq)$ has exactly one initial object.
3. $(A, \leq)$ has two initial objects.

Example 6.4. The initial object in FinSet is the empty set. Given any finite set $T$, there is a unique function $\varnothing \rightarrow T$, since $\varnothing$ has no elements.

Example 6.5. As seen in Exercise 6.3, a category $\mathcal{C}$ need not have an initial object. As a different sort of example, consider the category shown here:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-197.jpg?height=145&width=321&top_left_y=1586&top_left_x=897)

If there were to be an initial object $\varnothing$, it would either be $A$ or $B$. Either way, we need to show that for each object $T \in \operatorname{Ob}(\mathcal{C})$ (i.e. for both $T=A$ and $T=B$ ) there is a unique morphism $\varnothing \rightarrow T$. Trying the case $\varnothing=$ ? $A$ this condition fails when $T=B$ : there are two morphisms $A \rightarrow B$, not one. And trying the case $\varnothing=$ ? $B$ this condition fails when $T=A$ : there are zero morphisms $B \rightarrow A$, not one.

Exercise 6.6. For each of the graphs below, consider the free category on that graph, and say whether it has an initial object.

1. $a$

$$
\text { 2. } \stackrel{a}{\bullet} \rightarrow \stackrel{b}{\bullet} \rightarrow \bullet
$$

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-197.jpg?height=95&width=195&top_left_y=2191&top_left_x=1057)
4. $\stackrel{a}{\rightharpoonup}$ $\diamond$

Exercise 6.7. Recall the notion of rig from Chapter 5. A rig homomorphism from $\left(R, 0_{R},+_{R}, 1_{R}, *_{R}\right)$ to $\left(S, 0_{S},+S, 1_{S}, *_{S}\right)$ is a function $f: R \rightarrow S$ such that $f\left(0_{R}\right)=0_{S}$, $f\left(r_{1}+r_{R} r_{2}\right)=f\left(r_{1}\right)+s f\left(r_{2}\right)$, etc.

1. We said "etc." Guess the remaining conditions for $f$ to be a rig homomorphism.

2. Let Rig denote the category whose objects are rigs and whose morphisms are rig homomorphisms. We claim Rig has an initial object. What is it?

Exercise 6.8. Explain the statement "the hallmark of universality is the existence of a unique map to any other comparable object," in the context of Definition 6.1. In particular, what is being universal in Definition 6.1, and which is the "comparable object"?

Remark 6.9. As mentioned in Remark 3.85, we often speak of 'the' object that satisfies a universal property, such as 'the initial object', even though many different objects could satisfy the initial object condition. Again, the reason is that initial objects are unique up to unique isomorphism: any two initial objects will have a canonical isomorphism between them, which one finds using various applications of the universal property.

Exercise 6.10. Let $\mathcal{C}$ be a category, and suppose that $c_{1}$ and $c_{2}$ are initial objects. Find an isomorphism between them, using the universal property from Definition 6.1. $\diamond$

\subsection*{6.2.2 Coproducts}

Coproducts generalize both joins in a preorder and disjoint unions of sets.

Definition 6.11. Let $A$ and $B$ be objects in a category $\mathcal{C}$. A coproduct of $A$ and $B$ is an object, which we denote $A+B$, together with a pair of morphisms $\left(\iota_{A}: A \rightarrow\right.$ $\left.A+B, \iota_{B}: B \rightarrow A+B\right)$ such that for all objects $T$ and pairs of morphisms $(f: A \rightarrow$ $T, g: B \rightarrow T)$, there exists a unique morphism $[f, g]: A+B \rightarrow T$ such that the following diagram commutes:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-198.jpg?height=231&width=415&top_left_y=1500&top_left_x=844)

We call $[f, g]$ the copairing of $f$ and $g$.

Exercise 6.13. Explain why, in a preorder, coproducts are the same as joins.

Example 6.14. Coproducts in the categories FinSet and Set are disjoint unions. More precisely, suppose $A$ and $B$ are sets. Then the coproduct of $A$ and $B$ is given by the disjoint union $A \sqcup B$ together with the inclusion functions $\iota_{A}: A \longrightarrow A \sqcup B$ and
$\iota_{B}: B \rightarrow A \sqcup B$.

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-199.jpg?height=521&width=979&top_left_y=325&top_left_x=562)

Suppose we have functions $f: A \rightarrow T$ and $g: B \rightarrow T$ for some other set $T$, unpictured. The universal property of coproducts says there is a unique function $[f, g]: A \sqcup B \rightarrow T$ such that $\iota_{A} ̊[f, g]=f$ and $\iota_{B} \circ[f, g]=g$. What is it? Any element $x \in A \sqcup B$ is either 'from $A$ ' or 'from $B$ ', i.e. either there is some $a \in A$ with $x=\iota_{A}(a)$ or there is some $b \in B$ with $x=\iota_{B}(b)$. By Eq. (6.12), we must have:

$$
[f, g](x)= \begin{cases}f(x) & \text { if } x=\iota_{A}(a) \text { for some } a \in A \\ g(x) & \text { if } x=\iota_{B}(b) \text { for some } b \in B\end{cases}
$$

Exercise 6.16. Suppose $T=\{a, b, c, \ldots, z\}$ is the set of letters in the alphabet, and let $A$ and $B$ be the sets from Eq. (6.15). Consider the function $f: A \rightarrow T$ sending each element of $A$ to the first letter of its label, e.g. $f$ (apple) $=a$. Let $g: B \rightarrow T$ be the function sending each element of $B$ to the last letter of its label, e.g. $g$ (apple) $=e$. Write down the function $[f, g](x)$ for all eight elements of $A \sqcup B$.

Exercise 6.17. Let $f: A \rightarrow C, g: B \rightarrow C$, and $h: C \rightarrow D$ be morphisms in a category $\mathcal{C}$ with coproducts. Show that
1. $\iota_{A} \circ[f, g]=f$.
2. $\iota_{B} \circ[f, g]=g$.
3. $[f, g] ; h=[f ; h, g ; h]$.
4. $\left[\iota_{A}, \iota_{B}\right]=\operatorname{id}_{A+B}$.

Exercise 6.18. Suppose a category $\mathcal{C}$ has coproducts, denoted + , and an initial object, denoted $\varnothing$. Then $(\mathcal{C},+, \varnothing)$ is a symmetric monoidal category (recall Definition 4.45). In this exercise we develop the data relevant to this fact:

1. Show that + extends to a functor $\mathcal{C} \times \mathcal{C} \rightarrow \mathcal{C}$. In particular, how does it act on morphisms in $\mathcal{C} \times \mathcal{C}$ ?

2. Using the universal properties of the initial object and coproduct, show that there are isomorphisms $A+\varnothing \rightarrow A$ and $\varnothing+A \rightarrow A$.

3. Using the universal property of the coproduct, write down morphisms
a) $(A+B)+C \rightarrow A+(B+C)$.
b) $A+B \rightarrow B+A$.

If you like, check that these are isomorphisms.

It can then be checked that this data obeys the axioms of a symmetric monoidal category, but we'll end the exercise here.

\subsection*{6.2.3 Pushouts}

Pushouts are a way of combining sets. Like a union of subsets, a pushout can combine two sets in a non-disjoint way: elements of one set may be identified with elements of the other. The pushout construction, however, is much more general: it allows (and requires) the user to specify exactly which elements will be identified. We'll see a demonstration of this additional generality in Example 6.29.

Definition 6.19. Let $\mathcal{C}$ be a category and let $f: A \rightarrow X$ and $g: A \rightarrow Y$ be morphisms in $\mathcal{C}$ that have a common domain. The pushout $X+{ }_{A} Y$ is the colimit of the diagram

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-200.jpg?height=201&width=198&top_left_y=1008&top_left_x=953)

In more detail, a pushout consists of (i) an object $X+{ }_{A} Y$ and (ii) morphisms $\iota_{X}: X \rightarrow$ $X+_{A} Y$ and $\iota_{Y}: Y \rightarrow X+_{A} Y$ satisfying (a) and (b) below.

(a) The diagram

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-200.jpg?height=215&width=309&top_left_y=1432&top_left_x=946)

commutes. (We will explain the ' $\Gamma$ ' symbol below.)

(b) For all objects $T$ and morphisms $x: X \rightarrow T, y: Y \rightarrow T$, if the diagram

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-200.jpg?height=214&width=225&top_left_y=1796&top_left_x=993)

commutes, then there exists a unique morphism $t: X+_{A} Y \rightarrow T$ such that

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-200.jpg?height=306&width=414&top_left_y=2129&top_left_x=888)

commutes.

If $X+{ }_{A} Y$ is a pushout, we denote that fact by drawing the commutative square Eq. (6.20), together with the $\ulcorner$ symbol as shown; we call it a pushout square.

We further call $\iota_{X}$ the pushout of $g$ along $f$, and similarly $\iota_{Y}$ the pushout of $f$ along $g$.

Example 6.22. In a preorder, pushouts and coproducts have a lot in common. The pushout of a diagram $B \leftarrow A \rightarrow C$ is equal to the coproduct $B \sqcup C$ : namely, both are equal to the join $B \vee C$.

Example 6.23. Let $f: A \rightarrow X$ be a morphism in a category $\mathcal{C}$. For any isomorphisms $i: A \rightarrow A^{\prime}$ and $j: X \rightarrow X^{\prime}$, we can take $X^{\prime}$ to be the pushout $X+{ }_{A} A^{\prime}$, i.e. the following is a pushout square:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-201.jpg?height=228&width=228&top_left_y=867&top_left_x=946)

where $f^{\prime}:=i^{-1} g f ; j$. To see this, observe that if there is any object $T$ such that the following square commutes:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-201.jpg?height=209&width=222&top_left_y=1213&top_left_x=949)

then $f ; x=i ; a$, and so we are forced to take $x^{\prime}: X \rightarrow T$ to be $x^{\prime}:=j^{-1} ; x$. This makes the following diagram commute:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-201.jpg?height=295&width=331&top_left_y=1582&top_left_x=889)

because $f^{\prime} ; x^{\prime}=i^{-1} \rightrightarrows f ; j \circ j^{-1} ; x=i^{-1} ; i \circ a=a$.

Exercise 6.24. For any set $S$, we have the discrete category Disc $S$, with $S$ as objects and only identity morphisms.

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-201.jpg?height=49&width=911&top_left_y=2141&top_left_x=385)

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-201.jpg?height=54&width=873&top_left_y=2190&top_left_x=385)

Example 6.25. In the category FinSet, pushouts always exist. The pushout of functions $f: A \rightarrow X$ and $g: A \rightarrow Y$ is the set of equivalence classes of $X \sqcup Y$ under the equivalence relation generated by-that is, the reflexive, transitive, symmetric closure of-the
relation $\{f(a) \sim g(a) \mid a \in A\}$.

We can think of this in terms of interconnection too. Each element $a \in A$ provides a connection between $f(a)$ in $X$ and $g(a)$ in $Y$. The pushout is the set of connected components of $X \sqcup Y$.

Exercise 6.26. What is the pushout of the functions $f: \underline{4} \rightarrow \underline{5}$ and $g: \underline{4} \rightarrow \underline{3}$ pictured below?
![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-202.jpg?height=260&width=982&top_left_y=583&top_left_x=562)

Check your answer using the abstract description from Example 6.25.

Example 6.27. Suppose a category $\mathcal{C}$ has an initial object $\varnothing$. For any two objects $X, Y \in$ $\mathrm{Ob} \mathcal{C}$, there is a unique morphism $f: \varnothing \rightarrow X$ and a unique morphism $g: \varnothing \rightarrow Y$; this is what it means for $\varnothing$ to be initial.

The diagram $X \stackrel{f}{\leftarrow} \varnothing \xrightarrow{g} Y$ has a pushout in $\mathcal{C}$ iff $X$ and $Y$ have a coproduct in $\mathcal{C}$, and the pushout and the coproduct will be the same. Indeed, suppose $X$ and $Y$ have a coproduct $X+Y$; then the diagram to the left
![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-202.jpg?height=214&width=768&top_left_y=1340&top_left_x=669)

commutes (why? ${ }^{1}$ ), and for any object $T$ and commutative diagram as to the right, there is a unique map $X+Y \rightarrow T$ making the diagram as in Eq. (6.21) commute (why?2). This shows that $X+Y$ is a pushout, $X+\varnothing Y \cong X+Y$.

Similarly, if a pushout $X+{ }_{\varnothing} Y$ exists, then it satisfies the universal property of the coproduct (why? ${ }^{3}$ ).

Exercise 6.28. In Example 6.27 we asked "why?" three times.

1. Give a justification for "why? ${ }^{1 \text { ". }}$.

2. Give a justification for "why?2".

3. Give a justification for "why? ${ }^{3 "}$.

Example 6.29. Let $A=X=Y=\mathbb{N}$. Consider the functions $f: A \rightarrow X$ and $g: A \rightarrow Y$
given by the 'floor' functions, $f(a):=\lfloor a / 2\rfloor$ and $g(a):=\lfloor(a+1) / 2\rfloor$.

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-203.jpg?height=312&width=1157&top_left_y=321&top_left_x=473)

What is their pushout? Let's figure it out using the definition.

If $T$ is any other set and we have maps $x: X \rightarrow T$ and $y: Y \rightarrow T$ that commute with $f$ and $g$, i.e. $f \cong x=g \cong y$, then this commutativity implies that

$$
y(0)=y(g(0))=x(f(0))=x(0)
$$

In other words, $Y$ 's 0 and X's 0 go to the same place in $T$, say $t$. But since $f(1)=0$ and $g(1)=1$, we also have that $t=x(0)=x(f(1))=y(g(1))=y(1)$. This means $Y$ 's 1 goes to $t$ also. But since $g(2)=1$ and $f(2)=1$, we also have that $t=g(1)=y(g(2))=$ $x(f(2))=x(1)$, which means that $X$ 's 1 also goes to $t$. One can keep repeating this and find that every element of $Y$ and every element of $X$ go to $t$ ! Using mathematical induction, one can prove that the pushout is in fact a 1-element set, $X \sqcup_{A} Y \cong\{1\}$.

\subsection*{6.2.4 Finite colimits}

Initial objects, coproducts, and pushouts are all types of colimits. We gave the general definition of colimit in Section 3.5.4. Just as a limit in $\mathcal{C}$ is a terminal object in a category of cones over a diagram $D: \mathcal{J} \rightarrow \mathcal{C}$, a colimit is an initial object in a category of cocones over some diagram $D: \mathcal{J} \rightarrow \mathcal{C}$. For our purposes it is enough to discuss finite colimits-i.e. when $\mathscr{J}$ is a finite category-which subsume initial objects, coproducts, and pushouts. ${ }^{4}$

In Definition 3.102, cocones in $\mathcal{C}$ are defined to be cones in $\mathcal{C}^{\text {oop. For visualization }}$ purposes, if $D: \mathcal{J} \rightarrow \mathcal{C}$ looks like the diagram to the left, then a cocone on it shown in the diagram to the right:
![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-203.jpg?height=320&width=1416&top_left_y=1992&top_left_x=346)

Here, any two parallel paths that end at $T$ are equal in $\mathcal{C}$.
\footnotetext{
${ }^{4}$ If a category $\mathcal{J}$ has finitely many morphisms, we say that $\mathcal{J}$ is a finite category. Note that in this case it must have finitely many objects too, because each object $j \in \mathrm{Ob} \mathcal{J}$ has its own identity morphism id $j$.
}

Definition 6.30. We say that a category $\mathcal{C}$ has finite colimits if a colimit, $\operatorname{colim}_{\mathcal{J}} D$, exists whenever $\mathcal{J}$ is a finite category and $D: \mathcal{J} \rightarrow \mathcal{C}$ is a diagram.

Example 6.31. The initial object in a category $\mathfrak{C}$, if it exists, is the colimit of the functor !: $\mathbf{0} \rightarrow \mathcal{C}$, where $\mathbf{0}$ is the category with no objects and no morphisms, and ! is the unique such functor. Indeed, a cocone over $!$ is just an object of $\mathcal{C}$, and so the initial cocone over ! is just the initial object of $\mathcal{C}$.

Note that $\mathbf{0}$ has finitely many objects (none); thus initial objects are finite colimits.

We often want to know that a category $\mathcal{C}$ has all finite colimits (in which case, we often drop the 'all' and just say ' $\mathcal{C}$ has finite colimits'). To check that $\mathcal{C}$ has (all) finite colimits, it's enough to check it has a few simpler forms of colimit, which generate all the rest.

Proposition 6.32. Let $\mathcal{C}$ be a category. The following are equivalent:
1. $\mathcal{e}$ has all finite colimits.
2. $\mathcal{C}$ has an initial object and all pushouts.
3. $\mathcal{C}$ has all coequalizers and all finite coproducts.

Proof. We will not give precise details here, but the key idea is an inductive one: one can build arbitrary finite diagrams using some basic building blocks. Full details can be found in [Bor94, Prop 2.8.2].

Example 6.33. Let $\mathcal{C}$ be a category with all pushouts, and suppose we want to take the colimit of the following diagram in $\mathcal{C}$ :

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-204.jpg?height=304&width=355&top_left_y=1656&top_left_x=880)

In it we see two diagrams ready to be pushed out, and we know how to take pushouts. So suppose we do that; then we see another pushout diagram so we take the pushout again:
![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-204.jpg?height=312&width=982&top_left_y=2148&top_left_x=562)
is the result—consisting of the object $S$, together with all the morphisms from the original diagram to S-the colimit of the original diagram? One can check that it indeed has the correct universal property and thus is a colimit.

Exercise 6.35. Check that the pushout of pushouts from Example 6.33 satisfies the universal property of the colimit for the original diagram, Eq. (6.34).

We have already seen that the categories FinSet and Set both have an initial object and pushouts. We thus have the following corollary.

Corollary 6.36. The categories FinSet and Set have (all) finite colimits.

In Theorem 3.95 we gave a general formula for computing finite limits in Set. It is also possible to give a formula for computing finite colimits. There is a duality between products and coproducts and between subobjects and quotient objects, so whereas a finite limit is given by a subset of a product, a finite colimit is given by a quotient of a coproduct.

Theorem 6.37. Let $\mathcal{J}$ be presented by the finite graph $(V, A, s, t)$ and some equations, and let $D: \mathcal{J} \rightarrow$ Set be a diagram. Consider the set

$$
\underset{\gamma}{\operatorname{colim}} D:=\{(v, d) \mid v \in V \text { and } d \in D(v)\} / \sim
$$

where this denotes the set of equivalence classes under the equivalence relation $\sim$ generated by putting $(v, d) \sim(w, e)$ if there is an arrow $a: v \rightarrow w$ in $J$ such that $D(a)(d)=e$. Then this set, together with the functions $\iota_{v}: D(v) \rightarrow \operatorname{colim}_{\mathcal{J}} D$ given by sending $d \in D(v)$ to its equivalence class, constitutes a colimit of $D$.

Example 6.38. Recall that an initial object is the colimit on the empty graph. The formula thus says the initial object in Set is the empty set $\varnothing$ : there are no $v \in V$.

Example 6.39. A coproduct is a colimit on the graph $\mathcal{J}=\begin{array}{ll}v_{1} & v_{2} \\ \bullet & \bullet\end{array}$. A functor $D: \mathcal{J} \rightarrow$ Set can be identified with a choice of two sets, $X:=D\left(v_{1}\right)$ and $Y:=D\left(v_{2}\right)$. Since there are no arrows in $\mathcal{g}$, the equivalence relation $\sim$ is vacuous, so the formula in Theorem 6.37 says that a coproduct is given by

$$
\left\{(v, d) \mid d \in D(v), \text { where } v=v_{1} \text { or } v=v_{2}\right\}
$$

In other words, the coproduct of sets $X$ and $Y$ is their disjoint union $X \sqcup Y$, as expected.

Example 6.40. If $\mathcal{J}$ is the category $\mathbf{1}=\stackrel{\rightharpoonup}{\bullet}$, the formula in Theorem 6.37 yields the set

$$
\{(v, d) \mid d \in D(v)\}
$$

This is isomorphic to the set $D(v)$. In other words, if $X$ is a set considered as a diagram $X: \mathbf{1} \rightarrow$ Set, then its colimit (like its limit) is just $X$ again.

Exercise 6.41. Use the formula in Theorem 6.37 to show that pushouts-colimits on a diagram $X \stackrel{f}{\leftarrow} N \xrightarrow{g} Y$ —agree with the description we gave in Example 6.25. $\diamond$

Example 6.42. Another important type of finite colimit is the coequalizer. These are colimits over the graph $\bullet \rightrightarrows \bullet$ consisting of two parallel arrows.

Consider some diagram $X \underset{g}{\stackrel{f}{\Longrightarrow}} Y$ on this graph in Set. The coequalizer of this diagram is the set of equivalence classes of $Y$ under equivalence relation generated by declaring $y \sim y^{\prime}$ whenever there exists $x$ in $X$ such that $f(x)=y$ and $g(x)=y^{\prime}$.

Let's return to the example circuit in the introduction to hint at why colimits are useful for interconnection. Consider the following picture:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-206.jpg?height=178&width=835&top_left_y=1315&top_left_x=642)

We've redrawn this picture with one change: some of the arrows are now red, and others are now blue. If we let $X$ be the set of white circles $\circ$, and $Y$ be the set of black circles $\bullet$, the blue and red arrows respectively define functions $f, g: X \rightarrow Y$. Let's leave the actual circuit components out of the picture for now; we're just interested in the dots. What is the coequalizer?

It is a three element set, consisting of one element for each newly-connected pair of $\bullet$ 's. Thus the colimit describes the set of terminals after performing the interconnection operation. In Section 6.4 we'll see how to keep track of the circuit components too.

\subsection*{6.2.5 Cospans}

When a category $\mathcal{C}$ has finite colimits, an extremely useful way to package them is by considering the category of cospans in $\mathcal{C}$.

Definition 6.43. Let $\mathcal{C}$ be a category. A cospan in $\mathcal{C}$ is just a pair of morphisms to a common object $A \rightarrow N \leftarrow B$. The common object $N$ is called the apex of the cospan and the other two objects $A$ and $B$ are called its feet.

If we want to say that cospans form a category, we should begin by saying how composition would work. So suppose we have two cospans in $\mathcal{C}$

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-207.jpg?height=146&width=938&top_left_y=385&top_left_x=583)

Since the right foot of the first is equal to the left foot of the second, we might stick them together into a diagram like this:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-207.jpg?height=149&width=656&top_left_y=690&top_left_x=729)

Then, if a pushout of $N \stackrel{g}{\leftarrow} B \xrightarrow{h} P$ exists in $\mathcal{C}$, as shown on the left, we can extract a new cospan in $\mathcal{C}$, as shown on the right:
![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-207.jpg?height=236&width=1286&top_left_y=1020&top_left_x=431)

It might look like we have achieved our goal, but we're missing some things. First, we need an identity on every object $C \in \mathrm{Ob} \mathcal{C}$; but that's not hard: use $C \rightarrow C \leftarrow C$ where both maps are identities in $\mathcal{C}$. More importantly, we don't know that $\mathcal{C}$ has all pushouts, so we don't know that every two sequential morphisms $A \rightarrow B \rightarrow C$ can be composed. And beyond that, there is a technical condition that when we form pushouts, we only get an answer 'up to isomorphism': anything isomorphic to a pushout counts as a pushout (check the definition to see why). We want all these different choices to count as the same thing, so we define two cospans to be equivalent iff there is an isomorphism between their respective apexes. That is, the cospan $A \rightarrow P \leftarrow B$ and $A \rightarrow P^{\prime} \leftarrow B$ in the diagram shown left below are equivalent iff there is an isomorphism $P \cong P^{\prime}$ making the diagram to the right commute:
![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-207.jpg?height=156&width=802&top_left_y=1992&top_left_x=651)

Now we are getting somewhere. As long as our category $\mathcal{C}$ has pushouts, we are in business: Cospan ${ }_{\mathfrak{e}}$ will form a category. But in fact, we are very close to getting more. If we also demand that $\mathcal{C}$ has an initial object $\varnothing$ as well, then we can upgrade $\operatorname{Cospan}_{\mathcal{C}}$ to a symmetric monoidal category.

Recall from Proposition 6.32 that a category $\mathfrak{C}$ has all finite colimits iff it has an initial object and all pushouts.

Definition 6.45. Let $\mathcal{C}$ be a category with finite colimits. Then there exists a category $\operatorname{Cospan}_{\mathscr{C}}$ with the same objects as $\mathcal{C}$, i.e. $\mathrm{Ob}\left(\operatorname{Cospan}_{\mathscr{C}}\right)=\mathrm{Ob}(\mathcal{C})$, where the morphisms $A \rightarrow B$ are the (equivalence classes of) cospans from $A$ to $B$, and composition is given by the above pushout construction.

There is a symmetric monoidal structure on this category, denoted $\left(\operatorname{Cospan}_{\mathcal{C}}, \varnothing,+\right)$. The monoidal unit is the initial object $\varnothing \in \mathcal{C}$ and the monoidal product is given by coproduct. The coherence isomorphisms, e.g. $A+\varnothing \cong A$, can be defined in a similar way to those in Exercise 6.18.

It is a straightforward but time-consuming exercise to verify that $\left(\operatorname{Cospan}_{e}, \varnothing,+\right)$ from Definition 6.45 really does satisfy all the axioms of a symmetric monoidal category, but it does.

Example 6.46. The category FinSet has finite colimits (see 6.36). So, we can define a symmetric monoidal category Cospan $_{\text {FinSet }}$. What does it look like? It looks a lot like wires connecting ports.

The objects of Cospan $_{\text {FinSet }}$ are finite sets; here let's draw them as collections of $\bullet$ 's. The morphisms are cospans of functions. Let $A$ and $N$ be five element sets, and $B$ be a six element set. Below are two depictions of a cospan $A \xrightarrow{f} N \stackrel{g}{\leftarrow} B$.
![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-208.jpg?height=352&width=1100&top_left_y=1312&top_left_x=511)

In the depiction on the left, we simply represent the functions $f$ and $g$ by drawing arrows from each $a \in A$ to $f(a)$ and each $b \in B$ to $g(b)$. In the depiction on the right, we make this picture resemble wires a bit more, simply drawing a wire where before we had an arrow, and removing the unnecessary center dots. We also draw a dotted line around points that are connected, to emphasize an important perspective, that cospans establish that certain ports are connected, i.e. part of the same equivalence class.

The monoidal category Cospan $_{\text {FinSet }}$ then provides two operations for combining cospans: composition and monoidal product. Composition is given by taking the pushout of the maps coming from the common foot, as described in Definition 6.45. Here is an example of cospan composition, where all the functions are depicted with
arrow notation:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-209.jpg?height=349&width=1103&top_left_y=362&top_left_x=462)

The monoidal product is given simply by the disjoint union of two cospans; in pictures it is simply combining two cospans by stacking one above another.

Exercise 6.48. In Eq. (6.47) we showed morphisms $A \rightarrow B$ and $B \rightarrow C$ in Cospan $_{\text {FinSet }}$. Draw their monoidal product as a morphism $A+B \rightarrow B+C$ in Cospan $_{\text {FinSet }}$. $\diamond$

Exercise 6.49. Depicting the composite of cospans in Eq. (6.47) with the wire notation gives

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-209.jpg?height=274&width=1081&top_left_y=1126&top_left_x=522)

Comparing Eq. (6.47) and Eq. (6.50), describe the composition rule in Cospan $_{\text {FinSet }}$ in terms of wires and connected components.

\subsection*{6.3 Hypergraph categories}

A hypergraph category is a type of symmetric monoidal category whose wiring diagrams are networks. We will soon see that electric circuits can be organized into a hypergraph category; this is what we've been building up to. But to define hypergraph categories, it is useful to first introduce Frobenius monoids.

\subsection*{6.3.1 Frobenius monoids}

The pictures of cospans we saw above, e.g. in Eq. (6.50) look something like icons in signal flow graphs (see Section 5.3.2): various wires merge and split, initialize and terminate. And these follow the same rules they did for linear relations, which we briefly discussed in Exercise 5.84. There's a lot of potential for confusion, so let's start from scratch and build back up.

In any symmetric monoidal category $(\mathcal{C}, I, \otimes)$, recall from Section 4.4.2 that objects can be drawn as wires and morphisms can be drawn as boxes. Particularly noteworthy morphisms might be iconified as dots rather than boxes, to indicate that the morphisms
there are not arbitrary but notation-worthy. One case of this is when there is an object $X$ with special "abilities",e.g. the ability to duplicate into two, or disappear into nothing.

To make this precise, recall from Definition 5.65 that a commutative monoid $(X, \mu, \eta)$ in symmetric monoidal category $(\mathcal{C}, I, \otimes)$ is an object $X$ of $\mathcal{C}$ together with (noteworthy) morphisms

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-210.jpg?height=65&width=116&top_left_y=534&top_left_x=820)

$\mu: X \otimes X \rightarrow X$

$\eta: I \rightarrow X$

obeying

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-210.jpg?height=101&width=353&top_left_y=806&top_left_x=409)

(associativity)

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-210.jpg?height=79&width=344&top_left_y=817&top_left_x=869)

(unitality)

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-210.jpg?height=84&width=306&top_left_y=820&top_left_x=1316)

where $X$ is the symmetry on $X \otimes X$. A cocommutative cocomonoid $(X, \delta, \epsilon)$ is an object $X$ with maps $\delta: X \rightarrow X \otimes X, \epsilon: X \rightarrow I$, obeying the mirror images of the laws in Eq. (6.51).

Suppose $X$ has both the structure of a commutative monoid and cocommutative comonoid, and consider a wiring diagram built only from the icons $\mu, \eta, \delta$, and $\epsilon$, where every wire is labeled $X$. These diagrams have a left and right, and are pictures of how ports on the left are connected to ports on the right. The commutative monoid and cocommutative comonoid axioms thus both express when to consider two such connection pictures should be considered the same. For example, associativity says the order of connecting ports on the left doesn't matter; coassociativity (not drawn) says the same for the right.

If you want to go all the way and say "all I care about is which port is connected to which; I don't even care about left and right", then you need a few more axioms to say how the morphisms $\mu$ and $\delta$, the merger and the splitter, interact.

Definition 6.52. Let $X$ be an object in a symmetric monoidal category $(\mathcal{C}, \otimes, I)$. A Frobenius structure on $X$ consists of a 4-tuple $(\mu, \eta, \delta, \epsilon)$ such that $(X, \mu, \eta)$ is a commutative monoid and $(X, \delta, \epsilon)$ is a cocommutative comonoid, which satisfies the six equations above ((co-)associativity, (co-)unitality, (co-)commutativity), as well as the following three equations:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-210.jpg?height=120&width=542&top_left_y=2149&top_left_x=564)

(the Frobenius law)

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-210.jpg?height=68&width=349&top_left_y=2167&top_left_x=1208)

(the special law)

We refer to an object $X$ equipped with a Frobenius structure as a special commutative Frobenius monoid, or just Frobenius monoid for short.

With these two equations, it turns out that two morphisms $X^{\otimes m} \rightarrow X^{\otimes n}$-defined by composing and tensoring identities on $X$ and the noteworthy morphisms $\mu, \delta$, etc.are equal if and only if their string diagrams connect the same ports. This link between connectivity, and Frobenius monoids can be made precise as follows.

Definition 6.54. Let $(X, \mu, \eta, \delta, \epsilon)$ be a Frobenius monoid in a monoidal category $(\mathcal{C}, I, \otimes)$. Let $m, n \in \mathbb{N}$. Define $s_{m, n}: X^{\otimes m} \rightarrow X^{\otimes n}$ to be the following morphism

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-211.jpg?height=177&width=1000&top_left_y=627&top_left_x=557)

It can be written formally as $(m-1) \mu^{\prime}$ s followed by $(n-1) \delta$ 's, with special cases when $m=0$ or $n=0$.

We call $s_{m, n}$ the spider of type $(m, n)$, and can draw it more simply as the icon

$$
m \text { legs }\{\text { ¡ミ }\} n \text { legs }
$$

So a special commutative Frobenius monoid, aside from being a mouthful, is a 'spiderable' wire. You agree that in any monoidal category wiring diagram language, wires represent objects and boxes represent morphisms? Well in our weird way of talking, if a wire is spiderable, it means that we have a bunch of morphisms $\mu, \eta, \delta, \epsilon, \sigma$ that we can combine without worrying about the order of doing so: the result is just "how many in's, and how many out's": a spider. Here's a formal statement.

Theorem 6.55. Let $(X, \mu, \eta, \delta, \epsilon)$ be a Frobenius monoid in a monoidal category ( $\mathcal{C}, I, \otimes)$. Suppose that we have a map $f: X^{\otimes m} \rightarrow X^{\otimes n}$ each constructed from spiders and the symmetry map $\sigma: X^{\otimes 2} \rightarrow X^{\otimes 2}$ using composition and the monoidal product, and such that the string diagram of $f$ has only one connected component. Then it is a spider: $f=s_{m, n}$.

Example 6.56. As the following two morphisms both (i) have the same number of inputs and outputs, (ii) are constructed only from spiders, and (iii) are connected, Theorem 6.55 immediately implies they are equal:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-211.jpg?height=136&width=445&top_left_y=2087&top_left_x=840)

Exercise 6.57. Let $X$ be an object equipped with a Frobenius structure. Which of the morphisms $X \otimes X \rightarrow X \otimes X \otimes X$ in the following list are necessarily equal?

1 .

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-211.jpg?height=87&width=130&top_left_y=2423&top_left_x=1038)

2.

3.

4.

5.

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-212.jpg?height=143&width=247&top_left_y=747&top_left_x=988)

6.

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-212.jpg?height=171&width=344&top_left_y=955&top_left_x=934)

Back to cospans. Another way of understanding Frobenius monoids is to relate them to cospans. Recall the notion of prop presentation from Definition 5.33.

Theorem 6.58. Consider the four-element set $G:=\{\mu, \eta, \delta, \epsilon\}$ and define in, out: $G \rightarrow$ $\mathbb{N}$ as follows:

$$
\begin{array}{rrrr}
\operatorname{in}(\mu):=2, & \operatorname{in}(\eta):=0, & \operatorname{in}(\delta):=1, & \operatorname{in}(\epsilon):=1 \\
\operatorname{out}(\mu):=1, & \operatorname{out}(\eta):=1, & \operatorname{out}(\delta):=2, & \operatorname{out}(\epsilon):=0
\end{array}
$$

Let $E$ be the set of Frobenius axioms, i.e. the nine equations from Definition 6.52. Then the free prop on $(G, E)$ is equivalent, as a symmetric monoidal category, ${ }^{a}$ to

\section*{Cospan $_{\text {FinSet }}$.}
\footnotetext{
${ }^{a}$ We will not explain precisely what it means to be equivalent as a symmetric monoidal category, but you probably have some idea: "they are the same for all category-theoretic intents and purposes." The idea is similar to that of equivalence of categories, as explained in Remark 3.59.
}

Thus we see that ideal wires, connectivity, cospans, and objects with Frobenius structures are all intimately related. We use Frobenius structures (all that splitting, merging, initializing, and terminating stuff) as a way to capture the grammar of circuit diagrams.

\subsection*{6.3.2 Wiring diagrams for hypergraph categories}

We introduce hypergraph categories through their wiring diagrams. Just like for monoidal categories, the formal definition is just the structure required to unambiguously interpret these diagrams.

Indeed, our interest in hypergraph categories is best seen in their wiring diagrams. The key idea is that wiring diagrams for hypergraph categories are network diagrams. This means, in addition to drawing labeled boxes with inputs and outputs, as we can for monoidal categories, and in addition to bending these wires around as we can for compact closed categories, we are allowed to split, join, terminate, and initialize wires.

Here is an example of a wiring diagram that represents a composite of morphisms in a hypergraph category

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-213.jpg?height=458&width=1305&top_left_y=649&top_left_x=407)

We have suppressed some of the object/wire labels for readability, since all types can be inferred from the labeled ones.

Exercise 6.59.

1. What label should be on the input to $h$ ?

2. What label should be on the output of $g$ ?

3. What label should be on the fourth output wire of the composite?

Thus hypergraph categories are general enough to talk about all network-style diagrammatic languages, like circuit diagrams.

\subsection*{6.3.3 Definition of hypergraph category}

We are now ready to define hypergraph categories formally. Since the wiring diagrams for hypergraph categories are just those for symmetric monoidal categories with a few additional icons, the definition is relatively straightforward: we just want a Frobenius structure on every object. The only coherence condition is that these interact nicely with the monoidal product.

Definition 6.60. A hypergraph category is a symmetric monoidal category $(\mathcal{C}, I, \otimes)$ in which each object $X$ is equipped with a Frobenius structure $\left(X, \mu_{X}, \delta_{X}, \eta_{X}, \epsilon_{X}\right)$ such
that

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-214.jpg?height=504&width=1420&top_left_y=309&top_left_x=339)

for all objects $X, Y$, and such that $\eta_{I}=\operatorname{id}_{I}=\epsilon_{I}$.

A hypergraph prop is a hypergraph category that is also a prop, e.g. $\mathrm{Ob}(\mathcal{C})=\mathbb{N}$, etc.

Example 6.61. For any $\mathcal{C}$ with finite colimits, $\operatorname{Cospan}_{\mathcal{C}}$ is a hypergraph category. The Frobenius morphisms $\mu_{X}, \delta_{X}, \eta_{X}, \epsilon_{X}$ for each object $X$ are constructed using the universal properties of colimits:

$$
\begin{aligned}
& \mu_{X}:=\left(X+X \xrightarrow{\left[\mathrm{id}_{X}, \mathrm{id} X\right]} X \stackrel{\text { id } X}{\longleftarrow} X\right) \\
& \eta_{X}:=\left(\varnothing \xrightarrow{!_{X}} X \stackrel{\mathrm{id}_{X}}{\longleftrightarrow} X\right) \\
& \delta_{X}:=\left(X \xrightarrow{\mathrm{id}_{X}} X \stackrel{\left[\mathrm{id}_{X}, \mathrm{id}_{X}\right]}{\longleftrightarrow} X+X\right) \\
& \epsilon_{X}:=\left(X \xrightarrow{\operatorname{id}_{X}} X \stackrel{!_{X}}{\longleftrightarrow}\right)
\end{aligned}
$$

Exercise 6.62. By Example 6.61, the category Cospan $_{\text {FinSet }}$ is a hypergraph category. (In fact, it is equivalent to a hypergraph prop.) Draw the Frobenius morphisms for the object 1 in Cospan ${ }_{\text {FinSet }}$ using both the function and wiring depictions as in Example 6.46.

Exercise 6.63. Using your knowledge of colimits, show that the maps defined in Example 6.61 do indeed obey the special law (see Definition 6.52).

Example 6.64. Recall the monoidal category (Corel, $\varnothing, \sqcup$ ) from Example 4.61; its objects are finite sets and its morphisms are corelations. Given a finite set $X$, define the corelation $\mu_{X}: X \sqcup X \rightarrow X$ such that two elements of $X \sqcup X \sqcup X$ are equivalent if and only if they come from the same underlying element of $X$. Define $\delta_{X}: X \rightarrow X \sqcup X$ in the same way, and define $\eta_{X}: \varnothing \rightarrow X$ and $\epsilon_{X}: X \rightarrow \varnothing$ such that no two elements of $X=\varnothing \sqcup X=X \sqcup \varnothing$ are equivalent.

These maps define a special commutative Frobenius monoid $\left(X, \mu_{X}, \eta_{X}, \delta_{X}, \epsilon_{X}\right)$,
and in fact give Corel the structure of a hypergraph category.

Example 6.65. The prop of linear relations, which we briefly mentioned in Exercise 5.84, is a hypergraph category. In fact, it is a hypergraph category in two ways, by choosing either the black 'copy' and 'discard' generators or the white 'add' and 'zero' generators as the Frobenius maps.

We can generalize the construction we gave in Theorem 5.87.

Proposition 6.66. Hypergraph categories are self-dual compact closed categories, if we define the cup and cap to be

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-215.jpg?height=136&width=938&top_left_y=905&top_left_x=585)

Proof. The proof is a straightforward application of the Frobenius and unitality axioms:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-215.jpg?height=540&width=1303&top_left_y=1172&top_left_x=476)

Exercise 6.67. Fill in the missing diagram in the proof of Proposition 6.66 using the equations from Eq. (6.51), their opposites, and Eq. (6.53).

\subsection*{6.4 Decorated cospans}

The goal of this section is to show how we can construct a hypergraph category whose morphisms are electric circuits. To do this, we first must introduce the notion of structure-preserving map for symmetric monoidal categories, a generalization of monoidal monotones known as symmetric monoidal functors. Then we introduce a general method-that of decorated cospans-for producing hypergraph categories. Doing all this will tie up lots of loose ends: colimits, cospans, circuits, and hypergraph categories.

\subsection*{6.4.1 Symmetric monoidal functors}

Rough Definition 6.68. Let $\left(\mathcal{C}, I_{\mathcal{C}}, \otimes_{\mathcal{C}}\right)$ and $\left(\mathcal{D}, I_{\mathcal{D}}, \otimes_{\mathcal{D}}\right)$ be symmetric monoidal categories. To specify a symmetric monoidal functor $(F, \varphi)$ between them,

(i) one specifies a functor $F: \mathcal{C} \rightarrow \mathcal{D}$;

(ii) one specifies a morphism $\varphi_{I}: I_{\mathcal{D}} \rightarrow F\left(I_{\mathcal{e}}\right)$.

(iii) for each $c_{1}, c_{2} \in \mathrm{Ob}(\mathcal{C})$, one specifies a morphism

$$
\varphi_{c_{1}, c_{2}}: F\left(c_{1}\right) \otimes_{\mathcal{D}} F\left(c_{2}\right) \rightarrow F\left(c_{1} \otimes_{\mathcal{e}} c_{2}\right)
$$

natural in $c_{1}$ and $c_{2}$.

We call the various maps $\varphi$ coherence maps. We require the coherence maps to obey bookkeeping axioms that ensure they are well behaved with respect to the symmetric monoidal structures on $\mathcal{C}$ and $\mathcal{D}$. If $\varphi_{I}$ and $\varphi_{c_{1}, c_{2}}$ are isomorphisms for all $c_{1}, c_{2}$, we say that $(F, \varphi)$ is strong.

Example 6.69. Consider the power set functor $\mathrm{P}:$ Set $\rightarrow$ Set. It acts on objects by sending a set $S \in$ Set to its set of subsets $\mathrm{P}(S):=\{R \subseteq S\}$. It acts on morphisms by sending a function $f: S \rightarrow T$ to the image map $\operatorname{im}_{f}: \mathrm{P}(S) \rightarrow \mathrm{P}(T)$, which maps $R \subseteq S$ to $\{f(r) \mid r \in R\} \subseteq T$.

Now consider the symmetric monoidal structure $(\{1\}, \times)$ on Set from Example 4.49. To make $\mathrm{P}$ a symmetric monoidal functor, we need to specify a function $\varphi_{I}:\{1\} \rightarrow$ $\mathrm{P}(\{1\})$ and for all sets $S$ and $T$, a functor $\varphi_{S, T}: \mathrm{P}(S) \times \mathrm{P}(T) \rightarrow \mathrm{P}(S \times T)$. One possibility is to define $\varphi_{I}(1)$ to be the maximal subset $\{1\} \subseteq\{1\}$, and given subsets $A \subseteq S$ and $B \subseteq T$, to define $\varphi_{S, T}(A, B)$ to be the product subset $A \times B \subseteq S \times T$. With these definitions, $(\mathrm{P}, \varphi$ ) is a symmetric monoidal functor.

Exercise 6.70. Check that the maps $\varphi_{S, T}$ defined in Example 6.69 are natural in $S$ and $T$. In other words, given $f: S \rightarrow S^{\prime}$ and $g: T \rightarrow T^{\prime}$, show that the diagram below commutes:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-216.jpg?height=219&width=593&top_left_y=1842&top_left_x=755)

\subsection*{6.4.2 Decorated cospans}

Now that we have briefly introduced symmetric monoidal functors, we return to the task at hand: constructing a hypergraph category of circuits. To do so, we introduce the method of decorated cospans.

Circuits have lots of internal structure, but they also have some external ports-also called 'terminals'-by which to interconnect them with others. Decorated cospans are ways of discussing exactly that: things with external ports and internal structure.

To see how this works, let us start with the following example circuit:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-217.jpg?height=360&width=479&top_left_y=373&top_left_x=823)

We might formally consider this as a graph on the set of four ports, where each edge is labeled by a type of circuit component (for example, the top edge would be labeled as a resistor of resistance $2 \Omega$ ). For this circuit to be a morphism in some category, i.e. in order to allow for interconnection, we must equip the circuit with some notion of interface. We do this by marking the ports in the interface using functions from finite sets:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-217.jpg?height=371&width=924&top_left_y=1148&top_left_x=598)

Let $N$ be the set of nodes of the circuit. Here the finite sets $A, B$, and $N$ are sets consisting of one, two, and four elements respectively, drawn as points, and the values of the functions $A \rightarrow N$ and $B \rightarrow N$ are indicated by the grey arrows. This forms a cospan in the category of finite sets, for which the apex set $N$ has been decorated by our given circuit.

Suppose given another such decorated cospan with input $B$

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-217.jpg?height=249&width=788&top_left_y=2098&top_left_x=663)

Since the output of the first equals the input of the second (both are $B$ ), we can stick
them together into a single diagram:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-218.jpg?height=293&width=1112&top_left_y=333&top_left_x=455)

The composition is given by gluing the circuits along the identifications specified by $B$, resulting in the decorated cospan

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-218.jpg?height=358&width=1169&top_left_y=805&top_left_x=432)

We've seen this sort of gluing before when we defined composition of cospans in Definition 6.45. But now there's this whole 'decoration' thing; our goal is to formalize it.

Definition 6.75. Let $\mathcal{C}$ be a category with finite colimits, and $(F, \varphi):(\mathcal{C},+) \longrightarrow($ Set,$\times)$ be a symmetric monoidal functor. An $F$-decorated cospan is a pair consisting of a cospan $A \xrightarrow{i} N \stackrel{0}{\leftarrow} B$ in $\mathcal{C}$ together with an element $s \in F(N) .{ }^{5}$ We call $(F, \varphi)$ the decoration functor and $s$ the decoration.

The intuition here is to use $\mathcal{C}=$ FinSet, and, for each object $N \in$ FinSet, the functor $F$ assigns the set of all legal decorations on a set $N$ of nodes. When you choose an $F$ decorated cospan, you choose a set $A$ of left-hand external ports, a set $B$ of right-hand external ports, each of which maps to a set $N$ of nodes, and you choose one of the available decorations on $N$ nodes, taken from the set $F(N)$.

So, in our electrical circuit case, the decoration functor $F$ sends a finite set $N$ to the set of circuit diagrams-graphs whose edges are labeled by resistors, capacitors, etc.-that have $N$ vertices.

Our goal is still to be able to compose such diagrams; so how does that work exactly? Basically one combines the way cospans are composed with the structures defining our decoration functor: namely $F$ and $\varphi$.

Let $(A \xrightarrow{f} N \stackrel{g}{\leftarrow} B, s)$ and $(B \xrightarrow{h} P \stackrel{k}{\leftarrow} C, t)$ represent decorated cospans. Their composite is represented by the composite of the cospans $A \xrightarrow{f} N \stackrel{g}{\leftarrow} B$ and $B \xrightarrow{h} P \stackrel{k}{\leftarrow} C$,
\footnotetext{
${ }^{5}$ Just like in Definition 6.45, we should technically use equivalence classes of cospans. We will elide this point to get the bigger idea across. The interested reader should consult Section 6.6.
}
paired with the following element of $F\left(N+{ }_{B} P\right)$ :

$$
\begin{equation*}
F\left(\left[\iota_{N}, \iota_{P}\right]\right)\left(\varphi_{N, P}(s, t)\right) \tag{6.76}
\end{equation*}
$$

That's rather compact! We'll unpack it, in a concrete case, in just a second. But let's record a theorem first.

Theorem 6.77. Given a category $\mathcal{C}$ with finite colimits and a symmetric monoidal functor $(F, \varphi):(\mathcal{C},+) \longrightarrow($ Set,$\times)$, there is a hypergraph category Cospan $_{F}$ whose objects are the objects of $\mathcal{C}$, and whose morphisms are equivalence classes of $F$-decorated cospans.

The symmetric monoidal and hypergraph structures are derived from those on Cospan $_{\mathfrak{e}}$.

Exercise 6.78. Suppose you're worried that the notation Cospan ${ }_{\mathfrak{e}}$ looks like the notation Cospan $_{F}$, even though they're very different. An expert tells you "they're not so different; one is a special case of the other. Just use the constant functor $F(c):=\{*\}$." What does the expert mean?

\subsection*{6.4.3 Electric circuits}

In order to work with the above abstractions, we will get a bit more precise about the circuits example and then have a detailed look at how composition works in decorated cospan categories.

Let's build some circuits. To begin, we'll need to choose which components we want in our circuit. This is simply a matter of what's in our electrical toolbox. Let's say we're carrying some lightbulbs, switches, batteries, and resistors of every possible resistance. That is, define a set

$$
C:=\{\text { light, switch, battery }\} \sqcup\left\{x \Omega \mid x \in \mathbb{R}^{+}\right\}
$$

To be clear, the $\Omega$ are just labels; the above set is isomorphic to $\{$ light, switch, battery $\} \sqcup$ $\mathbb{R}^{+}$. But we write $C$ this way to remind us that it consists of circuit components. If we wanted, we could also add inductors, capacitors, and even elements connecting more than two ports, like transistors, but let's keep things simple for now.

Given our set $C$, a $C$-circuit is just a graph $(V, A, s, t)$, where $s, t: A \rightarrow V$ are the source and target functions, together with a function $\ell: A \rightarrow C$ labeling each edge with a certain circuit component from $C$.

For example, we might have the simple case of $V=\{1,2\}, A=\{e\}, s(e)=1$, $t(e)=2-$ so $e$ is an edge from 1 to 2 -and $\ell(e)=3 \Omega$. This represents a resistor with resistance $3 \Omega$ :

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-219.jpg?height=123&width=229&top_left_y=2375&top_left_x=945)

Note that in the formalism we have chosen, we have multiple ways to represent any circuit, as our representations explicitly choose directions for the edges. The above resistor could also be represented by the 'reversed graph', with data $V=\{1,2\}, A=\{e\}$, $s(e)=2, t(e)=1$, and $\ell(e)=3 F$.

Exercise 6.79. Write a tuple ( $V, A, s, t, \ell)$ that represents the circuit in Eq. (6.71). $\diamond$

A decoration functor for circuits. We want $C$-circuits to be our decorations, so let's use them to define a decoration functor as in Definition 6.75. We'll call the functor $($ Circ, $\psi$ ). We start by defining the functor part

$$
\text { Circ }:(\text { FinSet },+) \longrightarrow(\text { Set }, \times)
$$

as follows. On objects, simply send a finite set $V$ to the set of $C$-circuits:

$$
\operatorname{Circ}(V):=\{(V, A, s, t, \ell) \mid \text { where } s, t: A \rightarrow V, \ell: E \rightarrow C\}
$$

On morphisms, Circ sends a function $f: V \rightarrow V^{\prime}$ to the function

$$
\begin{aligned}
\operatorname{Circ}(f): \operatorname{Circ}(V) & \longrightarrow \operatorname{Circ}\left(V^{\prime}\right) \\
\quad(V, A, s, t, \ell) & \longmapsto\left(V^{\prime}, A,(s ; f),(t ; f), \ell\right) .
\end{aligned}
$$

This defines a functor; let's explore it a bit in an exercise.

Exercise 6.80. To understand this functor better, let $c \in \operatorname{Circ}(4)$ be the circuit

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-220.jpg?height=125&width=531&top_left_y=1325&top_left_x=794)

and let $f: \underline{4} \rightarrow \underline{3}$ be the function

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-220.jpg?height=203&width=547&top_left_y=1557&top_left_x=778)

Draw a picture of the circuit $\operatorname{Circ}(f)(c)$.

We're trying to get a decoration functor (Circ,$\psi$ ) and so far we have Circ. For the coherence maps $\psi_{V, V^{\prime}}$ for finite sets $V, V^{\prime}$, we define

$$
\begin{align*}
\psi_{V, V^{\prime}}: \operatorname{Circ}(V) \times \operatorname{Circ}\left(V^{\prime}\right) & \longrightarrow \operatorname{Circ}\left(V+V^{\prime}\right) ; \\
\left((V, A, s, t, \ell),\left(V^{\prime}, A^{\prime}, s^{\prime}, t^{\prime}, \ell^{\prime}\right)\right) & \longmapsto\left(V+V^{\prime}, A+A^{\prime}, s+s^{\prime}, t+t^{\prime},\left[\ell, \ell^{\prime}\right]\right) \tag{6.81}
\end{align*}
$$

This is simpler than it may look: it takes a circuit on $V$ and a circuit on $V^{\prime}$, and just considers them together as a circuit on the disjoint union of vertices $V+V^{\prime}$.

Exercise 6.82. Suppose we have circuits

$$
b:=\bullet \quad \mid \longmapsto \quad \text { and } s:=\bullet \quad \longrightarrow
$$

in $\operatorname{Circ}(2)$. Use the definition of $\psi_{V, V}$, from (6.81) to figure out what 4 -vertex circuit $\psi_{2,2}(b, s) \in \operatorname{Circ}(\underline{2}+\underline{2})=\operatorname{Circ}(\underline{4})$ should be, and draw a picture.

Open circuits using decorated cospans. From the above data, just a monoidal functor $($ Circ,$\psi)$ : (FinSet, + ) $\rightarrow($ Set, $\times$ ), we can construct our promised hypergraph category of circuits!

Our notation for this category is Cospan $_{\text {Circ }}$. Following Theorem 6.77, the objects of this category are the same as the objects of FinSet, just finite sets. We'll reprise our notation from the introduction and Example 6.42, and draw these finite sets as collections of white circles $\circ$. For example, we'll represent the object $\underline{2}$ of $\operatorname{Cospan}_{\text {Circ }}$ as two white circles:

$$
0 \quad 0
$$

These white circles mark interface points of an open circuit.

More interesting than the objects, however, are the morphisms in Cospan $_{\text {Circ }}$. These are open circuits. By Theorem 6.77, a morphism $\underline{m} \rightarrow \underline{n}$ is a Circ-decorated cospan: that is, cospan $\underline{m} \rightarrow \underline{p} \leftarrow \underline{n}$ together with an element $c$ of $\operatorname{Circ}(\underline{p})$. As an example, consider the cospan $\underline{1} \xrightarrow{i_{1}} \underline{2} \stackrel{i_{2}}{\leftarrow} \underline{1}$ where $i_{1}(1)=1$ and $i_{2}(1)=2$, equipped with the battery element of Circ(2) connecting node 1 and node 2. We'll depict this as follows:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-221.jpg?height=120&width=437&top_left_y=1179&top_left_x=841)

Exercise 6.84. Morphisms of Cospan $_{\text {Circ }}$ are Circ-decorated cospans, as defined in Definition 6.75. This means (6.83) depicts a cospan together with a decoration, which is some $C$-circuit $(V, A, s, t, \ell) \in \operatorname{Circ}(\underline{2})$. What is it?

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-221.jpg?height=59&width=1391&top_left_y=1540&top_left_x=386)
electric circuits.

Composition in Cospan Circ . First we'll consider composition. Consider the following decorated cospan from $\underline{1}$ to $\underline{1}$ :

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-221.jpg?height=124&width=441&top_left_y=1868&top_left_x=842)

Since this and the circuit in (6.83) are both morphisms $\underline{1} \rightarrow \underline{1}$, we may compose them to get another morphism $\underline{1} \rightarrow \underline{1}$. How do we do this? There are two parts: to get the new cospan, we simply compose the cospans of our two circuits, and to get the new decoration, we use the formula $\operatorname{Circ}\left(\left[\iota_{N}, \iota_{P}\right]\right)\left(\psi_{N, P}(s, t)\right)$ from (6.76). Again, this is rather compact! Let's unpack it together.

We'll start with the cospans. The cospans we wish to compose are

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-221.jpg?height=106&width=439&top_left_y=2408&top_left_x=409)

and

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-221.jpg?height=109&width=436&top_left_y=2404&top_left_x=1278)

(We simply ignore the decorations for now.) If we pushout over the common set $\underline{1}=\{\circ\}$, we obtain the pushout square

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-222.jpg?height=282&width=830&top_left_y=366&top_left_x=642)

This means the composite cospan is

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-222.jpg?height=117&width=436&top_left_y=744&top_left_x=839)

In the meantime, we already had you start us off unpacking the formula for the new decoration. You told us what the map $\psi_{2,2}$ does in Exercise 6.82. It takes the two decorations, both circuits in $\operatorname{Circ(2)}$, and turns them into the single, disjoint circuit
![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-222.jpg?height=82&width=630&top_left_y=1065&top_left_x=736)

in Circ(4). So this is what the $\psi_{N, P}(s, t)$ part means. What does the $\left[\iota_{N}, \iota_{P}\right]$ mean? Recall this is the copairing of the pushout maps, as described in Examples 6.14 and 6.25. In our case, the relevant pushout square is given by (6.85), and $\left[\iota_{N}, \iota_{P}\right]$ is in fact the function $f$ from Exercise 6.80! This means the decoration on the composite cospan is

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-222.jpg?height=84&width=439&top_left_y=1411&top_left_x=838)

Putting this all together, the composite circuit is

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-222.jpg?height=108&width=634&top_left_y=1602&top_left_x=737)

Exercise 6.86. Refer back to the example at the beginning of Section 6.4.2. In particular, consider the composition of circuits in Eq. (6.73). Express the two circuits in this diagram as morphisms in Cospan $_{\text {Circ, }}$ and compute their composite. Does it match the picture in Eq. (6.74)?

Monoidal products in Cospan $_{\text {Circ }}$. Monoidal products in Cospan Circ are much simpler than composition. On objects, we again just work as in FinSet: we take the disjoint union of finite sets. Morphisms again have a cospan, and a decoration. For cospans,

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-222.jpg?height=60&width=1440&top_left_y=2168&top_left_x=337)
we take their coproduct cospan $A+C \rightarrow M+N \leftarrow B+D$. And for decorations, we use the map $\psi_{M, N}: \operatorname{Circ}(M) \times \operatorname{Circ}(N) \rightarrow \operatorname{Circ}(M+N)$. So, for example, suppose we want to take the monoidal product of the open circuits

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-222.jpg?height=103&width=631&top_left_y=2407&top_left_x=736)
and

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-223.jpg?height=111&width=631&top_left_y=281&top_left_x=736)

The result is given by stacking them. In other words, their monoidal product is:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-223.jpg?height=217&width=621&top_left_y=480&top_left_x=752)

Easy, right?

We leave you to do two compositions of your own.

Exercise 6.88. Write $x$ for the open circuit in (6.87). Also define cospans $\eta: 0 \rightarrow 2$ and $\eta: 2 \rightarrow 0$ as follows:
![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-223.jpg?height=140&width=980&top_left_y=954&top_left_x=564)

where each of these are decorated by the empty circuit $(\underline{1}, \varnothing,!,!,!) \in \operatorname{Circ}(1) .{ }^{6}$

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-223.jpg?height=63&width=1390&top_left_y=1172&top_left_x=389)
such things closed circuits.

\subsection*{6.5 Operads and their algebras}

In Theorem 6.77 we described how decorating cospans builds a hypergraph category from a symmetric monoidal functor. We then explored how that works in the case that the decoration functor is somehow "all circuit graphs on a set of nodes".

In this book, we have devoted a great deal of attention to different sorts of compositional theories, from monoidal preorders to compact closed categories to hypergraph categories. Yet for an application you someday have in mind, it may be the case that none of these theories suffice. You need a different structure, customized to a particular situation. For example in [VSL15] the authors wanted to compose continuous dynamical systems with control-theoretic properties and realized that in order for feedback to make sense, the wiring diagrams could not involve what they called 'passing wires'.

So to close our discussion of compositional structures, we want to quickly sketch something we can use as a sort of meta-compositional structure, known as an operad. We saw in Section 6.4.3 that we can build electric circuits from a symmetric monoidal functor FinSet $\rightarrow$ Set. Similarly we'll see that we can build examples of new algebraic structures from operad functors $\mathcal{O} \rightarrow$ Set.

\subsection*{6.5.1 Operads design wiring diagrams}

Understanding that circuits are morphisms in a hypergraph category is useful: it means we can bring the machinery of category theory to bear on understanding electrical
circuits. For example, we can build functors that express the compositionality of circuit semantics, i.e. how to derive the functionality of the whole from the functionality and interaction pattern of the parts. Or we can use the category-theoretic foundation to relate circuits to other sorts of network systems, such as signal flow graphs. Finally, the basic coherence theorems for monoidal categories and compact closed categories tell us that wiring diagrams give sound and complete reasoning in these settings.

However, one perhaps unsatisfying result is that the hypergraph category introduces artifacts like the domain and codomain of a circuit, which are not inherent to the structure of circuits or their composition. Circuits just have a single boundary interface, not 'domains' and 'codomains'. This is not to say the above model is not useful: in many applications, a vector space does not have a preferred basis, but it is often useful to pick one so that we may use matrices (or signal flow graphs!). But it would be worthwhile to have a category-theoretic model that more directly represents the compositional structure of circuits. In general, we want the category-theoretic model to fit our desired application like a glove. Let us quickly sketch how this can be done.

Let's return to wiring diagrams for a second. We saw that wiring diagrams for hypergraph categories basically look like this:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-224.jpg?height=464&width=1328&top_left_y=1221&top_left_x=390)

Note that if you had a box with $A$ and $B$ on the left and $D$ on the right, you could plug the above diagram right inside it, and get a new open circuit. This is the basic move of operads.

But before we explain this, let's get where we said we wanted to go: to a model where there aren't ports on the left and ports on the right, there are just ports. We want a more succinct model of composition for circuit diagrams; something that looks more like this:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-224.jpg?height=393&width=605&top_left_y=2118&top_left_x=760)

Do you see how diagrams Eq. (6.89) and Eq. (6.90) are actually exactly the same in terms of interconnection pattern? The only difference is that the latter does not have left/right distinction: we have lost exactly what we wanted to lose.

The cost is that the 'boxes' $f, g, h, k$ in Eq. (6.90) no longer have a left/right distinction; they're just circles now. That wouldn't be bad except that it means they can no longer represent morphisms in a category-like they used to above, in Eq. (6.89)because morphisms in a category by definition have a domain and codomain. Our new circles have no such distinction. So now we need a whole new way to think about 'boxes' categorically: if they're no longer morphisms in a category, what are they? The answer is found in the theory of operads.

In understanding operads, we will find we need to navigate one of the level shifts that we first discussed in Section 1.4.5. Notice that for decorated cospans, we define a hypergraph category using a symmetric monoidal functor. This is reminiscent of our brief discussion of algebraic theories in Section 5.4.2, where we defined something called the theory of monoids as a prop $\mathcal{M}$, and define monoids using functors $\mathcal{M} \rightarrow$ Set; see Remark 5.74. In the same way, we can view the category Cospan $_{\text {FinSet }}$ as some sort of 'theory of hypergraph categories', and so define hypergraph categories as functors Cospan $_{\text {FinSet }} \rightarrow$ Set.

So that's the idea. An operad $\mathcal{O}$ will define a theory or grammar of composition, and operad functors $\mathcal{O} \rightarrow$ Set, known as $\mathcal{O}$-algebras, will describe particular applications that obey that grammar.

\section*{Rough Definition 6.91. To specify an operad $\mathcal{O}$,}

(i) one specifies a collection $T$, whose elements are called types;

(ii) for each tuple ( $\left.t_{1}, \ldots, t_{n}, t\right)$ of types, one specifies a set $\mathcal{O}\left(t_{1}, \ldots, t_{n} ; t\right)$, whose elements are called operations of arity $\left(t_{1}, \ldots, t_{n} ; t\right)$;

(iii) for each pair of tuples $\left(s_{1}, \ldots, s_{m}, t_{i}\right)$ and $\left(t_{1}, \ldots, t_{n}, t\right)$, one specifies a function

$$
\circ_{i}: \mathcal{O}\left(s_{1}, \ldots, s_{m} ; t_{i}\right) \times \mathcal{O}\left(t_{1}, \ldots, t_{n} ; t\right) \rightarrow \mathcal{O}\left(t_{1}, \ldots, t_{i-1}, s_{1}, \ldots, s_{m}, t_{i+1}, \ldots, t_{n} ; t\right)
$$

called substitution; and

(iv) for each type $t$, one specifies an operation $\mathrm{id}_{t} \in O(t ; t)$ called the identity operation. These must obey generalized identity and associativity laws. ${ }^{7}$

Let's ignore types for a moment and think about what this structure models. The intuition is that an operad consists of, for each $n$, a set of operations of arity $n$-that is, all the operations that accept $n$ arguments. If we take an operation $f$ of arity $m$, and plug the output into the $i$ th argument of an operation $g$ of arity $n$, we should get an operation of arity $m+n-1$ : we have $m$ arguments to fill in $m$, and the remaining $n-1$
\footnotetext{
${ }^{6}$ As usual ! denotes the unique function, in this case from the empty set to the relevant codomain.

${ }^{7}$ Often what we call types are called objects or colors, what we call operations are called morphisms, what we call substitution is called composition, and what we call operads are called multicategories. A formal definition can be found in [Lei04].
}
arguments to fill in $g$. Which operation of arity $m+n-1$ do we get? This is described by the substitution function $\circ_{i}$, which says we obtain the operation $f \circ_{i} g \in \mathcal{O}(m+n-1)$. The coherence conditions say that these functions $\circ_{i}$ capture the following intuitive picture:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-226.jpg?height=247&width=1152&top_left_y=478&top_left_x=476)

The types then allow us to specify the, well, types of the arguments-inputs-that each function takes. So making tea is a 2-ary operation, an operation with arity 2, because it takes in two things. To make tea you need some warm water, and you need some tea leaves.

Example 6.92. Context-free grammars are to operads as graphs are to categories. Let's sketch what this means. First, a context-free grammar is a way of describing a particular set of 'syntactic categories' that can be formed from a set of symbols. For example, in English we have syntactic categories like nouns, determiners, adjectives, verbs, noun phrases, prepositional phrases, sentences, etc. The symbols are words, e.g. cat, dog, the, chases.

To define a context-free grammar on some alphabet, one specifies some production rules, which say how to form an entity in some syntactic category from a bunch of entities in other syntactic categories. For example, we can form a noun phrase from a determiner (the), an adjective (happy), and a noun (boy). Context free grammars are important in both linguistics and computer science. In the former, they're a basic way to talk about the structure of sentences in natural languages. In the latter, they're crucial when designing parsers for programming languages.

So just like graphs present free categories, context-free grammars present free operads. This idea was first noticed in [HMP98].

\subsection*{6.5.2 Operads from symmetric monoidal categories}

We will see in Definition 6.97 that a large class of operads come from symmetric monoidal categories. Before we explain this, we give a couple of examples. Perhaps the most important operad is that of Set.

Example 6.93. The operad Set of sets has

(i) Sets $X$ as types.

(ii) Functions $X_{1} \times \cdots \times X_{n} \rightarrow Y$ as operations of arity $\left(X_{1}, \ldots, X_{n} ; Y\right)$.
(iii) Substitution defined by

$$
\begin{aligned}
\left(g \circ \circ_{i} f\right)\left(x_{1}, \ldots, x_{i-1}, w_{1}, \ldots, w_{m},\right. & \left.x_{i+1}, \ldots, x_{n}\right) \\
& =g\left(x_{1}, \ldots, x_{i-1}, f\left(w_{1}, \ldots, w_{m}\right), x_{i+1}, \ldots, x_{n}\right)
\end{aligned}
$$

where $f \in \operatorname{Set}\left(W_{1}, \ldots, W_{m} ; X_{i}\right), g \in \operatorname{Set}\left(X_{1}, \ldots, X_{n} ; Y\right)$, and hence $g \circ_{i} f$ is a function

$$
\left(g \circ \circ_{i} f\right): X_{1} \times \cdots \times X_{i-1} \times W_{1} \times \cdots \times W_{m} \times X_{i+1} \times \cdots \times X_{n} \longrightarrow Y
$$

(iv) Identities $\operatorname{id}_{X} \in \operatorname{Set}(X ; X)$ are given by the identity function $\operatorname{id}_{X}: X \rightarrow X$.

Next we give an example that reminds us what all this operad stuff was for: wiring diagrams.

Example 6.94. The operad Cospan of finite-set cospans has

(i) Natural numbers $a \in \mathbb{N}$ as types.

(ii) Cospans $\underline{a_{1}}+\cdots+\underline{a_{n}} \rightarrow \underline{p} \leftarrow \underline{b}$ of finite sets as operations of arity $\left(a_{1}, \ldots, a_{n} ; b\right)$.

(iii) Substitution defined by pushout.

(iv) Identities $\operatorname{id}_{a} \in \operatorname{Set}(a ; a)$ just given by the identity cospan $\underline{a} \xrightarrow{\mathrm{id}_{\underline{a}}} \underline{a} \stackrel{\mathrm{id}_{\underline{g}}}{\longleftarrow} \underline{a}$. This is the operadic analogue of the monoidal category $\left(\operatorname{Cospan}_{\text {FinSet }}, 0,+\right)$.

We can depict operations in this operad using diagrams like we drew above. For example, here's a picture of an operation:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-227.jpg?height=398&width=607&top_left_y=1460&top_left_x=756)

This is an operation of arity ( $\underline{3}, \underline{3}, \underline{4}, \underline{2} ; \underline{3})$. Why? The circles marked $f$ and $g$ have 3 ports, $h$ has 4 ports, $k$ has 2 ports, and the outer circle has 3 ports: $3,3,4,2 ; 3$.

So how exactly is Eq. (6.95) a morphism in this operad? Well a morphism of this arity is, by (ii), a cospan $\underline{3}+\underline{3}+\underline{4}+\underline{2} \xrightarrow{a} p \stackrel{b}{\leftarrow}$. In the diagram above, the apex $p$ is the set $\underline{7}$, because there are 7 nodes $\bullet$ in the diagram. The function $a$ sends each port on one of the small circles to the node it connects to, and the function $b$ sends each port of the outer circle to the node it connects to.

We are able to depict each operation in the operad Cospan as a wiring diagram. It is often helpful to think of operads as describing a wiring diagram grammar. The
substitution operation of the operad signifies inserting one wiring diagram into a circle or box in another wiring diagram.

\section*{Exercise 6.96.}

1. Consider the following cospan $f \in \operatorname{Cospan}(2,2 ; 2)$ :

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-228.jpg?height=241&width=222&top_left_y=508&top_left_x=992)

Draw it as a wiring diagram with two inner circles, each with two ports, and one outer circle with two ports.

2. Draw the wiring diagram corresponding to the following cospan $g \in \operatorname{Cospan}(2,2,2 ; 0)$ :

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-228.jpg?height=206&width=328&top_left_y=962&top_left_x=947)

$\varnothing$

3. Compute the cospan $g \circ_{1} f$. What is its arity?

4. Draw the cospan $g \circ_{1} f$. Do you see it as substitution?

We can turn any symmetric monoidal category into an operad in a way that generalizes the above two examples.

Definition 6.97. For any symmetric monoidal category $(\mathcal{C}, I, \otimes)$, there is an operad $\mathcal{O}_{\mathcal{C}}$, called the operad underlying $\mathcal{C}$, defined as having:

(i) $\mathrm{Ob}(\mathcal{C})$ as types.

(ii) morphisms $C_{1} \otimes \cdots \otimes C_{n} \rightarrow D$ in $\mathcal{C}$ as the operations of arity $\left(C_{1}, \ldots, C_{n} ; D\right)$.

(iii) substitution is defined by

$$
(f \circ i g):=f \circ(\mathrm{id}, \ldots, \mathrm{id}, g, \mathrm{id}, \ldots, \mathrm{id})
$$

(iv) identities $\operatorname{id}_{a} \in \mathcal{O}_{\mathcal{C}}(a ; a)$ defined by $\operatorname{id}_{a}$.

We can also turn any monoidal functor into what's called an operad functor.

\subsection*{6.5.3 The operad for hypergraph props}

An operad functor takes the types of one operad to the types of another, and then the operations of the first to the operations of the second in a way that respects this.

Rough Definition 6.98. Suppose given two operads $\mathcal{O}$ and $\mathcal{P}$ with type collections $T$ and $U$ respectively. To specify an operad functor $F: \mathcal{O} \rightarrow \mathcal{P}$,

(i) one specifies a function $f: T \rightarrow U$.

(ii) For all arities $\left(t_{1}, \ldots, t_{n} ; t\right)$ in $\mathcal{O}$, one specifies a function

$$
F: \mathcal{O}\left(t_{1}, \ldots, t_{n} ; t\right) \rightarrow \mathcal{P}\left(f\left(t_{1}\right), \ldots, f\left(t_{n}\right) ; f(t)\right)
$$

such that composition and identities are preserved.

Just as set-valued functors $\mathcal{C} \rightarrow$ Set from any category $\mathcal{C}$ are of particular interestwe saw them as database instances in Chapter 3-so to are Set-valued functors $\mathcal{O} \rightarrow$ Set from any operad $\mathcal{O}$.

Definition 6.99. An algebra for an operad $\mathcal{O}$ is an operad functor $F: \mathcal{O} \rightarrow$ Set.

We can think of functors $\mathcal{O} \rightarrow$ Set as defining a set of possible ways to fill the boxes in a wiring diagram. Indeed, each box in a wiring diagram represents a type $t$ of the given operad $\mathcal{O}$ and an algebra $F: \mathcal{O} \rightarrow$ Set will take a type $t$ and return a set $F(t)$ of fillers for box $t$. Moreover, given an operation (i.e., a wiring diagram) $f \in \mathcal{O}\left(t_{1}, \ldots, t_{n}\right.$; $t$ ), we get a function $F(f)$ that takes an element of each set $F\left(t_{i}\right)$, and returns an element of $F(t)$. For example, it takes $n$ circuits with interface $t_{1}, \ldots, t_{n}$ respectively, and returns a circuit with boundary $t$.

Example 6.100. For electric circuits, the types are again finite sets, $T=\mathrm{Ob}$ (FinSet), where each finite set $t \in T$ corresponds to a cell with $t$ ports. Just as before, we have a set $\operatorname{Circ}(t)$ of fillers, namely the set of electric circuits with that $t$-marked terminals. As an operad algebra, Circ: Cospan $\rightarrow$ Set transforms wiring diagrams like this one

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-229.jpg?height=255&width=477&top_left_y=1694&top_left_x=821)

into formulas that build a new circuit from a bunch of existing ones. In the abovedrawn case, we would get a morphism $\operatorname{Circ}(\varphi) \in \operatorname{Set}(\operatorname{Circ}(2), \operatorname{Circ}(2), \operatorname{Circ}(2) ; \operatorname{Circ}(0))$, i.e. a function

$$
\operatorname{Circ}(\varphi): \operatorname{Circ}(2) \times \operatorname{Circ}(2) \times \operatorname{Circ}(2) \rightarrow \operatorname{Circ}(0)
$$

We could apply this function to the three elements of Circ(2) shown here

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-229.jpg?height=173&width=830&top_left_y=2304&top_left_x=642)
and the result would be the closed circuit from the beginning of the chapter:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-230.jpg?height=178&width=618&top_left_y=323&top_left_x=751)

This is reminiscent of the story for decorated cospans: gluing fillers together to form hypergraph categories. An advantage of the decorated cospan construction is that one obtains an explicit category (where morphisms have domains and codomains and can hence be composed associatively), equipped with Frobenius structures that allow us to get around the strictures of domains and codomains. The operad perspective has other advantages. First, whereas decorated cospans can produce only some hypergraph categories, Cospan-algebras can produce any hypergraph category.

Proposition 6.101. There is an equivalence between Cospan-algebras and hypergraph props.

Another advantage of using operads is that one can vary the operad itself, from Cospan to something similar (like the operad of 'cobordisms'), and get slightly different compositionality rules.

In fact, operads-with the additional complexity in their definition-can be customized even more than all compositional structures defined so far. For example, we can define operads of wiring diagrams where the wiring diagrams must obey precise conditions far more specific than the constraints of a category, such as requiring that the diagram itself has no wires that pass straight through it. In fact, operads are strong enough to define themselves: roughly speaking, there is an operad for operads: the category of operads is equivalent to the category of algebras for a certain operad [Lei04, Example 2.2.23]. While operads can, of course, be generalized again, they conclude our march through an informal hierarchy of compositional structures, from preorders to categories to monoidal categories to operads.

\subsection*{6.6 Summary and further reading}

This chapter began with a detailed exposition of colimits in the category of sets; as we saw, these colimits describe ways of joining or interconnecting sets. Our second way of talking about interconnection was the use of Frobenius monoids and hypergraph categories; we saw these two themes come together in the idea of a decorated cospans. The decorated cospan construction uses a certain type of structured functor to construct a certain type of structured category. More generally, we might be interested in other types of structured category, or other compositional structure. To address this, we briefly saw how these ideas fit into the theory of operads.

Colimits are a fundamental concept in category theory. For more on colimits, one might refer to any of the introductory category theory textbooks we mentioned in Section 3.6.

Special commutative Frobenius monoids and hypergraph categories were first defined, under the names 'separable commutative Frobenius algebra' and 'well-supported compact closed category', by Carboni and Walters [CW87; Car91]. The use of decorated cospans to construct them is detailed in [Fon15; Fon18; Fon16]. The application to networks of passive linear systems, such as certain electrical circuits, is discussed in [BF15], while further applications, such as to Markov processes and chemistry can be found in [BFP16; BP17]. For another interesting application of hypergraph categories, we recommend the pixel array method for approximating solutions to nonlinear equations [Spi+16]. The story of this chapter is fleshed out in a couple of recent, more technical papers [FS18b; FS18a].

Operads were introduced by May to describe compositional structures arising in algebraic topology [May72]; Leinster has written a great book on the subject [Lei04]. More recently, with collaborators author-David has discussed using operads in applied mathematics, to model composition of structures in logic, databases, and dynamical systems [RS13; Spi13; VSL15].

\section*{Chapter 7}

\section*{Logic of behavior: Sheaves, toposes, and internal languages}

\subsection*{7.1 How can we prove our machine is safe?}

Imagine you are trying to design a system of interacting components. You wouldn't be doing this if you didn't have a goal in mind: you want the system to do something, to behave in a certain way. In other words, you want to restrict its possibilities to a smaller set: you want the car to remain on the road, you want the temperature to remain in a particular range, you want the bridge to be safe for trucks to pass. Out of all the possibilities, your system should only permit some.

Since your system is made of components that interact in specified ways, the possible behavior of the whole-in any environment-is determined by the possible behaviors of each of its components in their local environments, together with the precise way in which they interact. ${ }^{1}$ In this chapter, we will discuss a logic wherein one can describe general types of behavior that occur over time, and prove properties of a larger-scale system from the properties and interaction patterns of its components.

For example, suppose we want an autonomous vehicle to maintain a distance of some safe $\in \mathbb{R}$ from other objects. To do so, several components must interact: a sensor that approximates the real distance by an internal variable $S^{\prime}$, a controller that uses $S^{\prime}$ to decide what action $A$ to take, and a motor that moves the vehicle with an
\footnotetext{
${ }^{1}$ The well-known concept of emergence is not about possibilities, it is about prediction. Predicting the behavior of a system given predictions of its components is notoriously hard. The behavior of a double pendulum is chaotic-meaning extremely sensitive to initial conditions-whereas those of the two component pendulums are not. However, the set of possibilities for the double pendulum is completely understood: it is the set of possible angular positions and velocities of both arms. When we speak of a machine's properties in this chapter, we always mean the guarantees on its behaviors, not the probabilities involved, though the latter would certainly be an interesting thing to contemplate.
}
acceleration based on $A$. This in turn affects the real distance $S$, so there is a feedback loop.

Consider the following model diagram:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-234.jpg?height=217&width=1081&top_left_y=436&top_left_x=522)

In the diagram shown, the distance $S$ is exposed by the exterior interface. This just means we imagine $S$ as being a variable that other components of a larger system may want to interact with. We could have exposed no variables (making it a closed system) or we could have exposed $A$ and/or $S^{\prime}$ as well.

In order for the system to ensure $S \geq$ safe, we need each of the components to ensure a property of its own. But what are these components, 'sensor, controller, motor', and what do they do?

One way to think about any of the components is to open it up and see how it is put together; with a detailed study we may be able to say what it will do. For example, just as $S$ was exposed in the diagram above, one could imagine opening up the 'sensor' component box in Eq. (7.1) and seeing an interaction between subcomponents

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-234.jpg?height=304&width=784&top_left_y=1298&top_left_x=665)

This ability to zoom in and see a single unit as being composed of others is important for design. But at the end of the day, you eventually need to stop diving down and simply use the properties of the components in front of you to prove properties of the composed system. Have no fear: everything we do in this chapter will be fully compositional, i.e. compatible with opening up lower-level subsystems and using the fractal-like nature of composition. However at a given time, your job is to design the system at a given level, taking the component properties of lower-level systems as given.

We will think of each component in terms of the relationship it maintains (through time) between the changing values on its ports. "Whenever I see a flash, I will increase pressure on the button": this is a relationship I maintain through time between the changing values on my eye port and my finger port. We will make this more precise soon, but fleshing out the situation in Eq. (7.1) should help. The sensor maintains a relationship between $S$ and $S^{\prime}$, e.g. that the real distance $S$ and its internal representation $S^{\prime}$ differ by no more than $5 \mathrm{~cm}$. The controller maintains a relationship between $S^{\prime}$ and the action signal $A$, e.g. that if at any time $S<$ safe, then within one second it will
emit the signal $A=$ go. The motor maintains a relationship between $A$ and $S$, e.g. that A dictates the second derivative of $S$ by the formula

$$
\begin{equation*}
((A=\text { go }) \Rightarrow \ddot{S}>1) \wedge((A=\text { stop }) \Rightarrow \ddot{S}=0) \tag{7.2}
\end{equation*}
$$

If we want to prove properties of the whole interacting system, then the relationships maintained by each component need to be written in a formal logical language, something like what we saw in Eq. (7.2). From that basis, we can use standard proof techniques to combine properties of subsystems into properties of the whole. This is our objective in the present chapter.

We have said how component systems, wired together in some arrangement, create larger-scale systems. We have also said that, given the wiring arrangement, the behavioral properties of the component systems dictate the behavioral properties of the whole. But what exactly are behavioral properties?

In this chapter, we want to give a formal language and semantics for a very general notion of behavior. Mathematics is itself a formal language; the usual style of mathematical modeling is to use any piece of this vast language at any time and for any reason. One uses "human understanding" to ensure that the different models are fitting together in an appropriate way when different systems are combined. The present work differs in that we want to find a domain-specific language for modeling behavior, any sort of behavior, and nothing but behavior. Unlike in the wide world of math, we want a setting where the only things that can be discussed are behaviors.

For this, we will construct what is called a topos, which is a special kind of category. Our topos, let's call it BT, will have behavior types-roughly speaking, sets whose elements can change through time-as its objects. An amazing fact about toposes ${ }^{2}$ is that they come with an internal language that looks very much like the usual formal language of mathematics itself. Thus one can define graphs, groups, topological spaces, etc. in any topos. But in BT, what we call graphs will actually be graphs that change through time, and similarly what we call groups and spaces will actually be groups and spaces that change through time.

The topos BT not only has an internal language, but also a mathematical semantics using the notion of sheaves. Technically, a sheaf is a certain sort of functor, but one can imagine it as a space of possibilities, varying in a controlled way; in our case it will be a space of possible behaviors varying in a certain notion of time. Every property we prove in our logic of behavior types will have meaning in this category of sheaves.

When discussing systems and components-such as sensors, controllers, motors, etc.-we mentioned behavior types; these will be the objects in the topos BT. Every wire in the picture below will stand for a behavior type, and every box $X$ will stand for a behavioral property, a relation that $X$ maintains between the changing values on its
\footnotetext{
${ }^{2}$ The plural of topos is often written topoi, rather than toposes. This seems a bit fancy for our taste. As Johnstone suggests in [Joh77], we might ask those who "persist in talking about topoi whether, when they go out for a ramble on a cold day, they carry supplies of hot tea with them in thermoi." It's all in good fun; either term is perfectly reasonable and well-accepted.
}
ports.

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-236.jpg?height=206&width=1071&top_left_y=290&top_left_x=516)

For example we could imagine that
- $S$ (wire): The behavior of $S$ over a time-interval $[a, b]$ is that of all continuous real-valued functions $[a, b] \rightarrow \mathbb{R}$.
- $A$ (wire): The behavior of $A$ over a time-interval $[a, b]$ is all piecewise constant functions, taking values in the finite set such as $\{$ go, stop $\}$.
- controller (box): the relation $\left\{\left(S^{\prime}, A\right) \mid E q\right.$. (7.2)\}, i.e. all behavioral pairs $\left(S^{\prime}, A\right)$ that conform to what we said our controller is supposed to do in Eq. (7.2).

\subsection*{7.2 The category Set as an exemplar topos}

We want to think about a very abstract sort of thing, called a topos, because we will see that behavior types form a topos. To get started, we begin with one of the easiest toposes to think about, namely the topos Set of sets. In this section we will discuss commonalities between sets and every other topos. We will go into some details about the category of sets, so as to give intuition for other toposes. In particular, we'll pay careful attention to the logic of sets, because we eventually want to understand the logic of behaviors.

Indeed, logic and sets are closely related. For example, the logical statement-more formally known as a predicate-likes_cats defines a function from the set $P$ of people to the set $\mathbb{B}=\{$ false, true $\}$ of truth values, where Brendan $\in P$ maps to true because he likes cats whereas Ursula $\in P$ maps to false because she does not. Alternatively, likes_cats also defines a subset of $P$, consisting of exactly the people that do like cats

$$
\{p \in P \mid \text { likes_cats }(p)\}
$$

In terms of these subsets, logical operations correspond to set operations, e.g. AND corresponds to intersection: indeed, the set of people for (mapped to true by) the predicate likes_cats_AND_likes_dogs is equal to the intersection of the set for likes_cats and the set for likes_dogs.

We saw in Chapter 3 that such operations, which are examples of database queries, can be described in terms of limits and colimits in Set. Indeed, the category Set has many such structures and properties, which together make logic possible in that setting. In this section we want to identify these properties, and show how logical operations can be defined using them.

Why would we want to abstractly find such structures and properties? In the next section, we'll start our search for other categories that also have them. Such categories, called toposes, will be Set-like enough to do logic, but have much more complex and
interesting semantics. Indeed, we will discuss one whose logic allows us to reason not about properties of sets, but about behavioral properties of very general machines.

\subsection*{7.2.1 Set-like properties enjoyed by any topos}

Although we will not prove it in this book, toposes are categories that are similar to Set in many ways. Here are some facts that are true of any topos $\mathcal{E}$ :
1. $\mathcal{E}$ has all limits,
2. $\mathcal{E}$ has all colimits,
3. $\mathcal{E}$ is cartesian closed,
4. $\mathcal{E}$ has epi-mono factorizations,
5. $\mathcal{E}$ has a subobject classifier $1 \xrightarrow{\text { true }} \Omega$.

In particular, since Set is a topos, all of the above facts are true for $\mathcal{E}=$ Set. Our first goal is to briefly review these concepts, focusing most on the subobject classifier.

Limits and colimits. We discussed limits and colimits briefly in Section 3.4.2, but the basic idea is that one can make new objects from old by taking products, using equations to define subobjects, forming disjoint unions, and taking quotients.object 0 . One of the most important types of limit (resp. colimit) is that of pullbacks (resp. pushouts); see Example 3.99 and Definition 6.19. For our work below, we'll need to know a touch more about pullbacks than we have discussed so far, so let's begin there.

Suppose that $\mathcal{C}$ is a category and consider the diagrams below:
![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-237.jpg?height=176&width=916&top_left_y=1522&top_left_x=598)

In the left-hand square, the corner symbol $\lrcorner$ unambiguously means that the square $(B, C, E, F)$ is a pullback. But in the right-hand square, does the corner symbol mean that $(A, B, D, E)$ is a pullback or that $(A, C, D, F)$ is a pullback? It's ambiguous, but as we next show, it becomes unambiguous if the right-hand square is a pullback.

Proposition 7.3. In the commutative diagram below, suppose that the ( $\left.B, C, B^{\prime}, C^{\prime}\right)$ square is a pullback:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-237.jpg?height=176&width=379&top_left_y=2124&top_left_x=862)

Then the $\left(A, B, A^{\prime}, B^{\prime}\right)$ square is a pullback iff the $\left(A, C, A^{\prime}, C^{\prime}\right)$ rectangle is a pullback.

Epi-mono factorizations. The abbreviation 'epi' stands for epimorphism, and the abbreviation 'mono' stands for monomorphism. Epimorphisms are maps that act like surjections, and monomorphisms are maps that act like injections. ${ }^{3}$ We can define them formally in terms of pushouts and pullbacks.

Definition 7.5. Let $\mathcal{C}$ be a category, and let $f: A \rightarrow B$ be a morphism. It is called a monomorphism (resp. epimorphism) if the square to the left is a pullback (resp. the square to the right is a pushout):
![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-238.jpg?height=232&width=616&top_left_y=690&top_left_x=751)

Exercise 7.6. Show that in Set, monomorphisms are just injections:

1. Show that if $f$ is a monomorphism then it is injective.

2. Show that if $f: A \rightarrow B$ is injective then it is a monomorphism.

\section*{Exercise 7.7 .}

1. Show that the pullback of an isomorphism along any morphism is an isomorphism. That is, suppose that $i: B^{\prime} \rightarrow B$ is an isomorphism and $f: A \rightarrow B$ is any morphism. Show that $i^{\prime}$ is an isomorphism, in the following diagram:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-238.jpg?height=228&width=242&top_left_y=1409&top_left_x=974)

2. Show that for any map $f: A \rightarrow B$, the square shown is a pullback:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-238.jpg?height=223&width=217&top_left_y=1737&top_left_x=995)

Exercise 7.8. Suppose the following diagram is a pullback in a category $\mathfrak{C}$ :

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-238.jpg?height=211&width=244&top_left_y=2063&top_left_x=932)

Use Proposition 7.3 and Exercise 7.7 to show that if $f$ is a monomorphism, then so is $f^{\prime}$.
\footnotetext{
${ }^{3}$ Surjections are sometimes called 'onto' and injections are sometimes called 'one-to-one', hence the Greek prefixes epi and mono.
}

Now that we have defined epimorphisms and monomorphisms, we can say what epi-mono factorizations are. We say that a morphism $f: C \rightarrow D$ in $\mathcal{E}$ has an epi-mono factorization if it has an 'image'; that is, there is an object $\operatorname{im}(f)$, an epimorphism $C \rightarrow \operatorname{im}(f)$, and a monomorphism $\operatorname{im}(f) \hookrightarrow D$, whose composite is $f$.

In Set, epimorphisms are surjections and monomorphisms are injections. Every function $f: C \rightarrow D$ may be factored as a surjective function onto its image $\operatorname{im}(f)=$ $\{f(c) \mid c \in C\}$, followed by the inclusion of this image into the codomain $D$. Moreover, this factorization is unique up to isomorphism.

Exercise 7.9. Factor the following function $f: \underline{3} \rightarrow \underline{3}$ as an epimorphism followed by a monomorphism.

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-239.jpg?height=209&width=287&top_left_y=782&top_left_x=911)

This is the case in any topos $\mathcal{E}$ : for any morphism $f: c \rightarrow d$, there exists an epimorphism $e$ and a monomorphism $m$ such that $f=(e ; m)$ is their composite.

Cartesian closed. A category $\mathcal{C}$ being cartesian closed means that $\mathcal{C}$ has a symmetric monoidal structure given by products, and it is monoidal closed with respect to this. (We previously saw monoidal closure in Definition 2.79 (for preorders) and Proposition 4.60, as a corollary of compact closure.) Slightly more down-to-earth, cartesian closure means that for any two objects $C, D \in \mathcal{C}$, there is a 'hom-object' $D^{\mathcal{C}} \in \mathcal{C}$ and a natural isomorphism for any $A \in \mathcal{C}$ :

$$
\begin{equation*}
\mathcal{C}(A \times C, D) \cong \mathcal{C}\left(A, D^{C}\right) \tag{7.10}
\end{equation*}
$$

Think of it this way. Suppose you're $A$ and I'm $C$, and we're interacting through some game $f(-,-): A \times C \rightarrow D$ : for whatever action $a \in A$ that you take and action $c \in C$ that I take, $f(a, c)$ is some value in $D$. Since you're self-centered but loving, you think of this situation as though you're creating a game experience for me. When you do $a$, you make a game $f(a,-): C \rightarrow D$ for me alone. In the formalism, $D^{C}$ represents the set of games for me. So now you've transformed a two-player game, valued in $D$, into a one-player game, you're the player, valued in... one player games valued in $D$. This transformation is invertible-you can switch your point of view at will-and it's called currying. This is the content of Example 3.72.

Exercise 7.11. Let $\mathcal{V}=(V, \leq, I, \otimes)$ be a (unital, commutative) quantale-see Definition 2.90—and suppose it satisfies the following for all $v, w, x \in V$ :
- $v \leq I$,
- $v \otimes w \leq v$ and $v \otimes w \leq w$, and
- if $x \leq v$ and $x \leq w$ then $x \leq v \otimes w$.

1. Show that $\mathcal{V}$ is a cartesian closed category, in fact a cartesian closed preorder.

2. Can every cartesian closed preorder be obtained in this way?

Subobject classifier. The concept of a subobject classifier requires more attention, because its existence has huge consequences for a category $\mathcal{C}$. In particular, it creates the setting for a rich system of higher order logic to exist inside $\mathcal{C}$; it does so by providing some things called 'truth values'. The higher order logic manifests in its fully glory when $\mathcal{C}$ has finite limits and is cartesian closed, because these facts give rise to the logical operations on truth values. ${ }^{4}$ In particular, the higher order logic exists in any topos.

We will explain subobject classifiers in as much detail as we can; in fact, it will be our subject for the rest of Section 7.2.

\subsection*{7.2.2 The subobject classifier}

Before giving the definition of subobject classifiers, recall that monomorphisms in Set are injections, and any injection $X \nrightarrow Y$ is isomorphic to a subset of $Y$. This gives a simple and useful way to conceptualize monomorphisms into $Y$ when reading the following definition: it will do no harm to think of them as subobjects of $Y$.

Definition 7.12. Let $\mathcal{E}$ be a category with finite limits, i.e. with pullbacks and a terminal object 1. A subobject classifier in $\mathcal{E}$ consists of an object $\Omega \in \mathcal{E}$, together with a monomorphism true: $1 \rightarrow \Omega$, satisfying the following property: for any objects $X$ and $Y$ and monomorphism $m: X \hookrightarrow Y$ in $\mathcal{E}$, there is a unique morphism $\ulcorner m\urcorner: Y \rightarrow \Omega$ such that the diagram on the left of Eq. (7.13) is a pullback in $\mathcal{E}$ :
![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-240.jpg?height=246&width=964&top_left_y=1438&top_left_x=577)

We refer to $\ulcorner m\urcorner$ as the characteristic map of $m$, or we say that $\ulcorner m\urcorner$ classifies $m$. Conversely, given any map $p: Y \rightarrow \Omega$, we denote the pullback of true as on the right of Eq. (7.13).

A predicate on $Y$ is a morphism $Y \rightarrow \Omega$.

Definition 7.12 is a bit difficult to get one's mind around, partly because it is hard to imagine its consequences. It is like a superdense nugget from outer space, and through scientific explorations in the latter half of the 20th century, we have found that it brings super powers to whichever categories possess it. We will explain some of the consequences below, but very quickly, the idea is the following.

When a category has a subobject classifier, it provides a translator, turning subobjects of any object $Y$ into maps from that $Y$ to the particular object $\Omega$. Pullback of the
\footnotetext{
${ }^{4}$ A category that has finite limits, is cartesian closed, and has a subobject classifier is called an elementary topos. We will not discuss these further, but they are the most general notion of topos in ordinary category theory. When someone says topos, you might ask "Grothendieck topos or elementary topos?," because there does not seem to be widespread agreement on which is the default.
}
monomorphism true: $1 \rightarrow \Omega$ provides a translator going back, turning maps $Y \rightarrow \Omega$ into subobjects of $Y$. We can replace our fantasy of the superdense nugget with a slightly more refined story: "any object $Y$ understands itself—its parts and the logic of how they fit together-by asking questions of the oracle $\Omega$, looking for what's true." Or to fully be precise but dry, "subobjects of $Y$ are classified by predicates on $Y$."

Let's move from stories and slogans to concrete facts.

The subobject classifier in Set. Since Set is a topos, it has a subobject classifier. It will be a set with supposedly wonderful properties; what set is it?

The subobject classifier in Set is the set of booleans,

$$
\begin{equation*}
\Omega_{\text {Set }}:=\mathbb{B}=\{\text { true, false }\} \tag{7.14}
\end{equation*}
$$

So in Set, the truth values are true and false.

By definition (Def. 7.12), the subobject classifier comes equipped with a morphism, generically called true: $1 \rightarrow \Omega$; in the case of Set it is played by the function $1 \rightarrow$ \{true, false\} that sends 1 to true. In other words, the morphism true is aptly named in this case.

For sets, monomorphism just means injection, as we mentioned above. So Definition 7.12 says that for any injective function $m: X \hookrightarrow Y$ between sets, we are supposed to be able to find a characteristic function $\ulcorner m\urcorner: Y \rightarrow\{$ true, $\mathrm{false}\}$ with some sort of pullback property. We propose the following definition of $\ulcorner m\urcorner$ :

$$
\ulcorner m\urcorner(y):= \begin{cases}\text { true } & \text { if } m(x)=y \text { for some } x \in X \\ \text { false } & \text { otherwise }\end{cases}
$$

In other words, if we think of $X$ as a subobject of $Y$, then we make $\ulcorner m\urcorner(y)$ equal to true iff $y \in X$.

In particular, the subobject classifier property turns subsets $X \subseteq Y$ into functions $p: Y \rightarrow \mathbb{B}$, and vice versa. How it works is encoded in Definition 7.12, but the basic idea is that $X$ will be the set of all things in $Y$ that $p$ sends to true:

$$
\begin{equation*}
X=\{y \in Y \mid p(y)=\text { true }\} \tag{7.15}
\end{equation*}
$$

This might help explain our abstract notation $\{Y \mid p\}$ in Eq. (7.13).

Exercise 7.16. Let $X=\mathbb{N}=\{0,1,2, \ldots\}$ and $Y=\mathbb{Z}=\{\ldots,-1,0,1,2, \ldots\}$; we have $X \subseteq Y$, so consider it as a monomorphism $m: X \nrightarrow Y$. It has a characteristic function $\ulcorner m\urcorner: Y \rightarrow \mathbb{B}$, as in Definition 7.12.

1. What is $\ulcorner m\urcorner(-5) \in \mathbb{B}$ ?

2. What is $\ulcorner m\urcorner(0) \in \mathbb{B}$ ?

Exercise 7.17 .

1. Consider the identity function $\operatorname{id}_{\mathbb{N}}: \mathbb{N} \rightarrow \mathbb{N}$. It is an injection, so it has a characteristic function $\left\ulcorner\mathrm{id}_{\mathbb{N}}\right\urcorner: \mathbb{N} \rightarrow \mathbb{B}$. Give a concrete description of $\left\ulcorner\mathrm{id}_{\mathbb{N}}\right\urcorner$, i.e. its exact value for each natural number $n \in \mathbb{N}$.

2. Consider the unique function $!_{\mathbb{N}}: \varnothing \rightarrow \mathbb{N}$ from the empty set. Give a concrete description of $\left\ulcorner!_{\mathbb{N}}\right\urcorner: \mathbb{N} \rightarrow \mathbb{B}$.

\subsection*{7.2.3 Logic in the topos Set}

As we said above, the subobject classifier of any topos $\mathcal{E}$ gives the setting in which to do logic. Before we explain a bit about how topos logic works in general, we continue to work concretely by focusing on logic in the topos Set.

Obtaining the AND operation. Consider the function $1 \rightarrow \mathbb{B} \times \mathbb{B}$ picking out the element (true, true). This is a monomorphism, so it defines a characteristic function $\ulcorner$ (true, true) $\urcorner: \mathbb{B} \times \mathbb{B} \rightarrow \mathbb{B}$. What function is it? By Eq. (7.15) the only element of $\mathbb{B} \times \mathbb{B}$ that can be sent to true is (true, true). Thus $\ulcorner($ true, true $)\urcorner(P, Q) \in \mathbb{B}$ must be given by the following truth table

\begin{tabular}{cc||c}
$P$ & $Q$ & $\ulcorner$ (true, true $)\urcorner(P, Q)$ \\
\hline true & true & true \\
true & false & false \\
false & true & false \\
false & false & false
\end{tabular}

This is exactly the truth table for the AND of $P$ and $Q$, i.e. for $P \wedge Q$. In other words, $\ulcorner($ true, true $)\urcorner=\wedge$. Note that this defines $\wedge$ as a function $\wedge: \mathbb{B} \times \mathbb{B} \rightarrow \mathbb{B}$, and we use the usual infix notation $x \wedge y:=\wedge(x, y)$.

Obtaining the OR operation. Let's go backwards this time. The truth table for the OR of $P$ and $Q$, i.e. that of the function $\vee: \mathbb{B} \times \mathbb{B} \rightarrow \mathbb{B}$ defining $O R$, is:

\begin{tabular}{cc||c}
$P$ & $Q$ & $P \vee Q$ \\
\hline true & true & true \\
true & false & true \\
false & true & true \\
false & false & false
\end{tabular}

If we wanted to obtain this function as the characteristic function $\ulcorner m\urcorner$ of some subset $m: X \subseteq \mathbb{B} \times \mathbb{B}$, what subset would $X$ be? By Eq. (7.15), $X$ should be the set of $y \in Y$ that are sent to true. Thus $m$ is the characteristic map for the three element subset

$$
X=\{\text { (true }, \text { true }),(\text { true }, \text { false }),(\text { false }, \text { true })\} \subseteq \mathbb{B} \times \mathbb{B}
$$

To prepare for later generalization of this idea in any topos, we want a way of thinking of $X$ only in terms of properties listed at the beginning of Section 7.2.1. In fact, one can think of $X$ as the union of $\{$ true $\} \times \mathbb{B}$ and $\mathbb{B} \times\{$ true $\}$-a colimit of limits involving the subobject classifier and terminal object. This description will construct an analogous subobject of $\Omega \times \Omega$, and hence classify a map $\Omega \times \Omega \rightarrow \Omega$, in any topos $\mathcal{E}$.

Exercise 7.19. Every boolean has a negation, $\neg$ false $=$ true and $\neg$ true $=$ false. The function $\neg: \mathbb{B} \rightarrow \mathbb{B}$ is the characteristic function of some thing, (*?*).

1. What sort of thing should (*?*) be? For example, should $\neg$ be the characteristic function of an object? A topos? A morphism? A subobject? A pullback diagram?

2. Now that you know the sort of thing (*?*) is, which thing of that sort is it? $\diamond$

Exercise 7.20. Given two booleans $P, Q$, define $P \Rightarrow Q$ to mean $P=(P \wedge Q)$.

1. Write down the truth table for the statement $P=(P \wedge Q)$ :

\begin{tabular}{cc||c|c}
$P$ & $Q$ \\
true & true & $P \wedge Q$ & $P=(P \wedge Q)$ \\
true & false & $?$ & $?$ \\
false & true & $?$ & $?$ \\
false false & $?$ & $?$
\end{tabular}

2. If you already have an idea what $P \Rightarrow Q$ should mean, does it agree with the last column of table above?

3. What is the characteristic function $m: \mathbb{B} \times \mathbb{B} \rightarrow \mathbb{B}$ for $P \Rightarrow Q$ ?

4. What subobject does $m$ classify?

Exercise 7.21. Consider the sets $E:=\{n \in \mathbb{N} \mid n$ is even $\}, P:=\{n \in \mathbb{N} \mid n$ is prime $\}$, and $T:=\{n \in \mathbb{N} \mid n \geq 10\}$. Each is a subset of $\mathbb{N}$, so defines a function $\mathbb{N} \rightarrow \mathbb{B}$.

1. What is $\ulcorner E\urcorner(17)$ ?

2. What is $\ulcorner P\urcorner(17)$ ?

3. What is $\ulcorner T\urcorner(17)$ ?

4. Name the smallest three elements in the set classified by $(\ulcorner E\urcorner \wedge\ulcorner P\urcorner) \vee\ulcorner T\urcorner$. $\diamond$

Review. Let's take stock of where we are and where we're going. In Section 7.1, we set out our goal of proving properties about behavior, and we said that topos theory is a good mathematical setting for doing that. We are now at the end of Section 7.2, which was about Set as an examplar topos. What happened?

In Section 7.2.1, we talked about properties of Set that are enjoyed by any topos: limits and colimits, cartesian closure, epi-mono factorizations, and subobject classifiers. Then in Section 7.2.2 we launched into thinking about the subobject classifier in general and in the specific topos Set, where it is the set $\mathbb{B}$ of booleans because any subset of $Y$ is classified by a specific predicate $p: Y \rightarrow \mathbb{B}$. Finally, in Section 7.2.3 we discussed how to understand logic in terms of $\Omega$ : there are various maps $\wedge, \vee, \Rightarrow: \Omega \times \Omega \rightarrow \Omega$ and $\neg: \Omega \rightarrow \Omega$ etc., which serve as logical connectives. These are operations on truth values.

We have talked a lot about toposes, but we've only seen one so far: the category of sets. But we've actually seen more without knowing it: the category $\mathcal{C}$-Inst of instances on any database schema from Definition 3.60 is a topos. Such toposes are called presheaf toposes and are fundamental, but we will focus on sheaf toposes, because our topos of behavior types will be a sheaf topos.

Sheaves are fascinating, but highly abstract mathematical objects. They are not for the faint of mathematical heart (those who are faint of physical heart are welcome to proceed).

\subsection*{7.3 Sheaves}

Sheaf theory began before category theory, e.g. in the form of something called "local coefficient systems for homology groups." However its modern formulation in terms of functors and sites is due to Grothendieck, who also invented toposes.

The basic idea is that rather than study spaces, we should study what happens on spaces. A space is merely the 'site' at which things happen. For example, if we think of the plane $\mathbb{R}^{2}$ as a space, we might examine only points and regions in it. But if we think of $\mathbb{R}^{2}$ as a site where things happen, then we might think of things like weather systems throughout the plane, or sand dunes, or trajectories and flows of material. There are many sorts of things that can happen on a space, and these are the sheaves: a sheaf on a space is roughly "a sort of thing that can happen on the space." If we want to think about points or regions from the sheaf perspective, we would consider them as different points of view on what's happening. That is, it's all about what happens on a space: the parts of the space are just perspectives from which to watch the show.

This is reminiscent of databases. The schema of a database is not the interesting part; the data is what's interesting. To be clear, the schema of a database is a site-it's acting like the space-and the category of all instances on it is a topos. In general, we can think of any small category $\mathcal{C}$ as a site; the corresponding topos is the category of functors $\mathcal{C}^{\text {op }} \rightarrow$ Set. ${ }^{5}$ Such functors are called presheaves on $\mathcal{C}$.

Did you notice that we just introduced a huge class of toposes? For any category $\mathcal{C}$, we said there is a topos of presheaves on it. So before we go on to sheaves, let's discuss this preliminary topic of presheaves. We will begin to develop some terminology and ways of thinking that will later generalize to sheaves.

\subsection*{7.3.1 Presheaves}

Recall the definition of functor and natural transformation from Section 3.3. Presheaves are just functors, but they have special terminology that leads us to think about them in a certain geometric way.

Definition 7.22. Let $\mathcal{C}$ be a small category. A presheaf $P$ on $\mathcal{C}$ is a functor $P: \mathcal{C}^{\text {op }} \rightarrow$ Set. To each object $c \in \mathcal{C}$, we refer to the set $P(c)$ as the set of sections of $P$ over $c$. To each morphism $f: c^{\prime} \rightarrow c$, we refer to the function $P(f): P(c) \rightarrow P\left(c^{\prime}\right)$ as the restriction map along $f$. For any section $s \in P(c)$, we may denote $P(f)(s) \in P\left(c^{\prime}\right)$, i.e. its restriction along $f$, by $\left.s\right|_{f}$.
\footnotetext{
${ }^{5}$ The category of functors $\mathcal{C} \rightarrow$ Set is also a topos: use $\mathcal{C}^{\text {op }}$ as the defining site.
}

If $P$ and $Q$ are presheaves, a morphism $\alpha: P \rightarrow Q$ between them is a natural transformation of functors

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-245.jpg?height=165&width=266&top_left_y=335&top_left_x=927)

Example 7.23. Let ArShp be the category shown below:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-245.jpg?height=149&width=637&top_left_y=668&top_left_x=733)

The reason we call our category ArShp is that we can imagine of it as an 'arrow shape.'

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-245.jpg?height=323&width=463&top_left_y=912&top_left_x=820)

A presheaf on ArShp is a functor $I:$ ArShp ${ }^{\mathrm{op}} \rightarrow$ Set, which is a database instance on ArShp ${ }^{\text {op }}$. Note that ArShp ${ }^{\text {op }}$ is what we called Gr in Section 3.3.5; there we showed that database instances on Gr-i.e. presheaves on ArShp- are just directed graphs, e.g.

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-245.jpg?height=222&width=1065&top_left_y=1469&top_left_x=519)

Thinking of presheaves on any category $\mathfrak{C}$, it often makes sense to imagine the objects of $\mathcal{C}$ as shapes of some sort, and the morphisms of $\mathcal{C}$ as continuous maps between shapes, just like we did for the arrow shape in Eq. (7.24). In that context, one can think of a presheaf $P$ as a kind of lego construction: $P$ is built out of the shapes in $\mathcal{C}$, connected together using the morphisms in $\mathcal{C}$. In the case where $\mathcal{C}$ is the arrow shape, a presheaf is a graph. So this would say that a graph is a sort of lego construction, built out of vertices and arrows connected together using the inclusion of a vertex as the source or target of an arrow. Can you see it?

This statement can be made pretty precise; though we cannot go through it here, the above lego idea is summarized by the formal statement that "the category of presheaves on $\mathcal{C}$ is the free colimit completion of $\mathcal{C}$." Ask a friendly neighborhood category theorist for details.

However one thinks of presheaves-in terms of lego assemblies or database instancesthey're relatively straightforward. The difference between presheaves and sheaves is
that sheaves take into account some sort of 'covering information.' The trivial notion of covering is to say that every object covers itself and nothing more; if one uses this trivial covering, presheaves and sheaves are the same thing. In our behavioral context we will need a non-trivial notion of covering, so sheaves and presheaves will be slightly different. Our next goal is to understand sheaves on a topological space.

\subsection*{7.3.2 Topological spaces}

We said in Section 7.3 that, rather than study spaces, we consider spaces as mere 'sites' on which things happen. We also said the things that can happen on a space are called sheaves, and always form a type of category called a topos. To define a topos of sheaves, we must start with the site on which they exist.

Sites are very abstract mathematical objects, and we will not make them precise in this book. However, one of the easiest sorts of sites to think about are those coming from topological spaces: every topological space naturally has the structure of a site. We've talked about spaces for a while without making them precise; let's do so now.

Definition 7.25. Let $X$ be a set, and let $P(X)=\{U \subseteq X\}$ denote its set of subsets. A topology on $X$ is a subset $O p \subseteq P(X)$, elements of which we call open sets, ${ }^{6}$ satisfying the following conditions:

(a) Whole set: the subset $X \subseteq X$ is open, i.e. $X \in \mathbf{O p}$.

(b) Binary intersections: if $U, V \in \mathbf{O p}$ then $(U \cap V) \in \mathbf{O p}$.

(c) Arbitrary unions: if $I$ is a set and if we are given an open set $U_{i} \in \mathbf{O p}$ for each $i$, then their union is also open, $\left(\bigcup_{i \in I} U_{i}\right) \in \mathbf{O p}$. We interpret the particular case where $I=\varnothing$ to mean that the empty set is open: $\varnothing \in \mathbf{O p}$.

If $U=U_{i \in I} U_{i}$, we say that $\left(U_{i}\right)_{i \in I}$ covers $U$.

A pair ( $X, \mathbf{O p})$, where $X$ is a set and $\mathbf{O p}$ is a topology on $X$, is called a topological space.

A continuous function between topological spaces $\left(X, \mathbf{O} \mathbf{p}_{X}\right)$ and $\left(Y, \mathbf{O} \mathbf{p}_{Y}\right)$ is a function $f: X \rightarrow Y$ such that for every $U \in \mathbf{O p}_{Y}$, the preimage $f^{-1}(U)$ is in $\mathbf{O} \mathbf{p}_{X}$.

At the very end of Section 7.3.1 we mentioned how sheaves differ from presheaves in that they take into account 'covering information.' The notion of covering an open set by a union of other open sets was defined in Definition 7.25, and it will come into play when we define sheaves in Definition 7.35.

Example 7.26. The usual topology Op on $\mathbb{R}^{2}$ is based on ' $\epsilon$-balls.' For any $\epsilon \in \mathbb{R}$ with $\epsilon>0$, and any point $p=(x, y) \in \mathbb{R}^{2}$, define the $\epsilon$-ball centered at $p$ to be:

$$
B(p ; \epsilon):=\left\{p^{\prime} \in \mathbb{R}^{2} \mid d\left(p, p^{\prime}\right)<\epsilon\right\}^{7}
$$
\footnotetext{
${ }^{6}$ In other words, we refer to a subset $U \subseteq X$ as open if $U \in \mathbf{O p}$.
}

In other words, $B(x, y ; \epsilon)$ is the set of all points within $\epsilon$ of $(x, y)$.

For an arbitrary subset $U \subseteq \mathbb{R}^{2}$, we call it open and put it in $O$ p if, for every $(x, y) \in U$ there exists a (small enough) $\epsilon>0$ such that $B(x, y ; \epsilon) \subseteq U$.

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-247.jpg?height=420&width=963&top_left_y=446&top_left_x=581)

The same idea works if we replace $\mathbb{R}^{2}$ with any other metric space $X$ (Definition 2.51): it can be considered as a topological space where the open sets are subsets $U$ such that for any $p \in U$ there is an $\epsilon$-ball centered at $p$ and contained in $U$. So every metric space can be considered as a topological space.

Exercise 7.27. Consider the set $\mathbb{R}$. It is a metric space with $d\left(x_{1}, x_{2}\right):=\left|x_{1}-x_{2}\right|$.

1. What is the 1-dimensional analogue of $\epsilon$-balls as found in Example 7.26? That is, for each $x \in \mathbb{R}$, define $B(x, \epsilon)$.

2. When is an arbitrary subset $U \subseteq \mathbb{R}$ called open, in analogy with Example 7.26?

3. Find three open sets $U_{1}, U_{2}$, and $U$ in $\mathbb{R}$, such that $\left(U_{i}\right)_{i \in\{1,2\}}$ covers $U$.

4. Find an open set $U$ and a collection $\left(U_{i}\right)_{i \in I}$ of opens sets where $I$ is infinite, such that $\left(U_{i}\right)_{i \in I}$ covers $U$.

Example 7.28. For any set $X$, there is a 'coarsest' topology, having as few open sets as possible: $\mathbf{O} \mathbf{p}_{\text {crse }}=(\varnothing, X)$. There is also a 'finest' topology, having as many open sets as possible: $\mathbf{O p}$ fine $=P(X)$. The latter, $(X, P(X))$ is called the discrete space on the set $X$.

\section*{Exercise 7.29 .}

1. Verify that for any set $X$, what we called $\mathbf{O} \mathbf{p}_{\text {crse }}$ in Example 7.28 really is a topology, i.e. satisfies the conditions of Definition 7.25.

2. Verify also that $\mathbf{O p}_{\text {fine }}$ really is a topology.

3. Show that if $(X, P(X))$ is discrete and $\left(Y, O p_{Y}\right)$ is any topological space, then every function $X \rightarrow Y$ is continuous.

Example 7.30. There are four topologies possible on $X=\{1,2\}$. Two are $\mathbf{O} \mathbf{p}_{\text {crse }}$ and

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-247.jpg?height=57&width=1385&top_left_y=2416&top_left_x=392)
One can generalize $d$ to any metric.
}
$\mathbf{O p}_{\text {fine }}$ from Example 7.28. The other two are:

$$
\mathbf{O p}_{1}:=\{\varnothing,\{1\}, X\} \quad \text { and } \quad \mathbf{O p}_{2}:=\{\varnothing,\{2\}, X\}
$$

The two topological spaces $\left(\{1,2\}, \mathbf{O} \mathbf{p}_{1}\right)$ and $\left(\{1,2\}, \mathbf{O p}_{2}\right)$ are isomorphic; either one can be called the Sierpinski space.

The open sets of a topological space form a preorder. Given a topological space $(X, \mathrm{Op})$, the set $\mathrm{Op}$ has the structure of a preorder using the subset relation, $(\mathbf{O p}, \subseteq)$. It is reflexive because $U \subseteq U$ for any $U \in \mathbf{O p}$, and it is transitive because if $U \subseteq V$ and $V \subseteq W$ then $U \subseteq W$

Recall from Section 3.2.3 that we can regard any preorder, and hence Op, as a category: its objects are the open sets $U$ and for any $U, V$ the set of morphisms $\mathbf{O p}(U, V)$ is empty if $U \nsubseteq V$ and it has one element if $U \subseteq V$.

Exercise 7.31. Recall the Sierpinski space, say $\left(X, \mathbf{O p}_{1}\right)$ from Example 7.30.

1. Write down the Hasse diagram for its preorder of opens.

2. Write down all the covers.

Exercise 7.32. Given any topological space ( $X, \mathrm{Op}$ ), any subset $Y \subseteq X$ can be given the

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-248.jpg?height=59&width=1442&top_left_y=1239&top_left_x=336)
if there is an open set $B \in \mathbf{O p}$ such that $A=B \cap Y$.

1. Find a $B \in \mathbf{O p}$ that shows that the whole set $Y$ is open, i.e. $Y \in \mathbf{O} \mathbf{p}_{? \cap Y}$.

2. Show that $\mathbf{O} \mathbf{p}_{\text {?nY }}$ is a topology in the sense of Definition 7.25.8

3. Show that the inclusion function $Y \hookrightarrow X$ is a continuous function.

Remark 7.33. Suppose $(X, \mathbf{O p})$ is a topological space, and consider the preorder $(\mathbf{O p}, \subseteq$ ) of open sets. It turns out that $(\mathbf{O p}, \subseteq, X, \cap$ ) is always a quantale in the sense of Definition 2.79. We will not need this fact, but we invite the reader to think about it a bit in Exercise 7.34.

Exercise 7.34. In Sections 2.3.2 and 2.3.3 we discussed how Bool-categories are preorders and Cost-categories are Lawvere metric spaces, and in Section 2.3.4 we imagined interpretations of $\mathcal{V}$-categories for other quantales $\mathcal{V}$.

If $(X, \mathrm{Op})$ is a topological space and $\mathcal{V}$ the corresponding quantale as in Remark 7.33, how might we imagine a $\mathcal{V}$-category?

\subsection*{7.3.3 Sheaves on topological spaces}

To summarize where we are, a topological space ( $X, \mathrm{Op}$ ) is a set $X$ together with a bunch of subsets we call 'open'; these open subsets form a preorder-and hence categorydenoted Op. Sheaves on $X$ will be presheaves on $\mathrm{Op}$ with a special property, aptly named the 'sheaf condition.'
\footnotetext{
${ }^{8}$ Hint 1: for any set $I$, collection of sets $\left(U_{i}\right)_{i \in I}$ with $U_{i} \subseteq X$, and set $V \subseteq X$, one has $\left(\cup_{i \in I} U_{i}\right) \cap V=$ $U_{i \in I}\left(U_{i} \cap V\right)$. Hint 2: for any $U, V, W \subseteq X$, one has $(U \cap W) \cap(V \cap W)=(U \cap V) \cap W$.
}

Recall the terminology and notation for presheaves: a presheaf on $\mathbf{O p}$ is a functor $P: \mathbf{O p}^{\text {op }} \rightarrow$ Set. Thus to every open set $U \in \mathbf{O p}$ we have a set $P(U)$, called the set of sections over $U$, and to every inclusion of open sets $V \subseteq U$ we have a function $P(U) \rightarrow P(V)$ called the restriction. If $s \in P(U)$ is a section over $U$, we may denote its restriction to $V$ by $\left.s\right|_{V}$. Recall that we say a collection of open sets $\left(U_{i}\right)_{i \in I}$ covers an open set $U$ if $U=U_{i \in I} U_{i}$.

We are now ready to give the following definition, which comes in several waves: we first define matching families, then gluing, then sheaf condition, then sheaf, and finally the category of sheaves.

Definition 7.35. Let $(X, \mathbf{O p})$ be a topological space, and let $P: \mathbf{O p}^{\mathrm{op}} \rightarrow$ Set be a presheaf on Op.

Let $\left(U_{i}\right)_{i \in I}$ be a collection of open sets $U_{i} \in \mathbf{O p}$ covering $U$. A matching family $\left(s_{i}\right)_{i \in I}$ of $P$-sections over $\left(U_{i}\right)_{i \in I}$ consists of a section $s_{i} \in P\left(U_{i}\right)$ for each $i \in I$, such that for every $i, j \in I$ we have

$$
\left.s_{i}\right|_{U_{i} \cap U_{j}}=\left.s_{j}\right|_{U_{i} \cap U_{j}}
$$

Given a matching family $\left(s_{i}\right)_{i \in I}$ for the cover $U=\bigcup_{i \in I} U_{i}$, we say that $s \in P(U)$ is a gluing, or glued section, of the matching family if $\left.s\right|_{U_{i}}=s_{i}$ holds for all $i \in I$.

If there exists a unique gluing $s \in P(U)$ for every matching family $\left(s_{i}\right)_{i \in I}$, we say that $P$ satisfies the sheaf condition for the cover $U=\bigcup_{i \in I} U_{i}$. If $P$ satisfies the sheaf condition for every cover, we say that $P$ is a sheaf on $(X, O p)$.

Thus a sheaf is just a presheaf satisfying the sheaf condition for every open cover. If $P$ and $Q$ are sheaves, then a morphism $f: P \rightarrow Q$ between these sheaves is just a morphism-that is, a natural transformation-between their underlying presheaves. We denote by $\operatorname{Shv}(X, \mathbf{O p})$ the category of sheaves on $X$.

The category of sheaves on $X$ is a topos, but we'll get to that.

Example 7.36. Here is a funny-but very important-special case to which the notion of matching family applies. We do not give this example for intuition, but because (to emphasize) it's an important and easy-to-miss case. Just like the sum of no numbers is 0 and the product of no numbers is 1 , the union of no sets is the empty set. Thus if we take $U=\varnothing \subseteq X$ and $I=\varnothing$, then the empty collection of subsets (one for each $i \in I$, of which there are none) covers $U$. In this case the empty tuple () counts a matching family of sections, and it is the only matching family for the empty cover of the empty set.

In other words, in order for a presheaf $P: \mathbf{O p}{ }^{\text {op }} \rightarrow$ Set to be a sheaf, a necessary (but rarely sufficient) condition is that $P(\varnothing) \cong\{()\}$, i.e. $P(\varnothing)$ must be a set with one element.

Extended example: sections of a function. This example is for intuition, and gives a case where the 'section' and 'restriction' terminology are easy to visualize.

Consider the function $f: X \rightarrow Y$ shown below, where each element of $X$ is sent to the element of $Y$ immediately below it. For example, $f\left(a_{1}\right)=f\left(a_{2}\right)=a, f\left(b_{1}\right)=b$, and so on.

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-250.jpg?height=456&width=778&top_left_y=455&top_left_x=671)

For each point $y \in Y$, the preimage set $f^{-1}(y) \subseteq X$ above it is often called the fiber over $y$. Note that different $f^{\prime}$ 's would arrange the eight elements of $X$ differently over $Y$ : elements of $Y$ would have different fibers.

Exercise 7.38. Consider the function $f: X \rightarrow Y$ shown in Eq. (7.37).

1. What is the fiber of $f$ over $a$ ?

2. What is the fiber of $f$ over $c$ ?

3. What is the fiber of $f$ over $d$ ?

4. Gave an example of a function $f^{\prime}: X \rightarrow Y$ for which every fiber has either one or two elements.

Let's consider $X$ and $Y$ as discrete topological spaces, so every subset is open, and $f$ is automatically continuous (see Exercise 7.29). We will think of $f$ as an arrangement of $X$ over $Y$, in terms of fibers as above, and use it to build a sheaf on $Y$. To do this, we begin by building a presheaf-i.e. a functor $\mathrm{Sec}_{f}: \mathbf{O p}(Y)^{\text {op }} \rightarrow$ Set-and then we'll prove it's a sheaf.

Define the presheaf $\operatorname{Sec}_{f}$ on an arbitrary subset $U \subseteq Y$ by:

$$
\operatorname{Sec}_{f}(U):=\left\{s: U \rightarrow X \mid\left(s \_f\right)(u)=u \text { for all } u \in U\right\}
$$

One might describe $\operatorname{Sec}_{f}(U)$ as the set of all ways to pick a 'cross-section' of the $f$ arrangement over $U$. That is, an element $s \in \operatorname{Sec}_{f}(U)$ is a choice of one element per fiber over $U$.

As an example, let's say $U=\{a, b\}$. How many such s's are there in $\operatorname{Sec}_{f}(U)$ ? To
answer this, let's clip the picture (7.37) and look only at the relevant part:
![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-251.jpg?height=420&width=1406&top_left_y=358&top_left_x=344)

Looking at the picture (7.39), do you see how we get all cross-sections of $f$ over $U$ ?

Exercise 7.40. Refer to Eq. (7.37).

1. Let $V_{1}=\{a, b, c\}$. Draw all the sections over it, i.e. all elements of $\operatorname{Sec}_{f}\left(V_{1}\right)$, as we did in Eq. (7.39).

2. Let $V_{2}=\{a, b, c, d\}$. Again draw all the sections, $\operatorname{Sec}_{f}\left(V_{2}\right)$.

3. Let $V_{3}=\{a, b, d, e\}$. How many sections (elements of $\left.\operatorname{Sec}_{f}\left(V_{3}\right)\right)$ are there? $\diamond$

By now you should understand the sections of $\operatorname{Sec}_{f}(U)$ for various $U \subseteq X$. This is $\mathrm{Sec}_{f}$ on objects, so you are half way to understanding $\mathrm{Sec}_{f}$ as a presheaf. That is, as a presheaf, $\operatorname{Sec}_{f}$ also includes a restriction maps for every subset $V \subseteq U$. Luckily, the restriction maps are easy: if $V \subseteq U$, say $V=\{a\}$ and $U=\{a, b\}$, then given a section $s$ as in Eq. (7.39), we get a section over $V$ by 'restricting' our attention to what $s$ does on $\{a\}$.

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-251.jpg?height=387&width=959&top_left_y=1590&top_left_x=583)

\section*{Exercise 7.42 .}

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-251.jpg?height=62&width=1071&top_left_y=2094&top_left_x=386)

2. Draw lines from the first to the second to indicate the restriction map.

Now we have understood $\operatorname{Sec}_{f}$ as a presheaf; we next explain how to see that it is a sheaf, i.e. that it satisfies the sheaf condition for every cover. To understand the sheaf condition, consider the set $U_{1}=\{a, b\}$ and $U_{2}=\{b, e\}$. These cover the set $U=\{a, b, e\}=U_{1} \cup U_{2}$. By Definition 7.35, a matching family for this cover consists of a section over $U_{1}$ and a section over $U_{2}$ that agree on the overlap set, $U_{1} \cap U_{2}=\{b\}$.

So consider $s_{1} \in \operatorname{Sec}_{f}\left(U_{1}\right)$ and $s_{2} \in \operatorname{Sec}_{f}\left(U_{2}\right)$ shown below.
![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-252.jpg?height=548&width=520&top_left_y=326&top_left_x=797)

Since sections $g_{1}$ and $g_{2}$ agree on the overlap-they both send $b$ to $b_{2}$-the two sections shown in Eq. (7.43) can be glued to form a single section over $U=\{a, b, e\}$ :

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-252.jpg?height=556&width=279&top_left_y=1023&top_left_x=912)

Exercise 7.44. Again let $U_{1}=\{a, b\}$ and $U_{2}=\{b, e\}$, so the overlap is $U_{1} \cap U_{2}=\{b\}$.

1. Find a section $s_{1} \in \operatorname{Sec}_{f}\left(U_{1}\right)$ and a section $s_{2} \in \operatorname{Sec}_{f}\left(U_{2}\right)$ that do not agree on the overlap.

2. For your answer $\left(s_{1}, s_{2}\right)$ in part 1 , can you find a section $s \in \operatorname{Sec}_{f}\left(U_{1} \cup U_{2}\right)$ such that $\left.s\right|_{U_{1}}=s_{1}$ and $\left.s\right|_{U_{2}}=s_{2}$ ?

3. Find a section $h_{1} \in \operatorname{Sec}_{f}\left(U_{1}\right)$ and a section $h_{2} \in \operatorname{Sec}_{f}\left(U_{2}\right)$ that do agree on the overlap, but which are different than our choice in Eq. (7.43).

4. Can you find a section $h \in \operatorname{Sec}_{f}\left(U_{1} \cup U_{2}\right)$ such that $\left.h\right|_{U_{1}}=h_{1}$ and $\left.h\right|_{U_{2}}=h_{2}$ ? $\diamond$

Other examples of sheaves. The extended example above generalizes to any continuous function $f: X \rightarrow Y$ between topological spaces.

Example 7.45. Let $f:\left(X, \mathbf{O p}_{X}\right) \rightarrow\left(Y, \mathbf{O} \mathbf{p}_{Y}\right)$ be a continuous function. Consider the functor $\mathrm{Sec}_{f}: \mathbf{O p} \mathbf{p}_{Y}^{\mathrm{op}} \rightarrow$ Set given by

$\operatorname{Sec}_{f}(U):=\{g: U \rightarrow X \mid g$ is continuous and $(g ; f)(u)=u$ for all $u \in U\}$,

The morphisms of $\mathbf{O} p_{Y}$ are inclusions $V \subseteq U$. Given $g: U \rightarrow X$ and $V \subseteq U$, what we call the restriction of $g$ to $V$ is the usual thing we mean by restriction, the same as it was in Eq. (7.41). One can again check that $\operatorname{Sec}_{f}$ is a sheaf.

Example 7.46. A nice example of a sheaf on a space $M$ is that of vector fields on $M$. If you calculate the wind velocity at every point on Earth, you will have what's called a vector field on Earth. If you know the wind velocity at every point in Afghanistan and I know the wind velocity at every point in Pakistan, and our calculations agree around the border, then we can glue our information together to get the wind velocity over the union of the two countries. All possible wind velocity fields over all possible open sets of the Earth's surface together form the sheaf of vector fields.

Let's say this a bit more formally. A manifold $M$-you can just imagine a sphere such as the Earth's surface-always has something called a tangent bundle. It is a space $T M$ whose points are pairs $(m, v)$, where $m \in M$ is a point in the manifold and $v$ is a tangent vector emanating from it. Here's a picture of one tangent plane-all the tangent vectors emanating from some fixed point-on a sphere:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-253.jpg?height=467&width=650&top_left_y=1176&top_left_x=735)

The tangent bundle $T M$ includes the whole tangent plane shown above-including the three vectors drawn on it-as well as the tangent plane at every other point on the sphere.

The tangent bundle $T M$ on a manifold $M$ comes with a continuous map $\pi: T M \rightarrow$ $M$ back down to the manifold, sending $(m, v) \mapsto m$. One might say that $\pi$ "forgets the tangent vector and just remembers the point it emanated from." By Example 7.45, $\pi$ defines a sheaf $\operatorname{Sec}_{\pi}$. It could be called the sheaf of 'tangent vector sections on $M^{\prime}$ ', but its usual name is the sheaf of vector fields on $M$. This is what we were describing when we spoke of the sheaf of wind velocities on Earth, above. Given an open subset $U \subseteq M$, an element $v \in \operatorname{Sec}_{\pi}(U)$ is called a vector field over $U$ because it continuously assigns a tangent vector $v(u)$ to each point $u \in U$. The tangent vector at $u$ tells us the velocity of the wind at that point.

Here's a fun digression: in the case of a spherical manifold $M$ like the Earth, it's possible to prove that for every open set $U$, as long as $U \neq M$, there is a vector field
$v \in \operatorname{Sec}_{\pi}(U)$ that is never 0 : the wind could be blowing throughout $U$. However, a theorem of Poincaré says that if you look at the whole sphere, there is guaranteed to be a point $m \in M$ at which the wind is not blowing at all. It's like the eye of a hurricane or perhaps a cowlick. A cowlick in someone's hair occurs when the hair has no direction to go, so it sticks up! Hair sticking up would not count as a tangent vector: tangent vectors must start out lying flat along the head. Poincaré proved that if your head was covered completely with inch-long hair, there would be at least one cowlick. This difference between local sections (over arbitrary $U \subseteq X$ ) and global sections (over $X$ )namely that hair can be well combed whenever $U \neq X$ but cannot be well combed when $U=X$-can be thought of as a generative effect, and can be measured by cohomology (see Section 1.5).

Exercise 7.47. If $M$ is a sphere as in Example 7.46, we know from Definition 7.35 that we can consider the category $\operatorname{Shv}(M)$ of sheaves on $M$; in fact, such categories are toposes and these are what we're getting to.

But are the sheaves on $M$ the vector fields? That is, is there a one-to-one correspondence between sheaves on $M$ and vector fields on $M$ ? If so, why? If not, how are sheaves on $M$ and vector fields on $M$ related?

Example 7.48. For every topological space ( $\mathrm{X}, \mathrm{Op}$ ), we have the topos of sheaves on it. The topos of sets, which one can regard as the story of set theory, is the category of sheaves on the one-point space $\{*\}$. In topos theory, we see the category of sets-an huge, amazing, and rich category-as corresponding to a single point. Imagine how much more complex arbitrary toposes are, when they can take place on much more interesting topological spaces (and in fact even more general 'sites').

Exercise 7.49. Consider the Sierpinski space $\left(\{1,2\}, \mathbf{O p}_{1}\right)$ from Example 7.30.

1. What is the category $\mathrm{Op}$ for this space? (You may have already figured this out in Exercise 7.31; if not, do so now.)

2. What does a presheaf on Op consist of?

3. What is the sheaf condition for $\mathrm{Op}$ ?

4. How do we identify a sheaf on Op with a function?

\subsection*{7.4 Toposes}

A topos is defined to be a category of sheaves. ${ }^{9}$ So for any topological space $(X, \mathbf{O p})$, the category $\operatorname{Shv}(X, \mathbf{O p})$ defined in Definition 7.35 is a topos. In particular, taking the one-point space $X=\mathbf{1}$ with its unique topology, we find that the category Set is a topos, as we've been saying all along and saw again explicitly in Example 7.48. And for any
\footnotetext{
${ }^{9}$ This is sometimes called a sheaf topos or a Grothendieck topos. There is a more general sort of topos called an elementary topos due to Lawvere.
}
database schema-i.e. finitely presented category- $\mathcal{C}$, the category $\mathcal{C}$-Inst of database instances on $\mathcal{C}$ is also a topos. ${ }^{10}$ Toposes encompass both of these sources of examples, and many more.

Toposes are incredibly nice structures, for a variety of seemingly disparate reasons. In this sketch, the reason in focus is that every topos has many of the same structural properties that the category Set has. Indeed, we discussed in Section 7.2.1 that every topos has limits and colimits, is cartesian closed, has epi-mono factorizations, and has a subobject classifier (see Section 7.2.2). Using these properties, one can do logic with semantics in the topos $\mathcal{E}$. We explained this for sets, but now imagine it for sheaves on a topological space. There, the same logical symbols $\wedge, \vee, \neg, \Rightarrow, \exists, \forall$ become operations that mean something about sub-sheaves-e.g. vector fields, sections of continuous functions, etc.-not just subsets.

To understand this more deeply, we should say what the subobject classifier true: $1 \rightarrow$ $\Omega$ is in more generality. We said that, in the topos Set, the subobject classifier is the set of booleans $\Omega=\mathbb{B}$. In a sheaf topos $\mathcal{E}=\operatorname{Shv}(X, O p)$, the object $\Omega \in \mathcal{E}$ is a sheaf, not just a set. What sheaf is it?

\subsection*{7.4.1 The subobject classifier $\Omega$ in a sheaf topos}

In this subsection we aim to understand the subobject classifier $\Omega$, i.e. the object of truth values, in the sheaf topos $\operatorname{Shv}(X, \mathbf{O p})$. Since $\Omega$ is a sheaf, let's understand it by going through the definition of sheaf (Definition 7.35) slowly in this case. A sheaf $\Omega$ is a presheaf that satisfies the sheaf condition. As a presheaf it is just a functor $\Omega: \mathbf{O p}{ }^{\text {op }} \rightarrow$ Set; it assigns a set $\Omega(U)$ to each open $U \subseteq X$ and comes with a restriction map $\Omega(U) \rightarrow \Omega(V)$ whenever $V \subseteq U$. So in our quest to understand $\Omega$, we first ask the question: what presheaf is it?

The answer to our question is that $\Omega$ is the presheaf that assigns to $U \in \mathbf{O p}$ the set of open subsets of $U$ :

$$
\begin{equation*}
\Omega(U):=\left\{U^{\prime} \in \mathbf{O p} \mid U^{\prime} \subseteq U\right\} \tag{7.50}
\end{equation*}
$$

That was easy, right? And given the restriction map for $V \subseteq U$ is given by

$$
\begin{align*}
\Omega(U) & \rightarrow \Omega(V)  \tag{7.51}\\
U^{\prime} & \mapsto U^{\prime} \cap V .
\end{align*}
$$

One can check that this is functorial—see Exercise 7.53—and after doing so we will still need to see that it satisfies the sheaf condition. But at least we don't have to struggle to understand $\Omega$ : it's a lot like Op itself.
\footnotetext{
${ }^{10}$ We said that a topos is a category of sheaves, yet database instances are presheaves; so how is $\mathcal{C}$-Inst a topos? Well, presheaves in fact count as sheaves. We apologize that this couldn't be clearer. All of this could be made formal if we were to introduce sites. Unfortunately, that concept is simply too abstract for the scope of this chapter.
}

Exercise 7.52. Let $X=\{1\}$ be the one point space. We said above that its subobject classifier is the set $\mathbb{B}$ of booleans, but how does that align with the definition of $\Omega$ given in Eq. (7.50)?

\section*{Exercise 7.53 .}

1. Show that the definition of $\Omega$ given above in Eqs. (7.50) and (7.51) is functorial, i.e., that whenever $W \subseteq V \subseteq U$, the restriction map $\Omega(U) \rightarrow \Omega(V)$ followed by the restriction map $\Omega(V) \rightarrow \Omega(W)$ is the same as the restriction map $\Omega(U) \rightarrow \Omega(W)$.

2. Is that all that's necessary to conclude that $\Omega$ is a presheaf?

To see that $\Omega$ as defined in Eq. (7.50) satisfies the sheaf condition (see Definition 7.35), suppose that we have a cover $U=\bigcup_{i \in I} U_{i}$, and suppose given an element $V_{i} \in \Omega\left(U_{i}\right)$, i.e. an open set $V_{i} \subseteq U_{i}$, for each $i \in I$. Suppose further that for all $i, j \in I$, it is the case that $V_{i} \cap U_{j}=V_{j} \cap U_{i}$, i.e. that the elements form a matching family. Define $V:=\bigcup_{i \in I} V_{i}$; it is an open subset of $U$, so we can consider $V$ as an element of $\Omega(U)$. The following verifies that $V$ is indeed a gluing for the $\left(V_{i}\right)_{i \in I}$ :

$$
V \cap U_{j}=\left(\bigcup_{i \in I} V_{i}\right) \cap U_{j}=\bigcup_{i \in I}\left(V_{i} \cap U_{j}\right)=\bigcup_{i \in I}\left(V_{j} \cap U_{i}\right)=\left(\bigcup_{i \in I} U_{i}\right) \cap V_{j}=V_{j}
$$

In other words $V \cap U_{j}=V_{j}$ for any $j \in I$. So our $\Omega$ has been upgraded from presheaf to sheaf!

The eagle-eyed reader will have noticed that we haven't yet given all the data needed to define a subobject classifier. To turn the object $\Omega$ into a subobject classifier in good standing, we also need to give a sheaf morphism true: $\{1\} \rightarrow \Omega$. Here $\{1\}: \mathbf{O p}^{\mathrm{op}} \rightarrow$ Set is the terminal sheaf; it maps every open set to the terminal, one element set $\{1\}$. The correct morphism true: $\{1\} \rightarrow \Omega$ for the subobject classifier is the sheaf morphism that assigns, for every $U \in \mathbf{O p}$ the function $\{1\}=\{1\}(U) \rightarrow \Omega(U)$ sending $1 \mapsto U$, the largest open set $U \subseteq U$. From now on we denote $\{1\}$ simply as 1 .

Upshot: Truth values are open sets. The point is that the truth values in the topos of sheaves on a space $(X, \mathbf{O p})$ are the open sets of that space. When someone says "is property $P$ true?" the answer is not yes or no, but "it is true on the open subset $U$." If this $U$ is everything, $U=X$, then $P$ is really true; if $U$ is nothing, $U=\varnothing$, then $P$ is really false. But in general, it's just true some places and not others.

Example 7.54. The category Grph of graphs is a presheaf topos, and one can also think of it as the category of instances for a database schema, as we saw in Example 7.23. The subobject classifier $\Omega$ in the topos $\mathrm{Gr}$ is thus a graph, so we can draw it. Here's what it
looks like:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-257.jpg?height=278&width=725&top_left_y=265&top_left_x=689)

Finding $\Omega$ for oneself is easiest using something called the Yoneda Lemma, but we have not introduced it. For a nice, easy introduction to the topos of graphs, see [Vig03]. The terminal graph is a single vertex with a single loop, and the graph homomorphism true: $1 \rightarrow \Omega$ sends that loop to $(V, V ; A)$.

Given any graph $G$ and subgraph $i: H \subseteq G$, we need to construct a graph homomorphism $\ulcorner H\urcorner: G \rightarrow \Omega$ classifying $H$. The idea is that for each part of $G$, we decide "how much of it is in $H$. A vertex in $v$ in $G$ is either in $H$ or not; if so we send it to $V$ and if not we send it to 0 . But arrows $a$ are more complicated. If $a$ is in $H$, we send it $(V, V ; A)$. But if it is not in $H$, the mathematics requires us to ask more questions: is its source in $H$ ? is its target in $G^{\prime \prime}$ ? both? neither? Based on the answers to these questions we send $a$ to $(V, 0 ; 0),(0, V ; 0),(V, V ; 0)$, or $(0,0 ; 0)$, respectively.

Exercise 7.55. Consider the subgraph $H \subseteq G$ shown here:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-257.jpg?height=144&width=1039&top_left_y=1240&top_left_x=543)

Find the graph homomorphism $\ulcorner H\urcorner: G \rightarrow \Omega$ classifying it. See Example 7.54.

\subsection*{7.4.2 Logic in a sheaf topos}

Let's consider the logical connectives, AND, OR, IMPLIES, and NOT. Suppose we have a topological space $X \in$ Op. Given two open sets $U, V$, considered as truth values $U, V \in \Omega(X)$, then their conjunction ' $U$ AND $V^{\prime}$ ' is their intersection, and their disjunction ' $U$ OR $V$ ' is their union;

$$
\begin{equation*}
(U \wedge V):=U \cap V \quad \text { and } \quad(U \vee V):=U \cup V \tag{7.56}
\end{equation*}
$$

These formulas are easy to remember, because $\wedge$ looks like $\cap$ and $\vee$ looks like $\cup$. The implication $U \Rightarrow V$ is the largest open set $R$ such that $R \cap U \subseteq V$, i.e.

$$
\begin{equation*}
(U \Rightarrow V):=\bigcup_{\{R \in \mathbf{O p} \mid R \cap U \subseteq V\}} R \tag{7.57}
\end{equation*}
$$

In general, it is not easy to reduce Eq. (7.57) further, so implication is the hardest logical connective to think about topologically.

Finally, the negation of $U$ is given by $\neg U:=(U \Rightarrow$ false $)$, and this turns out to be relatively simple. By the formula in Eq. (7.57), it is the union of all $R$ such that $R \cap U=\varnothing$, i.e. the union of all open sets in the complement of $U$. If you know topology, you might recognize that $\neg U$ is the 'interior of the complement of $U$.'

Example 7.58. Consider the real line $X=\mathbb{R}$ as a topological space (see Exercise 7.27). Let $U, V \in \Omega(X)$ be the open sets $U=\{x \in \mathbb{R} \mid x<3\}$ and $V=\{x \in \mathbb{R} \mid-4<x<4\}$. Using interval notation, $U=(-\infty, 3)$ and $V=(-4,4)$. Then
- $U \wedge V=(-4,3)$.
- $U \vee V=(-\infty, 4)$
- $\neg U=(3, \infty)$.
- $\neg V=(-\infty,-4) \cup(4, \infty)$.
- $(U \Rightarrow V)=(-4, \infty)$
- $(V \Rightarrow U)=U$

Exercise 7.59. Consider the real line $\mathbb{R}$ as a topological space, and consider the open subset $U=\mathbb{R}-\{0\}$.

1. What open subset is $\neg U$ ?

2. What open subset is $\neg \neg U$ ?

3. Is it true that $U \subseteq \neg \neg U$ ?

4. Is it true that $\neg \neg U \subseteq U$ ?

Above we explained operations on open sets, one corresponding to each logical connective; there are also open sets corresponding to the the symbols true and false. We explore this in an exercise.

Exercise 7.60. Let $(X, \mathrm{Op})$ be a topological space.

1. Suppose the symbol true corresponds to an open set such that for any open set $U \in \mathbf{O p}$, we have $($ true $\wedge U)=U$. Which open set is it?

2. Other things we should expect from true include (true $\vee U)=$ true and $(U \Rightarrow$ true) $=$ true and $(\operatorname{true} \Rightarrow U)=U$. Do these hold for your answer to 1 ?

3. The symbol false corresponds to an open set $U \in \mathbf{O p}$ such that for any open set $U \in \mathbf{O p}$, we have (false $\vee U)=U$. Which open set is it?

4. Other things we should expect from false include (false $\wedge U$ ) $=$ false and (false $\Rightarrow U$ ) = true. Do these hold for your answer to 1 ?

Example 7.61. For a vector bundle $\pi: E \rightarrow X$ over a space $X$, the corresponding sheaf is $\mathrm{Sec}_{\pi}$ corresponding to its sections: to each open set $i_{U}: U \subseteq X$, we associate the set of functions $s: U \rightarrow E$ for which $s ; \pi=i_{U}$. For example, in the case of the tangent bundle $\pi: T M \rightarrow M$ (see Example 7.46), the corresponding sheaf, call it VF, associates to each $U$ the set $\operatorname{VF}(U)$ of vector fields on $U$.

The internal logic of the topos can then be used to consider properties of vector fields. For example, one could have a predicate Grad: VF $\rightarrow \Omega$ that asks for the largest subspace $\operatorname{Grad}(v)$ on which a given vector field $v$ comes from the gradient of some scalar function. One could also have a predicate that asks for the largest open set on which a vector field is non-zero. Logical operations like $\wedge$ and $\vee$ could then be applied to hone in on precise submanifolds throughout which various desired properties hold,
and to reason logically about what other properties are forced to hold there.

\subsection*{7.4.3 Predicates}

In English, a predicate is the part of the sentence that comes after the subject. For example "...is even" or "... likes the weather" are predicates. Not every subject makes sense for a given predicate; e.g. the sentence " 7 is even" may be false, but it makes sense. In contrast, the sentence " 2.7 is even" does not really make sense, and "2.7 likes the weather" certainly doesn't. In computer science, they might say "The expression ' 2.7 likes the weather' does not type check."

The point is that each predicate is associated to a type, namely the type of subject that makes sense for that predicate. When we apply a predicate to a subject of the appropriate type, the result has a truth value: "7 is even" is either true or false. Perhaps "Bob likes the weather" is true some days and false on others. In fact, this truth value might change by the year (bad weather this year), by the season, by the hour, etc. In English, we expect truth values of sentences to change over time, which is exactly the motivation for this chapter. We're working toward a logic where truth values change over time.

In a topos $\mathcal{E}=\mathbf{S h v}(X, \mathbf{O p})$, a predicate is a sheaf morphism $p: S \rightarrow \Omega$ where $S \in \mathcal{E}$ is a sheaf and $\Omega \in \mathcal{E}$ is the subobject classifier, the sheaf of truth values. By Definition 7.35 we get a function $p(U): S(U) \rightarrow \Omega(U)$ for any open set $U \subseteq X$. In the above examplewhich we will discuss more carefully in Section 7.5-if $S$ is the sheaf of people (people come and go over time), and Bob $\in S(U)$ is a person existing over a time $U$, and $p$ is the predicate "likes the weather," then $p(\mathrm{Bob})$ is the set of times during which Bob likes the weather. So the answer to "Bob likes the weather" is something like "in summers yes, and also in April 2018 and May 2019 yes, but in all other times no." That's $p$ (Bob), the temporal truth value obtained by applying the predicate $p$ to the subject Bob.

Exercise 7.62. Just now we described how a predicate $p: S \rightarrow \Omega$, such as "... likes the weather," acts on sections $s \in S(U)$, say $s=$ Bob. But by Definition 7.12, any predicate $p: S \rightarrow \Omega$ also defines a subobject of $\{S \mid p\} \subseteq S$. Describe the sections of this subsheaf.

The poset of subobjects. For a topos $\mathcal{E}=\operatorname{Shv}(X, \mathbf{O p})$ and object (sheaf) $S \in \mathcal{E}$, the set of $S$-predicates $\left|\Omega^{E}\right|=\mathcal{E}(S, \Omega)$ is naturally given the structure of a poset, which we denote

$$
\begin{equation*}
\left(\left|\Omega^{S}\right|, \leq^{S}\right) \tag{7.63}
\end{equation*}
$$

Given two predicates $p, q: S \rightarrow \Omega$, we say that $p \leq^{S} q$ if the first implies the second. More precisely, for any $U \in \mathbf{O p}$ and section $s \in S(U)$ we obtain two open subsets $p(s) \subseteq U$ and $q(s) \subseteq U$. We say that $p \leq^{S} q$ if $p(s) \subseteq q(s)$ for all $U \in \mathbf{O p}$ and $s \in S(U)$. We often drop the superscript from $\leq^{S}$ and simply write $\leq$. In formal logic notation,
one might write $p \leq^{S} q$ using the $\vdash$ symbol, e.g. in one of the following ways:

$$
s: S \mid p(s) \vdash q(s) \quad \text { or } \quad p(s) \vdash_{s: S} q(s)
$$

In particular, if $S=1$ is the terminal object, we denote $\left|\Omega^{S}\right|$ by $|\Omega|$, and refer to elements $p \in|\Omega|$ as propositions. They are just morphisms $p: 1 \rightarrow \Omega$.

This preorder is partially ordered-a poset-meaning that if $p \leq q$ and $q \leq p$ then $p=q$. The reason is that for any subsets $U, V \subseteq X$, if $U \subseteq V$ and $V \subseteq U$ then $U=V$.

Exercise 7.64. Give an example of a space $X$, a sheaf $S \in \operatorname{Shv}(X)$, and two predicates $p, q: S \rightarrow \Omega$ for which $p(s) \vdash_{s: S} q(s)$ holds. You do not have to be formal.

All of the logical symbols (true, false, $\wedge, \vee, \Rightarrow, \neg$ ) from Section 7.4.2 make sense in any such poset $\left|\Omega^{S}\right|$. For any two predicates $p, q: S \rightarrow \Omega$, we define $(p \wedge q): S \rightarrow \Omega$ by $(p \wedge q)(s):=p(s) \wedge q(s)$, and similarly for $\vee$. Thus one says that these operations are computed pointwise on $S$. With these definitions, the $\wedge$ symbol is the meet and the $\vee$ symbol is the join—in the sense of Definition 1.81-for the poset $\left|\Omega^{S}\right|$.

With all of the logical structure we've defined so far, the poset $\left|\Omega^{S}\right|$ of predicates on $S$ forms what's called a Heyting algebra. We will not define it here, but more information can be found in Section 7.6. We now move on to quantification.

\subsection*{7.4.4 Quantification}

Quantification comes in two flavors: universal and existential, or 'for all' and 'there exists.' Each takes in a predicate of $n+1$ variables and returns a predicate of $n$ variables.

Example 7.65. Suppose we have two sheaves $S, T \in \mathbf{S h v}(X, \mathbf{O p})$ and a predicate $p: S \times$ $T \rightarrow \Omega$. Let's say $T$ represents what's considered newsworthy and $S$ is again the set of people. So for a subset of time $U$, a section $t \in T(U)$ is something that's considered newsworthy throughout the whole of $U$, and a section $s \in S(U)$ is a person that lasts throughout the whole of $U$. Let's imagine the predicate $p$ as " $s$ is worried about $t$." Now recall from Section 7.4.3 that a predicate $p$ does not simply return true or false; given a person $s$ and a news-item $t$, it returns a truth value corresponding to the subset of times on which $p(s, t)$ is true.

"For all $t$ in $T, \ldots$ is worried about $t$ " is itself a predicate on just one variable, $S$, which we denote

$$
\forall(t: T) \cdot p(s, t)
$$

Applying this predicate to a person $s$ returns the times when that person is worried about everything in the news. Similarly, "there exists $t$ in $T$ such that $s$ is worried about $t^{\prime \prime}$ is also a predicate on $S$, which we denote $\exists(t: T)$. $p(s, t)$. If we apply this predicate to a person $s$, we get the times when person $s$ is worried about at least one thing in the news.

Exercise 7.66. In the topos Set, where $\Omega=\mathbb{B}$, consider the predicate $p: \mathbb{N} \times \mathbb{Z} \rightarrow \mathbb{B}$ given by

$$
p(n, z)= \begin{cases}\text { true } & \text { if } n \leq|z| \\ \text { false } & \text { if } n>|z|\end{cases}
$$

1. What is the set of $n \in \mathbb{N}$ for which the predicate $\forall(z: \mathbb{Z}) \cdot p(n, z)$ holds?

2. What is the set of $n \in \mathbb{N}$ for which the predicate $\exists(z: \mathbb{Z}) . p(n, z)$ holds?

3. What is the set of $z \in \mathbb{Z}$ for which the predicate $\forall(n: \mathbb{N})$. $p(n, z)$ holds?

4. What is the set of $z \in \mathbb{Z}$ for which the predicate $\exists(n: \mathbb{N})$. $p(n, z)$ holds?

So given $p$, we have a universally- and an existentially-quantified predicate $\forall(t$ : $T)$. $p(s, t)$ and $\exists(t: T)$. $p(s, t)$ on $S$. How do we formally understand them as sheaf morphisms $S \rightarrow \Omega$ or, equivalently, as subsheaves of $S$ ?

Universal quantification. Given a predicate $p: S \times T \rightarrow \Omega$, the universally-quantified predicate $\forall(t: T)$. $p(s, t)$ takes a section $s \in S(U)$, for any open set $U$, and returns a certain open set $V \in \Omega(U)$. Namely, it returns the largest open set $V \subseteq U$ for which $p\left(\left.s\right|_{V}, t\right)=V$ holds for all $t \in T(V)$.

Exercise 7.67. Suppose $s$ is a person alive throughout the interval $U$. Apply the above definition to the example $p(s, t)=$ "person $s$ is worried about news $t$ " from Example 7.65. Here, $T(V)$ is the set of items that are in the news throughout the interval $V$.

1. What open subset of $U$ is $\forall(t: T)$. $p(s, t)$ for a person $s$ ?

2. Does it have the semantic meaning you'd expect, given the less formal description in Section 7.4.4?

Abstractly speaking, the universally-quantified predicate corresponds to the subsheaf given by the following pullback:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-261.jpg?height=219&width=311&top_left_y=1712&top_left_x=907)

where $p^{\prime}: S \rightarrow \Omega^{T}$ is the currying of $S \times T \rightarrow \Omega$ and true ${ }^{T}$ is the currying of the composite $1 \times T \xrightarrow{!} 1 \xrightarrow{\text { true }} \Omega$. See Eq. (7.10).

Existential quantification. Given a predicate $p: S \times T \rightarrow \Omega$, the existentially quantified predicate $\exists(t: T)$. $p(s, t)$ takes a section $s \in S(U)$, for any open set $U$, and returns a certain open set $V \in \Omega(U)$, namely the union $V=U_{i} V_{i}$ of all the open sets $V_{i}$ for which there exists some $t_{i} \in T\left(V_{i}\right)$ satisfying $p\left(\left.s\right|_{V_{i}}, t_{i}\right)=V_{i}$. If the result is $U$ itself, you might be tempted to think "ah, so there exists some $t \in T(U)$ satisfying $p(t)$," but that is not necessarily so. There is just a cover of $U=U U_{i}$ and local sections $t_{i} \in T\left(U_{i}\right)$, each satisfying $p$, as explained above. Thus the existential quantifier is doing a lot of
work "under the hood," taking coverings into account without displaying that fact in the notation.

Exercise 7.68. Apply the above definition to the "person $s$ is worried about news $t$ " predicate from Example 7.65.

1. What open set is $\exists(t: T)$. $p(s, t)$ for a person $s$ ?

2. Does it have the semantic meaning you'd expect?

Abstractly speaking, the existentially-quantified predicate is given as follows. Start with the subobject classified by $p$, namely $\{(s, t) \in S \times T \mid p(s, t)\} \subseteq S \times T$, compose with the projection $\pi_{S}: S \times T \rightarrow S$ as on the upper right; then take the epi-mono factorization of the composite as on the lower left:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-262.jpg?height=195&width=417&top_left_y=851&top_left_x=843)

Then the bottom map is the desired subsheaf of $S$.

\subsection*{7.4.5 Modalities}

Back in Example 1.123 we discussed modal operators-also known as modalitiessaying they are closure operators on preorders which arise in logic. The preorders we were referring to are the ones discussed in Eq. (7.63): for any object $S \in \mathcal{E}$ there is the poset $\left(\left|\Omega^{S}\right|, \leq^{S}\right)$ of predicates on $S$, where $\left|\Omega^{S}\right|=\mathcal{E}(S, \Omega)$ is just the set of morphisms $S \rightarrow \Omega$ in the category $\mathcal{E}$.

Definition 7.69. A modality in $\operatorname{Shv}(X)$ is a sheaf morphism $j: \Omega \rightarrow \Omega$ satisfying three properties for all $U \subseteq X$ and $p, q \in \Omega(U)$ :

(a) $p \leq j(p)$;

(b) $(j \circ j)(p) \leq j(p)$; and

(c) $j(p \wedge q)=j(p) \wedge j(q)$.

Exercise 7.70. Suppose $j: \Omega \rightarrow \Omega$ is a morphism of sheaves on $X$, such that $p \leq j(p)$ holds for all $U \subseteq X$ and $p \in \Omega(U)$. Show that for all $q \in \Omega(U)$ we have $j(j(q)) \leq j(q)$ iff $j(j(q))=j(q)$.

In Example 1.123 we informally said that for any proposition $p$, e.g. "Bob is in San Diego," there is a modal operator "assuming $p, . .$. " Now we are in a position to make that formal.

Proposition 7.71. Fix a proposition $p \in|\Omega|$. Then

(a) the sheaf morphism $\Omega \rightarrow \Omega$ given by sending $q$ to $p \Rightarrow q$ is a modality.

(b) the sheaf morphism $\Omega \rightarrow \Omega$ given by sending $q$ to $p \vee q$ is a modality.

(c) the sheaf morphism $\Omega \rightarrow \Omega$ given by sending $q$ to $(q \Rightarrow p) \Rightarrow p$ is a modality.

We cannot prove Proposition 7.71 here, but we give references in Section 7.6.

Exercise 7.72. Let $S$ be the sheaf of people as in Section 7.4.3, and let $j: \Omega \rightarrow \Omega$ be "assuming Bob is in San Diego..."

1. Name any predicate $p: S \rightarrow \Omega$, such as "likes the weather."

2. Choose a time interval $U$. For an arbitrary person $s \in S(U)$, what sort of thing is $p(s)$, and what does it mean?

3. What sort of thing is $j(p(s))$ and what does it mean?

4. Is it true that $p(s) \leq j(p(s))$ ? Explain briefly.

5. Is it true that $j(j(p(s)))=j(p(s))$ ? Explain briefly.

6. Choose another predicate $q: S \rightarrow \Omega$. Is it true that $j(p \wedge q)=j(p) \wedge j(q)$ ? Explain briefly.

\subsection*{7.4.6 Type theories and semantics}

We have been talking about the logic of a topos in terms of open sets, but this is actually a conflation of two ideas that are really better left unconflated. The first is logic, or formal language, and the second is semantics, or meaning. The formal language looks like this:

$$
\begin{equation*}
\forall(t: T) . \exists(s: S) . f(s)=t \tag{7.73}
\end{equation*}
$$

and semantic statements are like "the sheaf morphism $f: S \rightarrow T$ is an epimorphism." In the former, logical world, all statements are linguistic expressions formed according to strict rules and all proofs are deductions that also follow strict rules. In the latter, semantic world, statements and proofs are about the sheaves themselves, as mathematical objects. We admit these are rough statements; again, our aim here is only to give a taste, an invitation to further reading.

To provide semantics for a logical system means to provide a compiler that converts each logical statement in the formal language into a mathematical statement about particular sheaves and their relationships. A computer can carry out logical deductions without knowing what any of them "mean" about sheaves. We say that semantics is sound if every formal proof is converted into a true fact about the relevant sheaves.

Every topos can be assigned a formal language, often called its internal language, in which to carry out constructions and formal proofs. This language has a sound semantics-a sort of logic-to-sheaf compiler-which goes under the name categorical semantics or Kripke-Joyal semantics. We gave the basic ideas in Section 7.4; we give references to the literature in Section 7.6.

Example 7.74. In every topos $\mathcal{E}$, and for every $f: S \rightarrow T$ in $\mathcal{E}$, the morphism $f$ is an epimorphism if and only if Eq. (7.73) holds. For example, consider the case of database instances on a schema $\mathcal{C}$, say with 100 tables (one of which might be denoted $c \in \operatorname{Ob}(\mathcal{C})$ ) and 500 foreign key columns (one of which might be denoted $f: c \rightarrow c^{\prime}$ in $\mathcal{C}$ ); see Eq. (3.2).

If $S$ and $T$ are two instances and $f$ is a natural transformation between them, then we can ask the question of whether or not Eq. (7.73) holds. This simple formula is compiled by the Kripke-Joyal semantics into asking:

Is it true that for every table $c \in \mathrm{Ob}(\mathcal{C})$ and every row $s \in S(c)$ there exists a row $t \in T(c)$ such that $f(s)=t$ ?

This is exactly what it means for $f$ to be surjective. Maybe this is not too impressive, but whether one is talking about databases or topological spaces, or complex ideas from algebraic geometry, Eq. (7.73) always compiles into the question of surjectivity. For topological spaces it would say something like:

Is it true that for every open set $U \subseteq X$ and every section $s \in S(U)$ of the bundle $S$, there exists an open covering of $\left(U_{i} \subseteq U\right)_{i \in I}$ of $U$ and a section $t_{i} \in T\left(U_{i}\right)$ of the bundle $T$ for each $i \in I$, such that $f\left(t_{i}\right)=\left.s\right|_{U_{i}}$ is the restriction of $s$ to $U_{i}$ ?

\subsection*{7.5 A topos of behavior types}

Now that we have discussed logic in a sheaf topos, we return to our motivating example, a topos of behavior types. We begin by discussing the topological space on which behavior types will be sheaves, a space called the interval domain.

Remark 7.75. Note that above, we were thinking very intuitively about time, e.g. when we discussed people being worried about the news. Now we will be thinking about time in a different way, but there is no need to change your answers or reconsider the intuitive thinking done above.

\subsection*{7.5.1 The interval domain}

The interval domain $\mathbb{I} \mathbb{R}$ is a specific topological space, which we will use to model intervals of time. In other words, we will be interested in the category $\operatorname{Shv}(\mathbb{I} \mathbb{R})$ of sheaves on the interval domain.

To give a topological space, one must give a pair $(X, \mathbf{O p})$, where $X$ is a set of 'points' and $\mathrm{Op}$ is a topology on $X$; see Definition 7.25. The set of points for $\mathbb{I} \mathbb{R}$ is that of all finite closed intervals

$$
\mathbb{R} \mathbb{R}:=\{[d, u] \subseteq \mathbb{R} \mid d \leq u\}
$$

For $a<b$ in $\mathbb{R}$, let $o_{[a, b]}$ denote the set $o_{[a, b]}:=\{[d, u] \in \mathbb{R} \mid a<d \leq u<b\}$; these are called basic open sets. The topology $\mathbf{O p}$ is determined by these basic open sets in that a subset $U$ is open if it is the union of some collection of basic open sets.

Thus for example, $o_{[0,5]}$ is an open set: it contains every $[d, u]$ contained in the open interval $\{x \in \mathbb{R} \mid 0<x<5\}$. Similarly $o_{[4,8]}$ is an open set, but note that $o_{[0,5]} \cup o_{[4,8]} \neq o_{[0,8]}$. Indeed, the interval [2,6] is in the right-hand side but not the left. Exercise 7.76 .

1. Explain why $[2,6] \in o_{[0,8]}$.

2. Explain why $[2,6] \notin o_{[0,5]} \cup o_{[4,8]}$.

Let Op denote the open sets of $\mathbb{I} \mathbb{R}$, as described above, and let BT := Shv $(\mathbb{R} \mathbb{R}, \mathbf{O p})$ denote the topos of sheaves on this space. We call it the topos of behavior types.

There is an important subspace of $\mathbb{R} \mathbb{R}$, namely the usual space of real numbers $\mathbb{R}$. We see $\mathbb{R}$ as a subspace of $\mathbb{I}$ via the isomorphism

$$
\mathbb{R} \cong\{[d, u] \in \mathbb{I} \mid d=u\}
$$

We discussed the usual topology on $\mathbb{R}$ in Example 7.26, but we also get a topology on $\mathbb{R}$ because it is a subset of $\mathbb{R}$; i.e. we have the subspace topology as described in Exercise 7.32. These agree, as the reader can check.

Exercise 7.77. Show that a subset $U \subseteq \mathbb{R}$ is open in the subspace topology of $\mathbb{R} \subseteq \mathbb{R}$ iff $U \cap \mathbb{R}$ is open in the usual topology on $\mathbb{R}$ defined in Example 7.26.

\subsection*{7.5.2 Sheaves on $\mathbb{R} \mathbb{R}$}

We cannot go into much depth about the sheaf topos $\mathbf{B T}=\mathbf{S h v}(\mathbb{I}, \mathbf{O p})$, for reasons of space; we refer the interested reader to Section 7.6. In this section we will briefly discuss what it means to be a sheaf on $\mathbb{R}$, giving a few examples including that of the subobject classifier.

What is a sheaf on $\mathbb{I}$ ? A sheaf $S$ on the interval domain ( $\mathbb{R}, \mathbf{O p}$ ) is a functor $S: \mathbf{O p}{ }^{\text {op }} \rightarrow$ Set: it assigns to each open set $U$ a set $S(U)$; how should we interpret this? An element $s \in S(U)$ is something that says is an "event that takes place throughout the interval $U$." Given this $U$-event $s$ together with an open subset of $V \subseteq U$, there is a $V$-event $\left.s\right|_{V}$ that tells us what $s$ is if we regard it as an event taking place throughout $V$. If $U=\bigcup_{i \in I} U_{i}$ and we can find matching $U_{i}$-events ( $\left.s_{i}\right)$ for each $i \in I$, then the sheaf condition (Definition 7.35) says that they have a unique gluing, i.e. a $U$-event $s \in S(U)$ that encompasses all of them: $\left.s\right|_{U_{i}}=s_{i}$ for each $i \in I$.

We said in Section 7.5.1 that every open set $U \subseteq \mathbb{R}$ can be written as the union of basic open sets $o_{[a, b]}$. This implies that any sheaf $S$ is determined by its values $S\left(o_{[a, b]}\right)$ on these basic open sets. The sheaf condition furthermore implies that these vary continuously in a certain sense, which we can express formally as

$$
S\left(o_{[a, b]}\right) \cong \lim _{\epsilon>0} S\left(o_{[a-\epsilon, b+\epsilon]}\right)
$$

However, rather than get into the details, we describe a few sorts of sheaves that may be of interest.

Example 7.78. For any set $A$ there is a sheaf $\mathrm{A} \in \mathbf{S h v}(\mathbb{I} \mathbb{R})$ that assigns to each open set $U$ the set $\mathrm{A}(U):=A$. This allows us to refer to integers, or real numbers, or letters of an alphabet, as though they were behaviors. What sort of behavior is $7 \in \mathbb{N}$ ? It is the sort
of behavior that never changes: it's always seven. Thus A is called the constant sheaf on A.

Example 7.79. Fix any topological space ( $X, \mathbf{O} \mathbf{p}_{X}$ ). Then there is a sheaf $F_{X}$ of local functions from $\mathbb{I} \mathbb{R}$ to $X$. That is, for any open set $U \in \mathbf{O p}_{\mathbb{R}}$, we assign the set $F_{X}(U):=$ $\{f: U \rightarrow X \mid f$ is continuous $\}$. There is also the sheaf $G_{X}$ of local functions on the subspace $\mathbb{R} \subseteq \mathbb{R}$. That is, for any open set $U \in \mathbf{O p}_{\mathbb{R}}$, we assign the set $G_{X}(U):=$ $\{f: U \cap \mathbb{R} \rightarrow X \mid f$ is continuous $\}$.

Exercise 7.80. Let's check that Example 7.78 makes sense. Fix any topological space $\left(X, \mathbf{O} p_{X}\right.$ ) and any subset $R \subseteq \mathbb{R}$ of the interval domain. Define $H_{X}(U):=\{f: U \cap R \rightarrow$ $X \mid f$ is continuous $\}$.

1. Is $H_{X}$ a presheaf? If not, why not; if so, what are the restriction maps?

2. Is $H_{X}$ a sheaf? Why or why not?

Example 7.81. Another source of examples comes from the world of open hybrid dynamical systems. These are machines whose behavior is a mixture of continuous movements-generally imagined as trajectories through a vector field-and discrete jumps. These jumps are imagined as being caused by signals that spontaneously arrive. Over any interval of time, a hybrid system has certain things that it can do and certain things that it cannot. Although we will not make this precise here, there is a construction for converting any hybrid system into a sheaf on $\mathbb{R}$; we will give references in Section 7.6.

We refer to sheaves on $\mathbb{I} \mathbb{R}$ as behavior types because almost any sort of behavior one can imagine is a behavior type. Of course, a complex behavior type-such as the way someone acts when they are in love-would be extremely hard to write down. But the idea is straightforward: for any interval of time, say a three-day interval $(d, d+3)$, let $L(d, d+3)$ denote the set of all possible behaviors a person who is in love could possibly do. Obviously it's a big, unwieldy set, and no one would want to make precise. But to the extent that one can imagine that sort of behavior as occurring through time, they could imagine the corresponding sheaf.

The subobject classifier as a sheaf on $\mathbb{R}$. In any sheaf topos, the subobject classifier $\Omega$ is itself a sheaf. It is responsible for the truth values in the topos. As we said in Section 7.4.1, when it comes to sheaves on a topological space ( $X, \mathrm{Op}$ ), truth values are open subsets $U \in \mathbf{O p}$.

BT is the topos of sheaves on the space ( $\mathbb{I} \mathbb{R}, \mathbf{O p}$ ), as defined in Section 7.5.1. As always, the subobject classifier $\Omega$ assigns to any $U \in \mathbf{O p}$ the set of open subsets of $U$, so these are the truth values. But what do they mean? The idea is that every proposition,
such as "Bob likes the weather" returns an open set $U$, as if to respond that Bob likes the weather "...throughout time period $U$." Let's explore this just a bit more.

Suppose Bob likes the weather throughout the interval $(0,5)$ and throughout the interval $(4,8)$. We would probably conclude that Bob likes the weather throughout the interval $(0,8)$. But what about the more ominous statement "a single pair of eyes has remained watching position $p$." Then just because it's true on $(0,5)$ and on $(4,8)$, does not imply that it's been true on $(0,8)$ : there may have been a change of shift, where one watcher was relieved from their post by another watcher. As another example, consider the statement "the stock market did not go down by more than 10 points." This might be true on $(0,5)$ and true on $(4,8)$ but not on $(0,8)$. In order to capture the semantics of statements like these-statements that take time to evaluate-we must use the space $\mathbb{I}$ rather than the space $\mathbb{R}$.

\subsection*{7.5.3 Safety proofs in temporal logic}

We now have at least a basic idea of what goes into a proof of safety, say for autonomous vehicles, or airplanes in the national airspace system. In fact, the underlying ideas of this chapter came out of a project between MIT, Honeywell Inc., and NASA [SSV18]. The background for the project was that the National Airspace System consists of many different systems interacting: interactions between airplanes, each of which is an interaction between physics, humans, sensors, and actuators, each of which is an interaction between still more basic parts. The same sort of story would hold for a fleet of autonomous vehicles, as in the introduction to this chapter.

Suppose that each of the systems-at any level-is guaranteed to satisfy some property. For example, perhaps we can assume that an engine is either out of gas, has a broken fuel line, or is following the orders of a human driver or pilot. If there is a rupture in the fuel line, the sensors will alert the human within three seconds, etc. Each of the components interact with a number of different variables. In the case of airplanes, a pilot interacts with the radio, the positions of the dials, the position of the thruster, and the visual data in front of her. The component-here the pilot-is guaranteed to keep these variables in some relation: "if I see something, I will say something" or "if the dials are in position bad_pos, I will engage the thruster within 1 second." We call these guarantees behavior contracts.

All of the above can be captured in the topos BT of behavior types. The variables are behavior types: the altimeter is a variable whose value $\theta \in \mathbb{R}_{\geq 0}$ is changing continuously with respect to time. The thruster is also a continuously-changing variable whose value is in the range $[0,1]$, etc.

The guaranteed relationships-behavior contracts-are given by predicates on variables. For example, if the pilot will always engage the thruster within one second of the display dials being in position bad_pos, this can be captured by a predicate $p$ : dials $\times$ thrusters $\rightarrow \Omega$. While we have not written out a formal language for $p$,
one could imagine the predicate $p(D, T)$ for $D$ : dials and $T$ : thrusters as

$$
\begin{align*}
\forall(t: \mathbb{R}) \cdot @_{t} & (\text { bad_pos }(D)) \Rightarrow \\
& \exists(r: \mathbb{R}) \cdot(0<r<1) \wedge \forall\left(r^{\prime}: \mathbb{R}\right) \cdot 0 \leq r^{\prime} \leq 5 \Rightarrow @_{t+r+r^{\prime}}(\operatorname{engaged}(T)) \tag{7.82}
\end{align*}
$$

Here $@_{t}$ is a modality, as we discussed in Definition 7.69; in fact it turns out to be one of type 3. from Proposition 7.71, but we cannot go into that. For a proposition $q$, the statement $@_{t}(q)$ says that $q$ is true in some small enough neighborhood around $t$. So (7.82) says "starting within one second of whenever the dials say that we are in a bad position, I'll engage the thrusters for five seconds."

Given an actual playing-out-of-events over a time period $U$, i.e. actual section $D \in$ dials $(U)$ and section $T \in$ thrusters( $U$ ), the predicate Eq. (7.82) will hold on certain parts of $U$ and not others, and this is the truth value of $p$. Hopefully the pilot upholds her behavior contract at all times she is flying, in which case the truth value will be true throughout that interval $U$. But if the pilot breaks her contract over certain intervals, then this fact is recorded in $\Omega$.

The logic allows us to record axioms like that shown in Eq. (7.82) and then reason from them: e.g. if the pilot and the airplane, and at least one of the three radars upholds its contract then safe separation will be maintained. We cannot give further details here, but these matters have been worked out in detail in [SS18]; see Section 7.6.

\subsection*{7.6 Summary and further reading}

This chapter was about modeling various sorts of behavior using sheaves on a space of time-intervals. Behavior may seem like it's something that occurs now in the present, but in fact our memory of past behavior informs what the current behavior means. In order to commit to anything, to plan or complete any sort of process, one needs to be able to reason over time-intervals. The nice thing about temporal sheaves-indeed sheaves on any site-is that they fit into a categorical structure called a topos, which has many useful formal properties. In particular, it comes equipped with a higher-order logic with which we can formally reason about how temporal sheaves work together when combined in larger systems. A much more detailed version of this story was presented in [SS18]. But it would have been impossible without the extensive edifice of topos theory and domain theory that has been developed over the past six decades.

Sheaf toposes were invented by Grothendieck and his school in the 1960s [AGV71] as an approach to proving conjectures at the intersection of algebraic geometry and number theory, called the Weil conjectures. Soon after, Lawvere and Tierney recognized that toposes had all the structure necessary to do logic, and with a whole host of other category theorists, the subject was developed to an impressive extent in many directions. For a much more complete history, see [McL90].

There are many sorts of references on topos theory. One that starts by introducing categories and then moves to toposes, focusing on logic, is [McL92]. Our favorite
treatment is perhaps [MM92], where the geometric aspects play a central role. Finally, Johnstone has done the field a huge favor by collecting large amounts of the theory into a single two-volume set [Joh02]; it is very dense, but an essential reference for the serious student or researcher. For just categorical (Kripke-Joyal) semantics of logic in a topos, one should see either [MM92], [Jac99], or [LS88].

We did not mention domain theory much in this chapter, aside from referring to the interval domain. But domains, in the sense of Dana Scott, play an important role in the deeper aspects of temporal type theory. A good reference is [Gie+03], but for an introduction we suggest [AJ94].

In some sense our application area has been a very general sort of dynamical system. Other categorical approaches to this subject include [JNW96], [HTP03], [AS05], and [Law86], though there are many others.

We hope you have enjoyed the seven sketches in this book. As a next step, consider running a reading course on applied category theory with some friends or colleagues. Simultaneously, we hope you begin to search out categorical ways of thinking about familiar subjects. Perhaps you'll find something you want to contribute to this growing field of applied category theory, or as we sometimes call it, the field of compositionality.

\section*{Appendix $A$}

\section*{Exercise solutions}

\section*{A. 1 Solutions for Chapter 1 .}

\section*{Solution to Exercise 1.1.}

For each of the following properties, we need to find a function $f: \mathbb{R} \rightarrow \mathbb{R}$ that preserves it, and another function-call it $g$-that does not.

order-preserving: Take $f(x)=x+5$; if $x \leq y$ then $x+5 \leq y+5$, so $f$ is order-preserving. Take $g(x):=-x$; even though $1 \leq 2$, the required inequality $-1 \leq^{?}-2$ does not hold, so $g$ is not order-preserving.

metric-preserving: Take $f(x):=x+5$; for any $x, y$ we have $|x-y|=|(x+5)-(y+5)|$ by the rules of arithmetic, so $|x-y|=|f(x)-f(y)|$, meaning $f$ preserves metric. Take $g(x):=2 * x$; then with $x=1$ and $y=2$ we have $|x-y|=1$ but $|2 x-2 y|=2$, so $g$ does not preserve the metric.

addition-preserving: Take $f(x):=3 * x$; for any $x, y$ we have $3 *(x+y)=(3 * x)+(3 * y)$, so $f$ preserves addition. Take $g(x):=x+1$; then with $x=0$ and $y=0$, we have $g(x+y)=1$, but $g(x)+g(y)=2$, so $g$ does not preserve addition.

Solution to Exercise 1.4.

Here is the join of the two systems:
![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-271.jpg?height=266&width=1246&top_left_y=1768&top_left_x=450)

Solution to Exercise 1.6.

1. Here is the Hasse diagram for partitions of the two element set $\{\bullet, *\}$ :

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-271.jpg?height=236&width=309&top_left_y=2267&top_left_x=968)

2. Here is a picture (using text, rather than circles) for partitions of the set $1,2,3,4$ :

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-272.jpg?height=575&width=1213&top_left_y=320&top_left_x=510)

For the remaining parts, we choose $A=(12)(3)(4)$ and $B=(13)(2)(4)$.
3. $A \vee B=(123)(4)$.

4. Yes, it is true that $A \leq(A \vee B)$ and that $B \leq(A \vee B)$.

5. The systems $C$ with $A \leq C$ and $B \leq C$ are: (123)(4) and (1234).

6. Yes, it is true that in each case $(A \vee B) \leq C$.

Solution to Exercise 1.7.

1. true $\vee$ false $=$ true.

2. false $\vee$ true $=$ true.

3. true $\vee$ true $=$ true.
4. $f a l s e \vee f a l s e=f a l s e$

Solution to Exercise 1.11.

1. The eight subsets of $B:=\{1,2,3\}$ are

$$
\varnothing, \quad\{1\}, \quad\{2\}, \quad\{3\}, \quad\{1,2\}, \quad\{1,3\}, \quad\{2,3\}, \quad\{1,2,3\}
$$

2. The union of $\{1,2,3\}$ and $\{1\}$ is $\{1,2,3\} \cup\{1\}=\{1,2,3\}$.

3. The six elements of $\{h, 1\} \times\{1,2,3\}$ are

$$
(h, 1), \quad(h, 2), \quad(h, 3), \quad(1,1), \quad(1,2), \quad(1,3)
$$

4. The five elements of $\{h, 1\} \sqcup\{1,2,3\}$ are

$$
(h, 1), \quad(1,1), \quad(1,2), \quad(2,2), \quad(3,2)
$$

5. The four elements of $\{h, 1\} \cup\{1,2,3\}$ are

$$
h, \quad 1, \quad 2, \quad 3
$$

Solution to Exercise 1.10.

1. This is true: a natural number is exactly an integer that is at least 0 .

2. This is false: $0 \in \mathbb{N}$ but $0 \notin\{n \in \mathbb{Z} \mid n \geq 1\}$.

3. This is true: no elements of $\mathbb{Z}$ are strictly between 1 and 2 .

Solution to Exercise 1.16.

Suppose that $A$ is a set and $\left\{A_{p}\right\}_{p \in P}$ and $\left\{A_{p^{\prime}}^{\prime}\right\}_{p^{\prime} \in P^{\prime}}$ are two partitions of $A$ such that for each $p \in P$ there exists a $p^{\prime} \in P^{\prime}$ with $A_{p}=A_{p^{\prime}}^{\prime}$.

1. Given $p \in P$, suppose we had $p_{1}^{\prime}, p_{2}^{\prime} \in P^{\prime}$ such that $A_{p}=A_{p_{1}^{\prime}}^{\prime}$ and $A_{p}=A_{p_{2}^{\prime}}^{\prime}$. Well then $A_{p_{1}^{\prime}}=A_{p_{2}^{\prime}}$, so in particular $A_{p_{1}^{\prime}} \cap A_{p_{2}^{\prime}}=A_{p_{1}^{\prime}}$. By the definition of partition (1.14), $A_{p_{1}^{\prime}} \neq \varnothing$, and yet if $p_{1} \neq p_{2}$ then $A_{p_{1}^{\prime}} \cap A_{p_{2}^{\prime}}=\varnothing$. This can't be, so we must have $p_{1}^{\prime}=p_{2}^{\prime}$, as desired.

2. Suppose given $p^{\prime} \in P^{\prime}$; we want to show that there is a $p \in P$ such that $A_{p}=A_{p^{\prime}}^{\prime}$. Since $A_{p^{\prime}}^{\prime} \neq \varnothing$ is nonempty by definition, we can pick some $a \in A_{p^{\prime}}^{\prime}$; since $A_{p^{\prime}}^{\prime} \subseteq A$, we have $a \in A$. Finally, since $A=\bigcup_{p \in P} A_{p}$, there is some $p$ with $a \in A_{p}$. This is our candidate $p$; now we show that $A_{p}=A_{p^{\prime}}^{\prime}$. By assumption there is some $p^{\prime \prime} \in P^{\prime}$ with $A_{p}=A_{p^{\prime \prime}}^{\prime}$, so now $a \in A_{p^{\prime \prime}}^{\prime}$ and $a \in A_{p^{\prime \prime}}^{\prime}$ so $a \in A_{p^{\prime}}^{\prime} \cap A_{p^{\prime \prime}}^{\prime}$. Again by definition, having a nonempty intersection means $p^{\prime}=p^{\prime \prime}$. So we conclude that $A_{p}=A_{p^{\prime}}$.

Solution to Exercise 1.17.

The pairs $(a, b)$ such that $a \sim b$ are:

$$
\begin{array}{lllll}
(11,11) & (11,12) & (12,11) & (12,12) & (13,13) \\
(21,21) & (22,22) & (12,23) & (23,22) & (23,23)
\end{array}
$$

Solution to Exercise 1.20.

1. One aspect in the definition of the parts is that they are connected, and one aspect of that is that they are nonempty. So each part $A_{p}$ is nonempty.

2. Suppose $p \neq q$, i.e. $A_{p}$ and $A_{q}$ are not exactly the same set. To prove $A_{p} \cap A_{q}=\varnothing$, we suppose otherwise and derive a contradiction. So suppose there exists $a \in A_{p} \cap A_{q}$; we will show that $A_{p}=A_{q}$, which contradicts an earlier hypothesis. To show that these two subsets are equal, it suffices to show that $a^{\prime} \in A_{p}$ iff $a^{\prime} \in A_{q}$ for all $a^{\prime} \in A$. Suppose $a^{\prime} \in A_{p}$; then because $A_{p}$ is connected, we have $a \sim a^{\prime}$. And because $A_{q}$ is closed, $a^{\prime} \in A_{q}$. In just the way, if $a^{\prime} \in A_{q}$ then because $A_{q}$ is connected and $A_{p}$ is closed, $a^{\prime} \in A_{p}$, and we are done.

3. To show that $A=\bigcup_{p \in P} A_{p}$, it suffices to show that for each $a \in A$, there is some $p \in P$ such that $a \in A_{p}$. We said that $P$ was the set of closed and connected subsets of $A$, so it suffices to show that there is some closed and connected subset containing $a$. Let $X:=\left\{a^{\prime} \in A \mid a^{\prime} \sim a\right\}$; we claim it is closed and connected and contains $a$. To see $X$ is closed, suppose $a^{\prime} \in X$ and $b \sim a^{\prime}$; then $b \sim a$ by transitivity and symmetry of $\sim$, so $b \in X$. To see that $X$ is connected, suppose $b, c \in X$; then $b \sim a$ and $c \sim a$ so $b \sim c$ by the transitivity and symmetry of $\sim$. Finally, $a \in X$ by the reflexivity of $\sim$.

\section*{Solution to Exercise 1.24.}

1. The unique function $\varnothing \rightarrow\{1\}$ is injective but not surjective.

2. The unique function $\{a, b\} \rightarrow\{1\}$ is surjective but not injective.

3. The second and third are not functions; the first and fourth are functions.
![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-273.jpg?height=196&width=1064&top_left_y=1934&top_left_x=583)

4. Neither the second nor third is 'total'. Moreover, the second one is not deterministic. The first one is a function which is not injective and not surjective. The fourth one is a function which is both injective and surjective.

\section*{Solution to Exercise 1.25.}

By Definition 1.22, a function $f: A \rightarrow \varnothing$ is a subset $F \subseteq A \times \varnothing$ such that for all $a \in A$, there exists a unique $b \in \varnothing$ with $(a, b) \in F$. But there are no elements $b \in^{?} \varnothing$, so if $F$ is to have the above property, there can be no $a \in A$ either; i.e. $A$ must be empty.

Solution to Exercise 1.27.

Below each partition, we draw a corresponding surjection out of $\{\bullet, *, \circ\}$ :

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-274.jpg?height=168&width=176&top_left_y=390&top_left_x=386)
- $\longmapsto p_{1}$

$* \longmapsto p_{2}$

$\circ \longmapsto p_{3}$
![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-274.jpg?height=180&width=1066&top_left_y=378&top_left_x=686)

$\bullet \longmapsto p_{1}$
$* \longmapsto p_{2}$

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-274.jpg?height=103&width=133&top_left_y=607&top_left_x=1603)

Solution to Exercise 1.38.

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-274.jpg?height=262&width=410&top_left_y=899&top_left_x=478)

\begin{tabular}{l||l|l} 
arrow $a$ & source $s(a) \in V$ & target $t(a) \in V$ \\
\hline$a$ & 1 & 2 \\
$b$ & 1 & 3 \\
$c$ & 1 & 3 \\
$d$ & 2 & 2 \\
$e$ & 2 & 3
\end{tabular}

Solution to Exercise 1.40.

The graph $G$ from Exercise 1.38 is a strange Hasse diagram because it has two arrows $1 \rightarrow 3$ and a loop, both of which are "useless" from a preorder point-of-view. But that does not prevent our formula from working. The preorder $(P, \leq)$ is given by taking $P:=V=\{1,2,3,4\}$ and writing $p \leq q$ whenever there exists a path from $p$ to $q$. So:

$$
1 \leq 1, \quad 1 \leq 2, \quad 1 \leq 3, \quad 2 \leq 2, \quad 2 \leq 3, \quad 3 \leq 3, \quad 4 \leq 4
$$

Solution to Exercise 1.41.

A collection of points, e.g. $\bullet \bullet$ is a Hasse diagram, namely for the discrete order, i.e. for the order where $x \leq y$ iff $x=y$.

\section*{Solution to Exercise 1.42}

Let's write the five elements of $X$ as

$$
(\bullet)(\circ)(*), \quad(\bullet \circ)(*), \quad(\bullet *)(\circ), \quad(\bullet)(\circ *), \quad(\bullet \circ *)
$$

Our job is to write down all 12 pairs of $x_{1}, x_{2} \in X$ with $x_{1} \leq x_{2}$. Here they are:

$$
\begin{aligned}
& (\bullet)(\circ)(*) \leq(\bullet)(\circ)(*) \\
& (\bullet)(\circ)(*) \leq(\bullet \circ)(*) \\
& (\bullet)(\circ)(*) \leq(\bullet *)(\circ) \\
& (\bullet)(\circ)(*) \leq(\bullet)(\circ *) \\
& (\bullet)(\circ)(*) \leq(\bullet \circ *) \\
& (\bullet \circ)(*) \leq(\bullet \circ)(*) \\
& (\bullet \circ)(*) \leq(\bullet \circ *) \\
& (\bullet *)(\circ) \leq(\bullet *)(\circ) \\
& (\bullet *)(\circ) \leq(\bullet \circ *) \\
& (\bullet)(\circ *) \leq(\bullet)(\circ *) \\
& (\bullet)(\circ *) \leq(\bullet \circ *) \\
& (\bullet \circ *) \leq(\bullet \circ *)
\end{aligned}
$$

\section*{Solution to Exercise 1.44.}

The statement in the text is almost correct. It is correct to say that a discrete preorder is one where $x$ and $y$ are comparable if and only if $x=y$.

Solution to Exercise 1.46.

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-275.jpg?height=360&width=553&top_left_y=327&top_left_x=800)

No, it is not a total order; for example $4 \not \leq 6$ and $6 \not \leq 4$.

Solution to Exercise 1.48.

Yes, the usual $\leq$ ordering is a total order on $\mathbb{R}$ : for every $a, b \in \mathbb{R}$ either $a \leq b$ or $b \leq a$.

\section*{Solution to Exercise 1.51.}

The Hasse diagrams for $P(\varnothing), P\{1\}$, and $P\{1,2\}$ are

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-275.jpg?height=284&width=653&top_left_y=1045&top_left_x=752)

Solution to Exercise 1.53.

The coarsest partition on $S$ corresponds to the unique function !: $S \rightarrow\{1\}$. The finest partition on $S$ corresponds to the identity function id $_{S}: S \rightarrow S$.

Solution to Exercise 1.55.

If $X$ has the discrete preorder, then every subset $U$ of $X$ is an upper set: indeed, if $p \in U$, the only $q$ such that $p \leq q$ is $p$ itself, so $q$ is definitely in $U$ ! This means that $U(X)$ contains all subsets of $X$, so it's exactly the power set, $\mathrm{U}(X)=\mathrm{P}(X)$.

Solution to Exercise 1.57.

The product preorder and its upper set preorder are:
![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-275.jpg?height=572&width=1494&top_left_y=1937&top_left_x=369)

Solution to Exercise 1.63.

With $X=\{0,1,2\}$, the Hasse diagram for $P(X)$, the preorder $0 \leq \cdots \leq 3$, and the cardinality map between them are shown below:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-276.jpg?height=374&width=833&top_left_y=385&top_left_x=662)

Solution to Exercise 1.65.

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-276.jpg?height=287&width=889&top_left_y=881&top_left_x=629)

Solution to Exercise 1.66.

1. Let $q \in \uparrow p$, and suppose $q \leq q^{\prime}$. Since $q \in \uparrow p$, we have $p \leq q$. Thus by transitivity $p \leq q^{\prime}$, so $q^{\prime} \in \uparrow p$. Thus $\uparrow p$ is an upper set.

2. Suppose $p \leq q$ in $P$; this means that $q \leq$ op $p$ in $P^{\text {op }}$. We must show that $\uparrow q \subseteq \uparrow p$. Take any $q^{\prime} \in \uparrow q$. Then $q \leq q^{\prime}$, so by transitivitiy $p \leq q^{\prime}$, and hence $q^{\prime} \in \uparrow p$. Thus $\uparrow q \subseteq \uparrow p$.

3. Monotonicity of $\uparrow$ says that $p \leq p^{\prime}$ implies $\uparrow\left(p^{\prime}\right) \subseteq \uparrow(p)$. We must prove the other direction, that if $p \not p^{\prime}$ then $\uparrow\left(p^{\prime}\right) \nsubseteq \uparrow(p)$. This is straightforward, since by reflexivity we always have $p^{\prime} \in \uparrow\left(p^{\prime}\right)$, but if $p \not p^{\prime}$, then $p^{\prime} \notin \uparrow(p)$, so $\uparrow\left(p^{\prime}\right) \nsubseteq \uparrow(p)$.

4. The map $\uparrow: P^{\mathrm{op}} \rightarrow \mathrm{U}(P)$ can be depicted:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-276.jpg?height=287&width=890&top_left_y=1605&top_left_x=669)

Solution to Exercise 1.67.

Suppose $\left(P, \leq_{P}\right)$ is a discrete preorder and that $\left(Q, \leq_{Q}\right)$ is any preorder. We want to show that every function $f: P \rightarrow Q$ is monotone, i.e. that if $p_{1} \leq_{P} p_{2}$ then $f\left(p_{1}\right) \leq_{Q} f\left(p_{2}\right)$. But in $P$ we have $p_{1} \leq_{P} p_{2}$ iff $p_{1}=p_{2}$; that's what discrete means. If $p_{1} \leq_{P} p_{2}$ then $p_{1}=p_{2}$, so $f\left(p_{1}\right)=f\left(p_{2}\right)$, so $f\left(p_{1}\right) \leq f\left(p_{2}\right)$.

Solution to Exercise 1.69.

Let $X=\mathbb{Z}=\{\ldots,-2,-1,0,1, \ldots\}$ be the set of all integers, and let $Y=\{n, z, p\}$; let $f: X \rightarrow Y$ send negative numbers to $n$, zero to $z$, and positive integers to $p$. This is surjective because all three elements of $Y$ are hit.

We consider two partitions of $Y$, namely $P:=(n z)(p)$ and $Q:=(n p)(z)$. Technically, these are notation for $\{\{n, z\},\{p\}\}$ and $\{\{n, p\},\{z\}\}$ as sets of disjoint subsets whose union is $Y$. Their pulled back partitions are $f^{*} P=(\ldots,-2,-1,0)(1,2, \ldots)$ and $f^{*} Q=(0)(\ldots,-2,-1,1,2, \ldots)$, or technically

$$
f^{*}(P)=\{\{x \in \mathbb{Z} \mid x \leq 0\},\{x \in \mathbb{Z} \mid x \geq 1\}\} \quad \text { and } \quad f^{*}(Q)=\{\{0\},\{x \in \mathbb{Z} \mid x \neq 0\}\}
$$

Solution to Exercise 1.71.

We have preorders $\left(P, \leq_{P}\right),\left(Q, \leq_{Q}\right)$, and $\left(R, \leq_{R}\right)$, and we have monotone maps $f: P \rightarrow Q$ and $g: Q \rightarrow R$.

1. To see that $\operatorname{id}_{P}$ is monotone, we need to show that if $p_{1} \leq_{P} p_{2}$ then $\operatorname{id}_{P}\left(p_{1}\right) \leq \operatorname{id}_{P}\left(p_{2}\right)$. But $\operatorname{id}_{P}(p)=p$ for all $p \in P$, so this is clear.

2. We have that $p_{1} \leq_{P} \quad p_{2}$ implies $f\left(p_{1}\right) \leq_{Q} f\left(p_{2}\right)$ and that $q_{1} \leq_{Q} q_{2}$ implies $g\left(q_{1}\right) \leq_{R} g\left(q_{2}\right)$. By substitution, $p_{1} \leq_{P} p_{2}$ implies $g\left(f\left(p_{1}\right)\right) \leq_{R} g\left(f\left(p_{2}\right)\right)$ which is exactly what is required for $(f ; g)$ to be monotone.

Solution to Exercise 1.73.

We need to show that if $\left(P, \leq_{P}\right)$ is both skeletal and dagger, then it is discrete. So suppose it is skeletal, i.e. $p_{1} \leq p_{2}$ and $p_{2} \leq p_{1}$ implies $p_{1}=p_{2}$. And suppose it is dagger, i.e. $p_{1} \leq p_{2}$ implies $p_{2} \leq p_{1}$. Well then $p_{1} \leq p_{2}$ implies $p_{1}=p_{2}$, and this is exactly the definition of $P$ being discrete.

\section*{Solution to Exercise 1.77.}

The map $\Phi$ from Section 1.1.1 took partitions of $\{\bullet, *, \circ\}$ and returned true or false based on whether or not $\bullet$ was in the same partition as $*$. We need to see that it's actually a monotone map $\Phi: \operatorname{Prt}(\{\bullet, *, \circ\}) \rightarrow$ $\mathbb{B}$. So suppose $P, Q$ are partitions with $P \leq Q$; we need to show that if $\Phi(P)=$ true then $\Phi(Q)=$ true. By definition $P \leq Q$ means that $P$ is finer than $Q$ : i.e. $P$ differentiates more stuff, and $Q$ lumps more stuff together. Technically, $x \sim_{P} y$ implies $x \sim Q$ for all $x, y \in\{\bullet, *, \circ\}$. Applying this to $\bullet, *$ gives the result.

\section*{Solution to Exercise 1.79.}

Given a function $f: P \rightarrow Q$, we have $f^{*}: U(Q) \rightarrow U(P)$ given by $U \mapsto f^{-1}(U)$. But upper sets in $Q$ are classified by monotone maps $u: Q \rightarrow \mathbb{B}$, and similarly for $P$; our job is to show that $f^{*}(U)$ is given by composing the classifier $u$ with $f$.

Given an upper set $U \subseteq Q$, let $u: Q \rightarrow \mathbb{B}$ be the corresponding monotone map, which sends $q \mapsto$ true iff $q \in U$. Then $(f ; u): P \rightarrow \mathbb{B}$ sends $p \mapsto$ true iff $f(p) \in U$; it corresponds to the upper set $\{p \in P \mid f(p) \in U\}$ which is exactly $f^{-1}(U)$.

Solution to Exercise 1.80.
1. 0 is a lower bound for $S=\left\{\left.\frac{1}{n+1} \right\rvert\, n \in \mathbb{N}\right\}$ because $0 \leq \frac{1}{n+1}$ for any $n \in \mathbb{N}$.

2. Suppose that $b$ is a lower bound for $S$; we want to see that $b \leq 0$. If one believes to the contrary that $0<b$, then consider $1 / b$; it is a real number so we can find a natural number $n$ that's bigger $1 / b<n<n+1$. This implies $1<b(n+1)$ and hence $\frac{1}{n+1}<b$, but that is a contradiction of $b$ being a lower bound for $S$. The false believer is defeated!

Solution to Exercise 1.85.

We have a preorder $(P, \leq)$, an element $p \in P$, and a subset $A=\{p\}$ with one element.

1. To see that $\bigwedge A \cong p$, we need to show that $p \leq a$ for all $a \in A$ and that if $q \leq a$ for all $a \in A$ then $q \leq p$. But the only $a \in A$ is $a=p$, so both are obvious.

2. We know $p$ is a meet of $A$, so if $q$ is also a meet of $A$ then $q \leq a$ for all $a \in A$ so $q \leq p$; similarly $p \leq a$ for all $a \in A$, so $p \leq q$. Then by definition we have $p \cong q$, and since $(P, \leq)$ is a partial order, $p=q$.

3. The analogous facts are true when $\wedge$ is replaced by $\vee$; the only change in the argument is to replace $\leq$ by $\geq$ and 'meet' by 'join' everywhere.

Solution to Exercise 1.90.

The meet of 4 and 6 is the highest number in the order that divides both of them; the numbers dividing both are 1 and 2 , and 2 is higher, so $4 \wedge 6=2$. Similar reasoning shows that $4 \vee 6=12$. The meet is the 'greatest common divisor' and the join is the 'least common multiple,' and this holds up for all pairs $m, n \in \mathbb{N}$ not just 4,6 .

Solution to Exercise 1.94.

Since $f$ is monotone, the facts that $a \leq a \vee b$ and $b \leq a \vee b$ imply that $f(a) \leq f(a \vee b)$ and $f(b) \leq f(a \vee b)$. But by definition of join, $f(a) \vee f(b)$ is the largest element with that property, so $f(a) \vee f(b) \leq f(a \vee b)$, as desired.

\section*{Solution to Exercise 1.98}

By analogy with Example 1.97, the right adjoint for $(3 \times-)$ should be $\lfloor-/ 3\rfloor$. But to prove this is correct, we must show that for any any $r \in \mathbb{R}$ and $z \in \mathbb{Z}$ we have $z \leq\lfloor r / 3\rfloor$ iff $3 * z \leq r$.

Suppose the largest integer below $r / 3$ is $z^{\prime}:=\lfloor r / 3\rfloor$. Then $z \leq z^{\prime}$ implies $3 * z \leq 3 * z^{\prime} \leq 3 * r / 3=r$, giving one direction. For the other, suppose $3 * z \leq r$. Then dividing both sides by 3 , we have $z=3 * z / 3 \leq r / 3$. Since $z$ is an integer below $r / 3$ it is below $\lfloor r / 3\rfloor$ because $\lfloor r / 3\rfloor$ is the greatest integer below $r / 3$, and we are done.

\section*{Solution to Exercise 1.99.}

1. We need to check that for all nine pairs $\{(p, q) \mid 1 \leq p \leq 3$ and $1 \leq q \leq 3\}$ we have $f(p) \leq q$ iff $p \leq g(q)$, where $f$ and $g$ are the functions shown here:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-278.jpg?height=200&width=550&top_left_y=1022&top_left_x=839)

When $p=q=1$ we have $f(p)=1$ and $g(q)=2$, so both $f(p)=1 \leq 1=q$ and $p=1 \leq g(q)$; it works! Same sort of story happens when $(p, q)$ is $(1,2),(1,3),(2,1),(2,2),(2,3)$, and $(3,3)$. A different story happens for $p=3, q=1$ and $p=3, q=2$. In those cases $f(p)=3$ and $g(q)=2$, and neither inequality holds: $f(p) \not q$ and $p \not g(q)$. But that's fine, we still have $f(p) \leq q$ iff $p \leq g(q)$ in all nine cases, as desired.

2.

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-278.jpg?height=190&width=550&top_left_y=1512&top_left_x=839)

Here $f$ is not left adjoint to $g$ because $f(2) \not 1$ but $2 \leq g(1)$.

Solution to Exercise 1.101.

1. Let's suppose we have a monotone map $L: \mathbb{Z} \rightarrow \mathbb{R}$ that's left adjoint to $\lceil-/ 3\rceil$ and see what happens. Writing $C(r):=\lceil r / 3\rceil$, then for all $z \in \mathbb{Z}$ and $r \in \mathbb{R}$ we have $L(z) \leq r$ iff $z \leq C(r)$ by definition of adjunction. So take $z=1$ and $r=.01$; then $\lceil r / 3\rceil=1$ so $z \leq C(r)$, and hence $L(z) \leq r$, i.e. $L(1) \leq 0.01$. In the same way $L(1) \leq r$ for all $r>0$, so $L(1) \leq 0$. By definition of adjunction $1 \leq C(0)=\lceil 0 / 3\rceil=0$, a contradiction.

2. There's no left adjoint, because starting with an arbitrary one, we derived a contradiction.

Solution to Exercise 1.103.

We have $S=\{1,2,3,4\}, T=\{12,3,4\}$, and $g: S \rightarrow T$ the "obvious" function between them; see Example 1.102. Take $c_{1}, c_{2}, c_{3}, c_{4}$ to be the following partitions:
![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-278.jpg?height=144&width=1268&top_left_y=2347&top_left_x=442)

Then the induced partitions $g!\left(c_{1}\right), g!\left(c_{2}\right), g!\left(c_{3}\right)$, and $g!\left(c_{4}\right)$ on $T$ are:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-279.jpg?height=143&width=1287&top_left_y=319&top_left_x=430)

Solution to Exercise 1.105.

Here are the partitions on $S=\{1,2,3,4\}$ induced via $g^{*}$ by the five partitions on $T=\{12,3,4\}$ :

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-279.jpg?height=149&width=1109&top_left_y=668&top_left_x=519)

Solution to Exercise 1.106.

1. We choose the following partition $c$ on $S$ and compute its pushforward $g!(c)$ :

$$
\begin{aligned}
& c=\underset{\sim}{1} 2_{\bullet}^{2} \nearrow^{3} \quad 4 \\
& g!(c)=\coprod_{\bullet}^{12} \underset{\bullet}{3} \prod_{\bullet}^{4}
\end{aligned}
$$

2. Let $d$ be the partition as shown, which was chosen to be coarser than $g_{!}(c)$

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-279.jpg?height=141&width=203&top_left_y=1277&top_left_x=1015)

3. Let $e$ be the partition as shown, which was chosen to not be coarser than $g_{!}(c)$.

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-279.jpg?height=134&width=232&top_left_y=1535&top_left_x=1006)

4. Here are $g^{*}(d)$ and $g^{*}(e)$ :

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-279.jpg?height=152&width=669&top_left_y=1783&top_left_x=782)

5. Comparing $c$, the left-hand partition in part 1., with $g^{*}(d)$ and $g^{*}(e)$, we indeed have $c \leq g^{*}(d)$ but $c \not g^{*}(e)$, as desired.

\section*{Solution to Exercise 1.109.}

Suppose $P$ and $Q$ are preorders, and that $f: P \leftrightarrows Q: g$ are monotone maps.

1. Suppose $f$ is left adjoint to $g$. By definition this means $f(p) \leq q$ iff $p \leq g(q)$, for all $p \in P$ and $q \in Q$. Then starting with the reflexivity fact $g(q) \leq g(q)$, the definition with $p:=g(q)$ gives $f(g(q)) \leq q$ for all $q$.

2. Suppose that $p \leq g(f(p))$ and $f(g(q)) \leq q$ for all $p \in P$ and $q \in Q$. We first want to show that $p \leq g(q)$ implies $f(p) \leq q$, so assume $p \leq g(q)$. Then applying the monotone map $f$ to both sides, we have $f(p) \leq f(g(q))$, and then by transitivity $f(g(q)) \leq q$ implies $f(p) \leq q$, as desired. The other direction is similar.

Solution to Exercise 1.110.

1. Suppose that $f: P \rightarrow Q$ has two right adjoints, $g, g^{\prime}: Q \rightarrow P$. We want to show that $g(q) \cong g^{\prime}(q)$ for all $q \in Q$. We will prove $g(q) \leq g^{\prime}(q)$; the inequality $g^{\prime}(q) \leq g(q)$ is similar. To do this, we use the fact that $p \leq g^{\prime}(f(p))$ and $f(g(q)) \leq q$ for all $p, q$ by Eq. (1.108). Then the trick is to reason as follows:

$$
g(q) \leq g^{\prime}(f(g(q))) \leq g^{\prime}(q)
$$

2. It is the same for left adjoints.

Solution to Exercise 1.112.

Suppose $f: P \rightarrow Q$ is left adjoint to $g: Q \rightarrow P$. Let $A \subseteq P$ be any subset and let $j:=\vee A$ be its join. Then since $f$ is monotone $f(a) \leq f(j)$ for all $a \in A$, so $f(j)$ is an upper bound for the set $f(A)$. We want to show it is the least upper bound, so take any other upper bound $b$ for $f(A)$, meaning we have $f(a) \leq b$ for all $a \in A$. Then by definition of adjunction, we also have $a \leq g(b)$ for all $a \in A$. By definition of join, we have $j \leq g(b)$. Again by definition of adjunction $f(j) \leq b$, as desired.

\section*{Solution to Exercise 1.114.}

We want to show that in the following picture, $g$ is really right adjoint to $f$ :

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-280.jpg?height=344&width=845&top_left_y=999&top_left_x=651)

Here $g$ preserves labels and $f$ rounds 3.9 to 4

There are twelve tiny things to check: for each $p \in P$ and $q \in Q$, we need to see that $f(p) \leq q$ iff $p \leq g(q)$.

\begin{tabular}{cc|cc|cc|c}
$p$ & $q$ & $f(p)$ & $g(q)$ & $f(p) \leq ? q$ & $p \leq$ ? $g(q)$ & same? \\
\hline 1 & 1 & 1 & 1 & yes & yes & yes! \\
1 & 2 & 1 & 2 & no & no & yes! \\
1 & 4 & 1 & 4 & yes & yes & yes! \\
2 & 1 & 2 & 1 & no & no & yes! \\
2 & 2 & 2 & 2 & yes & yes & yes! \\
2 & 4 & 2 & 4 & yes & yes & yes! \\
3.9 & 1 & 4 & 1 & no & no & yes! \\
3.9 & 2 & 4 & 2 & no & no & yes! \\
3.9 & 4 & 4 & 4 & yes & yes & yes! \\
4 & 1 & 4 & 1 & no & no & yes! \\
4 & 2 & 4 & 2 & no & no & yes! \\
4 & 4 & 4 & 4 & yes & yes & yes!
\end{tabular}

Solution to Exercise 1.118.

Consider the function shown below, which "projects straight down":

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-280.jpg?height=339&width=436&top_left_y=2189&top_left_x=866)

1. Let $B_{1}:=\{a, b\}$ and $B_{2}:=\{c\}$. Then $f^{*}\left(B_{1}\right)=\left\{a_{1}\right\}$ and $f^{*}\left(B_{2}\right)=\left\{c_{1}, c_{2}\right\}$.

2. Let $A_{1}:=\varnothing$ and $A_{2}:=\left\{a_{1}, c_{1}\right\}$. Then $f_{!}\left(A_{1}\right)=\varnothing$ and $f_{!}\left(A_{2}\right)=\{a, c\}$.

3. With the same $A_{1}$ and $A_{2}$, we compute $f_{*}\left(A_{1}\right)=\{b\}$ and $f_{*}\left(A_{2}\right)=\{a, b\}$.

Solution to Exercise 1.119.

Assume $f: P \rightarrow Q$ is left adjoint to $g: Q \rightarrow P$.

1. It is part of the definition of adjunction (Proposition 1.107) that $p \leq g(f(p)$ ), and of course $g(f(p))$ and $(f ; g)(p)$ mean the same thing.

2. We want to show that $g(f(g(f(p)))) \leq g(f(p))$ and $g(f(p)) \leq g(f(g(f(p))))$ for all $p$. The latter is just the fact that $p^{\prime} \leq g\left(f\left(p^{\prime}\right)\right)$ for any $p^{\prime}$, applied with $g(f(p))$ in place of $p^{\prime}$. The former uses that $f(g(q)) \leq q$, with $f(p)$ substituted for $q$ : this gives $f(g(f(p))) \leq f(p)$, and then we apply $g$ to both sides.

Solution to Exercise 1.124.

We denote tuples $(a, b)$ by $a b$ for space reasons. So the relation $\{(1,1),(1,2),(2,1)\}$ will be denoted $\{11,12,21\}$.

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-281.jpg?height=483&width=1290&top_left_y=973&top_left_x=428)

Solution to Exercise 1.125.

Let $S:=\{1,2,3\}$.

1. Let $\leq$ be the preorder with $1 \leq 2$, and of course $1 \leq 1,2 \leq 2$, and $3 \leq 3$. Then $U(\leq)=$ $\{(1,1),(1,2),(2,2),(3,3)\}$.

2. Let $Q:=\{(1,1)\}$ and $Q^{\prime}:=\{(2,1)\}$.

3. The closure $\mathrm{CI}(Q)$ of $Q$ is the smallest preorder containing $(1,1)$, which is $\mathrm{Cl}(Q)=\{(1,1),(2,2),(3,3)\}$. Similarly, $\mathrm{Cl}\left(Q^{\prime}\right)=\{(1,1),(2,1),(2,2),(3,3)\}$. It is easy to see that $\mathrm{Cl}(Q) \sqsubseteq \leq$ because every ordered pair in $\mathrm{Cl}(Q)$ is also in $\leq$.

4. It is easy to see that $\mathrm{Cl}\left(Q^{\prime}\right) \nsubseteq \leq$ because the ordered pair $(2,1)$ is in $\mathrm{Cl}\left(Q^{\prime}\right)$ but is not in $\leq$.

\section*{A. 2 Solutions for Chapter 2.}

\section*{Solution to Exercise 2.5.}

The expert is right! The proposal violates property (a) when $x_{1}=-1, x_{2}=0, y_{1}=-1$, and $y_{2}=1$. Indeed $-1 \leq-1$ and $0 \leq 1$, but $-1 * 0=0 \not-1=-1 * 1$.

Solution to Exercise 2.8.

To check that $(\operatorname{Disc}(M),=, *, e)$ is a symmetric monoidal preorder, we need to check our proposed data obeys conditions (a)-(d) of Definition 2.2. Condition (a) just states the tautology that $x_{1} \otimes x_{2}=x_{1} \otimes x_{2}$, conditions (b) and (c) are precisely the equations Eq. (2.7), and (d) is the commutativity condition. So we're done. We leave it to you to decide whether we were telling the truth when we said it was easy.

Solution to Exercise 2.20.

1. Here is a line by line proof, where we write the reason for each step in parentheses on the right. Recall we call the properties (a) and (c) in Definition 2.2 monotonicity and associativity respectively.

$$
\begin{aligned}
t+u & \leq(v+w)+u \\
& =v+(w+u) \\
& \leq v+(x+z) \\
& =(v+x)+z \\
& \leq y+z .
\end{aligned}
$$

(monotonicity, $t \leq v+w, u \leq u$ )

(associativity)

(monotonicity, $v \leq v, w+u \leq x+v$ )

(associativitiy)

(monotonicity, $v+x \leq y, z \leq z$ )

2. We use reflexivity when we assert that $u \leq u, v \leq v$ and $z \leq z$, and use transitivity to assert that the above sequence of inequalities implies the single inequality $t+u \leq y+z$.

3. We know that the symmetry axiom is not necessary because no pair of wires cross.

Solution to Exercise 2.21.

Condition (a), monotonicity, says that if $x \rightarrow y$ and $z \rightarrow w$ are reactions, then $x+z \rightarrow y+w$ is a reaction. Condition (b), unitality, holds as 0 represents having no material, and adding no material to some other material does not change it. Condition (c), associativity, says that when combining three collections $x, y$, and $z$ of molecules it doesn't matter whether you combine $x$ and $y$ and then $z$, or combine $x$ with $y$ already combined with $z$. Condition (d), symmetry, says that combining $x$ with $y$ is the same as combining $y$ with $x$. All these are true in our model of chemistry, so (Mat, $\rightarrow, 0,+$ ) forms a symmetric monoidal preorder.

\section*{Solution to Exercise 2.29.}

The monoidal unit must be false. The symmetric monoidal preorder does satisfy the rest of the conditions; this can be verified just by checking all cases.

\section*{Solution to Exercise 2.31.}

The monoidal unit is the natural number 1 . Since we know that $(\mathbb{N}, \leq)$ is a preorder, we just need to check that $*$ is monotonic, associative, unital with 1 , and symmetric. These are all familiar facts from arithmetic.

\section*{Solution to Exercise 2.33.}

This proposal is not monotonic: we have $1 \mid 1$ and $1 \mid 2$, but $(1+1) \nmid(1+2)$

Solution to Exercise 2.34.

1 .

\begin{tabular}{c|ccc} 
min & no & maybe & yes \\
\hline no & no & no & no \\
maybe & no & maybe & maybe \\
yes & no & maybe & yes
\end{tabular}

2. We need to show that (a) if $x \leq y$ and $z \leq w$, then $\min (x, z) \leq \min (y, w)$, (b) $\min (x, y e s)=x=$ $\min ($ yes, $x$ ), (c) $\min (\min (x, y), z)=\min (x, \min (y, z))$, and (d) $\min (x, y)=\min (y, z)$. The most straightforward way is to just check all cases.

Solution to Exercise 2.35.

Yes, $(\mathrm{P}(S), \leq, S, \cap)$ is a symmetric monoidal preorder.

Solution to Exercise 2.36.

Depending on your mood, you might come up with either of the following. First, we could take the monoidal unit to be some statement true that is true for all natural numbers, such as " $n$ is a natural number." We can pair this unit with the monoidal product $\wedge$, which takes statements $P$ and $Q$ and makes the statement $P \wedge Q$, where $(P \wedge Q)(n)$ is true if $P(n)$ and $Q(n)$ are true, and false otherwise. Then $\left(\operatorname{Prop}^{\mathbb{N}}, \leq\right.$, true,$\left.\wedge\right)$ forms a symmetric monoidal preorder.

Another option is to take define false to be some statement that is false for all natural numbers, such as " $n+10 \leq 1$ " or " $n$ is made of cheese." We can also define $\vee$ such that $(P \vee Q)(n)$ is true if and only if at least one of $P(n)$ and $Q(n)$ is true. Then $\left(\operatorname{Prop}^{\mathbb{N}}, \leq\right.$, false,$\left.\vee\right)$ forms a symmetric monoidal preorder.

Solution to Exercise 2.39.

Unitality and associativity have nothing to do with the order in $X^{\text {op }}$ : they simply state that $I \otimes x=$ $x=x \otimes I$, and $(x \otimes y) \otimes z=x \otimes(y \otimes z)$. Since these are true in $X$, they are true in $X$ op. Symmetry is slightly trickier, since in only asks that $x \otimes y$ is equivalent to $y \otimes x$. Nonetheless, this is still true in $X^{\text {op }}$ because it is true in $X$. Indeed the fact that $(x \otimes y) \cong(y \otimes x)$ in $X$ means that $(x \otimes y) \leq(y \otimes x)$ and $(y \otimes x) \leq(x \otimes y)$ in $X$, which respectively imply that $(y \otimes x) \leq(x \otimes y)$ and $(x \otimes y) \leq(y \otimes x)$ in $X^{\text {op }}$, and hence that $(x \otimes y) \cong(y \otimes x)$ in $X^{\text {op }}$ too.

Solution to Exercise 2.40.

1. The preorder Cost ${ }^{\mathrm{op}}$ has underlying set $[0, \infty]$, and the usual increasing order on real numbers $\leq$ as its order.

2. Its monoidal unit is 0 .

3. Its monoidal product is + .

Solution to Exercise 2.43.

1. The map $g$ is monotonic as $g$ (false) $=\infty \geq 0=g$ (true), satisfies condition (a) since $0 \geq 0=$ $g$ (true), and satisfies condition (b) since

$$
\begin{gathered}
g(\text { false })+g(\text { false })=\infty+\infty \geq \infty=g(\text { false } \wedge \text { false }) \\
g(\text { false })+g(\text { true })=\infty+0 \geq \infty=g(\text { false } \wedge \text { true }) \\
g(\text { true })+g(\text { false })=0+\infty \geq \infty=g(\text { true } \wedge \text { false }) \\
g(\text { true })+g(\text { true })=0+0 \geq 0=g(\text { true } \wedge \text { true })
\end{gathered}
$$

2. Since all the inequalities regarding (a) and (b) above are in fact equalities, $g$ is a strict monoidal monotone.

Solution to Exercise 2.44.

The answer to all these questions is yes: $d$ and $u$ are both strict monoidal monotones. Here is one way to interpret this. The function $d$ asks 'is $x=0$ ?'. This is monotonic, 0 is 0 , and the sum of two elements of $[0, \infty]$ is 0 if and only if they are both 0 . The function $u$ asks 'is $x$ finite?'. Similarly, this is monotonic, 0 is finite, and the sum of $x$ and $y$ is finite if and only if $x$ and $y$ are both finite.

Solution to Exercise 2.45.

1. Yes, multiplication is monotonic in $\leq$, unital with respect to 1 , associative, and symmetric, so $(\mathbb{N}, \leq, 1, *)$ is a monoidal preorder. We also met this preorder in Exercise 2.31.

2. The map $f(n)=1$ for all $n \in \mathbb{N}$ defines a monoidal monotone $f:(\mathbb{N}, \leq, 0,+) \rightarrow(\mathbb{N}, \leq, 1, *)$. (In fact, it is unique! Why?)
3. $(\mathbb{Z}, \leq, *, 1)$ is not a monoidal preorder because $*$ is not monotone. Indeed $-1 \leq 0$ but $(-1 *-1) \not \leq$ $(0 * 0)$.

Solution to Exercise 2.50.

1. Let $(P, \leq)$ be a preorder. How is this a Bool-category? Following Example 2.47, we can construct a Bool-category $X_{P}$ with $P$ as its set of objects, and with $X_{P}(p, q):=$ true if $p \leq q$, and $X_{P}(p, q):=$ false otherwise. How do we turn this back into a preorder? Following the proof of Theorem 2.49, we construct a preorder with underlying set $\mathrm{Ob}\left(\mathcal{X}_{P}\right)=P$, and with $p \leq q$ if and only if $X_{P}(p, q)=$ true. This is precisely the preorder $(P, \leq)$ !

2. Let $X$ be a Bool-category. By the proof of Theorem 2.49, we construct a preorder $(\mathrm{Ob}(X), \leq)$, where $x \leq y$ if and only if $X(x, y)=$ true. Then, following our generalization of Example 2.47 in 1., we construct a Bool-category $X^{\prime}$ whose set of objects is $\operatorname{Ob}(X)$, and such that $X^{\prime}(x, y)=$ true if and only if $x \leq y$ in $(\operatorname{Ob}(X), \leq)$. But by construction, this means $X^{\prime}(x, y)=X(x, y)$. So we get back the Bool-category we started with.

\section*{Solution to Exercise 2.52.}

The distance $d$ (US, Spain) is bigger: the distance from, for example, San Diego to anywhere is Spain is bigger than the distance from anywhere in Spain to New York City.

Solution to Exercise 2.55.

The difference between a Lawvere metric space-that is, a category enriched over $([0, \infty], \geq, 0,+)$-and a category enriched over $\left(\mathbb{R}_{\geq 0}, \geq, 0,+\right)$ is that in the latter, infinite distances are not allowed between points. You might thus call the latter a finite-distance Lawvere metric space.

Solution to Exercise 2.58.

The table of distances for $X$ is

\begin{tabular}{c|cccc}
$d(\nearrow)$ & $A$ & $B$ & $C$ & $D$ \\
\hline$A$ & 0 & 6 & 3 & 11 \\
$B$ & 2 & 0 & 5 & 5 \\
$C$ & 5 & 3 & 0 & 8 \\
$D$ & 11 & 9 & 6 & 0
\end{tabular}

Solution to Exercise 2.60.

The matrix of edge weights of $X$ is

$$
M_{X}=\begin{array}{c|cccc}
\nearrow & A & B & C & D \\
\hline A & 0 & \infty & 3 & \infty \\
B & 2 & 0 & \infty & 5 \\
C & \infty & 3 & 0 & \infty \\
D & \infty & \infty & 6 & 0
\end{array}
$$

Solution to Exercise 2.61.

A NMY-category $X$ is a set $X$ together with, for all pairs of elements $x, y$ in $X$, a value $X(x, y)$ equal to no, maybe, or yes. Moreover, we must have $X(x, x)=$ yes and $\min (X(x, y), X(y, z)) \leq X(x, z)$ for all $x, y, z$. So a NMY-category can be thought of as set of points together with an statement-no, maybe, or yes-of whether it is possible to get from one point to another. In particular, it's always possible to get to a point if you're already there, and it's as least as possible to get from $x$ to $z$ as it is to get from $x$ to $y$ and then $y$ to $z$.

Solution to Exercise 2.62.

Here is one way to do this task.

1.

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-285.jpg?height=336&width=536&top_left_y=279&top_left_x=841)

2. The corresponding $\mathcal{M}$-category, call it $X$, has hom-objects:

\begin{tabular}{c|cccc}
$X(\nearrow)$ & $A$ & $B$ & $C$ & $D$ \\
\hline$A$ & $M$ & $\{$ boat $\}$ & $\varnothing$ & $\{$ boat $\}$ \\
$B$ & $\varnothing$ & $M$ & $\varnothing$ & $\{$ boat $\}$ \\
$C$ & $\{$ foot, boat $\}$ & $\{$ boat $\}$ & $M$ & $M$ \\
$D$ & $\{$ foot $\}$ & $\varnothing$ & \{foot, car $\}$ & $M$
\end{tabular}

For example, to compute the hom-object $X(C, D)$, we notice that there are two paths: $C \rightarrow A \rightarrow$ $B \rightarrow D$ and $C \rightarrow D$. For the first path, the intersection is the set \{boat\}. For the second path, the intersection in the set $\{$ foot, car\}. Their union, and thus the hom-object $\chi(C, D)$, is the entire set M.

This computation contains the key for why $\mathcal{X}$ is a $\mathcal{M}$-category: by taking the union over all paths, we ensure that $X(x, y) \cap X(y, z) \subseteq X(x, z)$ for all $x, y, z$.

3. The person's interpretation looks right to us.

Solution to Exercise 2.63.

1.

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-285.jpg?height=236&width=323&top_left_y=1362&top_left_x=955)

2. The matrix $M$ with $(x, y)^{\text {th }}$ entry equal to the maximum, taken over all paths $p$ from $x$ to $y$, of the minimum edge label in $p$ is

\begin{tabular}{c|ccc}
$M(\nearrow)$ & $A$ & $B$ & $C$ \\
\hline$A$ & $\infty$ & 6 & 10 \\
$B$ & 10 & $\infty$ & 10 \\
$C$ & 10 & 6 & $\infty$
\end{tabular}

3. This is a matrix of hom-objects for a $\mathcal{W}$-category since the diagonal values are all equal to the monoidal unit $\infty$, and because $\min (M(x, y), M(y, z)) \leq M(x, y)$ for all $x, y, z \in\{A, B, C\}$.

4. One interpretation is as a weight limit (not to be confused with 'weighted limit,' a more advanced categorical notion), for example for trucking cargo between cities. The hom-object indexed by a pair of points $(x, y)$ describes the maximum cargo weight allowed on that route. There is no weight limit on cargo that remains at some point $x$, so the hom-object from $x$ to $x$ is always infinite. The maximum weight that can be trucked from $x$ to $z$ is always at least the minimum of that that can be trucked from $x$ to $y$ and then $y$ to $z$. (It is 'at least' this much because there may be some other, better route that does not pass through $y$.)

Solution to Exercise 2.67.

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-285.jpg?height=117&width=436&top_left_y=2392&top_left_x=861)

This preorder describes the 'is a part of' relation. That is, $x \leq y$ when $d(x, y)=0$, which happens when $x$ is a part of $y$. So Boston is a part of the US, and Spain is a part of Spain, but the US is not a part of Boston.

Solution to Exercise 2.68.

1. Recall the monoidal monotones $d$ and $u$ from Exercise 2.44. The function $f$ is equal to $d$; let $g$ be equal to $u$.

2. Let $\mathcal{X}$ be the Lawvere metric space with two objects $A$ and $B$, such that $d(A, B)=d(B, A)=5$. Then we have $x_{f}=\stackrel{A}{\bullet} \quad \stackrel{B}{\bullet}$ while $x_{g}=\stackrel{A}{\bullet} \longrightarrow \stackrel{B}{\bullet}$.

Solution to Exercise 2.73.

1. An extended metric space is a Lawvere metric space that obeys in addition the properties (b) if $d(x, y)=0$ then $x=y$, and (c) $d(x, y)=d(y, x)$ of Definition 2.51. Let's consider the dagger condition first. It says that the identity function to the opposite Cost-category is a functor, and so for all $x, y$ we must have $d(x, y) \leq d(y, x)$. But this means also that $d(y, x) \leq d(x, y)$, and so $d(x, y)=d(y, x)$. This is exactly property (c).

Now let's consider the skeletality condition. This says that if $d(x, y)=0$ and $d(y, x)=0$, then $x=y$. Thus when we have property (c), $d(x, y)=d(y, x)$, this is equivalent to property (b). Thus skeletal dagger Cost-categories are the same as extended metric spaces!

2. Recall from Exercise 1.73 that skeletal dagger preorders are sets. The analogy "preorders are to sets as Lawvere metric spaces are to extended metric spaces" is thus the observation that just as extended metric spaces are skeletal dagger Cost-categories, sets are skeletal dagger Boolcategories.

Solution to Exercise 2.75.

1. Let $(x, y) \in X \times y$. Since $\mathcal{X}$ and $y$ are $\mathcal{V}$-categories, we have $I \leq X(x, x)$ and $I \leq y(y, y)$. Thus $I=I \otimes I \leq X(x, x) \otimes y(y, y)=(X \times y)((x, y),(x, y))$.

2. Using the definition of product hom-objects, and the symmetry and monotonicity of $\otimes$ we have

$$
\begin{aligned}
(X & \times y)\left(\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right)\right) \otimes(X \times y)\left(\left(x_{2}, y_{2}\right),\left(x_{3}, y_{3}\right)\right) \\
& =X\left(x_{1}, x_{2}\right) \otimes y\left(y_{1}, y_{2}\right) \otimes X\left(x_{2}, x_{3}\right) \otimes y\left(y_{2}, y_{3}\right) \\
& =X\left(x_{1}, x_{2}\right) \otimes X\left(x_{2}, x_{3}\right) \otimes y\left(y_{1}, y_{2}\right) \otimes y\left(y_{2}, y_{3}\right) \\
& \leq X\left(x_{1}, x_{3}\right) \otimes y\left(y_{1}, y_{3}\right) \\
& =(X \times y)\left(\left(x_{1}, y_{1}\right),\left(x_{3}, y_{3}\right)\right) .
\end{aligned}
$$

3. In particular, we use the symmetry, to conclude that $y\left(y_{1}, y_{2}\right) \otimes X\left(x_{2}, x_{3}\right)=X\left(x_{2}, x_{3}\right) \otimes y\left(y_{1}, y_{2}\right)$.

Solution to Exercise 2.78.

We just apply Definition $2.74($ ii): $(\mathbb{R} \times \mathbb{R})((5,6),(-1,4))=\mathbb{R}(5,-1)+\mathbb{R}(6,4)=6+2=8$.

Solution to Exercise 2.82.

1. The function $-\otimes v: V \rightarrow V$ is monotone, because if $u \leq u^{\prime}$ then $u \otimes v \leq u^{\prime} \otimes v$ by the monotonicity condition (a) in Definition 2.2.

2. Let $a:=(v \multimap w)$ in Eq. (2.80). The right hand side is thus $(v \multimap w) \leq(v \multimap w)$, which is true by reflexivity. Thus the left hand side is true too. This gives $((v \multimap w) \otimes v) \leq w$.

3. Let $u \leq u^{\prime}$. Then, using 2., $(v \multimap u) \otimes v \leq u \leq u^{\prime}$. Applying Eq. (2.80), we thus have $(v \multimap u) \leq\left(v \multimap u^{\prime}\right)$. This shows that the map $(v \multimap-): V \rightarrow V$ is monotone.

4. Eq. (2.80) is exactly the adjointness condition from Definition 1.95, except for the fact that we do not know $(-\otimes v)$ and $(v \multimap-)$ are monotone maps. We proved this, however, in items 1 and 3 above.

\section*{Solution to Exercise 2.84 .}

We need to find the hom-element. This is given by implication. That is, define the function $x \Rightarrow y$ by the table

\begin{tabular}{c|cc}
$\Rightarrow$ & false & true \\
\hline false & true & true \\
true & false & true
\end{tabular}

Then $(a \wedge v) \leq w$ if and only if $a \leq(v \Rightarrow w)$. Indeed, if $v=$ false then $a \wedge$ false $=$ false, and so the left hand side is always true. But (false $\Rightarrow w$ ) = true, so the right hand side is always true too. If $v=$ true, then $a \wedge$ true $=a$ so the left hand side says $a \leq w$. But (true $\Rightarrow w)=w$, so the right hand side is the same. Thus $\Rightarrow$ defines a hom-element as per Eq. (2.80).

\section*{Solution to Exercise 2.93.}

We showed in Exercise 2.84 that Bool is symmetric monoidal closed, and in Exercise 1.7 and Example 1.88 that the join is given by the OR operation $\vee$. Thus Bool is a quantale.

Solution to Exercise 2.94.

Yes, the powerset monoidal preorder $(\mathrm{P}(S), \subseteq, S, \cap)$ is a quantale. The hom-object $B \multimap C$ is given by $\bar{B} \cup C$, where $\bar{B}$ is the complement of $B$ : it contains all elements of $S$ not contained in $B$. To see that this satisfies Eq. (2.80), note that if $(A \cap B) \subseteq C$, then

$$
A=(A \cap \bar{B}) \cup(A \cap B) \subseteq \bar{B} \cup C
$$

On the other hand, if $A \subseteq(\bar{B} \cup C)$, then

$$
A \cap B \subseteq(\bar{B} \cup C) \cap B=(\bar{B} \cap B) \cup(C \cap B)=C \cap B \subseteq C
$$

So $(P(S), \subseteq, S, \cap$ ) is monoidal closed. Furthermore, joins are given by union of subobjects, so it is a quantale.

Solution to Exercise 2.92.

1a. In Bool, $(\vee \varnothing)=$ false, the least element.

1b. In Cost, $(\bigvee \varnothing)=\infty$. This is because we use the opposite order $\geq$ on $[0, \infty]$, so $\bigvee \varnothing$ is the greatest element of $[0, \infty]$. Note that in this case our convention from Definition 2.90, where we denote $(\vee \varnothing)=0$, is a bit confusing! Beware!

2a. In Bool, $x \vee y$ is the usual join, OR.

2b. In Cost, $x \vee y$ is the minimum $\min (x, y)$. Again because we use the opposite order on $[0, \infty]$, the join is the greatest number less than or equal to $x$ and $y$.

Solution to Exercise 2.103.

The $2 \times 2$-identity matrices for $(\mathbb{N}, \leq, 1, *)$, Bool, and Cost are respectively

$$
\left(\begin{array}{ll}
1 & 0 \\
0 & 1
\end{array}\right), \quad\left(\begin{array}{cc}
\text { true } & \text { false } \\
\text { false } & \text { true }
\end{array}\right), \quad \text { and }\left(\begin{array}{cc}
0 & \infty \\
\infty & 0
\end{array}\right)
$$

Solution to Exercise 2.104.

1. We first use Proposition 2.87 (2) and symmetry to show that for all $v \in V, 0 \otimes v=0$.

$$
0 \otimes v \cong v \otimes 0 \cong\left(v \otimes \bigvee_{a \in \varnothing} a\right)=\bigvee_{a \in \varnothing}(v \otimes a)=0
$$

Then we may just follow the definition in Eq. (2.101):

$$
\begin{aligned}
I_{X} * M(x, y) & =\bigvee_{x^{\prime} \in X} I_{X}\left(x, x^{\prime}\right) \otimes M\left(x^{\prime}, y\right) \\
& =\left(I_{X}(x, x) \otimes M(x, y)\right) \vee\left(\bigvee_{x^{\prime} \in X, x^{\prime} \neq x} I_{X}\left(x, x^{\prime}\right) \otimes M\left(x^{\prime}, y\right)\right) \\
& =(I \otimes M(x, y)) \vee\left(\bigvee_{x^{\prime} \in X, x^{\prime} \neq x} 0 \otimes M\left(x^{\prime}, y\right)\right) \\
& =M(x, y) \vee 0=M(x, y) .
\end{aligned}
$$

2. This again follows from Proposition 2.87 (2) and symmetry, making use also of the associativity of $\otimes$ :

$$
\begin{aligned}
((M * N) * P)(w, z) & =\bigvee_{y \in Y}\left(\bigvee_{x \in X} M(w, x) \otimes N(x, y)\right) \otimes P(y, z) \\
& \cong \bigvee_{y \in Y, x \in X} M(w, x) \otimes N(x, y) \otimes P(y, z) \\
& \cong \bigvee_{x \in X} M(w, x) \otimes\left(\bigvee_{y \in Y} N(x, y) \otimes P(y, z)\right) \\
& =(M *(N * P))(w, z)
\end{aligned}
$$

Solution to Exercise 2.105.

We have the matrices

$$
M_{X}=\left(\begin{array}{cccc}
0 & \infty & 3 & \infty \\
2 & 0 & \infty & 5 \\
\infty & 3 & 0 & \infty \\
\infty & \infty & 6 & 0
\end{array}\right) \quad M_{X}^{2}=\left(\begin{array}{cccc}
0 & 6 & 3 & \infty \\
2 & 0 & 5 & 5 \\
5 & 3 & 0 & 8 \\
\infty & 9 & 6 & 0
\end{array}\right) \quad M_{X}^{3}=M_{X}^{4}=\left(\begin{array}{cccc}
0 & 6 & 3 & 11 \\
2 & 0 & 5 & 5 \\
5 & 3 & 0 & 8 \\
11 & 9 & 6 & 0
\end{array}\right)
$$

\section*{A. 3 Solutions for Chapter 3 .}

\section*{Solution to Exercise 3.3.}

There are five non-ID columns in Eq. (3.1)and five arrows in Eq. (3.2). This is not a coincidence: there is always one arrow for every non-ID column.

\section*{Solution to Exercise 3.9.}

To do this precisely, we should define concatenation technically. If $G=(V, A, s, t)$ is a graph, define a path in $G$ to be a tuple of the form $\left(v, a_{1}, \ldots, a_{n}\right)$ where $v \in V$ is a vertex, $s\left(a_{1}\right)=v$, and $t\left(a_{i}\right)=s\left(a_{i+1}\right)$ for all $i \in\{1, \ldots, n-1\}$; the length of this path is $n$, and this definition makes sense for any $n \in \mathbb{N}$. We say that the source of $p$ is $s(p):=v$ and the target of $p$ is defined to be

$$
t(p):=\left\{\begin{array}{l}
v \text { if } n=0 \\
t\left(a_{n}\right) \text { if } n \geq 1
\end{array}\right.
$$

Two paths $p=\left(v, a_{1}, \ldots, a_{m}\right)$ and $q=\left(w, b_{1}, \ldots, b_{n}\right)$ can be concatenated if $t(p)=s(q)$, in which case the concatenated path $p ; q$ is defined to be

$$
(p ; q):=\left(v, a_{1}, \ldots, a_{m}, b_{1}, \ldots, b_{n}\right)
$$

We are now ready to check unitality and associativity. A path $p$ is an identity in Free(G) iff $p=(v)$ for some $v \in V$. It is easy to see from the above that $(v)$ and $\left(w, b_{1}, \ldots, b_{n}\right)$ can be concatenated iff $v=w$, in which case the result is $\left(w, b_{1}, \ldots, b_{n}\right)$. Similarly $\left(v, a_{1}, \ldots, a_{m}\right)$ and ( $w$ ) can be concatenated iff $w=t\left(a_{m}\right)$, in which case the result is $\left(v, a_{1}, \ldots, a_{m}\right)$. Finally, for associativity with $p$ and $q$ as above and $r=\left(x, c_{1}, \ldots, c_{o}\right)$, the formula readily reads that whichever way they are concatenated, $(p ; q) \circ r$ or $p ;(q ; r)$, the result is

$$
\left(v, a_{1}, \ldots, a_{m}, b_{1}, \ldots, b_{n}, c_{1}, \ldots, c_{o}\right)
$$

\section*{Solution to Exercise 3.10.}

We often like to name identity morphisms by the objects they're on, and we do that here: $v_{2}$ means $\operatorname{id}_{v_{2}}$. We write $\boxtimes$ when the composite does not make sense (i.e. when the target of the first morphism does not agree with the source of the second).

\begin{tabular}{c|cccccc}
$\nearrow$ & $v_{1}$ & $f_{1}$ & $f_{1} \cong f_{2}$ & $v_{2}$ & $f_{2}$ & $v_{3}$ \\
\hline$v_{1}$ & $v_{1}$ & $f_{1}$ & $f_{1} \Im f_{2}$ & $\boxtimes$ & $\boxtimes$ & $\boxtimes$ \\
$f_{1}$ & $\boxtimes$ & $\boxtimes$ & $\boxtimes$ & $f_{1}$ & $f_{1} \cong f_{2}$ & $\boxtimes$ \\
$f_{1} \Im f_{2}$ & $\boxtimes$ & $\boxtimes$ & $\boxtimes$ & $\boxtimes$ & $\boxtimes$ & $f_{1} \cong f_{2}$ \\
$v_{2}$ & $\boxtimes$ & $\boxtimes$ & $\boxtimes$ & $v_{2}$ & $f_{2}$ & $\boxtimes$ \\
$f_{2}$ & $\boxtimes$ & $\boxtimes$ & $\boxtimes$ & $\boxtimes$ & $\boxtimes$ & $f_{2}$ \\
$v_{3}$ & $\boxtimes$ & $\boxtimes$ & $\boxtimes$ & $\boxtimes$ & $\boxtimes$ & $v_{3}$
\end{tabular}

\section*{Solution to Exercise 3.12.}

1. The category 1 has one object $v_{1}$ and one morphism, the identity $\mathrm{id}_{v_{1}}$.

2. The category $\mathbf{0}$ is empty; it has no objects and no morphisms.

3. The pattern for number of morphisms in $\mathbf{0 , 1 , 2 , 3}$ is $0,1,3,6$; does this pattern look familiar? These are the first few 'triangle numbers,' so we could guess that the number of morphisms in $\mathbf{n}$, the free category on the following graph

$$
\stackrel{v_{1}}{\bullet} \xrightarrow{f_{1}} \stackrel{v_{2}}{\bullet} \xrightarrow{f_{2}} \cdots \xrightarrow{f_{n-1}} \stackrel{v_{n}}{\bullet}
$$

is $1+2+\cdots+n$. This makes sense because (and the proof strategy would be to verify that) the above graph has $n$ paths of length 0 , it has $n-1$ paths of length 1 , and so on: it has $n-i$ paths of length $i$ for every $0 \leq i \leq n$.

Solution to Exercise 3.15.

The correspondence was given by sending a path to its length. Concatenating a path of length $m$ with a path of length $n$ results in a path of length $m+n$.

Solution to Exercise 3.16.

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-290.jpg?height=284&width=442&top_left_y=514&top_left_x=858)

1. The ten paths are as follows

$$
A, \quad A ; f, \quad A ; g, \quad A ; f ; h, A ; g ; i, \quad B, \quad B ; h, C, \quad C ; i, \quad D
$$
2. $A ; f ; h$ is parallel to $A \because g ; i$, in that they both have the same domain and both have the same codomain.

3. A is not parallel to any of the other nine paths.

Solution to Exercise 3.17.

The morphisms in the given diagram are as follows:

$$
A, \quad A ; f, \quad A ; g, \quad A \nsubseteq j, \quad B, \quad B ; h, \quad C, \quad C ; i, \quad D
$$

Note that $A \AA f ; h=j=A \AA g \circ i$.

Solution to Exercise 3.19.

There are four morphisms in $\mathcal{D}$, shown below, namely $z, s, s ; s$, and $s ; s ; s$ :

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-290.jpg?height=221&width=338&top_left_y=1551&top_left_x=904)

Solution to Exercise 3.21.

The equations that make the graphs into preorders are shown below

$$
G_{1}=\begin{gathered}
\underset{g}{\stackrel{f}{\rightrightarrows}} \bullet \\
f=g
\end{gathered}
$$
![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-290.jpg?height=260&width=932&top_left_y=1957&top_left_x=781)

\section*{Solution to Exercise 3.22}

The preorder reflection of a category $\mathcal{C}$ has the same objects and either one morphism or none between two objects, depending on whether or not a morphism between them exists in $\mathcal{C}$. So the preorder reflection of $\mathbb{N}$ has one object and one morphism from it to itself, which must be the identity. In other words, the preorder reflection of $\mathbb{N}$ is $\mathbf{1}$.

\section*{Solution to Exercise 3.25.}

A function $f: \underline{2} \rightarrow \underline{3}$ can be described as an ordered pair $(f(1), f(2))$. The nine such functions are given by the following ordered pairs, which we arrange into a 2-dimensional grid with 3 entries in each dimension, just for "funzies": 1

$$
\begin{array}{lll}
(1,1) & (1,2) & (1,3) \\
(2,1) & (2,2) & (2,3) \\
(3,1) & (3,2) & (3,3)
\end{array}
$$

Solution to Exercise 3.30.

1. The inverse to $f(a)=2, f(b)=1, f(c)=3$ is given by

$$
f^{-1}(1)=b, \quad f^{-1}(2)=a, \quad f^{-1}(3)=c
$$

2. There are 6 distinct isomorphisms. In general, if $A$ and $B$ are sets, each with $n$ elements, then the number of isomorphisms between them is $n$-factorial, often denoted $n!$. So for example there are $5 * 4 * 3 * 2 * 1=120$ isomorphisms between $\{1,2,3,4,5\}$ and $\{a, b, c, d, e\}$.

Solution to Exercise 3.31.

We have to show that for any object $c \in \mathcal{C}$, the identity id ${ }_{c}$ has an inverse, i.e. a morphism $f: c \rightarrow c$ such that $f \circ \mathrm{id}_{c}=\operatorname{id}_{c}$ and $\operatorname{id}_{c} ̊ f=\operatorname{id}_{c}$. Take $f=\operatorname{id}_{c}$; this works.

Solution to Exercise 3.32.

1. The monoid in Example 3.13 is not a group, because the morphism $s$ has no inverse. Indeed each morphism is of the form $s^{n}$ for some $n \in \mathbb{N}$ and composing it with $s$ gives $s^{n+1}$, which is never $s^{0} . p$
2. $\mathcal{C}$ from Example 3.18 is a group: the identity is always an isomorphism, and the other morphism $s$ has inverse $s$.

\section*{Solution to Exercise 3.33.}

You may have found a person whose mathematical claims you can trust! Whenever you compose two morphisms in Free $(G)$, their lengths add, and the identities are exactly those morphisms whose length is 0 . In order for $p$ to be an isomorphism, there must be some $q$ such that $p ; q=$ id and $q \circ p=\mathrm{id}$, in which case the length of $p$ (or $q$ ) must be 0 .

\section*{Solution to Exercise 3.37.}

The other three functors $2 \rightarrow 3$ are shown here:
![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-291.jpg?height=358&width=1336&top_left_y=1762&top_left_x=404)

\section*{Solution to Exercise 3.39}

There are nine morphisms in $\mathcal{F}$; as usual we denote identities by the object they're on. These morphisms are sent to the following morphisms in $\mathcal{C}$ :

$$
\begin{gathered}
A^{\prime} \mapsto A, \quad f^{\prime} \mapsto f, \quad g^{\prime} \mapsto g, \quad f^{\prime} ; h^{\prime} \mapsto f ; h, \quad g^{\prime} ; i^{\prime} \mapsto f ; h, \\
B^{\prime} \mapsto B, \quad h^{\prime} \mapsto h, \quad C^{\prime} \mapsto C, \quad i^{\prime} \mapsto i, \quad D^{\prime} \mapsto D .
\end{gathered}
$$
\footnotetext{
${ }^{1}$ Of course, this is not mere funzies; this is category theory!
}

If one of these seems different from the rest, it's probably $g^{\prime} ; i^{\prime} \mapsto f ; h$. But note that in fact also $g^{\prime} ; i^{\prime} \mapsto g ; i$ because $g ; i=f ; h$, so it's not an outlier after all.

Solution to Exercise 3.40.

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-292.jpg?height=81&width=1409&top_left_y=407&top_left_x=369)
whose on-morphisms parts are different. There are only two ways to do this, and we choose one of them:

$$
F(a):=a^{\prime}, \quad G(a):=a^{\prime}, \quad F(b):=b^{\prime}, \quad G(b):=b^{\prime}, \quad F(f):=f_{1}, \text { and } G(f):=f_{2}
$$

The other way reverses $f_{1}$ and $f_{2}$.

Solution to Exercise 3.43.

1. Let $\mathcal{C}$ be a category. Then defining id $\mathcal{C}: \mathcal{C} \rightarrow \mathcal{C}$ by id $\mathcal{C}(x)=x$ for every object and morphism in $\mathcal{C}$ is a functor because it preserves identities $\operatorname{id}_{\mathcal{C}}\left(\mathrm{id}_{c}\right)=\operatorname{id}_{c}=\operatorname{id}_{\mathrm{ide}}(c)$ for each object $c \in \mathrm{Ob}(\mathcal{C})$, and it preserves composition $\operatorname{id}_{\mathcal{C}}(f ; g)=f ; g=\operatorname{id}_{\mathcal{C}}(f) ; \operatorname{id}_{\mathcal{C}}(g)$ for each pair of composable morphisms $f, g$ in $\mathcal{C}$.

2. Given functors $F: \mathcal{C} \rightarrow \mathcal{D}$ and $G: \mathcal{D} \rightarrow \mathcal{E}$, we need to show that $F ; G$ is a functor, i.e. that it preserves preserves identities and compositions. If $c \in \mathcal{C}$ is an object then $(F ; G)\left(\mathrm{id}_{c}\right)=$ $G\left(F\left(\mathrm{id}_{c}\right)\right)=G\left(\operatorname{id}_{F(c)}\right)=\operatorname{id}_{G(F(c))}$ because $F$ and $G$ preserve identities. If $f, g$ are composable morphisms in $\mathcal{C}$ then

$$
(F ; G)(f ; g)=G(F(f) ; F(g))=G(F(f)) ; G(F(g))
$$

because $F$ and $G$ preserve composition.

3. We have proposed objects, morphisms, identities, and a composition formula for a category Cat: they are categories, functors, and the identities and compositions given above. We need to check that the two properties, unitality and associativity, hold. So suppose $F: \mathcal{C} \rightarrow \mathcal{D}$ is a functor and we pre-compose it as above with ide ; it is easy to see that the result will again be $F$, and similarly if we post-compose $F$ with id $_{\mathcal{D}}$. This gives unitality, and associativity is just as easy, though more wordy. Given $F$ as above and $G: \mathcal{D} \rightarrow \mathcal{E}$ and $H: \mathcal{E} \rightarrow \mathcal{F}$, we need to show that $(F \circ G) \AA H=F \circ(G \because H)$. It's a simple application of the definition: for any $x \in \mathcal{C}$, be it an object or morphism, we have

$$
((F ; G) \nsubseteq H)(c)=H((F ; G)(c))=H(G(F(c)))=(G \varsubsetneqq H)(F(c))=(F \varsubsetneqq(G ̊ H))(c)
$$

Solution to Exercise 3.45.

Let $S \in$ Set be a set. Define $F_{S}: \mathbf{1} \rightarrow$ Set by $F_{S}(1)=S$ and $F_{S}\left(\mathrm{id}_{1}\right)=\mathrm{id}$. With this definition, $F_{S}$ preserves identities and compositions (the only compositions in $\mathbf{1}$ is the composite of the identity with itself), so $F_{S}$ is a functor with $F_{S}(1)=S$ as desired.

Solution to Exercise 3.48.

We are asked what sort of data "makes sense" for the schemas below?
![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-292.jpg?height=226&width=762&top_left_y=1974&top_left_x=367)

This is a subjective question, so we propose an answer for your consideration.

1. Data on this schema, i.e. a set-valued functor, assigns a set $D(z)$ and a function $D(s): D(z) \rightarrow D(z)$, such that applying that function twice is the identity. This sort of function is called an involution of the set $D_{z}$ :

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-292.jpg?height=132&width=423&top_left_y=2368&top_left_x=905)

It's a do-si-do, a "partner move," where everyone picks a partner (possibly themselves) and exchanges with them. One example one could take $D$ to be the set of pixels in a photograph, and take $s$ to be the function sending each pixel to its mirror image across the vertical center line of the photograph.

2. We could make $D(c)$ the set of people at a "secret Santa" Christmas party, where everyone gives a gift to someone, possibly themselves. Take $D(b)$ to be the set of gifts, $g$ the giver function (each gift is given by a person), and $h$ the receiver function (each gift is received by a person), $D(a)$ is the set of people who give a gift to themselves, and $d(f): D(a) \rightarrow D(b)$ is the inclusion.

\section*{Solution to Exercise 3.55.}

1. The expert packs so much information in so little space! Suppose given three objects $F, G, H \in \mathcal{D}^{\mathcal{E}}$; these are functors $F, G, H: \mathcal{C} \rightarrow \mathcal{D}$. Morphisms $\alpha: F \rightarrow G$ and $\beta: G \rightarrow H$ are natural transformations. Most beginners seem to think about a natural transformation in terms of its naturality squares, but the main thing to keep in mind is its components; the naturality squares constitute a check that comes later.

So for each $c \in \mathcal{C}, \alpha$ has a component $\alpha_{c}: F(c) \rightarrow G(c)$ and $\beta$ has a component $\beta_{c}: G(c) \rightarrow H(c)$ in $\mathcal{D}$. The expert has told us to define $(\alpha \nsubseteq \beta)_{c}:=\left(\alpha_{c} \varsubsetneqq \beta_{c}\right)$, and indeed that is a morphism $F(c) \rightarrow H(c)$.

Now we do the check. For any $f: c \rightarrow c^{\prime}$ in $\mathcal{C}$, the inner squares of the following diagram commute because $\alpha$ and $\beta$ are natural; hence the outer rectangle does too:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-293.jpg?height=198&width=458&top_left_y=1262&top_left_x=882)

2. We propose that the identity natural transformation $\operatorname{id}_{F}$ on a functor $F: \mathcal{C} \rightarrow \mathcal{D}$ has as its $c$-component the morphism $\left(\operatorname{id}_{F}\right)_{c}:=\operatorname{id}_{F(c)}$ in $\mathcal{D}$, for any $c$. The naturality square

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-293.jpg?height=203&width=306&top_left_y=1628&top_left_x=969)

obviously commutes for any $f: c \rightarrow c^{\prime}$. And it is unital: post-composing id ${ }_{F}$ with any $\beta: F \rightarrow G$ (and similarly for precomposing with any $\alpha: E \rightarrow F$ ) results in a natural transformation $\operatorname{id}_{F} \AA \beta$ with components $\left(\mathrm{id}_{F}\right)_{c} ๆ \beta_{c}=\left(\operatorname{id}_{F(c)} ๆ \beta_{c}\right)=\beta_{c}$, and this is just $\beta$ as desired.

\section*{Solution to Exercise 3.58.}

We have a category $\mathcal{C}$ and a preorder $\mathcal{P}$, considered as a category.

1. Suppose that $F, G: \mathcal{C} \rightarrow \mathcal{P}$ are functors and $\alpha, \beta: F \rightarrow G$ are natural transformations; we need to show that $\alpha=\beta$. It suffices to check that $\alpha_{\mathcal{c}}=\beta_{\mathcal{c}}$ for each object $c \in \operatorname{Ob}(\mathcal{C})$. But $\alpha_{\mathcal{c}}$ and $\beta_{c}$ are morphisms $F(c) \rightarrow G(c)$ in $\mathcal{P}$, which is a preorder, and the definition of a preorder-considered as a category-is that it has at most one morphism between any two objects. Thus $\alpha_{c}=\beta_{c}$, as desired.

2. This is false. Let $\mathcal{P}:=\mathbf{1}$, let $\mathcal{C}:=\underset{a}{a} \underset{f_{2}}{f_{1}} \bullet$, let $F(1):=a$, let $G(1):=b$, let $\alpha_{1}:=f_{1}$, and let $\beta_{1}:=g_{2}$.

Solution to Exercise 3.62.

We need to write down the following

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-294.jpg?height=252&width=743&top_left_y=373&top_left_x=707)

as a Gr-instance, as in Eq. (3.61). The answer is as follows:

\begin{tabular}{r|llr|} 
Arrow & source & target & Vertex \\
\cline { 1 - 2 } Mngr & Employee & Employee & Department \\
WorksIn & Employee & Department & Employee \\
Secr & Department & Employee & string \\
FName & Employee & string & \\
DName & Department & string &
\end{tabular}

Solution to Exercise 3.64.

Let $G, H$ be the following graphs:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-294.jpg?height=110&width=893&top_left_y=1177&top_left_x=624)

and let's believe the authors that there is a unique graph homomorphism $\alpha: G \rightarrow H$ for which $\alpha_{\text {Arrow }}(a)=d$.

1. We have $\alpha_{\text {Arrow }}(b)=e$ and $\alpha_{\text {Vertex }}(1)=4, \alpha_{\text {Vertex }}(2)=5$, and $\alpha_{\text {Vertex }}(3)=5$.

2. We roughly copy the tables and then draw the lines (shown in black; ignore the dashed lines for now):

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-294.jpg?height=339&width=569&top_left_y=1522&top_left_x=838)

3. It works! One example of the naturality is shown with the help of dashed blue lines above. See how both paths starting at $a$ end at 5?

\section*{Solution to Exercise 3.67.}

We just need to write out the composite of the following functors

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-294.jpg?height=360&width=845&top_left_y=2140&top_left_x=651)
in the form of a database, and then draw the graph. The results are given below.

\begin{tabular}{c|cc} 
Arrow & source & target \\
\hline 1 & 4 & 1 \\
2 & 4 & 2 \\
3 & 5 & 3 \\
4 & 5 & 4 \\
5 & 5 & 5 \\
6 & 7 & 6 \\
7 & 6 & 7
\end{tabular}

\begin{tabular}{c|} 
Vertex \\
\hline 1 \\
2 \\
3 \\
4 \\
5 \\
6 \\
7
\end{tabular}

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-295.jpg?height=339&width=632&top_left_y=324&top_left_x=1126)

Solution to Exercise 3.73.

We are interested in how the functors $-\times B$ and $(-)^{B}$ should act on morphisms for a given set $B$. We didn't specify this in the text-we only specified $-\times B$ and $(-)^{B}$ on objects-so in some sense this exercise is open: you can make up anything you want, under the condition that it is functorial. However, the authors cannot think of any such answers except the one we give below.

1. Given an arbitrary function $f: X \rightarrow Y$, we need a function $X \times B \rightarrow Y \times B$. We suggest the function which might be denoted $f \times B$; it sends $(x, b)$ to $(f(x), b)$. This assignment is functorial: applied to id $X$ it returns id ${ }_{X \times B}$ and it preserves composition.

2. Given a function $f: X \rightarrow Y$, we need a function $X^{B} \rightarrow Y^{B}$. The canonical function would be denoted $f^{B}$; it sends a function $g: B \rightarrow X$ to the composite $(g ; f): B \rightarrow X \rightarrow Y$. This is functorial: applied to id $\mathrm{id}_{X}$ it sends $g$ to $g$, i.e. $f^{B}\left(\mathrm{id}_{X}\right)=\operatorname{id}_{X^{B}}$, and applied to the composite $\left(f_{1} ; f_{2}\right): X \rightarrow Y \rightarrow Z$, we have

$$
\left(f_{1} ; f_{2}\right)^{B}(g)=g ;\left(f_{1} ; f_{2}\right)=\left(g ; f_{1}\right) ; f_{2}=\left(f_{1}^{B} ; f_{2}^{B}\right)(g)
$$

for any $g \in X^{B}$.

3. If $p: \mathbb{N} \rightarrow \mathbb{N}^{\mathbb{N}}$ is the result of currying $+: \mathbb{N} \times \mathbb{N} \rightarrow \mathbb{N}$, then $p(3)$ is an element of $\mathbb{N}^{\mathbb{N}}$, i.e. we have $p(3): \mathbb{N} \rightarrow \mathbb{N}$; what function is it? It is the function that adds three. That is $p(3)(n):=n+3$.

Solution to Exercise 3.76.

The functor !: $\mathcal{C} \rightarrow \mathbf{1}$ from Eq. (3.75) sends each object $c \in \mathcal{C}$ to the unique object $1 \in \mathbf{1}$ and sends each morphism $f: c \rightarrow d$ in $\mathcal{C}$ to the unique morphism $\operatorname{id}_{1}: 1 \rightarrow 1$ in 1 .

\section*{Solution to Exercise 3.78.}

We want to draw the graph corresponding to the instance $I: \mathcal{G} \rightarrow$ Set shown below:

\begin{tabular}{c|ccc} 
Email & sent_by & received_by & \\
\cline { 1 - 2 } Em_1 & Bob & Grace & Address \\
\cline { 4 - 4 } Em_2 & Grace & Pat & Bob \\
Em_3 & Bob & Emory & Doug \\
Em_4 & Sue & Doug & Emory \\
Em_5 & Doug & Sue & Grace \\
Em_6 & Bob & Bob & Pat \\
\cline { 4 - 4 } & & Sue
\end{tabular}

Here it is, with names and emails shortened (e.g. $\mathrm{B}=\mathrm{Bob}, 3=E m \_3$ ):

$$
E \stackrel{3}{\stackrel{R}{R}} \stackrel{1}{\longrightarrow} G \xrightarrow{2} P \quad S \overbrace{\sqrt{5}}^{4} D
$$

Solution to Exercise 3.81.

An object $z$ is terminal in some category $\mathcal{C}$ if, for every $c \in \mathcal{C}$ there exists a unique morphism $c \rightarrow z$. When $\mathcal{C}$ is the category underlying a preorder, there is at most one morphism between any two objects,
so the condition simplifies: an object $z$ is terminal iff, for every $c \in \mathcal{C}$ there exists a morphism $c \rightarrow z$. The morphisms in a preorder are written with $\leq$ signs, so $z$ is terminal iff, for every $c \in P$ we have $c \leq z$, and this is the definition of top element.

Solution to Exercise 3.82.

The terminal object in Cat is $\mathbf{1}$ because by Exercise 3.76 there is a unique morphism (functor) $\mathcal{C} \rightarrow \mathbf{1}$ for any object (category) $\mathcal{C} \in$ Cat.

\section*{Solution to Exercise 3.83}

Consider the graph $2 V:=\bullet$ with two vertices and no arrows, and let $\mathcal{C}=\operatorname{Free}(2 V)$; it has two objects and two morphisms (the identities). This category does not have a terminal object because it does not have any morphisms from one object to the other.

Solution to Exercise 3.88.

A product of $x$ and $y$ in $\mathcal{P}$ is an object $z \in \mathcal{P}$ equipped with maps $z \rightarrow x$ and $z \rightarrow y$ such that for any other object $z^{\prime}$ and maps $z^{\prime} \rightarrow x$ and $z^{\prime} \rightarrow y$, there is a unique morphism $z^{\prime} \rightarrow z$ making the evident triangles commute. But in a preorder, the maps are denoted $\leq$, they are unique if they exist, and all diagrams commute. Thus the above becomes: a product of $x$ and $y$ in $\mathcal{P}$ is an object $z$ with $z \leq x$ and $z \leq y$ such that for any other $z^{\prime}$, if $z^{\prime} \leq x$ and $z^{\prime} \leq y$ then $z^{\prime} \leq z$. This is exactly the definition of meet, $z=x \wedge y$.

Solution to Exercise 3.90.

1. The identity morphism on the object $(c, d)$ in the product category $\mathcal{C} \times \mathcal{D}$ is $\left(\mathrm{id}_{c}, \mathrm{id}_{d}\right)$.

2. Suppose given three composable morphisms in $\mathcal{C} \times \mathcal{D}$

$$
\left(c_{1}, d_{1}\right) \xrightarrow{\left(f_{1}, g_{1}\right)}\left(c_{2}, d_{2}\right) \xrightarrow{\left(f_{2}, g_{2}\right)}\left(c_{3}, d_{3}\right) \xrightarrow{\left(f_{3}, g_{3}\right)}\left(c_{4}, d_{4}\right) .
$$

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-296.jpg?height=46&width=1325&top_left_y=1343&top_left_x=454)
a product category is given component-wise. That means the left-hand side is $\left(\left(f_{1} ๆ f_{2}\right)\right.$ ๆ $f_{3},\left(g_{1}\right.$ ๆ $\left.\left.g_{2}\right) \nsubseteq g_{3}\right)$, whereas the right-hand side is $\left(f_{1} \nsubseteq\left(f_{2} \nsubseteq f_{3}\right), g_{1} \nsubseteq\left(g_{2} \nsubseteq g_{3}\right)\right)$, and these are equal because both $\mathcal{C}$ and $\mathcal{D}$ individually have associative composition.

3. The product category $\mathbf{1} \times \mathbf{2}$ has two objects $(1,1)$ and $(1,2)$ and one non-identity morphism $(1,1) \rightarrow(1,2)$. It is not hard to see that it looks the same as 2 . In fact, for any $\mathcal{C}$ there is an isomorphism of categories $\mathbf{1} \times \mathcal{C} \cong \mathcal{C}$.

4. Let $P$ and $Q$ be preorders, let $X=P \times Q$ be their product preorder as defined in Example 1.56, and let $\mathcal{P}, \mathcal{Q}$, and $\mathcal{X}$ be the corresponding categories. Then $\mathcal{X}=\mathcal{P} \times \mathcal{Q}$.

Solution to Exercise 3.91.

A product of $X$ and $Y$ is an object $Z$ equipped with morphisms $X \stackrel{p_{X}}{\longleftrightarrow} Z \xrightarrow{p_{Y}} Y$ such that for any other object $Z^{\prime}$ equipped with morphisms $X \stackrel{p_{X}^{\prime}}{\longleftrightarrow} Z^{\prime} \xrightarrow{p_{Y}^{\prime}} Y$, there is a unique morphism $f: Z^{\prime} \rightarrow Z$ making the triangles commute, $f ; p_{X}=p_{X}^{\prime}$ and $f ; p_{Y}=p_{Y}^{\prime}$. But "an object equipped with morphisms to $X$ and $Y^{\prime \prime}$ is exactly the definition of an object in $\operatorname{Cone}(X, Y)$, and a morphism $f$ making the triangles commute is exactly the definition of a morphism in $\operatorname{Cone}(X, Y)$. So the definition above becomes: a product of $X$ and $Y$ is an object $Z \in \operatorname{Cone}(X, Y)$ such that for any other object $Z^{\prime}$ there is a unique morphism $Z^{\prime} \rightarrow Z$ in Cone $(X, Y)$. This is exactly the definition of $Z$ being terminal in Cone $(X, Y)$.

Solution to Exercise 3.97.

Suppose $\mathcal{J}$ is the graph $\begin{aligned} & v_{1} \\ & \bullet v_{2} \\ & \bullet\end{aligned}$ and $D: \mathcal{J} \rightarrow$ Set is given by two sets, $D\left(v_{1}\right)=A$ and $D\left(v_{2}\right)=B$ for sets $A, B$. The product of these two sets is $A \times B$. Let's check that the limit formula in Theorem 3.95 gives the same answer. It says

$$
\lim _{\mathcal{J}} D:=\left\{\left(d_{1}, \ldots, d_{n}\right) \mid d_{i} \in D\left(v_{i}\right) \text { for all } 1 \leq i \leq n\right. \text { and }
$$

for all $a: v_{i} \rightarrow v_{j} \in A$, we have $\left.D(a)\left(d_{i}\right)=d_{j}\right\}$.

But in our case $n=2$, there are no arrows in the graph, and $D\left(v_{1}\right)=A$ and $D\left(v_{2}\right)=B$. So the formula reduces to

$$
\lim _{\mathcal{Z}} D:=\left\{\left(d_{1}, d_{2}\right) \mid d_{1} \in A \text { and } d_{2} \in B\right\}
$$

which is exactly the definition of $A \times B$.

Solution to Exercise 3.101.

Given a functor $F: \mathcal{C} \rightarrow \mathcal{D}$, we define its opposite $F^{\text {op }}: \mathcal{C}$ op $\rightarrow \mathcal{D}^{\text {op }}$ as follows. For each object $c \in$ $\operatorname{Ob}\left(\mathcal{C}^{\mathrm{op}}\right)=\operatorname{Ob}(\mathcal{C})$, put $F^{\mathrm{op}}(c):=F(c)$. For each morphism $f: c_{1} \rightarrow c_{2}$ in $\mathcal{C}^{\text {op }}$, we have a corresponding morphism $f^{\prime}: c_{2} \rightarrow c_{1}$ in $\mathcal{C}$ and thus a morphism $F\left(f^{\prime}\right): F\left(c_{2}\right) \rightarrow F\left(c_{1}\right)$ in $\mathcal{D}$, and thus a morphism $F\left(f^{\prime}\right)^{\prime}: F^{\mathrm{op}}\left(c_{1}\right) \rightarrow F^{\mathrm{op}}\left(c_{2}\right)$. Hence we can define $F^{\mathrm{op}}(f):=F\left(f^{\prime}\right)^{\prime}$. Note that the primes $\left(-^{\prime}\right)$ are pretty meaningless, we only put them there to differentiate between things that are very closely related.

It is easy to check that our definition of $F^{\mathrm{op}}$ is functorial: it sends identities to identities and composites to composites.

\section*{A. 4 Solutions for Chapter 4 .}

\section*{Solution to Exercise 4.4.}

1. The Hasse diagram for $x^{\mathrm{op}} \times y$ is shown here (ignore the colors):

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-298.jpg?height=379&width=680&top_left_y=434&top_left_x=777)

2. There is a profunctor $\Lambda: x \rightarrow y$, i.e. a functor $x^{\circ} \times y \rightarrow \mathbb{B}$, such that, in the picture above, blue is sent to true and black is sent to false, i.e.

$$
\begin{aligned}
\Lambda(\text { monoid, nothing }) & =\Lambda(\text { monoid, this book }) \\
& =\Lambda(\text { preorder, this book }) \\
\Lambda(\text { preorder, nothing }) & =\Lambda(\text { category, nothing })=\text { false. }
\end{aligned}
$$

The preorder $x^{\circ} \times y$ describes tasks in decreasing difficulty. For example, (we hope) it is easier for my aunt to explain a monoid given this book than for her to explain a monoid without this book. The profunctor $\Lambda$ describes possible states of knowledge for my aunt: she can describe monoids without help, categories with help from the book, etc. It is an upper set because we assume that if she can do a task, she can also do any easier task.

Solution to Exercise 4.7.

We've done this one before! We hope you remembered how to do it. If not, see Exercise 2.84.

\section*{Solution to Exercise 4.9.}

Recall from Definition 2.41 that a $\mathcal{V}$-functor $\Phi: X^{\mathrm{op}} \times y \rightarrow \mathcal{V}$ is a function $\Phi: \operatorname{Ob}\left(X^{\circ p} \times y\right) \rightarrow \operatorname{Ob}(\mathcal{V})$ such that for all $(x, y)$ and $\left(x^{\prime}, y^{\prime}\right)$ in $X^{\text {op }} \times y$ we have

$$
\left(X^{\mathrm{op}} \times y\right)\left((x, y),\left(x^{\prime}, y^{\prime}\right)\right) \leq \mathcal{V}\left(\Phi(x, y), \Phi\left(x^{\prime}, y^{\prime}\right)\right)
$$

Using the definitions of product $\mathcal{V}$-category (Definition 2.74) and opposite $\mathcal{V}$-category (Exercise 2.73) on the left hand side, and using Remark 2.89, which describes how we are viewing the quantale $\mathcal{V}$ as enriched over itself, on the right hand side, this unpacks to

$$
x\left(x^{\prime}, x\right) \otimes y\left(y, y^{\prime}\right) \leq \Phi(x, y) \multimap \Phi\left(x^{\prime}, y^{\prime}\right)
$$

Using symmetry of $\otimes$ and the definition of hom-element, Eq. (2.80), we see that $\Phi$ is a profunctor if and only if

$$
x\left(x^{\prime}, x\right) \otimes \Phi(x, y) \otimes y\left(y, y^{\prime}\right) \leq \Phi\left(x^{\prime}, y^{\prime}\right)
$$

Solution to Exercise 4.10.

Yes, since a Bool-functor is exactly the same as a monotone map, the definition of Bool-profunctor and that of feasibility relation line up perfectly!

\section*{Solution to Exercise 4.12.}

The feasibility matrix for $\Phi$ is

\begin{tabular}{c|ccccc}
$\Phi$ & $a$ & $b$ & $c$ & $d$ & $e$ \\
\hline$N$ & true & false & true & false & true \\
$E$ & true & true & true & true & true \\
$W$ & true & false & true & false & true \\
$S$ & true & true & true & true & true
\end{tabular}

\section*{Solution to Exercise 4.15.}

The Cost-matrix for $\Phi$ is

\begin{tabular}{c|ccc}
$\Phi$ & $x$ & $y$ & $z$ \\
\hline$A$ & 17 & 20 & 20 \\
$B$ & 11 & 14 & 14 \\
$C$ & 14 & 17 & 17 \\
$D$ & 12 & 9 & 15
\end{tabular}

Solution to Exercise 4.17.

$$
\begin{aligned}
\Phi=M_{X}^{3} * M_{\Phi} * M_{Y}^{2} & =\left(\begin{array}{cccc}
0 & 6 & 3 & 11 \\
2 & 0 & 5 & 5 \\
5 & 3 & 0 & 8 \\
11 & 9 & 6 & 0
\end{array}\right)\left(\begin{array}{ccc}
\infty & \infty & \infty \\
11 & \infty & \infty \\
\infty & \infty & \infty \\
\infty & 9 & \infty
\end{array}\right)\left(\begin{array}{lll}
0 & 4 & 3 \\
3 & 0 & 6 \\
7 & 4 & 0
\end{array}\right) \\
& =\left(\begin{array}{ccc}
17 & 20 & \infty \\
11 & 14 & \infty \\
14 & 17 & \infty \\
20 & 9 & \infty
\end{array}\right)\left(\begin{array}{lll}
0 & 4 & 3 \\
3 & 0 & 6 \\
7 & 4 & 0
\end{array}\right) \\
& =\left(\begin{array}{ccc}
17 & 20 & 20 \\
11 & 14 & 14 \\
14 & 17 & 17 \\
12 & 9 & 15
\end{array}\right)
\end{aligned}
$$

Solution to Exercise 4.18.

Yes, this is valid: it just means that the profunctor $\Phi:(T \times E) \rightarrow \$$ does not relate (good-natured, funny) to any element of $\$$. More formally, it means that $\Phi(($ good-natured, funny), $p)=$ false for all $p \in \$=\{\$ 100 \mathrm{~K}, \$ 500 \mathrm{~K}, \$ 1 \mathrm{M}\}$. We can interpret this to mean that it is not feasible to produce a good-natured, funny movie for any of the cost options presented-so at least not for less than a million dollars

\section*{Solution to Exercise 4.22}

There are a number of methods that can be used to get the correct answer. One way that works well for this example is to search for the shortest paths on the diagram: it so happens that all the shortest paths go through the bridges from $D$ to $y$ and $y$ to $r$, so in this case $(\Phi ; \Psi)(-,-)=X(-, D)+9+z(r,-)$. This gives:

\begin{tabular}{c|cccc}
$\Phi \circ \Psi$ & $p$ & $q$ & $r$ & $s$ \\
\hline$A$ & 22 & 24 & 20 & 21 \\
$B$ & 16 & 18 & 14 & 15 \\
$C$ & 19 & 21 & 17 & 18 \\
$D$ & 11 & 13 & 9 & 10
\end{tabular}

A more methodical way is to use matrix multiplication. Here's one way you might do the multiplication, using a few tricks.

$$
\begin{aligned}
\Phi ; \Psi & =\left(M_{X}^{3} * M_{\Phi} * M_{Y}^{2}\right) *\left(M_{Y}^{2} * M_{\Psi} * M_{Z}^{3}\right) \\
& =M_{X}^{3} * M_{\Phi} * M_{Y}^{4} * M_{\Psi} * M_{Z}^{3} \\
& =\left(M_{X}^{3} * M_{\Phi} * M_{Y}^{2}\right) * M_{\Psi} * M_{Z}^{3} \\
& =\Phi * M_{\Psi} * M_{Z}^{3} \\
& =\left(\begin{array}{ccc}
17 & 20 & 20 \\
11 & 14 & 14 \\
14 & 17 & 17 \\
12 & 9 & 15
\end{array}\right)\left(\begin{array}{cccc}
\infty & \infty & \infty & \infty \\
\infty & \infty & 0 & \infty \\
4 & \infty & \infty & 4
\end{array}\right)\left(\begin{array}{llll}
0 & 2 & 4 & 5 \\
4 & 0 & 2 & 3 \\
2 & 4 & 0 & 1 \\
1 & 3 & 5 & 0
\end{array}\right) \\
& =\left(\begin{array}{cccc}
17 & 20 & 20 \\
11 & 14 & 14 \\
14 & 17 & 17 \\
12 & 9 & 15
\end{array}\right)\left(\begin{array}{cccc}
\infty & \infty & \infty & \infty \\
2 & 4 & 0 & 1 \\
4 & 6 & 8 & 4
\end{array}\right) \\
& =\left(\begin{array}{cccc}
22 & 24 & 20 & 21 \\
16 & 18 & 14 & 15 \\
19 & 21 & 17 & 18 \\
11 & 13 & 9 & 10
\end{array}\right)
\end{aligned}
$$

\section*{Solution to Exercise 4.26.}

We choose the Cost-category $X$ from Eq. (2.56). The unit profunctor $U_{X}$ on $X$ is described by the bridge diagram

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-300.jpg?height=342&width=857&top_left_y=1347&top_left_x=645)

Solution to Exercise 4.30.

1. The first equality is the unitality of $\mathcal{V}$ (Definition $2.2(\mathrm{~b})$ ). The second step uses the monotonicity of $\otimes$ (Definition 2.2(a)) applied to the inequalities $I \leq \mathcal{P}(p, p)$ (the identity law for $\mathcal{P}$ at $p$, Definition $2.46(\mathrm{a})$ ) and $\Phi(p, q) \leq \Phi(p, q)$ (reflexivity of preorder $\mathcal{V}$, Definition 1.30 (a)). The third step uses the definition of join: for all $x$ and $y$, since any $x \leq x$, we have $x \leq x \vee y$. The final equality is just the definition of profunctor composition, Definition 4.21.

2. Note that in Bool, $I=$ true. Since the identity law at $p$ says true $\leq \mathcal{P}(p, p)$, and true is the largest element of the preorder Bool, we thus have $\mathcal{P}(p, p)=$ true for all $p$. This shows that the first inequality in Eq. (4.28) is an equality.

The second inequality is more involved. Note that by the above, we can assume the left hand side of the inequality is equal to $\Phi(p, q)$. We split into two cases. Suppose $\Phi(p, q)=$ true. Then, again since true is the largest element of $\mathbb{B}$, we must have equality.

Next, suppose $\Phi(p, q)=$ false. Note that since $\Phi$ is a monotone map $\mathcal{P}^{\text {op }} \times \mathcal{Q} \rightarrow$ Bool, if $p \leq p_{1}$ in $\mathcal{P}$, then $\Phi\left(p_{1}, q\right) \leq \Phi(p, q)$ in Bool. Thus if $\mathcal{P}\left(p, p_{1}\right)=$ true then $\Phi\left(p_{1}, q\right)=\Phi(p, q)=$ false. This implies that for all $p_{1} \in \mathcal{P}$, we have either $\mathcal{P}\left(p, p_{1}\right)=$ false or $\Phi\left(p_{1}, q\right)=$ false, and hence $\bigvee_{p_{1} \in \mathcal{P}} \mathcal{P}\left(p, p_{1}\right) \wedge \Phi\left(p_{1}, q\right)=\bigvee_{p_{1} \in \mathcal{P}}$ false $=$ false.

Thus, in either case, we see that $\Phi(p, q)=\bigvee_{p_{1} \in \mathcal{P}} \mathcal{P}\left(p, p_{1}\right) \wedge \Phi\left(p_{1}, q\right)$, as required.

3. The first equation is unitality in monoidal categories, $v \otimes I=v$ for any $v \in V$. The second is that $I \leq Q(q, q)$ by unitality of enriched categories, see Definition 2.46 , together with monotonicity of monoidal product: $v_{1} \leq v_{2}$ implies $v \otimes v_{1} \leq v \otimes v_{2}$. The third was shown in Exercise 4.9.

\section*{Solution to Exercise 4.32 .}

This is very similar to Exercise 2.104: we exploit the associativity of $\otimes$. Note, however, we also require $\mathcal{V}$ to be symmetric monoidal closed, since this implies the distributivity of $\otimes$ over $\vee$ (Proposition 2.87 2), and $\mathcal{V}$ to be skeletal, so we can turn equivalences into equalities.

$$
\begin{aligned}
((\Phi ; \Psi) \nsubseteq \Upsilon)(p, s) & =\bigvee_{r \in \mathcal{R}}\left(\bigvee_{q \in \mathcal{Q}} \Phi(p, q) \otimes \Psi(q, r)\right) \otimes \Upsilon(r, s) \\
& =\bigvee_{r \in \mathcal{R}, q \in \mathcal{Q}} \Phi(p, q) \otimes \Psi(q, r) \otimes \Upsilon(r, s) \\
& =\bigvee_{q \in \mathcal{Q}} \Phi(p, q) \otimes \bigvee_{r \in \mathcal{R}}(\Psi(q, r) \otimes \Upsilon(r, s)) \\
& =(\Phi \circ(\Psi ; \Upsilon))(p, s)
\end{aligned}
$$

Solution to Exercise 4.36.

This is very straightforward. We wish to check $\widehat{\mathrm{id}}: \mathcal{P} \rightarrow \mathcal{P}$ has the formula $\widehat{\mathrm{id}}(p, q)=\mathcal{P}(p, q)$. By Definition $4.34, \widehat{\operatorname{id}}(p, q):=\mathcal{P}(\operatorname{id}(p), q)=\mathcal{P}(p, q)$. So they're the same.

Solution to Exercise 4.38 .

The conjoint $ॅ: \mathbb{R} \rightarrow \mathbb{R} \times \mathbb{R} \times \mathbb{R}$ sends $(a, b, c, d)$ to $\mathbb{R}(a, b+c+d)$, which is true if $a \leq b+c+d$, and false otherwise.

Solution to Exercise 4.41.

1. By Definition $4.34, \widehat{F}(p, q)=Q(F(p), q)$ and $\check{G}(p, q)=\mathcal{Q}(p, G(q))$. Since $\mathcal{V}$ is skeletal, $F$ and $G$ are $\mathcal{V}$ adjoints if and only if $Q(F(p), q)=Q(p, G(q))$. Thus $F$ and $G$ are adjoints if and only if $\widehat{F}=\breve{G}$.

2. Note that id: $\mathcal{P} \rightarrow \mathcal{P}$ is $\mathcal{V}$-adjoint to itself, since both sides of Eq. (4.40) then equal $\mathcal{P}(p, q)$. Thus $\widehat{\mathrm{id}}=\mathrm{id}$.

\section*{Solution to Exercise 4.44.}

The Hasse diagram for the collage of the given profunctor is quite simply this:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-301.jpg?height=290&width=669&top_left_y=1758&top_left_x=739)

Solution to Exercise 4.48.

Since we only have a rough definition, we can only roughly check this: we won't bother with the notion of well-behaved. Nonetheless, we can still compare Definition 2.2 with Definition 4.45.

First, recall from Section 3.2.3 that a preorder is a category $\mathcal{P}$ such that for every $p, q \in \mathcal{P}$, the set $\mathcal{P}(p, q)$ has at most one element.

On the surface, all looks promising: both definitions have two constituents and four properties. In constituent (i), both Definition 2.2 and Definition 4.45 call for the same: an element, or object, of the preorder $\mathcal{P}$. So far so good. Constituent (ii), however, is where it gets interesting: Definition 2.2 calls for merely a function $\otimes: \mathcal{P} \times \mathcal{P} \rightarrow \mathcal{P}$, while Definition 4.45 calls for a functor.

Recall from Example 3.42 that functors between preorders are exactly monotone maps. So we need for the function $\otimes$ in Definition 2.2 to be a monotone map. This is exactly property (a) of Definition 2.2: it says that if $\left(x_{1}, x_{2}\right) \leq\left(y_{1}, y_{2}\right)$ in $\mathcal{P} \otimes \mathcal{P}$, then we must have $x_{1} \otimes x_{2} \leq y_{1} \otimes y_{2}$ in $\mathcal{P}$. So it is also the case that in Definition 2.2 that $\otimes$ is a functor.

The remaining properties compare easily, taking the natural isomorphisms to be equality or equivalence in $\mathcal{P}$. Indeed, property (b) of Definition 2.2 corresponds to both properties (a) and (b) of Definition 4.45, and then the respective properties (c) and (d) similarly correspond.

\section*{Solution to Exercise 4.50 .}
1. $g_{E}(5,3)=$ false, $g_{F}(5,3)=2$.
2. $g_{E}(3,5)=$ true, $g_{F}(3,5)=-2$.
3. $h(5$, true $)=5$.
4. $h(-5$, true $)=-5$.
5. $h(-5$, false $)=6$.
6. $q_{G}(-2,3)=2, q_{F}(-2,3)=-13$.
7. $q_{G}(2,3)=-1, q_{F}(2,3)=7$.

\section*{Solution to Exercise 4.52.}

Yes, the rough definition roughly agrees: plain old categories are Set-categories! In detail, we need to compare Definition 4.51 when $\mathcal{V}=($ Set, $\{1\}, \times)$ with Definition 3.6. In both cases, we see that (i) asks for a collection of objects and (ii) asks for, for all pairs of objects $x, y$, a set $\mathcal{C}(x, y)$ of morphisms. Moreover, recall that the monoidal unit $I$ is the one element set $\{1\}$. This means a morphism $\mathrm{id}_{x}: I \rightarrow \mathcal{C}(x, x)$ is a function $\operatorname{id}_{x}:\{1\} \rightarrow \mathcal{C}(x, x)$. This is the same data as simply an element $\operatorname{id}_{x}=\operatorname{id}_{x}(1) \in \mathcal{C}(x, x)$; we call this data the identity morphism on $x$. Finally, a morphism $9: \mathcal{C}(x, y) \otimes \mathcal{C}(y, z) \rightarrow \mathcal{C}(x, z)$ is a function $9: \mathcal{C}(x, y) \times \mathcal{C}(y, z) \rightarrow \mathcal{C}(x, z)$; this is exactly the composite required in Definition 3.6 (iv).

So in both cases the data agrees. In Definition 3.6 we also require this data to satify two conditions, unitality and associativity. This is what is meant by the last sentence of Definition 4.51.

\section*{Solution to Exercise 4.54.}

An identity element in a Cost-category $X$ is a morphism $I \rightarrow X(x, x)$ in Cost $=([0, \infty], \geq, 0,+)$, and hence the condition that $0 \geq X(x, x)$. This implies that $X(x, x)=0$. In terms of distances, we interpret this to mean that the distance from any point to itself is equal to 0 . We think this is a pretty sensible condition for a notion of distance to obey.

\section*{Solution to Exercise 4.62.}

1. Here is a picture of the unit corelation $\varnothing \rightarrow \underline{3} \sqcup \underline{3}$, where we have drawn the empty set with an empty dotted rectangle on the left:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-302.jpg?height=323&width=548&top_left_y=2191&top_left_x=840)

2. Here is a picture of the counit corelation $\underline{3} \sqcup \underline{3} \rightarrow \varnothing$ :
![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-303.jpg?height=308&width=532&top_left_y=323&top_left_x=844)

3. Here is a picture of the snake equation on the left of Eq. (4.59).

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-303.jpg?height=445&width=1076&top_left_y=729&top_left_x=579)

\section*{Solution to Exercise 4.64.}

Given two resource preorders $x$ and $y$, the preorder $x \times y$ represents the set of all pairs of resources, $x \in \mathcal{X}$ and $y \in y$, with $(x, y) \leq\left(x^{\prime}, y^{\prime}\right)$ iff $x \leq x^{\prime}$ and $y \leq y^{\prime}$. That is, if $x$ is available given $x^{\prime}$ and $y$ is available given $y^{\prime}$, then $(x, y)$ is available given $\left(x^{\prime}, y^{\prime}\right)$.

Given two profunctors $\Phi: x_{1} \rightarrow X_{2}$ and $\Psi: y_{1} \rightarrow y_{2}$, the profunctor $\Phi \times \Psi$ represents their conjunction, i.e. AND. In other words, if $y_{1}$ can be obtained given $x_{1}$ AND $y_{2}$ can be obtained given $x_{2}$, then $\left(y_{1}, y_{2}\right)$ can be obtained given $\left(x_{1}, x_{2}\right)$.

\section*{Solution to Exercise 4.65.}

The profunctor $X \times 1 \rightarrow X$ defined by the functor $\alpha:(X \times 1)^{\mathrm{op}} \times X \rightarrow \mathcal{V}$ that maps $\alpha((x, 1), y):=X(x, y)$ is an isomorphism. It has inverse $\alpha^{-1}: X \rightarrow x \times 1$ defined by $\alpha^{-1}(x,(y, 1)):=X(x, y)$. To see that $\alpha^{-1} ; \alpha=\mathrm{U}_{X}$, note first that the unit law for $X$ at $z$ and the definition of join imply

$$
X(x, z)=X(x, z) \otimes I \leq X(x, z) \otimes X(z, z) \leq \bigvee_{y \in X} X(x, y) \otimes X(y, z)
$$

while composition says $X(x, y) \otimes X(y, z) \leq X(x, z)$ and hence

$$
\bigvee_{y \in X} x(x, y) \otimes X(y, z) \leq \bigvee_{y \in X} x(x, z)=X(x, z)
$$

Thus, unpacking the definition of composition of profunctor, we have

$$
\left(\alpha^{-1} ; \alpha\right)(x, z)=\bigvee_{(y, 1) \in X \times 1} \alpha(x,(y, 1)) \otimes \alpha^{-1}((y, 1), z)=\bigvee_{y \in X} X(x, y) \otimes X(y, z)=X(x, z)
$$

Similarly we can show $\alpha ; \alpha^{-1}=\mathrm{U}_{X \times 1}$, and hence that $\alpha$ is an isomorphism $X \times 1 \rightarrow X$.

Moreover, we can similarly show that $\beta((1, x), y):=X(x, y)$ defines an isomorphism $\beta: \mathbf{1} \times \mathcal{X} \nrightarrow \mathcal{X}$.

\section*{Solution to Exercise 4.66.}

We check the first snake equation, the one on the left hand side of Eq. (4.59). The proof of the one on the right hand side is analogous.

We must show that the composite $\Phi$ of profunctors

$$
x \xrightarrow{\alpha^{-1}} x \times \mathbf{1} \xrightarrow{\mathrm{U}_{x} \times \eta x} x \times X^{\mathrm{op}} \times x \xrightarrow{\epsilon x \times \mathrm{U}_{x}} \mathbf{1} \times x \xrightarrow{\alpha} x
$$

is itself the identity (ie. the unit profunctor on $X$ ), where $\alpha$ and $\alpha^{-1}$ are the isomorphisms defined in the solution to Exercise 4.65 above.

Freely using the distributivity of $\otimes$ over $\vee$, the value $\Phi(x, y)$ of this composite at $(x, y) \in X^{\text {op }} \times \mathcal{X}$ is given by

$$
\begin{aligned}
& \bigvee_{a, b, c, d, e \in X} \alpha^{-1}(x,(a, 1)) \otimes\left(\mathrm{U}_{X} \times \eta X\right)((a, 1),(b, c, d)) \\
= & \bigvee_{a, b, c, d, e \in X} \alpha^{-1}(x,(a, 1)) \otimes \mathrm{U}_{X}(a, b) \otimes \eta_{X}(1, c, d) \quad \otimes\left(\epsilon X \times \mathrm{U}_{X}\right)((b, c, d),(1, e)) \otimes \alpha((1, e), y) \\
= & \bigvee_{a, b, c, d, e \in X} X(x, a) \otimes X(a, b) \otimes X(c, d) \otimes X(b, c) \otimes X(d, e) \otimes X(e, y) \\
= & X(x, y)
\end{aligned}
$$

where in the final step we repeatedly use the argument the Lemma 4.27 that shows that composing with the unit profunctor $\mathrm{U}_{X}(a, b)=X(a, b)$ is the identity.

This shows that $\Phi(x, y)$ is the identity profunctor, and hence shows the first snake equation holds. Again, checking the other snake equation is analogous.

\section*{A. 5 Solutions for Chapter 5.}

Solution to Exercise 5.5.

1. Below we draw a morphism $f: 3 \rightarrow 2$ and a morphism $g: 2 \rightarrow 4$ in FinSet:
![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-305.jpg?height=206&width=800&top_left_y=444&top_left_x=714)

2. Here is a picture of $f+g$

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-305.jpg?height=206&width=466&top_left_y=718&top_left_x=884)

3. The composite of morphisms $f: m \rightarrow n$ and $g: n \rightarrow p$ in FinSet is the function $(f ; g): m \rightarrow p$ given by $(f ; g)(i)=g(f(i))$ for all $1 \leq i \leq m$.

4. The identity id $m: m \rightarrow m$ is given by $\operatorname{id}_{m}(i)=i$ for all $1 \leq i \leq m$. Here is a picture of $\operatorname{id}_{2}$ and $\operatorname{id}_{8}$ :
![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-305.jpg?height=210&width=1108&top_left_y=1092&top_left_x=560)

5. Here is a picture of the symmetry $\sigma_{3,5}: 8 \rightarrow 8$ :
![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-305.jpg?height=200&width=550&top_left_y=1399&top_left_x=838)

\section*{Solution to Exercise 5.9.}

We need to give examples posetal props, i.e. each will be a poset whose set of objects is $\mathbb{N}$, whose order is denoted $m \leq n$, and with the property that whenever $m_{1} \leq n_{1}$ and $m_{2} \leq n_{2}$ hold then $m_{1}+m_{2} \leq n_{1}+n_{2}$ does too.

The question only asks for three, but we will additionally give a quasi-example and a non-example

1. Take $\leq$ to be the discrete order: $m \leq n$ iff $m=n$.

2. Take $\leq$ to be the usual order, $m \leq n$ iff there exists $d \in \mathbb{N}$ with $d+m=n$.

3. Take $\leq$ to be the reverse of the usual order, $m \leq n$ iff there exists $d \in \mathbb{N}$ with $m=n+d$.

4. Take $\leq$ to be the co-discrete order $m \leq n$ for all $m, n$. Some may object that this is a preorder, not a poset, so we call it a quasi-example.

5. (Non-example.) Take $\leq$ to be the division order, $m \leq n$ iff there exists $q \in \mathbb{N}$ with $m * q=d$. This is a perfectly good poset, but it does not satisfy the monotonicity property: we have $2 \leq 4$ and $3 \leq 3$ but not $5 \leq$ ? 7 .

Solution to Exercise 5.10.

Example 5.6: The prop Bij has
1. $\mathbf{B i j}(m, n):=\{f: \underline{m} \rightarrow \underline{n} \mid f$ is a bijection $\}$. Note that $\operatorname{Bij}(m, n)=\varnothing$ if $m \neq n$ and it has $n!$ elements if $m=n$.

2. The identity map $n \rightarrow n$ is the bijection $\underline{n} \rightarrow \underline{n}$ sending $i \mapsto i$.

3. The symmetry map $m+n \rightarrow n+m$ is the bijection $\sigma_{m, n}: \underline{m+n} \rightarrow \underline{n+m}$ given by

$$
\sigma_{m, n}(i):= \begin{cases}i+n & \text { if } i \leq m \\ i-m & \text { if } m+1 \leq i\end{cases}
$$

4. Composition of bijections $m \rightarrow n$ and $n \rightarrow p$ is just their composition as functions, which is again a bijection.

5. Given bijections $f: m \rightarrow m^{\prime}$ and $g: n \rightarrow n^{\prime}$, their monoidal product $(f+g):(m+n) \rightarrow$ $\left(m^{\prime}+n^{\prime}\right)$ is given by

$$
(f+g)(i):= \begin{cases}f(i) & \text { if } i \leq m \\ g(i-m) & \text { if } m+1 \leq i\end{cases}
$$

Example 5.7: The prop Corel has

1. Corel $(m, n)$ is the set of equivalence relations on $\underline{m+n}$.

2. The identity map $n \rightarrow n$ is the smallest equivalence relation, which is the smallest reflexive relation, i.e. where $i \sim j$ iff $i=j$.

3. The symmetry map $\sigma_{m, n}$, as an equivalence relation on $m+n+n+m$ is "the obvious thing," namely "equating corresponding $m$ 's together and also equating corresponding $n$ 's together." To be pedantic, $i \sim j$ iff either
- $|i-j|=m+n+n$, or
- $m+1 \leq i \leq m+n+n$ and $m+1 \leq j \leq m+n+n$ and $|i-j|=n$.

4. Composition of an equivalence relation $\sim$ on $\underline{m+n}$ and an equivalence relation $\dot{\sim}$ on $n+p$ is the equivalence relation $\simeq$ on $m+p$ given by $i \simeq k$ iff there exists $j \in \underline{n}$ with $i \sim j$ and $j \dot{\sim k}$.

5. Given equivalence relations $\sim$ on $\underline{m+n}$ and $\sim^{\prime}$ on $\underline{m^{\prime}+n^{\prime}}$, we need an equivalence relation $\left(\sim+\sim^{\prime}\right)$ on $\underline{m+n+m^{\prime}+n^{\prime}}$. We take it to be "the obvious thing," namely "using $\sim$ on the unprimed stuff and using $\sim^{\prime}$ on the primed stuff, with no other interaction." To be pedantic, $i \sim j$ iff either
- $i \leq m+n$ and $j \leq m+n$ and $i \sim j$, or
- $m+n+1 \leq i$ and $m+n+1 \leq j$ and $i \sim^{\prime} j$.

Example 5.8: The prop Rel has
1. $\operatorname{Rel}(m, n)$ is the set of relations on the set $\underline{m} \times \underline{n}$, i.e. the set of subsets of $\underline{m} \times \underline{n}$, i.e. its powerset.

2. The identity map $n \rightarrow n$ is the subset $\{(i, j) \in \underline{n} \times \underline{n} \mid i=j\}$

3. The symmetry map $m+n \rightarrow n+m$ is the subset of pairs $(i, j) \in(\underline{m+n}) \times(\underline{n+m})$ such that either
- $i \leq m$ and $m+1 \leq j$ and $i+m=j$, or
- $m+1 \leq i$ and $j \leq m$ and $j+m=i$.

4. Composition of relations is as in Example 5.8.

5. Given a relation $R \subseteq \underline{m} \times \underline{n}$ and a relation $R^{\prime} \subseteq \underline{m^{\prime}} \times \underline{n^{\prime}}$, we need a relation $\left(R+R^{\prime}\right) \subseteq$ $\underline{m+m^{\prime}} \times \underline{n+n^{\prime}}$. As stated in the example (footnote), this can be given by a universal property: The monoidal product $R_{1}+R_{2}$ of relations $R_{1} \subseteq \underline{m_{1}} \times \underline{n_{1}}$ and $R_{2} \subseteq \underline{m_{2}} \times \underline{n_{2}}$ is given by $R_{1} \sqcup R_{2} \subseteq\left(\underline{m_{1}} \times \underline{n_{1}}\right) \sqcup\left(\underline{m_{2}} \times \underline{n_{2}}\right) \subseteq\left(\underline{m_{1}} \sqcup \underline{m_{2}}\right) \times\left(\underline{n_{1}} \sqcup \underline{n_{2}}\right)$.

Solution to Exercise 5.16.

Composition of an $(m, n)$-port graph $G$ and an $(n, p)$-port graph $H$ looks visually like sticking them end to end, connecting the wires in order, removing the two outer boxes, and adding a new outer box. For example, suppose we want to compose the following in the order shown:
![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-306.jpg?height=186&width=1110&top_left_y=2346&top_left_x=518)

The result is:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-307.jpg?height=182&width=935&top_left_y=310&top_left_x=606)

Solution to Exercise 5.18.

The monoidal product of two morphisms is drawn by stacking the corresponding port graphs. For this problem, we just stack the left-hand picture on top of itself to obtain the righthand picture:
![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-307.jpg?height=250&width=796&top_left_y=699&top_left_x=674)

Solution to Exercise 5.20.

We have a relation $R \subseteq P \times P$ which generates a preorder $\leq_{P}$ on $P$, we have an arbitrary preorder $\left(Q, \leq_{Q}\right)$ and a function $f: P \rightarrow Q$, not necessarily monotonic.

1. Assume that for every $x, y \in P$, if $R(x, y)$ then $f(x) \leq f(y)$; we want to show that $f$ is monotone, i.e. that for every $x \leq_{P} \quad y$ we have $f(x) \leq_{Q} f(y)$. By definition of $P$ being the reflexive, transitive closure of $R$, we have $x \leq_{P} \quad y$ iff there exists $n \in \mathbb{N}$ and $x_{0}, \ldots, x_{n}$ in $P$ with $x_{0}=x$ and $x_{n}=y$ and $R\left(x_{i}, x_{i+1}\right)$ for each $0 \leq i \leq n-1$. (The case $n=0$ handles reflexivity.) But then by assumption, $R\left(x_{i}, x_{i+1}\right)$ implies $f\left(x_{i}\right) \leq_{Q} f\left(x_{i+1}\right)$ for each $i$. By induction on $i$ we show that $f\left(x_{0}\right) \leq_{Q} f\left(x_{i}\right)$ for all $0 \leq i \leq n$, at which point we are done.

2. Suppose now that $f$ is monotone, and take $x, y \in P$ for which $R(x, y)$ holds. Then $x \leq_{P} y$ because $\leq_{P}$ is the smallest preorder relation containing $R$. (Another way to see this based on the above description is with $n=1, x_{0}=x$, and $x_{n}=y$, which we said implies $x \leq_{P} y$.) Since $f$ is monotone, we indeed have $f(x) \leq_{Q} f(y)$.

\section*{Solution to Exercise 5.21.}

Suppose that $P, Q$, and $R$ are as in Exercise 5.20 and we have a function $g: Q \rightarrow P$.

1. If $R(g(a), g(b))$ holds for all $a \leq_{Q} b$ then $g$ is monotone, because $R(x, y)$ implies $x \leq_{P} \quad y$.

2. It is possible for $g:\left(Q, \leq_{Q}\right) \rightarrow\left(P, \leq_{P}\right)$ to be monotone and yet have some $a, b \in Q$ with $a \leq_{Q} b$ and $(g(a), g(b)) \notin R$. Indeed, take $Q:=\{1\}$ to be the free preorder on one element, and take $P:=\{1\}$ with $R=\varnothing$. Then the unique function $g: Q \rightarrow P$ is monotone (because $\leq_{P}$ is reflexive even though $R$ is empty), and yet $(g(1), g(1)) \notin R$.

Solution to Exercise 5.23.

Let $G=(V, A, s, t)$ be a graph, let $\mathcal{G}$ be the free category on $G$, and let $\mathcal{C}$ be another category, whose set of morphisms is denoted Mor(C).

1. To give a function $\operatorname{Mor}(\mathcal{C}) \rightarrow \mathrm{Ob}(\mathcal{C})$ means that for every element $\operatorname{Mor}(\mathcal{C})$ we need to give exactly one element of $\mathrm{Ob}(\mathcal{C})$. So for dom we take any $q \in \operatorname{Mor}(\mathcal{C})$, view it as a morphism $q: y \rightarrow z$, and send it to its domain $y$. Similarly for cod: we put $\operatorname{cod}(q):=z$.

2. Suppose first that we are given a functor $F: \mathcal{G} \rightarrow \mathcal{C}$. On objects we have a function $\mathrm{Ob}(\mathcal{G}) \rightarrow \mathrm{Ob}(\mathcal{C})$, and this defines $f$ since $\mathrm{Ob}(\mathcal{G})=V$. On morphisms, first note that the arrows of graph $G$ are exactly the length=1 paths in $G$, whereas $\operatorname{Mor}(\mathcal{G})$ is the set of all paths in $G$, so we have an inclusion $A \subseteq \operatorname{Mor}(\mathcal{G})$. The functor $F$ provides a function $\operatorname{Mor}(\mathcal{G}) \rightarrow \operatorname{Mor}(\mathcal{C})$, which we can restrict to $A$ to obtain $g: A \rightarrow \operatorname{Mor}(\mathcal{C})$. All functors satisfy $\operatorname{dom}(F(r))=F(\operatorname{dom}(r))$ and $\operatorname{cod}(F(r))=F(\operatorname{cod}(r))$ for any $r: w \rightarrow x$. In particular when $r \in A$ is an arrow we have $\operatorname{dom}(r)=s(r)$ and $\operatorname{cod}(r)=t(r)$.

Thus we have found $(f, g)$ with the required properties.

Suppose second that we are given a pair of functions $(f, g)$ where $f: V \rightarrow \mathrm{Ob}(\mathcal{C})$ and $g: A \rightarrow$ $\operatorname{Mor}(\mathcal{C})$ such that $\operatorname{dom}(g(a))=f(s(a))$ and $\operatorname{cod}(g(a))=f(t(a))$ for all $a \in A$. Define $F: \mathcal{G} \rightarrow \mathcal{C}$ on objects by $f$. An arbitrary morphism in $\mathcal{G}$ is a path $p:=\left(v_{0}, a_{1}, a_{2}, \ldots, a_{n}\right)$ in $G$, where $v_{0} \in V$, $a_{i} \in A, v_{0}=s\left(a_{1}\right)$, and $t\left(a_{i}\right)=s\left(a_{i+1}\right)$ for all $1 \leq i \leq n-1$. Then $g\left(a_{i}\right)$ is a morphism in $\mathcal{C}$ whose domain is $f\left(v_{0}\right)$ and the morphisms $g\left(a_{i}\right)$ and $g\left(a_{i+1}\right)$ are composable for every $1 \leq i \leq n-1$. We then take $F(p):=\operatorname{id}_{f\left(v_{0}\right)} \circ g\left(a_{1}\right) \circ \cdots \circ g\left(a_{n}\right)$ to be the composite. It is easy to check that this is indeed a functor (preserves identities and compositions).

Third, we want to see that the two operations we just gave are mutually inverse. On objects this is straightforward, and on morphisms it is straightforward to see that, given $(f, g)$, if we turn them into a functor $F: \mathcal{G} \rightarrow \mathcal{C}$ and then extract the new pair of functions $\left(f^{\prime}, g^{\prime}\right)$, then $f=f^{\prime}$ and $g=g^{\prime}$. Finally, given a functor $F: \mathcal{G} \rightarrow \mathcal{C}$, we extract the pair of functions $(f, g)$ as above and then turn them into a new functor $F^{\prime}: \mathcal{G} \rightarrow \mathcal{C}$. It is clear that $F$ and $F^{\prime}$ act the same on objects, so what about on morphisms. The formula says that $F^{\prime}$ acts the same on morphisms of length 1 in $\mathcal{G}$ (i.e. on the elements of $A$ ). But an arbitrary morphism in $\mathcal{G}$ is just a path, i.e. a sequence of composable arrows, and so by functoriality, both $F$ and $F^{\prime}$ must act the same on arbitrary paths.

3. (Mor( $\mathcal{C}), \mathrm{Ob}(\mathcal{C})$, dom, cod) is a graph; let's denote it $U(\mathcal{C}) \in$ Grph. We have functors Free: Grph $\leftrightarrows$ Cat $: U$, and Free is left adjoint to $U$.

Solution to Exercise 5.24.

1. The elements of the free monoid on the set $\{a\}$ are:

$$
a^{0}, a^{1}, a^{2}, a^{3}, \ldots, a^{2019}, \ldots
$$

with monoid multiplication * given by the usual natural number addition on the exponents, $a^{i} * a^{j}=a^{i+j}$.

2. This is isomorphic to $\mathbb{N}$, by sending $a^{i} \mapsto i$.

3. The elements of the free monoid on the set $\{a, b\}$ are 'words in $a$ and $b$,' each of which we will represent as a list whose entries are either $a$ or $b$. Here are some:

$$
[], \quad[a], \quad[b], \quad[a, a], \quad[a, b], \quad \ldots, \quad[b, a, b, b, a, b, a, a, a, a], \quad \ldots
$$

\section*{Solution to Exercise 5.28}

We have two props: the prop of port graphs and the free prop Free $(G, s, t)$ where

$$
G:=\left\{\rho_{m, n}: m \rightarrow n \mid m, n \in \mathbb{N}\right\}, \quad s\left(\rho_{m, n}\right):=m, \quad t\left(\rho_{m, n}\right):=n
$$

we want to show they are the same prop. As categories they have the same set of objects (in both cases, $\mathbb{N}$ ), so we need to show that for every $m, n \in \mathbb{N}$, they have the same set of morphisms (and that their composition formulas and monoidal product formulas agree).

By Definition 5.25, a morphism $m \rightarrow n$ in $\operatorname{Free}(G)$ is a $G$-labeled port graph, i.e. a pair $(\Gamma, \ell)$, where $\Gamma=(V$, in, out, $\iota)$ is an $(m, n)$-port graph and $\ell: V \rightarrow G$ is a function, such that the 'arities agree.' What does this mean? Recall that every vertex $v \in V$ is drawn as a box with some left-hand ports and some right-hand ports-an arity-and $\ell(v) \in G$ is supposed to have the correct arity; precisely, $s(\ell(v))=\operatorname{in}(v)$ and $t(\ell(v))=\operatorname{out}(v)$. But $G$ was chosen so that it has exactly one element with any given arity, so the function $\ell$ has only one choice, and thus contributes nothing: it neither increases nor decreases the freedom. In other words, a morphism in our particular Free $(G)$ can be identified with an $(m, n)$ port graph $\Gamma$, as desired.

Again by definition Definition 5.25, the 'composition and the monoidal structure are just those for port graphs PG (see Eq. (5.17)); the labelings (the $\ell$ 's) are just carried along.' So we are done.

Solution to Exercise 5.32.

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-309.jpg?height=46&width=1409&top_left_y=302&top_left_x=369)
$1, g: 2 \rightarrow 2, h: 2 \rightarrow 1\}:$

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-309.jpg?height=198&width=669&top_left_y=381&top_left_x=736)

\section*{Solution to Exercise 5.35.}

The free prop on generators $(G, s, t)$, defined in Definition 5.25 , is-for all intents and purposesthe same thing as the prop presented by $(G, s, t, \varnothing)$, having no relations. The only possible "subtle difference" we might have to admit is if someone said that a set $S$ is "subtly different" than its quotient by the trivial equivalence relation. In the latter, the elements are the singleton subsets of $S$. So for example the quotient of $S=\{1,2,3\}$ by the trivial equivalence relation is the set $\{\{1\},\{2\},\{3\}\}$. It is subtly different than $S$, but the two are naturally isomorphic, and category-theoretically, the difference will never make a difference.

Solution to Exercise 5.41.

1. If $(R, 0,+, 1, *)$ is a rig, then the multiplicative identity $1 \in \operatorname{Mat}_{n}(R)$ is the usual $n$-by- $n$ identity matrix: 1 's on the diagonal and 0 's everywhere else (where by ' 1 ' and ' 0 ', we mean those elements of $R)$. So for $n=4$ it is:

$$
\left(\begin{array}{llll}
1 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1
\end{array}\right)
$$

2. We choose $n=2$ and hence need to find two elements $A, B \in \operatorname{Mat}_{2}(\mathbb{N})$ such that $A * B \neq B * A$.

$$
A * B=\left(\begin{array}{ll}
0 & 1 \\
0 & 0
\end{array}\right) *\left(\begin{array}{ll}
0 & 1 \\
1 & 0
\end{array}\right) \neq\left(\begin{array}{ll}
0 & 1 \\
1 & 0
\end{array}\right) *\left(\begin{array}{ll}
0 & 1 \\
0 & 0
\end{array}\right)=B * A
$$

One can calculate from the multiplication formula (recalled in Example 5.40) says $(A * B)(1,1)=$ $0 * 0+1 * 1=1$ and $(B * A)(1,1)=0 * 0+0 * 0=0$, which are not equal.

\section*{Solution to Exercise 5.43.}

Semantically, if we apply the flow graph below to the input signal $(x, y)$

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-309.jpg?height=212&width=634&top_left_y=1870&top_left_x=756)

the resulting output signal is $(16 x+4 y, x+4 y)$.

Solution to Exercise 5.51.

The monoidal product of $A=\left(\begin{array}{lll}3 & 3 & 1 \\ 2 & 0 & 4\end{array}\right)$ and $B=\left(\begin{array}{llll}2 & 5 & 6 & 1\end{array}\right)$ is

$$
A+B=\left(\begin{array}{lllllll}
3 & 3 & 1 & 0 & 0 & 0 & 0 \\
2 & 0 & 4 & 0 & 0 & 0 & 0 \\
0 & 0 & 0 & 2 & 5 & 6 & 1
\end{array}\right)
$$

Solution to Exercise 5.55.

1. The signal flow graph on the left represents the matrix on the right:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-310.jpg?height=97&width=200&top_left_y=358&top_left_x=735)

$\left(\begin{array}{lll}1 & 1 & 1\end{array}\right)$

2. The signal flow graph on the left represents the matrix on the right:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-310.jpg?height=117&width=220&top_left_y=538&top_left_x=730)

$\left(\begin{array}{lll}1 & 1 & 1\end{array}\right)$

3. They are equal.

Solution to Exercise 5.58.

1.

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-310.jpg?height=168&width=422&top_left_y=816&top_left_x=903)

2.

$$
\left(\begin{array}{ll}
0 & 0 \\
0 & 0
\end{array}\right) \quad \sim \quad \longrightarrow 0
$$

3.

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-310.jpg?height=277&width=857&top_left_y=1141&top_left_x=688)

Solution to Exercise 5.59.
- For the first layer $g_{1}$, take the monoidal product of $m$ copies of $c_{n}$,

$$
g_{1}:=c_{n}+\cdots+c_{n}: m \rightarrow(m \times n)
$$

where $c_{n}$ is the signal flow diagram that makes $n$ copies of a single input:

$$
c_{n}:=-\subset ;(1+-c) \AA(1+1+-c) \nsubseteq \cdots q(1+\cdots+1+-c): 1 \rightarrow n
$$
- Next, define

$$
\begin{aligned}
g_{2}:= & s_{M(1,1)}+\cdots+s_{M(1, n)} \\
& +s_{M(2,1)}+\cdots+s_{M(2, n)} \\
& +\cdots \\
& +s_{M(m, 1)}+\cdots+s_{M(m, n)}:(m \times n) \rightarrow(m \times n)
\end{aligned}
$$

where $s_{a}: 1 \rightarrow 1$ is the signal flow graph generator "scalar multiplication by $a$. ." This layer amplifies each copy of the input signal by the relevant rig element.
- The third layer rearranges wires. We will not write this down explicitly, but simply say it is the signal flow graph $g_{3}: m \times n \rightarrow m \times n$, that is the composite and monoidal product of swap and identity maps, such that the $(i-1) m+j$ th input is sent to the $(j-1) n+i$ th output, for all $1 \leq i \leq n$ and $1 \leq j \leq m$.
- Finally, the fourth layer is similar to the first, but instead adds the amplified input signals. We define

$$
g_{4}:=a_{m}+\cdots+a_{m}:(m \times n) \rightarrow n
$$
where $a_{m}$ is the signal flow graph that adds $m$ inputs to produce a single output:

$$
a_{m}:=(1+\cdots+1+\supset-) ; \cdots \nsubseteq(1+1+>-) ;(1+>-) q>-: m \rightarrow 1
$$

Using Proposition 5.54, it is a straightforward but tedious calculation to show that $g=g_{1} \because g_{2} \circ g_{3} \because g_{4}: m \rightarrow$ $n$ has the property that $S(g)=M$.

Solution to Exercise 5.62.

1. The matrices in Exercise 5.58 may also be drawn as the following signal flow graphs:
a)

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-311.jpg?height=159&width=508&top_left_y=601&top_left_x=901)
b)

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-311.jpg?height=99&width=524&top_left_y=788&top_left_x=887)
c)

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-311.jpg?height=238&width=935&top_left_y=922&top_left_x=687)

2. Here are graphical proofs that the representations we chose in our solution to Exercise 5.58 agree with those chosen in Part 1 above.
a)

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-311.jpg?height=163&width=1019&top_left_y=1282&top_left_x=645)
b)

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-311.jpg?height=73&width=897&top_left_y=1487&top_left_x=706)
c)
![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-311.jpg?height=558&width=980&top_left_y=1606&top_left_x=691)

Solution to Exercise 5.63.

1. The signal flow graphs
![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-311.jpg?height=210&width=1238&top_left_y=2296&top_left_x=500)
cannot represent the same morphism because one has a path from a vertex on the left to one on the right, and the other does not. To prove this, observe that the only graphical equation in Theorem 5.60 that breaks a path from left to right is the equation

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-312.jpg?height=46&width=352&top_left_y=405&top_left_x=946)

So a 0 scalar must within a path from left to right before we could rewrite the diagram to break that path. No such 0 scalar can appear, however, because the diagram does not contain any, and the sum and product of any two nonzero natural numbers is always nonzero.

2. Replacing each of the 3 s with 0 allows us to rewrite the diagram to

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-312.jpg?height=114&width=157&top_left_y=669&top_left_x=1038)

\section*{Solution to Exercise 5.67.}

The three conditions of Definition 5.65 are

(a) $(\mu \otimes \mathrm{id}) \AA \mu=(\mathrm{id} \otimes \mu) \AA \mu$,

(b) $(\eta \otimes \mathrm{id}) \AA \mu=\mathrm{id}=(\mathrm{id} \otimes \eta) \AA \mu$, and

(c) $\sigma_{M, M} \circ \mu=\mu$.

where $\sigma_{M, M}$ is the swap map on $M$ in $\mathcal{C}$.

1. Suppose $\mu: \mathbb{R} \times \mathbb{R} \rightarrow \mathbb{R}$ is defined by $\mu(a, b)=a * b$ and $\eta \in \mathbb{R}$ is defined to be $\eta=1$. The conditions, written diagrammatically, say that starting in the upper left of each diagram below, the result in the lower right is the same regardless of which path you take:
![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-312.jpg?height=186&width=1128&top_left_y=1264&top_left_x=542)

and this is true for $(\mathbb{R}, *, 1)$.

2. The same reasoning works for $(\mathbb{R},+, 0)$, shown below:
![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-312.jpg?height=192&width=1114&top_left_y=1582&top_left_x=556)

\section*{Solution to Exercise 5.69.}

The functor $U: \operatorname{Mat}(R) \rightarrow$ Set is given on objects by sending $n$ to the set $R^{n}$, and on morphisms by matrix-vector multiplication. Here $R^{n}$ means the set of $n$-tuples or $n$-dimensional vectors in $R$. In particular, $R^{0}=\{()\}$ consists of a single vector of dimension 0 .
1. $U$ preserves the monoidal unit because 0 is the monoidal unit of any prop $(\operatorname{Mat}(R)$ is a prop), $\{1\}$ is the monoidal unit of Set, and $R^{0}$ is canonically isomorphic to $\{1\} . U$ also preserves the monoidal product because there is a canonical isomorphism $R^{m} \times R^{n} \cong R^{m+n}$.

2. A monoid object in $\operatorname{Mat}(R)$ is a tuple $(m, \mu, \eta)$ where $m \in \mathbb{N}, \mu: m+m \rightarrow m$, and $\eta: 0 \rightarrow m$ satisfy the properties $\mu(\eta, x)=x=\mu(x, \eta)$ and $\mu(x, \mu(y, z))=\mu(\mu(x, y), z)$. Note that there is only one morphism $0 \rightarrow m$ in $\operatorname{Mat}(R)$ for any $m$. It is not hard to show that for any $m \in \mathbb{N}$ there is only one monoid structure. For example, when $m=2, \mu$ must be the following matrix

$$
\mu:=\left(\begin{array}{ll}
1 & 0 \\
0 & 1 \\
1 & 0 \\
0 & 1
\end{array}\right)
$$

Anyway, for any monoid $(m, \mu, \eta)$, the morphism $U(\eta): R^{0} \rightarrow R^{m}$ is given by $U(\eta)(1):=$ $(0, \ldots, 0)$, and the morphism $U(\mu): R^{m} \times R^{m} \rightarrow R^{m}$ is given by

$$
U(\mu)\left(\left(a_{1}, \ldots, a_{m}\right),\left(b_{1}, \ldots, b_{m}\right)\right):=\left(a_{1}+b_{1}, \ldots, a_{m}+b_{m}\right)
$$

These give $R^{m}$ the structure of a monoid.

3. The triple $(1,2-\infty)$ corresponds to the additive monoid structure on $\mathbb{R}$, e.g. with $(5,3) \mapsto 8$.

Solution to Exercise 5.77.

1. The behavior $\mathrm{B}(-\subset)$ of the reversed addition icon $-\subset: 1 \rightarrow 2$ is the relation $\left\{(x, y, z) \in R^{3} \mid x=\right.$ $y+z\}$.

2. The behavior $B(\supset-)$ of the reversed copy icon, $\supset-: 2 \rightarrow 1$ is the relation $\left\{(x, y, z) \in R^{3} \mid x=y=\right.$ $z\}$.

Solution to Exercise 5.80.

If $B \subseteq R^{m} \times R^{n}$ and $C \subseteq R^{p} \times R^{q}$ are morphisms in $\operatorname{Rel}_{R}$, then take $B+C \subseteq R^{m+p} \times R^{n+q}$ to be the set

$$
B+C:=\left\{(w, y, x, z) \in R^{m+p} \times R^{n+q} \mid(w, x) \in B \text { and }(y, z) \in C\right\}
$$

Solution to Exercise 5.82.

The behavior of $g: m \rightarrow n$ and $h^{\mathrm{op}}: n \rightarrow \ell$ are respectively

$$
\begin{aligned}
\mathrm{B}(g) & =\left\{(x, z) \in R^{m} \times R^{n} \mid S(g)(x)=z\right\} \\
\mathrm{B}\left(h^{\mathrm{op}}\right) & =\left\{(z, y) \in R^{n} \times R^{\ell} \mid z=S(h)(y)\right\}
\end{aligned}
$$

and by Eq. (5.78), the composite $\mathrm{B}\left(\mathrm{g} \circ\left(h^{\mathrm{op}}\right)\right)=\mathrm{B}(g) \circ \mathrm{B}\left(h^{\mathrm{op}}\right)$ is:

$$
\left\{(x, y) \mid \text { there exists } z \in R^{n} \text { such that } S(g)(x)=z \text { and } z=S(h)(y)\right\}
$$

Since $S(g)$ and $S(h)$ are functions, the above immediately reduces to the desired formula:

$$
\mathrm{B}\left(g ;\left(h^{\mathrm{op}}\right)\right)=\{(x, y) \mid S(g)(x)=S(h)(y)\}
$$

Solution to Exercise 5.83 .

The behavior of $g^{\text {op }}: n \rightarrow m$ and $h: m \rightarrow p$ are respectively

$$
\begin{aligned}
\mathrm{B}\left(g^{\mathrm{op}}\right) & =\left\{(y, x) \in R^{n} \times R^{m} \mid y=S(g)(x)\right\} \\
\mathrm{B}(h) & =\left\{(x, z) \in R^{m} \times R^{p} \mid S(h)(x)=z\right\}
\end{aligned}
$$

and by Eq. (5.78), the composite $\mathrm{B}\left(\left(g^{\circ \mathrm{op}}\right) ; h\right)=\mathrm{B}\left(g^{\circ \mathrm{op}}\right) ; \mathrm{B}(h)$ is:

$$
\left\{(y, z) \mid \text { there exists } x \in R^{m} \text { such that } y=S(g)(x) \text { and } S(h)(x)=z\right\}
$$

This immediately reduces to the desired formula:

$$
\mathrm{B}\left(\left(g^{\mathrm{op}}\right) \risingdotseq h\right)=\left\{(S(g)(x), S(h)(x)) \mid x \in R^{m}\right\}
$$

\section*{Solution to Exercise 5.84.}

1. The behavior of the 0 -reverse $-\circ$ is the subset $\{y \in R \mid y=0\}$, and its $n$-fold tensor is similarly $\left\{y \in R^{n} \mid y=0\right\}$. Composing this relation with $S(g) \subseteq R^{m} \times R^{n}$ gives $\left\{x \in R^{m} \mid S(g)=0\right\}$, which is the kernel of $S(g)$.

2. The behavior of the discard-inverse - is the subset $\{x \in R\}$, i.e. the largest subset of $R$, and similarly its $m$-fold tensor is $R^{n} \subseteq R^{n}$. Composing this relation with $S(g) \subseteq R^{m} \times R^{n}$ gives $\left\{y \in R^{n} \mid\right.$ there exists $x \in R^{m}$ such that $\left.S(g)(x)=y\right\}$, which is exactly the image of $S(g)$.

3. For any $g: m \rightarrow n$, we first claim that the behavior $\mathrm{B}(g)=\{(x, y) \mid S(g)(x)=y\}$ is linear, i.e. it is closed under addition and scalar multiplication. Indeed, $S(g)$ is multiplication by a matrix, so if $S(g)(x)=y$ then $S(g)(r x)=r y$ and $S(g)\left(x_{1}+x_{2}\right)=S(g)\left(x_{1}\right)+S(g)\left(x_{2}\right)$. Thus we conclude that $(x, y) \in \mathrm{B}(g)$ implies $(r x, r y) \in \mathrm{B}(g)$, so it's closed under scalar multiplication, and $\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right) \in \mathrm{B}(g)$ implies $\left(x_{1}+x_{2}, y_{1}+y_{2}\right) \in \mathrm{B}(g)$ so it's closed under addition. Similarly, the behavior $\mathrm{B}\left(\mathrm{g}^{\mathrm{op}}\right)$ is also linear; the proof is similar.

Finally, we need to show that the composite of any two linear relations is linear. Suppose that $B \subseteq R^{m} \times R^{n}$ and $C \subseteq R^{n} \times R^{p}$ are linear. Take $\left(x_{1}, z_{1}\right),\left(x_{2}, z_{2}\right) \in B ; C$ and take $r \in R$. By definition, there exist $y_{1}, y_{2} \in R^{n}$ such that $\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right) \in B$ and $\left(y_{1}, z_{1}\right),\left(y_{2}, z_{2}\right) \in C$. Since $B$ and $C$ are linear, $\left(r x_{1}, r y_{1}\right) \in B$ and $\left(r y_{1}, r z_{1}\right) \in C$, and also $\left(x_{1}+x_{2}, y_{1}+y_{2}\right) \in B$ and $\left(y_{1}+y_{2}, z_{1}+z_{2}\right) \in C$. Hence $\left(r x_{1}, r z_{1}\right) \in(B ; C)$ and $\left(x_{1}+x_{2}, z_{1}+z_{2}\right) \in(B ; C)$, as desired.

Solution to Exercise 5.85.

Suppose that $B \subseteq R^{m} \times R^{n}$ and $C \subseteq R^{n} \times R^{p}$ are linear. Their composite is the relation $(B ; C) \subseteq R^{m} \times R^{p}$ consisting of all $(x, z)$ for which there exists $y \in R^{n}$ with $(x, y) \in B$ and $(y, z) \in C$. We want to show that the set $(B ; C)$ is linear, i.e. closed under scalar multiplication and addition.

For scalar multiplication, take an $(x, z) \in(B ; C)$ and any $r \in R$. Since $B$ is linear, we have $(r * x, r * y) \in B$ and since $C$ is linear we have $(r * y, r * z) \in C$, so then $(r * x, r * z) \in(B ; C)$. For addition, if we also have $\left(x^{\prime}, z^{\prime}\right) \in(B ; C)$ then there is some $y^{\prime} \in R^{n}$ with $\left(x^{\prime}, y^{\prime}\right) \in B$ and $\left(y^{\prime}, z^{\prime}\right) \in C$, so since $B$ and $C$ are linear we have $\left(x+x^{\prime}, y+y^{\prime}\right) \in B$ and $\left(y+y^{\prime}, z+z^{\prime}\right) \in C$, hence $\left(x+x^{\prime}, z+z^{\prime}\right) \in(B ; C)$.

\section*{A. 6 Solutions for Chapter 6.}

\section*{Solution to Exercise 6.3.}

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-315.jpg?height=89&width=1190&top_left_y=362&top_left_x=370)

1. The left-most (the discrete preorder on $A$ ) has no initial object, because $a \neq b$ and $b \not a$.

2. The middle one has one initial object, namely $a$.

3. The right-most (the co-discrete preorder on $A$ ) has two initial objects.

\section*{Solution to Exercise 6.6.}

Recall that the objects of a free category on a graph are the vertices of the graph, and the morphisms are paths. Thus the free category on a graph $G$ has an initial object if there exists a vertex $v$ that has a unique path to every object. In 1. and 2., the vertex $a$ has this property, so the free categories on graphs 1. and 2. have initial objects. In graph 3., neither $a$ nor $b$ have a path to each other, and so there is no initial object. In graph 4., the vertex $a$ has many paths to itself, and hence its free category does not have an initial object either.

\section*{Solution to Exercise 6.7.}

1. The remaining conditions are that $f\left(1_{R}\right)=1_{S}$, and that $f\left(r_{1}{ }_{R} r_{2}\right)=f\left(r_{1}\right) * S f\left(r_{2}\right)$.

2. The initial object in the category Rig is the natural numbers rig $(\mathbb{N}, 0,+, 1, *)$. The fact that is initial means that for any other rig $R=\left(R, 0_{R},{ }_{R}, 1_{R}, *_{R}\right)$, there is a unique rig homomorphism $f: \mathbb{N} \rightarrow R$.

What is this homomorphism? Well, to be a rig homomorphism, $f$ must send 0 to $0_{R}, 1$ to $1_{R}$. Furthermore, we must also have $f(m+n)=f(m)+_{R} f(n)$, and hence

$$
f(m)=f(\underbrace{1+1+\cdots+1}_{m \text { summands }})=\underbrace{f(1)+f(1)+\cdots+f(1)}_{m \text { summands }}=\underbrace{1_{R}+1_{R}+\cdots+1_{R}}_{m \text { summands }} .
$$

So if there is a rig homomorphism $f: \mathbb{N} \rightarrow R$, it must be given by the above formula. But does this formula work correctly for multiplication?

It remains to check $f(m * n)=f(m) *_{R} f(n)$, and this will follow from distributivity. Noting that $f(m * n)$ is equal to the sum of $m n$ copies of $1_{R}$, we have

$$
\begin{aligned}
f(m) * R f(n) & =(\underbrace{1_{R}+\cdots+1_{R}}_{m \text { summands }}) * R(\underbrace{1_{R}+\cdots+1_{R}}_{n \text { summands }}) \\
& =\underbrace{1_{R} *(\underbrace{1_{R}+\cdots+1_{R}}_{n \text { summands }})+\cdots+1_{R} *(\underbrace{1_{R}+\cdots+1_{R}}_{n \text { summands }})}_{m \text { summands }} \\
& =\underbrace{1_{R}+\cdots+1_{R}}_{m n \text { summands }}=f(m * n) .
\end{aligned}
$$

Thus $(\mathbb{N}, 0,+, 1, *)$ is the initial object in Rig.

\section*{Solution to Exercise 6.8.}

In Definition 6.1, it is the initial object $\varnothing \in \mathcal{C}$ that is universal. In this case, all objects $c \in \mathcal{C}$ are 'comparable objects'. So the universal property of the initial object is that to any object $c \in \mathcal{C}$, there is a unique map $\varnothing \rightarrow c$ coming from the initial object.

\section*{Solution to Exercise 6.10.}

If $c_{1}$ is initial then by the universal property, for any $c$ there is a unique morphism $c_{1} \rightarrow c$; in particular, there is a unique morphism $c_{1} \rightarrow c_{2}$, call it $f$. Similarly, if $c_{2}$ is initial then there is a unique morphism $c_{2} \rightarrow c_{1}$, call it $g$. But how do we know that $f$ and $g$ are mutually inverse? Well since $c_{1}$ is initial
there is a unique morphism $c_{1} \rightarrow c_{1}$. But we can think of two: $\mathrm{id}_{c_{1}}$ and $f ; g$. Thus they must be equal. Similarly for $c_{2}$, so we have $f ; g=\operatorname{id}_{\mathcal{c}_{1}}$ and $g \circ f=\operatorname{id}_{c_{2}}$, which is the definition of $f$ and $g$ being mutually inverse.

Solution to Exercise 6.13.

Let $(P, \leq)$ be a preorder, and $p, q \in P$. Recall that a preorder is a category with at most one morphism, denoted $\leq$, between any two objects. Also recall that all diagrams in a preorder commute, since this means any two morphisms with the same domain and codomain are equal.

Translating Definition 6.11 to this case, a coproduct $p+q$ is $P$ is an element of $P$ such that $p \leq p+q$ and $q \leq p+q$, and such that for all elements $x \in P$ with maps $p \leq x$ and $q \leq x$, we have $p+q \leq x$. But this says exactly that $p+q$ is a join: it is a least element above both $p$ and $q$. Thus coproducts in preorders are exactly the same as joins.

Solution to Exercise 6.16.

The function $[f, g]$ is defined by

$$
\begin{aligned}
{[f, g]: A \sqcup B } & \longrightarrow T \\
\text { apple1 } & \longmapsto a \\
\text { banana1 } & \longmapsto b \\
\text { pear1 } & \longmapsto p \\
\text { cherry1 } & \longmapsto c \\
\text { orange1 } & \longmapsto o \\
\text { apple2 } & \longmapsto e \\
\text { tomato2 } & \longmapsto o \\
\text { mango2 } & \longmapsto o
\end{aligned}
$$

Solution to Exercise 6.17.

1. The equation $\iota_{A} \varrho[f, g]=f$ is the commutativity of the left hand triangle in the commutative diagram (6.12) defining $[f, g]$.

2. The equation $\iota_{B} \circ[f, g]=g$ is the commutativity of the right hand triangle in the commutative diagram (6.12) defining $[f, g]$.

3. The equation $[f, g] \% h=[f ; h, g \circ h]$ follows from the universal property of the coproduct. Indeed, the diagram

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-316.jpg?height=320&width=409&top_left_y=1778&top_left_x=912)

commutes, and the universal property says there is a unique map $[f ; h, g ; h]: A+B \rightarrow D$ for which this occurs. Hence we must have $[f, g] ; h=[f ; h, g ; h]$.

4. Similarly, to show $\left[\iota_{A}, \iota_{B}\right]=\mathrm{id}_{A+B}$, observe that the diagram

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-316.jpg?height=200&width=328&top_left_y=2250&top_left_x=953)

trivially commutes. Hence by the uniqueness in (6.12), $\left[\iota_{A}, \iota_{B}\right]=\mathrm{id}_{A+B}$.

\section*{Solution to Exercise 6.18.}

This exercise is about showing that coproducts and an initial object give a symmetric monoidal category. Since all we have are coproducts and an initial object, and since these are defined by their universal properties, the solution is to use these universal properties over and over, to prove that all the data of Definition 4.45 can be constructed.

1. To define a functor $+: \mathcal{C} \times \mathcal{C} \rightarrow \mathcal{C}$ we must define its action on objects and morphisms. In both cases, we just take the coproduct. If $(A, B)$ is an object of $\mathcal{C} \times \mathcal{C}$, its image $A+B$ is, as usual, the coproduct of the two objects of $\mathcal{C}$. If $(f, g):(A, B) \rightarrow(C, D)$ is a morphism, then we can form a morphism $f+g=\left[f ; \iota_{C}, g ; \iota_{D}\right]: A+B \rightarrow C+D$, where $\iota_{C}: C \rightarrow C+D$ and $\iota_{D}: D \rightarrow C+D$ are the canonical morphisms given by the definition of the coproduct $A+B$.

Note that this construction sends identity morphisms to identity morphisms, since by Exercise 6.174 we have

$$
\mathrm{id}_{A}+\mathrm{id}_{B}=\left[\mathrm{id}_{A} \because \iota_{A}, \mathrm{id}_{B} \circ \iota_{B}\right]=\left[\iota_{A}, \iota_{B}\right]=\operatorname{id}_{A+B}
$$

To show that + is a functor, we need to also show it preserves composition. Suppose we also have a morphism $(h, k):(C, D) \rightarrow(E, F)$ in $\mathcal{C} \times \mathcal{C}$. We need to show that $(f+g) \circ(h+k)=(f \circ h)+(g \circ k)$. This is a slightly more complicated version of the argument in Exercise 6.173. It follows from the fact the diagram below commutes:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-317.jpg?height=288&width=596&top_left_y=1081&top_left_x=819)

Indeed, we again use the uniqueness of the copairing in (6.12), this time to show that $(f \circ h)+\left(g^{\circ} k\right)=$

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-317.jpg?height=44&width=694&top_left_y=1447&top_left_x=458)

2. Recall the universal property of the initial object gives a unique map $!_{A}: \varnothing \rightarrow A$. Then the copairing $\left[\operatorname{id}_{A},!_{A}\right]$ is a map $A+\varnothing \rightarrow A$. Moreover, it is an isomorphism with inverse $\iota_{A}: A \rightarrow$ $A+\varnothing$. Indeed, using the properties in Exercise 6.17 and the universal property of the initial object, we have $\iota_{A} 9\left[\mathrm{id}_{A},!_{A}\right]=\operatorname{id}_{A}$, and

$$
\left[\operatorname{id}_{A},!_{A}\right] \because \iota_{A}=\left[\operatorname{id}_{A} \because \iota_{A},!_{A} \nsubseteq \iota_{A}\right]=\left[\iota_{A},!_{A+\varnothing}\right]=\left[\iota_{A}, \iota_{\varnothing}\right]=\operatorname{id}_{A+\varnothing}
$$

An analogous argument shows $\left[!_{A}, \mathrm{id}_{A}\right]: \varnothing+A \rightarrow A$ is an isomorphism.

3. We'll just write down the maps and their inverses; we leave it to you, if you like, to check that they indeed are inverses.

a) The map $\left[\operatorname{id}_{A}+\iota_{B}, \iota_{C}\right]=\left[\left[\iota_{A}, \iota_{B} \circ \iota_{B+C}\right], \iota_{C} \circ \iota_{B+C}\right]:(A+B)+C \rightarrow A+(B+C)$ is an isomorphism, with inverse $\left[\iota_{A}, \iota_{B}+\operatorname{id}_{C}\right]: A+(B+C) \rightarrow(A+B)+C$.

b) The map $\left[\iota_{A}, \iota_{B}\right]: A+B \rightarrow B+A$ is an isomorphism. Note our notation here is slightly confusing: there are two maps named $\iota_{A}$, (i) $\iota_{A}: A \rightarrow A+B$, and (ii) $\iota_{A}: A \rightarrow B+A$, and similarly for $\iota_{B}$. In the above we mean the map (ii). It has inverse $\left[\iota_{A}, \iota_{B}\right]: B+A \rightarrow A+B$, where in this case we mean the map (i).

\section*{Solution to Exercise 6.24.}

1. Suppose given an arbitrary diagram of the form $B \leftarrow A \rightarrow C$ in Disc ; we need to show that it has a pushout. The only morphisms in Disc ${ }_{S}$ are identities, so in particular $A=B=C$, and the square consisting of all identities is its pushout.

2. Suppose Disc $S$ has an initial object $s$. Then $S$ cannot be empty! But it also cannot have more than one object, because if $s^{\prime}$ is another object then there is a morphism $s \rightarrow s^{\prime}$, but the only morphisms in $S$ are identities so $s=s^{\prime}$. Hence the set $S$ must consist of exactly one element.

Solution to Exercise 6.26.

The pushout is the set $\underline{4}$, as depicted in the top right in the diagram below, equipped also with the depicted functions:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-318.jpg?height=537&width=631&top_left_y=382&top_left_x=758)

We want to see that this checks out with the description from Example 6.25, i.e. that it is the set of equivalence classes in $\underline{5} \sqcup \underline{3}$ generated by the relation $\{f(a) \sim g(a) \mid a \in \underline{4}\}$. If we denote elements of $\underline{5}$ as $\{1, \ldots, 5\}$ and those of $\underline{3}$ as $\left\{1^{\prime}, 2^{\prime}, 3^{\prime}\right\}$, we can redraw the functions $f, g$ :

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-318.jpg?height=228&width=285&top_left_y=1095&top_left_x=931)

which says we take the equivalence relation on $\underline{5} \sqcup \underline{3}$ generated by: $1 \sim 1^{\prime},, 3 \sim 1^{\prime}, 5 \sim 2^{\prime}$, and $5 \sim 3^{\prime}$. The equivalence classes are $\left\{1,1^{\prime}, 3\right\},\{2\},\{4\}$, and $\left\{5,2^{\prime}, 3^{\prime}\right\}$. These four are exactly the four elements in the set labeled 'pushout' in Eq. (A.1).

\section*{Solution to Exercise 6.28.}

1. The diagram to the left commutes because $\varnothing$ is initial, and so has a unique map $\varnothing \rightarrow X+Y$. This implies we must have $f \circ \iota_{X}=g \circ \iota_{Y}$.

2. There is a unique map $X+Y \rightarrow T$ making the diagram in (6.21) commute simply by the universal property of the coproduct (6.12) applied to the maps $x: X \rightarrow T$ and $y: Y \rightarrow T$.

3. Suppose $X+\varnothing Y$ exists. By the universal property of $\varnothing$, given any pair of arrows $x: X \rightarrow T$ and $y: Y \rightarrow T$, the diagram

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-318.jpg?height=176&width=198&top_left_y=1820&top_left_x=1018)

commutes. This means, by the universal property of the pushout $X+\varnothing Y$, there exists a unique map $t: X+\varnothing Y \rightarrow T$ such that $\iota_{X} \circ t=x$ and $\iota_{Y} ; t=y$. Thus $X+\varnothing Y$ is the coproduct $X+Y$.

\section*{Solution to Exercise 6.35.}

We have to check that the colimit of the diagram shown left really is given by taking three pushouts as shown right:
![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-318.jpg?height=252&width=850&top_left_y=2259&top_left_x=649)

That is, we need to show that $S$, together with the maps from $A, B, X, Y$, and $Z$, has the required universal property. So suppose given an object $T$ with two commuting diagrams as shown:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-319.jpg?height=252&width=295&top_left_y=367&top_left_x=926)

We need to show there is a unique map $S \rightarrow T$ making everything commute. Since $Q$ is a pushout of $X \leftarrow A \rightarrow Y$, there is a unique map $Q \rightarrow T$ making a commutative triangle with $Y$, and since $R$ is the pushout of $Y \leftarrow B \rightarrow Z$, there is a unique map $R \rightarrow T$ making a commutative triangle with $Y$. This implies that there is a commuting $(Y, Q, R, T)$ square, and hence a unique map $S \rightarrow T$ from its pushout making everything commute. This is what we wanted to show.

\section*{Solution to Exercise 6.41.}

The formula in Theorem 6.37 says that the pushout $X+_{N} Y$ is given by the set of equivalence classes of $X \sqcup N \sqcup Y$ under the equivalence relation generated by $x \sim n$ if $x=f(n)$, and $y \sim n$ if $y=g(n)$, where $x \in X, y \in Y, n \in N$. Since for every $n \in N$ there exists an $x \in X$ such that $x=f(n)$, this set is equal to the set of equivalence classes of $X \sqcup Y$ under the equivalence relation generated by $x \sim y$ if there exists $n$ such that $x=f(n)$ and $y=g(n)$. This is exactly the description of Example 6.25.

Solution to Exercise 6.48.

The monoidal product is

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-319.jpg?height=612&width=462&top_left_y=1277&top_left_x=845)

Solution to Exercise 6.49.

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-319.jpg?height=44&width=1402&top_left_y=1992&top_left_x=371)
the composition rule in Cospan $_{\text {FinSet }}$ says that (i) the composite cospan has a unique element in the apex for every connected component of the concatenation of the wire diagrams $x$ and $y$, and (ii) in the wire diagram for $x ; y$, each element of the feet is connected by a wire to the element representing the connected component to which it belongs.

\section*{Solution to Exercise 6.57.}

Morphisms 1, 4, and 6 are equal, and morphisms 3 and 5 are equal. Morphism 3 is not equal to any other depicted morphism. This is an immediate consequence of Theorem 6.55.

Solution to Exercise 6.59.

1. The input to $h$ should be labelled $B$.

2. The output of $g$ should be labelled $D$, since we know from the labels in the top right that $h$ is a morphism $B \rightarrow D \otimes D$.

3. The fourth output wire of the composite should be labelled $D$ too!

\section*{Solution to Exercise 6.62.}

We draw the function depictions above, and the wiring depictions below. Note that we depict the empty set with blank space.

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-320.jpg?height=242&width=269&top_left_y=605&top_left_x=429)

multiplication $\mu_{1}$

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-320.jpg?height=233&width=228&top_left_y=610&top_left_x=797)

unit $\eta_{1}$

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-320.jpg?height=320&width=597&top_left_y=604&top_left_x=1119)

\section*{Solution to Exercise 6.63.}

The special law says that the composite of cospans

$$
\longrightarrow \sim X \xrightarrow{\mathrm{id}} X \stackrel{[\mathrm{id}, \mathrm{id}]}{\longleftrightarrow} X+X \xrightarrow{[\mathrm{id}, \mathrm{id}]} X \stackrel{\mathrm{id}}{\longleftarrow} X
$$

is the identity. This comes down to checking that the square

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-320.jpg?height=182&width=287&top_left_y=1302&top_left_x=930)

is a pushout square. It is trivial to see that the square commutes. Suppose now that we have maps $f: X \rightarrow Y$ and $g: X \rightarrow Y$ such that

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-320.jpg?height=174&width=282&top_left_y=1599&top_left_x=932)

Write $\iota_{1}: X \rightarrow X+X$ for the map into the first copy of $X$ in $X+X$, given by the definition of coproduct. Then, using the fact that $\iota_{1} \because[\mathrm{id}, \mathrm{id}]=$ id from Exercise 6.171 , and the commutativity of the above square, we have $f=\iota_{1} \cong[\mathrm{id}, \mathrm{id}] \because f=\iota_{1} \fallingdotseq[\mathrm{id}, \mathrm{id}] \because g=g$. This means that $f: X \rightarrow T$ is the unique map such that

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-320.jpg?height=252&width=328&top_left_y=1956&top_left_x=907)

commutes, and so (A.2) is a pushout square.

\section*{Solution to Exercise 6.67.}

The missing diagram is

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-320.jpg?height=111&width=352&top_left_y=2406&top_left_x=903)

Solution to Exercise 6.70.

Let $A \subseteq S$ and $B \subseteq T$. Then

$$
\begin{aligned}
\left.\varphi_{S^{\prime}, T^{\prime}}\left(\operatorname{im}_{f} \times \operatorname{im}_{g}\right)(A \times B)\right) & =\varphi_{S^{\prime}, T^{\prime}}(\{f(a) \mid a \in A\} \times\{g(b) \mid b \in B\}) \\
& =\{(f(a), g(b)) \mid a \in A, b \in B\} \\
& =\operatorname{im}_{f \times g}(A \times B) \\
& =\operatorname{im}_{f \times g}\left(\varphi_{S, T}(A, B)\right) .
\end{aligned}
$$

Thus the required square commutes.

\section*{Solution to Exercise 6.78.}

They mean that every category $\operatorname{Cospan}_{\mathcal{C}}$ is equal to a category $\operatorname{Cospan}_{F}$, for some well-chosen $F$. They also tell you how to choose this $F$ : take the functor $F: \mathcal{C} \rightarrow$ Set that sends every object of $\mathcal{C}$ to the set $\{*\}$, and every morphism of $\mathcal{C}$ to the identity function on $\{*\}$. Of course, you will have to check this functor is a lax symmetric monoidal functor, but in fact this is not hard to do.

To check that $\operatorname{Cospan}_{\mathcal{C}}$ is equal to Cospan $_{F}$, first observe that they have the same objects: the objects of $\mathcal{C}$. Next, observe that a morphism in Cospan $_{F}$ is a cospan $X \leftarrow N \rightarrow Y$ in $\mathcal{C}$ together with an element of $F N=\{*\}$. But $F N$ also has a unique element, *! So there's no choice here, and we can consider morphisms of $\operatorname{Cospan}_{F}$ just to be cospans in $\mathcal{C}$. Moreover, composition of morphisms in $\operatorname{Cospan}_{F}$ is simply the usual composition of cospans via pushout, so $\operatorname{Cospan}_{F}=\operatorname{Cospan}_{\mathcal{C}}$.

(More technically, we might say that $\operatorname{Cospan}_{\mathcal{C}}$ and $\operatorname{Cospan}_{F}$ are isomorphic, where the isomorphism is the identity-on-objects functor $\operatorname{Cospan}_{\mathcal{C}} \rightarrow$ Cospan $_{F}$ that simply decorates each cospan with *, and its inverse is the one that forgets this *. But this is close enough to equal that many category theorists, us included, don't mind saying equal in this case.)

\section*{Solution to Exercise 6.79.}

We can represent the circuit in Eq. (6.71) by the tuple $(V, A, s, t, \ell)$ where $V=\{\mathrm{ul}, \mathrm{ur}, \mathrm{dl}, \mathrm{dr}\}, A=$ $\{\mathrm{r} 1, \mathrm{r} 2, \mathrm{r} 3, \mathrm{c} 1, \mathrm{i} 1\}$, and $s, t$, and $\ell$ are defined by the table

\begin{tabular}{c|ccccc} 
& $\mathrm{r} 1$ & $\mathrm{r} 2$ & $\mathrm{r} 3$ & $\mathrm{c} 1$ & $\mathrm{i} 1$ \\
\hline$s(-)$ & $\mathrm{dl}$ & $\mathrm{ul}$ & $\mathrm{ur}$ & $\mathrm{ul}$ & $\mathrm{dl}$ \\
$t(-)$ & $\mathrm{ul}$ & $\mathrm{ur}$ & $\mathrm{dr}$ & $\mathrm{ur}$ & $\mathrm{dr}$ \\
$\ell(-)$ & $1 \Omega$ & $2 \Omega$ & $1 \Omega$ & $3 F$ & $1 H$
\end{tabular}

Solution to Exercise 6.80.

The circuit $\operatorname{Circ}(f)(c)$ is

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-321.jpg?height=108&width=439&top_left_y=1903&top_left_x=859)

Solution to Exercise 6.82.

The circuit $\psi_{2,2}(b, s)$ is the disjoint union of the two labelled graphs $b$ and $s$ :
![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-321.jpg?height=82&width=630&top_left_y=2206&top_left_x=758)

\section*{Solution to Exercise 6.84}

The cospan is the cospan $\underline{1} \stackrel{f}{\rightarrow} \underline{2} \stackrel{g}{\leftarrow} \underline{1}$, where $f(1)=1$ and $g(1)=2$. The decoration is the $C$-circuit $(\underline{2},\{a\}, s, t, \ell)$, where $s(a)=1, t(a)=2$ and $\ell(a)=$ battery.

Solution to Exercise 6.86.

Recall the circuit $C:=(V, A, s, t, \ell)$ from the solution to Exercise 6.79. Then the first decorated cospan is given by the cospan $\underline{1} \stackrel{f}{\rightarrow} V \stackrel{g}{\leftarrow} \underline{2}, f(1)=\mathrm{ul}, g(1)=\mathrm{ur}$, and $g(2)=$ ur, decorated by circuit $C$. The second decorated cospan is given by the cospan $\underline{2} \xrightarrow{f^{\prime}} V^{\prime} \stackrel{g^{\prime}}{\rightleftarrows} \underline{2}$ and the circuit $C^{\prime}:=\left(V^{\prime}, A^{\prime}, s^{\prime}, t^{\prime}, \ell^{\prime}\right)$, where $V^{\prime}=\{l, r, d\}, A^{\prime}=\left\{\mathrm{r} 1^{\prime}, \mathrm{r} 2^{\prime}\right\}$, and the functions are given by the tables

$$
\begin{array}{l|ll} 
& 1 & 2 \\
\hline f^{\prime}(-) & 1 & \mathrm{~d} \\
g^{\prime}(-) & \mathrm{r} & \mathrm{r}
\end{array}
$$

\begin{tabular}{c|cc} 
& $\mathrm{r}^{\prime}$ & $\mathrm{r}^{\prime}$ \\
\hline$s(-)$ & $\mathrm{l}$ & $\mathrm{r}$ \\
$t(-)$ & $\mathrm{r}$ & $\mathrm{d}$ \\
$\ell(-)$ & $5 \Omega$ & $8 \Omega$
\end{tabular}

To compose these, we first take the pushout of $V \stackrel{g}{\leftarrow} \xrightarrow{f^{\prime}} V^{\prime}$. This gives the a new apex $V^{\prime \prime}=$ $\{\mathrm{ul}, \mathrm{dl}, \mathrm{dr}, \mathrm{m}, \mathrm{r}\}$ with five elements, and composite cospan $\underline{1} \xrightarrow{h} V^{\prime \prime} \stackrel{k}{\leftarrow} \underline{2}$ given by $h(1)=\mathrm{ul}, k(1)=\mathrm{r}$ and $k(2)=\mathrm{m}$. The new circuit is given by $\left(V,{ }^{\prime \prime} A+A^{\prime}, s,{ }^{\prime \prime} t,{ }^{\prime \prime} \ell^{\prime \prime}\right)$ where the functions are given by

\begin{tabular}{c|ccccccc} 
& $\mathrm{r} 1$ & $\mathrm{r} 2$ & $\mathrm{r} 3$ & $\mathrm{c} 1$ & $\mathrm{i} 1$ & $\mathrm{r}^{\prime}$ & $\mathrm{r} 2^{\prime}$ \\
\hline$s^{\prime \prime}(-)$ & $\mathrm{dl}$ & $\mathrm{ul}$ & $\mathrm{m}$ & $\mathrm{ul}$ & $\mathrm{dl}$ & $\mathrm{m}$ & $\mathrm{r}$ \\
$t^{\prime \prime}(-)$ & $\mathrm{ul}$ & $\mathrm{m}$ & $\mathrm{dr}$ & $\mathrm{m}$ & $\mathrm{dr}$ & $\mathrm{r}$ & $\mathrm{m}$ \\
$\ell^{\prime \prime}(-)$ & $1 \Omega$ & $2 \Omega$ & $1 \Omega$ & $3 F$ & $1 H$ & $5 \Omega$ & $8 \Omega$
\end{tabular}

This is exactly what is depicted in Eq. (6.74).

Solution to Exercise 6.88.

Composing $\eta$ and $x$ we have

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-322.jpg?height=423&width=957&top_left_y=1252&top_left_x=600)

and composing the result of $\epsilon$ gives

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-322.jpg?height=420&width=984&top_left_y=1731&top_left_x=576)

Solution to Exercise 6.96.

1. The cospan shown left corresponds to the wiring diagram shown right:
![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-322.jpg?height=212&width=1040&top_left_y=2292&top_left_x=606)

It has two inner circles, each with two ports. One port of the first is wired to a port of the second. One port of the first is wired to the outside circle, and one port of the second is wired to the outside circle. This is exactly what the cospan says to do.

2. The cospan shown left corresponds to the wiring diagram shown right:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-323.jpg?height=249&width=1049&top_left_y=445&top_left_x=598)

3. The composite $g \circ_{1} f$ has arity $(2,2,2,2 ; 0)$; there is a depiction on the left:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-323.jpg?height=285&width=1160&top_left_y=779&top_left_x=537)

4. The associated wiring diagram is shown on the right above. One can see that one diagram has been substituted in to a circle of the other.

\section*{A. 7 Solutions for Chapter 7 .}

Solution to Exercise 7.4.

In the commutative diagram below, suppose the $\left(B, C, B^{\prime}, C^{\prime}\right)$ square is a pullback:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-324.jpg?height=196&width=333&top_left_y=436&top_left_x=907)

We need to show that the $\left(A, B, A^{\prime}, B^{\prime}\right)$ square is a pullback iff the $\left(A, C, A^{\prime}, C^{\prime}\right)$ rectangle is a pullback. Suppose first that $\left(A, B, A^{\prime}, B^{\prime}\right)$ is a pullback, and take any $(X, p, q)$ as in the following diagram:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-324.jpg?height=257&width=409&top_left_y=755&top_left_x=869)

where $q \circ f^{\prime} ; g^{\prime}=p ; h_{3}$. Then by the universal property of the $\left(B, C, B^{\prime}, C^{\prime}\right)$ pullback, we get a unique dotted arrow $r$ making the left-hand diagram below commute:
![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-324.jpg?height=260&width=986&top_left_y=1141&top_left_x=581)

In other words $r \circ h_{2}=g 9 f^{\prime}$ and $r \circ g=p$. Then by the universal property of the $\left(A, B, A^{\prime}, B^{\prime}\right)$ pullback, we get a unique dotted arrow $r^{\prime}: X \rightarrow A$ making the right-hand diagram commute, i.e. $r^{\prime} \circ f=r$ and $r^{\prime} ; h_{1}=q$. This gives the existence of an $r$ with the required property, $r^{\prime} ; f=r$ and $r^{\prime} ; f ; g=r ; g=p$. To see uniqueness, suppose given another morphisms $r_{0}$ such that $r_{0} \cong f \circ g=p$ and $r_{0} \risingdotseq h_{1}=q$ :

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-324.jpg?height=266&width=409&top_left_y=1607&top_left_x=869)

Then by the uniqueness of $r$, we must have $r_{0} ̊ f=r$, and then by the uniqueness of $r^{\prime}$, we must have $r_{0}=r^{\prime}$. This proves the first result.

The second is similar. Suppose that $\left(A, C, A^{\prime}, C^{\prime}\right)$ and $\left(B, C, B^{\prime}, C^{\prime}\right)$ are pullbacks and suppose given a commutative diagram of the following form:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-324.jpg?height=258&width=409&top_left_y=2126&top_left_x=866)

i.e. where $r \AA h_{2}=q \S f^{\prime}$. Then letting $p:=r \circ g$, we have

$$
p ; h_{3}=r \nsubseteq g ; h_{3}=r \AA h_{2} \nsubseteq g^{\prime}=q \ddagger f^{\prime} ; g^{\prime}
$$
so by the universal property of the $\left(A, C, A^{\prime}, C^{\prime}\right)$ pullback, there is a unique morphism $r^{\prime}: X \rightarrow A$ such that $r^{\prime} ; f ; g=p$ and $r_{0} \circ h_{1}=q$, as shown:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-325.jpg?height=258&width=409&top_left_y=359&top_left_x=869)

But now let $r_{0}:=r^{\prime} \rightrightarrows f$. It satisfies $r_{0} \cong g=p$ and $r_{0} \cong h_{2}=q \cong f^{\prime}$, and $r$ satisfies the same equations: $r \circ g=p$ and $r \circ h_{2}=q \circ f^{\prime}$. Hence by the universal property of the (B,C, $\left.B^{\prime}, C^{\prime}\right)$ pullback $r_{0}=r^{\prime}$. It follows that $r^{\prime}$ is a pullback of the $\left(A, B, A^{\prime}, B^{\prime}\right)$ square, as desired.

\section*{Solution to Exercise 7.6.}

A function $f: A \rightarrow B$ is injective iff for all $a_{1}, a_{2} \in A$, if $f\left(a_{1}\right)=f\left(a_{2}\right)$ then $a_{1}=a_{2}$. It is a monomorphism iff for all sets $X$ and functions $g_{1}, g_{2}: X \rightarrow A$, if $g_{1} \fallingdotseq f=g_{2} \because f$ then $g_{1}=g_{2}$. Indeed, this comes directly from the universal property of the pullback from Definition 7.5,

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-325.jpg?height=241&width=279&top_left_y=991&top_left_x=934)

because the dashed arrow is forced to equal both $g_{1}$ and $g_{2}$, thus forcing $g_{1}=g_{2}$.

1. Suppose $f$ is a monomorphism, let $a_{1}, a_{2} \in A$ be elements, and suppose $f\left(a_{1}\right)=f\left(a_{2}\right)$. Let $X=\{*\}$ be a one element set, and let $g_{1}, g_{2}: X \rightarrow A$ be given by $g_{1}(*):=a_{1}$ and $g_{2}(*):=a_{2}$. Then $g_{1} \fallingdotseq f=g_{2} \because f$, so $g_{1}=g_{2}$, so $a_{1}=a_{2}$.

2. Suppose that $f$ is an injection, let $X$ be any set, and let $g_{1}, g_{2}: X \rightarrow A$ be such that $g_{1} ๆ f=g_{2} ๆ f$. We will have $g_{1}=g_{2}$ if we can show that $g_{1}(x)=g_{2}(x)$ for every $x \in X$. So take any $x \in X$; since $f\left(g_{1}(x)\right)=f\left(g_{2}(x)\right)$ and $f$ is injective, we have $g_{1}(x)=g_{2}(x)$ as desired.

Solution to Exercise 7.7.

1. Suppose we have a pullback as shown, where $i$ is an isomorphism:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-325.jpg?height=185&width=198&top_left_y=1691&top_left_x=1018)

Let $j:=i^{-1}$ be the inverse of $i$, and consider $g:=(f ; j): A \rightarrow B^{\prime}$. Then $g ; i=f$, so by the existence part of the universal property, there is a map $j^{\prime}: A \rightarrow A^{\prime}$ such that $j^{\prime} ; i^{\prime}=\operatorname{id}_{A}$ and $j^{\prime} \circ f^{\prime}=f \circ j$. We will be done if we can show $i^{\prime} \circ j^{\prime}=\operatorname{id}_{A^{\prime}}$. One checks that $\left(i^{\prime} \nsubseteq j^{\prime}\right) \AA i^{\prime}=i^{\prime}$ and that $\left(i^{\prime} 9 j^{\prime}\right) \circ f^{\prime}=i^{\prime} \nsubseteq f \circ j=f^{\prime} \circ i \circ j=f^{\prime}$. But id ${ }_{A^{\prime}}$ also satisfies those properties: $\operatorname{id}_{A^{\prime}} \circ i^{\prime}=i^{\prime}$ and $\operatorname{id}_{A^{\prime}} \circ f^{\prime}=f^{\prime}$, so by the uniqueness part of the universal property, $\left(i^{\prime} \circ j^{\prime}\right)=\operatorname{id}_{A^{\prime}}$.

2. We need to show that the following diagram is a pullback:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-325.jpg?height=190&width=162&top_left_y=2176&top_left_x=1033)

So take any object $X$ and morphisms $g: X \rightarrow A$ and $h: X \rightarrow B$ such that $g ; f=h ; \mathrm{id}_{B}$. We need to show there is a unique morphism $r: X \rightarrow A$ such that $r \circ \operatorname{id}_{A}=g$ and $r ; f=h$. That's easy: the first requirement forces $r=g$ and the second requirement is then fulfilled.

Solution to Exercise 7.8.

Consider the diagram shown left, in which all three squares are pullbacks:
![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-326.jpg?height=300&width=936&top_left_y=362&top_left_x=606)

The front and bottom squares are the same-the assumed pullback-and the right-hand square is a pullback because $f$ is assumed monic. We can complete it to the commutative diagram shown right, where the back square and top square are pullbacks by Exercise 7.7. Our goal is to show that the left-hand square is a pullback.

To do this, we use two applications of the pasting lemma, Exercise 7.4. Since the right-hand face is a pullback and the back face is a pullback, the diagonal rectangle (lightly drawn) is also a pullback. Since the front face is a pullback, the left-hand face is also a pullback.

\section*{Solution to Exercise 7.9.}

The following is an epi-mono factorization of $f$ :

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-326.jpg?height=184&width=466&top_left_y=1117&top_left_x=840)

Solution to Exercise 7.11.

1. If $\mathcal{V}$ is a quantale with the stated properties, then
- I serves as a top element: $v \leq I$ for all $v \in V$.
- $v \otimes w$ serves as a meet operation, i.e. it satisfies the same universal property as $\wedge$, namely $v \otimes w$ is a greatest lower bound for $v$ and $w$.

Now the $\multimap$ operation satisfies the same universal property as exponentiation (hom-object) does, namely $v \leq(w \multimap x)$ iff $v \otimes W \leq x$. So $\mathcal{V}$ is a cartesian closed category, and of course it is a preorder.

2. Not every cartesian closed preorder comes from a quantale with the stated properties, because quantales have all joins and cartesian closed preorders need not. Finding a counterexample-a cartesian closed preorder that is missing some joins-takes some ingenuity, but it can be done. Here's one we came up with:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-326.jpg?height=349&width=569&top_left_y=1907&top_left_x=821)

This is the product preorder $\mathbb{N}^{\text {op }} \times \mathbb{N}^{\text {op }}$ : its objects are pairs $(a, b) \in \mathbb{N} \times \mathbb{N}$ with $(a, b) \leq\left(a^{\prime}, b^{\prime}\right)$ iff, in the usual ordering on $\mathbb{N}$ we have $a^{\prime} \leq a$ and $b^{\prime} \leq b$. But you can just look at the diagram.

It has a top element, $(0,0)$, and it has binary meets, $(a, b) \wedge\left(a^{\prime}, b^{\prime}\right)=\left(\max \left(a, a^{\prime}\right), \max \left(b, b^{\prime}\right)\right)$. But it has no bottom element, so it has no empty join. Thus we will be done if we can show that for
each $x, y$, the hom-object $x \multimap y$ exists. The formula for it is $x \multimap y=\bigvee\{w \mid w \wedge x \leq y\}$, i.e. we need these particular joins to exist. Since $y \wedge x \leq y$, we have $y \leq x \multimap y$. So we can replace the formula with $x \multimap y=\bigvee\{w \mid y \leq w$ and $w \wedge x \leq y\}$. But the set of elements in $\mathbb{N}$ op $\times \mathbb{N}$ op that are bigger than $y$ is finite and nonempty. ${ }^{2}$ So this is a finite nonempty join, and $\mathbb{N}$ op $\times \mathbb{N}^{\circ p}$ has all finite nonempty joins: they are given by inf.

Solution to Exercise 7.16.

Let $m: \mathbb{Z} \rightarrow \mathbb{B}$ be the characteristic function of the inclusion $\mathbb{N} \subseteq \mathbb{Z}$.

$$
\begin{array}{ll}
\text { 1. }\ulcorner m\urcorner(-5)=\text { false. } \quad \text { 2. }\ulcorner m\urcorner(0)=\text { true. }
\end{array}
$$

\section*{Solution to Exercise 7.17.}

1. The characteristic function $\left\ulcorner\operatorname{id}_{\mathbb{N}}\right\urcorner: \mathbb{N} \rightarrow \mathbb{B}$ sends each $n \in \mathbb{N}$ to true.

2. Let $!_{\mathbb{N}}: \varnothing \rightarrow \mathbb{N}$ be the inclusion of the empty set. The characteristic function $\left\ulcorner!_{\mathbb{N}}\right\urcorner: \mathbb{N} \rightarrow \mathbb{B}$ sends each $n \in \mathbb{N}$ to false.

Solution to Exercise 7.19.

1. The sort of thing (*?*) we're looking for is a subobject of $\mathbb{B}$, say $A \subseteq \mathbb{B}$. This would have a characteristic function, and we're trying to find the $A$ for which the characteristic function is $\neg: \mathbb{B} \rightarrow \mathbb{B}$.

2. The question now asks "what is $A$ ?" The answer is $\{$ false $\} \subseteq \mathbb{B}$.

Solution to Exercise 7.20.

1. Here is the truth table for $P=(P \wedge Q)$ :

\begin{tabular}{cc||c|c}
$P$ & $Q$ & $P \wedge Q$ & $P=(P \wedge Q)$ \\
true & true & true & true \\
true & false & false & false \\
false & true & false & true \\
false & false & false & true
\end{tabular}

2. Yes!

3. The characteristic function for $P \Rightarrow Q$ is the function $\ulcorner\Rightarrow\urcorner: \mathbb{B} \times \mathbb{B} \rightarrow \mathbb{B}$ given by the first, second, and fourth column of Eq. (A.3).

4. It classifies the subset $\{$ (true, true), (false, true), (false, false) $\subseteq \mathbb{B} \times \mathbb{B}$.

Solution to Exercise 7.21.

Say that $\ulcorner E\urcorner,\ulcorner P\urcorner,\ulcorner T\urcorner: \mathbb{N} \rightarrow \mathbb{B}$ classify respectively the subsets $E:=\{n \in \mathbb{N} \mid n$ is even $\}, P:=\{n \in \mathbb{N} \mid$ $n$ is prime $\}$, and $T:=\{n \in \mathbb{N} \mid n \geq 10\}$ of $\mathbb{N}$.
1. $\ulcorner E\urcorner(17)=$ false because 17 is not even.
2. $\ulcorner P\urcorner(17)=$ true because 17 is prime.
3. $\ulcorner T\urcorner(17)=$ true because $17 \geq 10$.

4. The set classified by $(\ulcorner E\urcorner \wedge\ulcorner P\urcorner) \vee\ulcorner T\urcorner$ is that of all natural numbers that are either above 10 or an even prime. The smallest three elements of this set are 2,10,11.

Solution to Exercise 7.27.

1. The 1-dimensional analogue of an $\epsilon$-ball around a point $x \in \mathbb{R}$ is $B(x, \epsilon):=\left\{x^{\prime} \in \mathbb{R}|| x-x^{\prime} \mid<\epsilon\right\}$, i.e. the set of all points within $\epsilon$ of $x$.

2. A subset $U \subseteq \mathbb{R}$ is open if, for every $x \in U$ there is some $\epsilon>0$ such that $B(x, \epsilon) \subseteq U$.

3. Let $U_{1}:=\{x \in \mathbb{R} \mid 0<x<2\}$ and $U_{2}:=\{x \in \mathbb{R} \mid 1<x<3\}$. Then $U:=U_{1} \cup U_{2}=\{x \in \mathbb{R} \mid 0<$ $x<3\}$.
\footnotetext{
${ }^{2}$ If $y=(a, b)$ then there are exactly $(a+1) *(b+1)$ elements $y^{\prime}$ for which $y \leq y^{\prime}$.
}

4. Let $I=\{1,2,3,4, \ldots\}$ and for each $i \in I$ let $U_{i}:=\left\{x \in \mathbb{R} \left\lvert\, \frac{1}{i}<x<1\right.\right\}$, so we have $U_{1} \subseteq U_{2} \subseteq$ $U_{3} \subseteq \cdots$. Their union is $U:=\bigcup_{i \in I} U_{i}=\{x \in \mathbb{R} \mid 0<x<1\}$.

Solution to Exercise 7.29.

1. The coarse topology on $X$ is the one whose only open sets are $X \subseteq X$ and $\varnothing \subseteq X$. This is a topology because it contains the top and bottom subsets, it is closed under finite intersection (the intersection $A \cap B$ is $\varnothing$ iff one or the other is $\varnothing$ ), and it is closed under arbitrary union (the union $\cup_{i \in I} A_{i}$ is $X$ iff $A_{i}=X$ for some $i \in I$ ).

2. The fine topology on $X$ is the one where every subset $A \subseteq X$ is considered open. All the conditions on a topology say "if such-and-such then such-and-such is open," but these are all satisfied because everything is open!

3. If $(X, P(X))$ is discrete, $\left(Y, \mathbf{O} \mathbf{p}_{Y}\right)$ is any topological space, and $f: X \rightarrow Y$ is any function then it is continuous. Indeed, this just means that for any open set $U \subseteq Y$ the preimage $f^{-1}(U) \subseteq X$ is open, and everything in $X$ is open.

Solution to Exercise 7.31.

1. The Hasse diagram for the Sierpinsky topology is $\varnothing \rightarrow\{1\} \rightarrow\{1,2\}$.

2. A set $\left(U_{i}\right)_{i \in I}$ covers $U$ iff either
- $I=\varnothing$ and $U=\varnothing$; or
- $U_{i}=U$ for some $i \in I$.

In other words, the only way that some collection of these sets could cover another set $U$ is if that collection contains $U$ or if $U$ is empty and the collection is also empty.

\section*{Solution to Exercise 7.32.}

Let $(X, \mathbf{O p})$ be a topological space, suppose that $Y \subseteq X$ is a subset, and consider the subspace topology $\mathrm{Op}_{\text {? } \cap Y}$.

1. We want to show that $Y \in \mathbf{O} p_{\text {? } \cap Y}$. We need to find $B \in \mathbf{O p}$ such that $Y=B \cap Y$; this is easy, you could take $B=Y$ or $B=X$, or anything in between.

2. We still need to show that $\mathrm{Op}_{\text {? } Y Y}$ contains $\varnothing$ and is closed under finite intersection and arbitrary union. $\varnothing=\varnothing \cap Y$, so according to the formula, $\varnothing \in \mathbf{O p} \mathbf{p}_{\text {? } Y Y}$. Suppose that $A_{1}, A_{2} \in \mathbf{O} \mathbf{p}_{? \cap \gamma}$. Then there exist $B_{1}, B_{2} \in \mathbf{O} \mathbf{p}$ with $A_{1}=B_{1} \cap Y$ and $A_{2}=B_{2} \cap Y$. But then $A_{1} \cap A_{2}=\left(B_{1} \cap Y\right) \cap\left(B_{2} \cap Y\right)=$ $\left(B_{1} \cap B_{2}\right) \cap Y$, so it is in $\mathbf{O p}_{\text {? } \cap Y}$ since $B_{1} \cap B_{2} \in \mathbf{O p}$. The same idea works for arbitrary unions: given a set $I$ and $A_{i}$ for each $i \in I$, we have $A_{i}=B_{i} \cap Y$ for some $B_{i} \in \mathbf{O p}$, and

$$
\bigcup_{i \in I} A_{i}=\bigcup_{i \in I}\left(B_{i} \cap Y\right)=\left(\bigcup_{i \in i} B_{i}\right) \cap Y \in \mathbf{O} \mathbf{p}_{? \cap Y}
$$

\section*{Solution to Exercise 7.34.}

Let's imagine a $\mathcal{V}$-category $\mathcal{C}$, where $\mathcal{V}$ is the quantale corresponding to the open sets of a topological space $(X, \mathbf{O p})$. Its Hasse diagram would be a set of dots and some arrows between them, each labeled by an open set $U \subseteq \mathbf{O p}$. It might look something like this:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-328.jpg?height=382&width=418&top_left_y=2124&top_left_x=867)

Recall from Section 2.3 that the 'distance' between two points is computed by taking the join, over all paths between them, of the monoidal product of distances along that path. For example, $\mathcal{C}(B, C)=$ $\left(U_{3} \wedge U_{1}\right) \vee\left(U_{4} \wedge U_{2}\right)$, because $\wedge$ is the monoidal product in $\mathcal{V}$.

In general, we can thus imagine the open set $\mathcal{C}(a, b)$ as a kind of 'size restriction' for getting from $a$ to $b$, like bridges that your truck needs to pass under. The size restriction for getting from $a$ to itself is $X$ : no restriction. In general, to go on any given route (path) from $a$ to $b$, you have to fit under every bridge in the path, so we take their meet. But we can go along any path, so we take the join over all paths.

Solution to Exercise 7.38 .

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-329.jpg?height=420&width=699&top_left_y=744&top_left_x=732)

1. The fiber of $f$ over $a$ is $\left\{a_{1}, a_{2}\right\}$.

2. The fiber of $f$ over $c$ is $\left\{c_{1}\right\}$.

3. The fiber of $f$ over $d$ is $\varnothing$.

4. A function $f^{\prime}: X \rightarrow Y$ for which every fiber has either one or two elements is shown below.

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-329.jpg?height=414&width=705&top_left_y=1338&top_left_x=726)

Solution to Exercise 7.40.

Refer to Eq. (A.4).

1. Here is a drawing of all six sections over $V_{1}=\{a, b, c\}$ :
![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-329.jpg?height=378&width=1218&top_left_y=1984&top_left_x=494)

2. When $V_{2}=\{a, b, c, d\}$, there are no sections: $\operatorname{Sec}_{f}\left(V_{2}\right)=\varnothing$.

3. When $V_{3}=\{a, b, d, e\}$, the set $\left.\operatorname{Sec}_{f}\left(V_{3}\right)\right)$ has $2 * 3 * 1 * 2=12$ elements.

Solution to Exercise 7.42.

$\operatorname{Sec}_{f}(\{a, b, c\})$ and $\operatorname{Sec}_{f}(\{a, c\})$ are drawn as the top row (six-element set) and bottom row (two-element set) below, and the restriction map is also shown:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-330.jpg?height=159&width=1149&top_left_y=403&top_left_x=496)

Solution to Exercise 7.44.

1. Let $g_{1}:=\left(a_{1}, b_{1}\right)$ and $g_{2}:=\left(b_{2}, e_{1}\right)$; these do not agree on the overlap.

2. No, there's no section $g \in \operatorname{Sec}_{f}\left(U_{1} \cup U_{2}\right)$ for which $\left.g\right|_{U_{1}}=g_{1}$ and $\left.g\right|_{U_{2}}=g_{2}$.
![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-330.jpg?height=514&width=1132&top_left_y=781&top_left_x=512)

Solution to Exercise 7.47.

No, there is not a one-to-one correspondence between sheaves on $M$ and vector fields on $M$. The relationship between sheaves on $M$ and vector fields on $M$ is that the set of all vector fields on $M$ corresponds to one sheaf, namely $\operatorname{Sec}_{\pi}$, where $\pi: T M \rightarrow M$ is the tangent bundle as described in Example 7.46. There are so many sheaves on $M$ that they don't even form a set (it's just a 'collection'); again, one member of this gigantic collection is the sheaf $\mathrm{Sec}_{\pi}$ of all possible vector fields on $M$.

Solution to Exercise 7.49.

1. The Hasse diagram for the Sierpinsky topology is $\varnothing \rightarrow\{1\} \rightarrow\{1,2\}$.

2. A presheaf $F$ on Op consists of any three sets and any two functions $F(\{1,2\}) \rightarrow F(\{1\}) \rightarrow F(\varnothing)$ between them.

3. Recall from Exercise 7.31 that the only non-trivial covering (a covering of $U$ is non-trivial if it does not contain $U$ ) occurs when $U=\varnothing$ in which case the empty family over $U$ is a cover.

4. As explained in Example 7.36, $F$ will be a sheaf iff $F(\varnothing) \cong\{1\}$. Thus we the category of sheaves is equivalent to that of just two sets and one function $F(\{1,2\}) \rightarrow F(\{1\})$.

Solution to Exercise 7.52.

The one-point space $X=\{1\}$ has two open sets, $\varnothing$ and $\{1\}$, and every sheaf $S \in \operatorname{Shv}(X)$ assigns $S(\varnothing)=\{()\}$ by the sheaf condition (see Example 7.36). So the only data in a sheaf $S \in \operatorname{Shv}(X)$ is the set $S(\{1\})$. This is how we get the correspondence between sets and sheaves on the one point space.

According to Eq. (7.50), the subobject classifier $\Omega: \mathbf{O p}(X)^{\mathrm{op}} \rightarrow$ Set in $\operatorname{Shv}(X)$ should be the functor where $\Omega(\{1\})$ is the set of open sets of $\{1\}$. So we're hoping to see that there is a one-to-one correspondence between the set $\mathbf{O p}(\{1\})$ and the set $\mathbb{B}=\{$ true, false $\}$ of booleans. Indeed there is: there are two open sets of $\{1\}$, as we said, $\varnothing$ and $\{1\}$, and these correspond to false and true respectively.

Solution to Exercise 7.53.

By Eqs. (7.50) and (7.51) the definition of $\Omega(U)$ is $\Omega(U):=\left\{U^{\prime} \in \mathbf{O p} \mid U^{\prime} \subseteq U\right\}$, and the definition of the restriction map for $V \subseteq U$ is $U^{\prime} \mapsto U^{\prime} \cap V$.

1. It is functorial: given $W \subseteq V \subseteq U$ and $U^{\prime} \subseteq U$, we indeed have $\left(U^{\prime} \cap V\right) \cap W=U^{\prime} \cap W$, since $W \subseteq V$. For functoriality, we also need preservation of identities, and this amounts to $U^{\prime} \cap U=U^{\prime}$ for all $U^{\prime} \subseteq U$.

2. Yes, a presheaf is just a functor; the above check is enough.

\section*{Solution to Exercise 7.55.}

We need a graph homomorphism of the following form:

![](https://cdn.mathpix.com/cropped/2024_06_10_9da5663bd726f3cff245g-331.jpg?height=239&width=1119&top_left_y=569&top_left_x=514)

There is only one that classifies $G^{\prime}$, and here it is. Let's write $\gamma:=\left\ulcorner G^{\prime}\right\urcorner$.
- Since $D$ is missing from $G^{\prime}$, we have $\gamma(D)=0$ (vertex: missing).
- Since vertices $A, B, C$ are present in $G^{\prime}$ we have $\gamma(A)=\gamma(B)=\gamma(C)=V$ (vertex: present).
- The above forces $\gamma(i)=(V, 0 ; 0)$ (arrow from present vertex to missing vertex: missing).
- Since the arrow $f$ is in $G^{\prime}$, we have $\gamma(f)=(V, V ; A)$ (arrow from present vertex to present vertex: present).
- Since the arrows $g$ and $h$ are missing in $G^{\prime}$, we have $\gamma(g)=\gamma(h)=(V, V ; 0)$ (arrow from present vertex to present vertex: missing).

Solution to Exercise 7.59.

With $U=\mathbb{R}-\{0\} \subseteq \mathbb{R}$, we have:

1. The complement of $U$ is $\mathbb{R}-U=\{0\}$ and $\neg U$ is its interior, which is $\neg U=\varnothing$.

2. The complement of $\neg U$ is $\mathbb{R}-\varnothing=\mathbb{R}$, and this is open, so $\neg \neg U=\mathbb{R}$.

3. It is true that $U \subseteq \neg \neg U$.

4. It is false that $\neg \neg U \subseteq$ ? $U$.

Solution to Exercise 7.60.

1. If for any $V \in O p$ we have $\top \wedge V=V$ then when $V=X$ we have $\top \wedge X:=\top \cap X=X$, but anything intersected with $X$ is itself, so $T=\top \cap X=X$.
2. $(\top \vee V):=(X \cup V)=X$ holds and $(V \Rightarrow X)=\bigcup_{\{R \in \mathbf{O p} \mid R \cap V \subseteq X\}} R=X$ holds because $(X \cap V) \subseteq X$.

3. If for any set $V \in \mathbf{O p}$ we have $(\perp \vee V)=V$, then when $V=\emptyset$ we have $(\perp \vee \varnothing)=(\perp \cup \varnothing)=\varnothing$, but anything unioned with $\varnothing$ is itself, so $\perp=\perp \cup \varnothing=\varnothing$.
4. $(\perp \wedge V)=(\varnothing \cap V)=\varnothing$ holds, and $(\perp \Rightarrow V)=\bigcup_{\{R \in \mathbf{O p} \mid R \cap \varnothing \subseteq V\}} R=X$ holds because $(X \cap \varnothing) \subseteq V$.

Solution to Exercise 7.62.

$S$ is the sheaf of people, the set of which changes over time: a section in $S$ over any interval of time is a person who is alive throughout that interval. A section in the subobject $\{S \mid p\}$ over any interval of time is a person who is alive and likes the weather throughout that interval of time.

\section*{Solution to Exercise 7.64.}

We need an example of a space $X$, a sheaf $S \in \operatorname{Shv}(X)$, and two predicates $p, q: S \rightarrow \Omega$ for which $p(s) \vdash_{s: S} q(s)$ holds. Take $X$ to be the one-point space, take $S$ to be the sheaf corresponding to the set $S=\mathbb{N}$, let $p(s)$ be the predicate " $24 \leq s \leq 28$," and let $q(s)$ be the predicate " $s$ is not prime." Then $p(s) \vdash_{s: S} q(s)$ holds.

As an informal example, take $X$ to be the surface of the earth, take $S$ to be the sheaf of vector fields as in Example 7.46 thought of in terms of wind-blowing. Let $p$ be the predicate "the wind is blowing due east at somewhere between 2 and 5 kilometers per hour" and let $q$ be the predicate "the wind is blowing at somewhere between 1 and 5 kilometers per hour." Then $p(s) r_{s: S} q(s)$ holds. This means
that for any open set $U$, if the wind is blowing due east at somewhere between 2 and 5 kilometers per hour throughout $U$, then the wind is blowing at somewhere between 1 and 5 kilometers per hour throughout $U$ as well.

Solution to Exercise 7.66.

We have the predicate $p: \mathbb{N} \times \mathbb{Z} \rightarrow \mathbb{B}$ given by $p(n, z)$ iff $n \leq|z|$.

1. The predicate $\forall(z: \mathbb{Z}) \cdot p(n, z)$ holds for $\{0\} \subseteq \mathbb{N}$.

2. The predicate $\exists(z: \mathbb{Z})$. $p(n, z)$ holds for $\mathbb{N} \subseteq \mathbb{N}$.

3. The predicate $\forall(n: \mathbb{N})$. $p(n, z)$ holds for $\varnothing \subseteq \mathbb{Z}$.

4. The predicate $\exists(n: \mathbb{N})$. $p(n, z)$ holds for $\mathbb{Z} \subseteq \mathbb{Z}$.

Solution to Exercise 7.67.

Suppose $s$ is a person alive throughout the interval $U$. Apply the above definition to the example $p(s, t)=$ "person $s$ is worried about news $t$ " from above.

1. The formula says that $\forall(t: T)$. $p(s, t)$ "returns the largest open set $V \subseteq U$ for which $p\left(\left.s\right|_{V}, t\right)=V$ for all $t \in T(V) . "$ Note that $T(V)$ is the set of items that are in the news throughout the interval $V$. Substituting, this becomes "the largest interval of time $V \subseteq U$ over which person $s$ is worried about news $t$ for every item $t$ that is in the news throughout $V$." In other words, for $V$ to be nonempty, the person $s$ would have to be worried about every single item of news throughout $V$. My guess is that there's a festival happening or a happy kitten somewhere that person $s$ is not worried about, but maybe I'm assuming that person $s$ is sufficiently mentally "normal." There may be people who are sometimes worried about literally everything in the news; we ask you to please be kind to them.

2. Yes, it is exactly the same description.

Solution to Exercise 7.68.

Suppose $s$ is a person alive throughout the interval $U$. Apply the above definition to the example $p(s, t)=$ "person $s$ is worried about news $t$ " from above.

1. The formula says that $\exists(t: T)$. $p(s, t)$ "returns the union $V=U_{i} V_{i}$ of all the open sets $V_{i}$ for which there exists some $t_{i} \in T\left(V_{i}\right)$ satisfying $p\left(\left.s\right|_{V_{i}}, t_{i}\right)=V_{i}$." Substituting, this becomes "the union of all time intervals $V_{i}$ for which there is some item $t_{i}$ in the news about which $s$ is worried throughout $V_{i}$." In other words it is all the time that $s$ is worried about at least one thing in the news. Perhaps when $s$ is sleeping or concentrating on something, she is not worried about anything, in which case intervals of sleeping or concentrating would not be subsets of $V$. But if $s$ said "there's been such a string of bad news this past year, it's like I'm always worried about something!," she is saying that it's like $V=$ "this past year."

2. This seems like a good thing for "there exists a piece of news that worries $s$ " to mean: the news itself is allowed to change as long as the person's worry remains. Someone might disagree and think that the predicate should mean "there is one piece of news that worries $s$ throughout the whole interval $V$." In that case, perhaps this person is working within a different topos, e.g. one where the site has fewer coverings. Indeed, it is the notion of covering that makes existential quantification work the way it does.

Solution to Exercise 7.70.

It is clear that if $j(j(q))=j(q)$ then $j(j(q)) \leq j(q)$ by reflexivity. On the other hand, assume the hypothesis, that $p \leq j(p)$ for all $U \subseteq X$ and $p \in \Omega(U)$. If $j(j(q)) \leq j(q)$, then letting $p:=j(q)$ we have both $j(p) \leq p$ and $p \leq j(p)$. This means $p \cong j(p)$, but $\Omega$ is a poset (not just a preorder) so $p=j(p)$, i.e. $j(j(q))=j(q)$ as desired.

Solution to Exercise 7.72.

Let $S$ be the sheaf of people and $j$ be "assuming Bob is in San Diego..."

1. Take $p(s)$ to be " $s$ likes the weather."

2. Let $U$ be the interval 2019/01/01-2019/02/01. For an arbitrary person $s \in S(U), p(s)$ is a subset of $U$, and it means the subset of $U$ throughout which $s$ likes the weather.

3. Similarly $j(p(s))$ is a subset of $U$, and it means the subset of $U$ throughout which, assuming Bob is in San Diego, $s$ liked the weather. In other words, $j(p(s))$ is true whenever Bob is not in San Diego, and it is true whenever $s$ likes the weather.

4. It is true that $p(s) \leq j(p(s))$, by the 'in other words' above.

5. It is true that $j(j(p(s))=j(p(s)$, because suppose given a time during which "if Bob is in San Diego then if Bob is in San Diego then $s$ likes the weather." Then if Bob is in San Diego during this time then $s$ likes the weather. But that is exactly what $j(p(s))$ means.

6. Take $q(s)$ to be " $s$ is happy." Suppose "if Bob is in San Diego then both $s$ likes the weather and $s$ is happy." Then both "if Bob is in San Diego then $s$ likes the weather" and "if Bob is in San Diego then $s$ is happy" are true too. The converse is equally clear.

Solution to Exercise 7.76.

We have $o_{[a, b]}:=\{[d, u] \in \mathbb{I} \mid a<d \leq u<b\}$.

1. Since $0 \leq 2 \leq 6 \leq 8$, we have $[2,6] \in o_{[0,8]}$ by the above formula.

2. In order to have $[2,6] \epsilon^{?} o_{[0,5]} \cup o_{[4,8]}$, we would need to have either $[2,6] \epsilon^{?} o_{[0,5]}$ or $[2,6] \epsilon^{?} o_{[4,8]}$. But the formula does not hold in either case.

Solution to Exercise 7.77.

A subset $U \subseteq \mathbb{R}$ is open in the subspace topology of $\mathbb{R} \subseteq \mathbb{R}$ iff there is an open set $U^{\prime} \subseteq \mathbb{R} \mathbb{R}$ with $U=U^{\prime} \cap \mathbb{R}$. We want to show that this is the case iff $U$ is open in the usual topology.

Suppose that $U$ is open in the subspace topology. Then $U=U^{\prime} \cap \mathbb{R}$, where $U^{\prime} \subseteq \mathbb{R} \mathbb{R}$ is the union of some basic opens, $U^{\prime}=\bigcup_{i \in I} o_{\left[a_{i}, b_{i}\right]}$, where $o_{\left[a_{i}, b_{i}\right]}=\left\{[d, u] \in \mathbb{R} \mid a_{i}<d<u<b_{i}\right\}$. Since $\mathbb{R}=\{[x, x] \in \mathbb{R}\}$, the intersection $U^{\prime} \cap \mathbb{R}$ will then be

$$
U=\bigcup_{i \in I}\left\{x \in \mathbb{R} \mid a_{i}<x<b_{i}\right\}
$$

and this is just the union of open balls $B\left(m_{i}, r_{i}\right)$ where $m_{i}:=\frac{a_{i}+b_{i}}{2}$ is the midpoint and $r_{i}:=\frac{b_{i}-a_{i}}{2}$ is the radius of the interval $\left(a_{i}, b_{i}\right)$. The open balls $B\left(m_{i}, r_{i}\right)$ are open in the usual topology on $\mathbb{R}$ and the union of opens is open, so $U$ is open in the usual topology.

Suppose that $U$ is open in the usual topology. Then $U=\bigcup_{j \in J} B\left(m_{j}, \epsilon_{j}\right)$ for some set $J$. Let $a_{j}:=m_{j}-\epsilon_{j}$ and $b_{j}:=m_{j}+\epsilon_{j}$. Then

$$
U=\bigcup_{j \in J}\left\{x \in \mathbb{R} \mid a_{j}<x<b_{j}\right\}=\bigcup_{j \in J}\left(o_{\left[a_{j}, b_{j}\right]} \cap \mathbb{R}\right)=\left(\bigcup_{j \in J} o_{\left[a_{j}, b_{j}\right]}\right) \cap \mathbb{R}
$$

which is open in the subspace topology.

\section*{Solution to Exercise 7.80.}

Fix any topological space $\left(X, \mathbf{O} \mathbf{p}_{X}\right)$ and any subset $R \subseteq \mathbb{R}$ of the interval domain. Define $H_{X}(U):=$ $\{f: U \cap R \rightarrow X \mid f$ is continuous $\}$.
1. $H_{X}$ is a presheaf: given $V \subseteq U$ the restriction map sends the continuous function $f: U \cap R \rightarrow X$ to its restriction along the subset $V \cap R \subseteq U \cap R$.

2. It is a sheaf: given any family $U_{i}$ of open sets with $U=U_{i} U_{i}$ and a continuous function $f_{i}: U_{i} \cap R \rightarrow X$ for each $i$, agreeing on overlaps, they can be glued together to give a continuous function on all of $U \cap R$, since $U \cap R=\left(\bigcup_{i} U_{i}\right) \cap R=\bigcup_{i}\left(U_{i} \cap R\right)$.

\section*{Bibliography}

[Ada17] Elie M. Adam. "Systems, Generativity and Interactional Effects". Available online: http://www.mit.edu/ eadam/eadam_PhDThesis.pdf. PhD thesis. Massachusetts Institute of Technology, July 2017 (cit. on pp. 2, 26, 36).

[AGV71] Michael Artin, Alexander Grothendieck, and Jean-Louis Verdier. Theorie de Topos et Cohomologie Etale des Schemas I, II, III. Vol. 269, 270, 305. Lecture Notes in Mathematics. Springer, 1971 (cit. on p. 256).

[AJ94] Samson Abramsky and Achim Jung. "Domain theory". In: Handbook of logic in computer science. Oxford University Press. 1994 (cit. on p. 257).

[AS05] Aaron D. Ames and Shankar Sastry. "Characterization of Zeno behavior in hybrid systems using homological methods". In: American Control Conference, 2005. Proceedings of the 2005. IEEE. 2005, pp. 1160-1165 (cit. on p. 257).

[AV93] Samson Abramsky and Steven Vickers. "Quantales, observational logic and process semantics". In: Mathematical Structures in Computer Science 3.2 (1993), pp. 161-227 (cit. on p. 76).

[Awo10] Steve Awodey. Category theory. Second. Vol. 52. Oxford Logic Guides. Oxford University Press, Oxford, 2010, pp. xvi+311 (cit. on p. 114).

[BD98] John C Baez and James Dolan. Categorification. 1998. eprint: math/9802029 (cit. on p. 145).

[BE15] John C. Baez and Jason Erbele. "Categories in control". In: Theory and Applications of Categories 30 (2015), Paper No. 24, 836-881 (cit. on pp. 170, 179).

[BF15] John C. Baez and Brendan Fong. "A compositional framework for passive linear networks". In: (2015). urL: https: / /arxiv .org / abs / 1504 . 05625 (cit. on p. 219).

[BFP16] John C. Baez, Brendan Fong, and Blake S Pollard. “A compositional framework for Markov processes". In: Journal of Mathematical Physics 57.3 (2016) (cit. on p. 219).

[BH08] Philip A Bernstein and Laura M Haas. "Information integration in the enterprise". In: Communications of the ACM 51.9 (2008), pp. 72-79 (cit. on p. 77).

[Bor94] Francis Borceux. Handbook of categorical algebra 1. Vol. 50. Encyclopedia of Mathematics and its Applications. Basic category theory. Cambridge University Press, Cambridge, 1994 (cit. on pp. 114, 192).

[BP17] John C Baez and Blake S Pollard. "A compositional framework for reaction networks". In: Reviews in Mathematical Physics 29.09 (2017) (cit. on p. 219).

[Bro61] Ronnie Brown. "Some problems of algebraic topology: a study of function spaces, function complexes, and FD-complexes". PhD thesis. University of Oxford, 1961 (cit. on p. 76).

[BS17] Filippo Bonchi and Fabio Sobociński Pawełand Zanasi. "The calculus of signal flow diagrams I: Linear relations on streams". In: Information and Computation 252 (2017), pp. 2-29 (cit. on pp. 170, 179).

[BSZ14] Filippo Bonchi, Paweł Sobociński, and Fabio Zanasi. "A categorical semantics of signal flow graphs". In: International Conference on Concurrency Theory. 2014, pp. 435-450 (cit. on p. 179).

[BSZ15] Filippo Bonchi, Pawel Sobocinski, and Fabio Zanasi. "Full abstraction for signal flow graphs". In: ACM SIGPLAN Notices. Vol. 50. 1. ACM. 2015, pp. 515-526 (cit. on p. 179).

[BW90] Michael Barr and Charles Wells. Category theory for computing science. Vol. 49. Prentice Hall New York, 1990 (cit. on p. 114).

[Car91] Aurelio Carboni. "Matrices, relations, and group representations". In: Journal of Algebra 136.2 (1991), pp. 497-529. URL: http: //www. sciencedirect. com/science/article/pii/002186939190057F (cit. on p. 219).

[CD95] Boris Cadish and Zinovy Diskin. "Algebraic graph-based approach to management of multibase systems, I: Schema integration via sketches and equations". In: proceedings of Next Generation of Information Technologies and Systems, NGITS. Vol. 95.1995 (cit. on p. 114).

[Cen15] Andrea Censi. A mathematical theory of co-design. 2015. eprint: arXiv:1512. 08055 (cit. on pp. 117,145).

[Cen17] Andrea Censi. "Uncertainty in Monotone Co-Design Problems". In: IEEE Robotics and Automation Letters (Feb. 2017). URL: https://arxiv.org/abs/ 1609.03103 (cit. on p. 145).

[CFS16] Bob Coecke, Tobias Fritz, and Robert W. Spekkens. "A mathematical theory of resources". In: Information and Computation 250 (2016), pp. 59-86 (cit. on pp. 48, 75).

[CK17] Bob Coecke and Aleks Kissinger. Picturing quantum processes. Cambridge University Press, 2017 (cit. on p. iv).

[CP10] Bob Coecke and Eric Oliver Paquette. "Categories for the practising physicist". In: New Structures for Physics. Springer, 2010, pp. 173-286 (cit. on p. iv).

[CW87] A. Carboni and R.F.C. Walters. "Cartesian bicategories I". In: Journal of Pure and Applied Algebra 49.1 (1987),pp.11-32. urL: http://www. sciencedirect. com/science/article/pii/0022404987901216 (cit. on p. 219).

[CY96] Louis Crane and David N Yetter. Examples of categorification. 1996. eprint: q-alg/9607028 (cit. on p. 145).

[FGR03] Michael Fleming, Ryan Gunther, and Robert Rosebrugh. "A database of categories". In: Journal of Symbolic Computation 35 (2003), Paper No. 2, 127135 (cit. on p. 114).

[Fon15] Brendan Fong. "Decorated cospans". In: Theory and Applications of Categories 30.33 (2015), pp. 1096-1120 (cit. on p. 219).

[Fon16] Brendan Fong. "The Algebra of Open and Interconnected Systems". PhD thesis. University of Oxford, 2016 (cit. on p. 219).

[Fon18] Brendan Fong. "Decorated corelations". In: Theory and Applications of Categories 33.22 (2018), pp. 608-643 (cit. on p. 219).

[Fra67] John B. Fraleigh. A first course in abstract algebra. Addison-Wesley Publishing Co., Reading, Mass.-London-Don Mills, Ont., 1967, pp. xvi+447 (cit. on p. 179).

[Fri17] Tobias Fritz. "Resource convertibility and ordered commutative monoids". In: Mathematical Structures in Computer Science 27.6 (2017), pp. 850-938 (cit. on pp. 48,75).

[FS18a] Brendan Fong and Maru Sarazola. A recipe for black box functors. 2018 (cit. on p. 219).

[FS18b] Brendan Fong and David I Spivak. Hypergraph Categories. 2018. eprint: arXiv: 1806.08304 (cit. on p. 219).

[FSR16] Brendan Fong, Paweł Sobociński, and Paolo Rapisarda. "A categorical approach to open and interconnected dynamical systems". In: Proceedings of the 31st Annual ACM/IEEE Symposium on Logic in Computer Science. ACM. 2016, pp. 495-504 (cit. on pp. 168, 179).

[Gie+03] G. Gierz, K. H. Hofmann, K. Keimel, J. D. Lawson, M. Mislove, and D. S. Scott. Continuous lattices and domains. Vol. 93. Encyclopedia of Mathematics and its Applications. Cambridge University Press, Cambridge, 2003, pp. xxxvi+591 (cit. on p. 257).

[Gla13] K Glazek. A Guide to the Literature on Semirings and their Applications in Mathematics and Information Sciences: With Complete Bibliography. Springer Science \& Business Media, 2013 (cit. on p. 179).

[Gra18] Marco Grandis. Category Theory and Applications. World Scientific, 2018 (cit. on p. 114).

[HMP98] Claudio Hermida, Michael Makkai, and John Power. "Higher-dimensional multigraphs". In: Thirteenth Annual IEEE Symposium on Logic in Computer Science (Indianapolis, IN, 1998). IEEE Computer Soc., Los Alamitos, CA, 1998, pp. 199-206 (cit. on p. 214).

[HTP03] Esfandiar Haghverdi, Paulo Tabuada, and George Pappas. "Bisimulation relations for dynamical and control systems". In: Electronic Notes in Theoretical Computer Science 69 (2003), pp. 120-136 (cit. on p. 257).

[IP94] Amitavo Islam and Wesley Phoa. "Categorical models of relational databases I: Fibrational formulation, schema integration". In: International Symposium on Theoretical Aspects of Computer Software. Springer. 1994, pp. 618-641 (cit. on $\mathrm{p} .114)$.

[Jac99] Bart Jacobs. Categorical logic and type theory. Vol. 141. Studies in Logic and the Foundations of Mathematics. North-Holland Publishing Co., Amsterdam, 1999, pp. xviii+760 (cit. on p. 257).

[JNW96] André Joyal, Mogens Nielsen, and Glynn Winskel. “Bisimulation from open maps". In: Information and Computation 127.2 (1996), pp. 164-185 (cit. on p. 257).

[Joh02] Peter T. Johnstone. Sketches of an elephant: a topos theory compendium. Vol. 43. Oxford Logic Guides. New York: The Clarendon Press Oxford University Press, 2002, pp. xxii+468+71 (cit. on p. 257).

[Joh77] P. T. Johnstone. Topos theory. London Mathematical Society Monographs, Vol. 10. Academic Press [Harcourt Brace Jovanovich, Publishers], LondonNew York, 1977, pp. xxiii+367 (cit. on p. 223).

[JR02] Michael Johnson and Robert Rosebrugh. "Sketch Data Models, Relational Schema and Data Specifications". In: Electronic Notes in Theoretical Computer Science 61 (2002). CATS'02, Computing, pp. 51-63 (cit. on p. 114).

[JS93] André Joyal and Ross Street. "Braided tensor categories". In: Advances in Mathematics 102.1 (1993), pp. 20-78 (cit. on pp. 40, 145).

[JSV96] André Joyal,Ross Street, and Dominic Verity. "Traced monoidal categories". In: Mathematical Proceedings of the Cambridge Philosophical Society 119 (1996), Paper No. 3, 447-468 (cit. on p. 145).

[Kel05] G. M. Kelly. "Basic concepts of enriched category theory". In: Reprints in Theory and Applications of Categories (2005), Paper No. 10. URL: http : //www.tac.mta.ca/tac/reprints/articles/10/tr10abs.html (cit. on pp. 76, 139).

[Law04] F William Lawvere. "Functorial Semantics of Algebraic Theories and Some Algebraic Problems in the context of Functorial Semantics of Algebraic Theories". In: Reprints in Theory and Applications of Categories 5 (2004), pp. 1121 (cit. on p. 179).

[Law73] F William Lawvere. "Metric spaces, generalized logic, and closed categories". In: Rendiconti del seminario matématico e fisico di Milano 43.1 (1973), pp. 135-166 (cit. on pp. 76, 145).

[Law86] Bill Lawvere. "State categories and response functors". 1986 (cit. on p. 257).

[Lei04] Tom Leinster. Higher operads, higher categories. London Mathematical Society Lecture Note Series 298. Cambridge University Press, Cambridge, 2004 (cit. on pp. 213, 218, 219).

[Lei14] Tom Leinster. Basic category theory. Vol. 143. Cambridge University Press, 2014 (cit. on p. 114).

[LS88] J. Lambek and P. J. Scott. Introduction to higher order categorical logic. Vol. 7. Cambridge Studies in Advanced Mathematics. Reprint of the 1986 original. Cambridge University Press, Cambridge, 1988, pp. x+293 (cit. on p. 257).

[Mac98] Saunders Mac Lane. Categories for the working mathematician. 2nd ed. Graduate Texts in Mathematics 5. New York: Springer-Verlag, 1998 (cit. on pp. 114, $115,145)$.

[May72] J Peter May. The geometry of iterated loop spaces, volume 271 of Lecture Notes in Mathematics. 1972 (cit. on p. 219).

[McL90] Colin McLarty. "The uses and abuses of the history of topos theory". In: The British Journal for the Philosophy of Science 41.3 (1990), pp.351-375 (cit. on p. 256).

[McL92] Colin McLarty. Elementary categories, elementary toposes. Clarendon Press, 1992 (cit. on p. 256).

[MM92] Saunders MacLane and Ieke Moerdijk. Sheaves in Geometry and Logic: A First Introduction to Topos Theory. Springer, 1992 (cit. on p. 257).

[nLa18] Contributors To nLab. Symmetric monoidal category. 2018. URL: https : / ncatlab.org/nlab/revision/symmetric+monoidal+category/30 (cit. on p. 139).

[NNH99] Flemming Nielson, Hanne R. Nielson, and Chris Hankin. Principles of Program Analysis. Secaucus, NJ, USA: Springer-Verlag New York, Inc., 1999 (cit. on p.37).

[Pie91] Benjamin C. Pierce. Basic Category Theory for Computer Scientists. MIT Press, 1991 (cit. on p. 114).

[PS95] Frank Piessens and Eric Steegmans. "Categorical data specifications". In: Theory and Applications of Categories 1.8 (1995), pp. 156-173 (cit. on p. 114).

[Rie17] Emily Riehl. Category theory in context. Courier Dover Publications, 2017 (cit. on p. 114).

[Ros90] Kimmo I Rosenthal. Quantales and their applications. Vol. 234. Longman Scientific and Technical, 1990 (cit. on p. 76).

[RS13] Dylan Rupel and David I. Spivak. The operad of temporal wiring diagrams: formalizing a graphical language for discrete-time processes. 2013. eprint: arXiv: 1307.6894 (cit. on p. 219).

[RW92] Robert Rosebrugh and R. J. Wood. "Relational Databases and Indexed Categories". In: Canadian Mathematical Society Conference Procedings. International Summer Category Theory Meeting. (June 23-30, 1991). Ed. by R. A. G. Seely. Vol. 13. American Mathematical Society, 1992, pp. 391-407 (cit. on p. 114).

[S+15] Eswaran Subrahmanian, Christopher Lee, Helen Granger, et al. “Managing and supporting product life cycle through engineering change management for a complex product". In: Research in Engineering Design 26.3 (2015), pp. 189-217 (cit. on p. 118).

[Sch+17] Patrick Schultz, David I. Spivak, Christina Vasilakopoulou, and Ryan Wisnesky. "Algebraic Databases". In: Theory and Applications of Categories 32 (2017), Paper No. 16, 547-619 (cit. on p. 114).

[Sel10] Peter Selinger. "A survey of graphical languages for monoidal categories". In: New structures for physics. Springer, 2010, pp. 289-355 (cit. on p. 146).

[Shu08] Michael Shulman. "Framed bicategories and monoidal fibrations". In: Theory and Applications of Categories 20 (2008), Paper No. 18, 650-738 (cit. on p. 145).

[Shu10] Michael Shulman. Constructing symmetric monoidal bicategories. 2010. eprint: arXiv: 1004.0993 (cit. on p. 145).

[Sob] Graphical Linear Algebra. URL: https://graphicallinearalgebra.net/ (visited on 03/11/2018) (cit. on p. 179).

[Spi+16] David I. Spivak, Magdalen R. C. Dobson, Sapna Kumari, and Lawrence Wu. Pixel Arrays: A fast and elementary method for solving nonlinear systems. 2016. eprint: arXiv: 1609.00061 (cit. on p. 219).

[Spi12] David I. Spivak. "Functorial data migration". In: Information and Computation 217 (2012), pp. 31-51 (cit. on p. 114).

[Spi13] David I. Spivak. The operad of wiring diagrams: formalizing a graphical language for databases, recursion, and plug-and-play circuits. 2013. eprint: arXiv: 1305 . 0297 (cit. on p. 219).

[Spi14a] David I Spivak. Category theory for the sciences. MIT Press, 2014 (cit. on pp. 105, 114).

[Spi14b] David I. Spivak. "Database queries and constraints via lifting problems". In: Mathematical Structures in Computer Science 24.6 (2014), e240602, 55. urL: http://dx.doi.org/10.1017/S0960129513000479 (cit. on p. 94).

[SS18] Patrick Schultz and David I. Spivak. Temporal Type Theory: A topos-theoretic approach to systems and behavior. Springer, Berkhäuser, To appear, 2018 (cit. on p. 256).

[SSV18] Alberto Speranzon, David I. Spivak, and Srivatsan Varadarajan. Abstraction, Composition and Contracts: A Sheaf Theoretic Approach. 2018. eprint: arXiv : 1802.03080 (cit. on p. 255).

[SVS16] David I. Spivak, Christina Vasilakopoulou, and Patrick Schultz. Dynamical Systems and Sheaves. 2016. eprint: arXiv:1609.08086.

[SW15a] Patrick Schultz and Ryan Wisnesky. Algebraic Data Integration. 2015. eprint: arXiv: 1503.03571 (cit. on p. 114).

[SW15b] David I. Spivak and Ryan Wisnesky. "Relational Foundations for Functorial Data Migration". In: Proceedings of the 15th Symposium on Database Programming Languages. DBPL. Pittsburgh, PA: ACM, 2015, pp.21-28 (cit. on p. 114).

[TG96] Chris Tuijn and Marc Gyssens. "CGOOD, a categorical graph-oriented object data model". In: Theoretical Computer Science 160.1-2 (1996), pp. 217-239 (cit. on p. 114).

[Vig03] Sebastiano Vigna. A Guided Tour in the Topos of Graphs. 2003. eprint: arXiv: math/0306394 (cit. on p. 245).

[VSL15] Dmitry Vagner, David I. Spivak, and Eugene Lerman. "Algebras of open dynamical systems on the operad of wiring diagrams". In: Theory and Applications of Categories 30 (2015), Paper No. 51, 1793-1822 (cit. on pp. 211, 219).

[Wal92] R. F. C. Walters. Categories and Computer Science. Cambridge University Press, 1992 (cit. on p. 114).

[Wik18] Contributors To Wikipedia. Symmetric monoidal category - Wikipedia, The Free Encyclopedia. [Online; accessed 22-February-2018]. 2018. urL: https : / / en . wikipedia . org / wiki / Symmetric_monoidal _category (cit. on p. 139).

[Wil07] Jan C Willems. "The behavioral approach to open and interconnected systems". In: IEEE Control Systems 27.6 (2007), pp. 46-99 (cit. on p. 179).

[Wis+15] Ryan Wisnesky, David I. Spivak, Patrick Schultz, and Eswaran Subrahmanian. Functorial Data Migration: From Theory to Practice. Report G2015-1701. National Institute of Standards and Technology, 2015. arXiv: 1502.05947v2 (cit. on p. 114).

[Zan15] Fabio Zanasi. "Interacting Hopf Algebras- the Theory of Linear Systems". Theses. Ecole normale supérieure de lyon - ENS LYON, Oct. 2015. urL: https://tel.archives-ouvertes.fr/tel-01218015 (cit. on p. 179).

\section*{Index}

adjoint, see adjunction

adjoint functor theorem, 32,73

adjunction, 35, 69, 99-107

examples of, $27,28,35,86,102,104$

from closure operator, 34

Galois connection as, 27

of preorders, 26-36

preservation of meets and joins, 31

relationship to companions and conjoints, 131

algebraic theory, 172

AND operation, 230, 245

applied category theory, 1-257

associativity, $82,83,125,135$

as coherence condition, 115

in enriched categories, 139

in product category, 110

in wiring diagrams, 46

of composition in an operad, 213

of function composition, 87

of monoid operation, wiring diagram for, 198

of monoidal product, 42

of morphism composition, 81

of profunctor composition, 129

of quantale matrix multiplication, 74

property vs. structure, 136

weak, 130

behavior, 221-225 contract, 255

properties of, 224

topos for, 252

behavioral approach, 174

Beyoncé, 51

bicategory, 130

binary relation, see relation, binary, 150

booleans, 4

alternative monoidal structure, 53

and feasibility, 119

as base of enrichment for preorders, 58,236

as monoidal closed, 70,231

as preorder, 13,22

as quantale, 72

as rig, 160

as set, 6,7

as subobject classifier, 229, 244

meets and joins in, 25

usual monoidal structure, 52

cardinality, 19

and isomorphisms, 88

categorification, 129, 132-134

category, 81-89

as database schema, 89

codiscrete, 104

compact closed, see compact closed category

composition in, 81

discrete, 104
examples of, 87

finitely presented, see presentation of

free, $82,104,154$

having finite colimits, 192, 206

hypergraph structure on, 201

identity in, 81

indexing, 95,111

monoidal structure on, 136

morphism in, 81

object in, 81

of algebras for an operad, 218

of bijections, 150

of categories, 93

of cocones, 191

of cones, 110

of cospans, 195

of database instances, 232

of finite sets, $87,150,205$

of functors, 96

of graphs, 97

of instances on a database schema, 97

of operads, 218

of preorders, 21

of presheaves, 233

of sets, 87,224

of sheaves, 223, 237

opposite of, 88

preorder reflection of, 86

presentation of, $84,93,158$

category of categories, see category, of categories

category theory

as central hub of mathematics, iii

books on, iv

ceiling function, 27

change of base, see enrichment, change of base

chemistry, 39, 70, 219

catalysis, 49 monoidal preorder of, 48

closed category

cartesian, $76,225,227$

compact, see compact closed category

compact implies monoidal, 142

hom object in, see hom object

monoidal, see monoidal closed category, 76

closure operator, 33-35, 250

co-design, see also feasibility, 117-146

diagram, 118, 124

problem, 119

cocone, 114

codomain, see morphism, codomain

coequalizer, 194

coherence, v, 114, 204

as bookkeeping, 136

conditions, 136

Mac Lane's theorem, 137

colimit, 113, 182-194, 224, 225, 231

and interconnection, 194

coequalizer as, 194

coproduct as, 186

finite, 191-194

formula for finite colimits in Set, 193

initial object as, 184

presheaves form colimit completion, 233

pushout as, 188

collage, 122, 131-132

collection, 81

commutative diagram, 95

commutative square, $84,91,95$

comonoid, 198

compact closed category, 139-145, 150, 178

duals in, 141

hypergraph category as, 203

companion profunctor, 130
comparable, 15

completeness

of proof system, 170

composition

in a category, see category, composition in

in enriched categories, see enriched categories, composition in

compositionality, 1, 222, 257

cone, 111

conjoint profunctor, 130

connected, 10

connectedness, 3

connection

as colimit, 183

context free grammar, 214

continuous function, 234

control theory, 149, 178

cooking, iv

coproduct, 186-188

corelation, 142, 150

corelations

hypergraph category of, see hypergraph category, of corelations

cospan, 194-197

apex of, 194

decorated, 206

foot of, 194

cospans

as theory of Frobenius monoids, 200

category of, 196

composition of, 195

decorated, 203

hypergraph category of, see hypergraph category, of cospans

Cost, 54, 69, 71

counit, 141

cover, 234,237

empty, 237

cowlick

inevitability of, 242 cross section, see section

currying, 103, 227, 249

cyber-physical system, 147

dagger, 21, 66

data migration, 99,104

adjoints, 102

left pushforward, 104

pullback, 100, 102

right pushforward, 104

database, 77-81, 224

as interlocking tables, 77

communication between, 79

constraints, 79,94

data migration, 80

foreign key, 78

ID columns of, 77

instance, 93-94, 97

instance homomorphism, 97

instances form a topos, 232

query, 106, 112

schema, see database schema, 158

database schema, 78, 93

as category presentation, 79,85

free, 78

mapping between, 80

design, 117

diagram

as functor, 95

commutative, 95

differential equation, 162

discrete dynamical system, 100

induced graph of, 105

disjoint union, see union, disjoint

divides relation, 8,25

as preorder, 15

domain, see morphism, domain

dual, see also properties, dual

as opposite, 72

category as opposite, in Prof, 145

double, 142

object, 141
of lax monoidal monotone, 56

self, 143, 178, 203

dual notions

colimits and limits as, 113

subobjects and quotients, 193

dynamical system, 257

continuous, 211

discrete, 101 , see discrete dynamical system

hybrid, 254

effort

as metric, 60

electric circuit, 181

electrical circuit, 182, 194, 204-211

closed, 211

via cospans, 209

enriched categories, 139

enriched category

composition in, 139

general definition, 138

identity in, 139

metric space as, 61

preorders as, 57

vs category, 87

enriched functor, 65

enrichment, 57-69

base of, 57

change of base, 64

epi-mono factorization, $28,225,227,243$

and existential quantification, 250

epimorphism, 225,226

surjection as, 227

equivalence of categories, 66,97

equivalence relation, $9,193,194$

and partition, $8-10,16$

as corelation, 142

as binary relation, 9

as symmetric preorder, 13

generated by a preorder, 13,55

feasibility relation, 119-125 as Bool-profunctor, 121, 127

compact closed category of, 143-

145

feedback, 174

fiber, see preimage

flip-flop, 181

floor function, 27

foreign key, see database, foreign key

free

category, 82, 89, 154

monoid, 155

preorder, 153

prop, 155

schema, 78

Frobenius

algebra, 172

law, 198

monoid, 198

structure, 197-201

function, 2,10

as database instance, 90

as relation, 10

bijective, 10, 150, 156

composite, 12

identity, 11

injective, 10, 226

structure preserving, 2

surjective, 10

functor, $91-93$

data migration, see data migration

diagram as, 95

enriched, see enriched functor

operad, 217

presheaf as, 232

prop, 151

Set-valued, 98, 99, 102, 112, 217

functorial query language, FQL, 79

functorial semantics, 168

future

as not yet arrived, 183

Galois connection, 26-36
generative effect, 2, 4, 6, 26, 37, 242

generators and relations, see presentation

gluing, 237

graph, 14

arrow, 14

as Set-valued functor, 97

complete, 104

discrete, 104

free category on, 82

homomorphism, 98

paths in, 14

vertex, 14

weighted, 62

graphs

database schema for, 97-102

homomorphism of, 97

topos of, 233

graphs

homomorphism of, 99

greatest common divisor, 25

greatest lower bound, 23

group, 84,88

commutator subgroup, 104

free, 104

Hasse diagram, 5, 14, 16-18, 35, 54

database schema as, 78

for metric spaces, 62

for preorders, 5

for profunctors, 132

weighted graph as, 62

Hausdorff distance, 60, 73

hom object, 57, 58, 64, 128, 138, 139

matrix of, 64

hom-set, 81

hypergraph category, 197-203

of corelations, 202

of cospans, 202

of linear relations, 203

hypergraph prop

operad for, 216 theory of, 218

icon, 44, 140, 157, 160-163, 183, 197

copy, 51

crossing wires, 46

cup and cap, 140

discard, 50

spider, 184, 199

identity

function, 11 , see also function, identity

functor, 93

in enriched categories, see enriched category, identity in

in wiring diagrams, 135

matrix, 74,166

morphism, 81, 82

natural transformation, 96

port graph, 153

profunctor, see also unit

iff, 14

IMPLIES operation, 231, 245

induction, 166, 191

infimum, 25

infix notation, $8,9,13,42,230$

informatics

discarding in, 52

duplication in, 52

monoidal preorder of, 51

initial object, 184-186

as colimit, 192

empty set as, 185

interaction, 221

interconnection, 182

as variable sharing, 147

network-type, 183

via Frobenius structures, 197

interface, 182,222

intersection, $8,54,224$

as meet, 25

interval domain, $\mathbb{I}$, 252-255

involution, 280
isomorphism, 88-89

adjunction as relaxed version of, 27

as stable under pullback, 226

as stable under pushout, 189

bijection as, 88

of preorders, 21

join, 4, 23-25, 73

as coproduct, 186

joins

preservation of, 5,31

required in a quantale, 71

Kan extension, 115

language, 223

internal, 223, 251

Lawvere metric space, 59-65

as Cost-category, 59

of regions, 60

least common multiple, 25

least upper bound, 23

level shift, 18,35 , see also primordial ooze

lifting problems, 94

limit, 107-113, 224, 225, 231

formula for finite limits in Set, 112

product as, 108

pullback as, 112

terminal object as, 107

linear relation, 177

linear relations

hypergraph category of, see hypergraph category, of linear relations

logic, 34, 224, 230-232

implication in, 6

in a topos, 243

manufacturing, 39

discard operation in, 50

monoidal preorder of, 49

map monotone, see also monotone map, 18-22

order-preserving, see map, monotone

structure preserving, 1,18

mapping object

see hom object, 57

matching family, 237

matrices

multiplication of, 74

rig of, 160

matrix, $63,73,123$

associated to a signal flow graph, $165-168$

feasibility, 122

identity, 74

of distances, 62

meet, 23-25, 109

meets

preservation of, 26,31

metric space, 66

as Cost-category, 61

as topological space, 235

discrete, 104

extended, 59

ordinary, 59

presentation of, 62

mirror image, see transpose

modal operator, 250

modes of transport, 63

monoid, 42,83 , see also monoidal category, monoid object in

as one-object category, 83

free, 104,155

group as, 88

monoidal category, iv, 136-138

monoid object in, 172-173

monoidal closed category, 69

booleans as, 70

Cost, see Cost

monoidal functor, 173, 204, 206, 209, 213
monoidal monotone as, 56

monoidal monotone, 55-57

monoidal preorder, 41-57

opposite of, 55

monoidal product, 41

as stacking, 197, 211

monoidal structure, 41

weak, 42

monoidal unit, 41

drawn as nothing, 45

monomorphism, 225, 226

as stable under pullback, 226

injection as, 226

monotone map, see also map, monotone

as Bool-functor, 66

as functor, 92

morphism, v

codomain, 81

domain, 81

identity, 81

in free category, 82

inequality as mere existence of, 86

invertible, 88

natural numbers, $15,42,53$

as free category, 83

as free monoid, 155

as rig, 159

natural numbers as

as set, 7

natural transformation, 95-97

as presheaf morphism, 233

between monotone maps, 97

component of, 95,96

graph homomorphism as, 98

identity, 96

naturality condition, 95

navigator, 72,126

network

diagram, 181

language, 201

NOT operation, 231, 245 notation, see also icon

for classified subobjects, 229

for common sets, 7

for monoidal structures, 42

infix, see infix

set builder, 7

obvious

conventional mathematical meaning of, 150

ooze

primordial, see primordial ooze

open set, 234

open system, 182

operad, 184, 211-218

algebra of , 217

as custom compositionality, 184

from monoidal category, 214

of cospans, 215

of sets, 214

of wiring diagrams, 211

operation in, 213

operation, see operad

logical, 224

opposite

V-category, 66

category, see category, opposite

preorder, 18

opposite category

and presheaves, 232

as dual, 145

OR operation, 230, 245

order, see also preorder

preservation of, 5

total, 16

ordinals

as categories, 83

partial order, 13 , see also preorder, skeletal, 127

partition, 9, 20, 28-30

as surjection, 11
associated equivalence relation of, 10

from preorder, 16

label irrelevance of, 9

part of, 9

pullback of, 29

pushforward of, 29

pie

lemon meringue, 40,47

poker, 43

port graph, 151-153, 155

acyclicity condition, 151

as morphism, 152, 156

poset, see partial order

power set, $16,17,19,54$, see also preorder, of subobjects, 204

predicate, $224,228,247,256$

prediction vs. possibility, 221

preimage, 10, 11, 32, 120, 234, 238

preorder, 12-18

as Bool-category, 57, 58

as category, 85

codiscrete, $13,104,303$

Cost, see Cost

dagger, see dagger

discrete, 13, 20, 104, 303

free, 104

free on a relation, 153

monoidal, see monoidal preorder

of open sets, 236

of partitions, 17

of subobjects, 247

partial order as, 13

presentation of, 14

skeletal, 13,21

symmetric monoidal, see monoidal

preorder, 137

preorder relation

as binary relation, 12

presentation

of linear algebra, 168 of metric space, 62

of monoid, 85

of preorder, 14

of prop, 158

presheaf, 232-234

as database instance, 233

restriction maps, 232

sections, 232

presheaves

morphism of, 233

topos of, 232

primordial ooze, see also ooze, primordial, $93,97,134,173$

product

as limit, 111

in a category, 108

meet as, 109

monoidal, 136

of $\mathcal{V}$-categories, 67

of categories, 110, 144

of preorders, 17

of sets, 8,109

profunctor, 119-132

Cost, 122 , see also Cost

as bridges, 122,131

Bool, 119

collage of, see collage

enriched, 121, see profunctor

unit, 128,130

profunctors

category of, 125-130

compact closed category of, 139

composition of, 125, 127

program semantics, 34,37

prop, 149-179

FinSet, 150

expression in, 157

free, 155

hypergraph, 202

of $R$-relations, 175

of matrices, 164, 170
posetal, 151

presentation of, 158

signature of, 155

properties

dual, 33

proposition, 54 , see also logic

pullback, 225

along a map, 22, 32, 101

as limit, 112

monomorphism in terms of, 226

of isomorphism, 226

pullbacks

pasting of, 225

pushout, 28, 188-191

along isomorphism, 189

as colimit, 188

epimorphism in terms of, 226

in cospan composition, 195

quantale, 68, 71-76, 121, 123, 160

commutative, 71

Cost as, see Cost

matrix multiplication in, 73

of open sets, 236

quantales

as self-enriched, 71

quantification, 248

quotient, 10, 104,157, 193

as data migration, 107

real numbers, $16,42,45,172$

as metric space, 61

as preorder, 23

as set, 7

topology on, 234

recipes, 40

reflexivity, 9,12

as identity in a preorder, 85

relation, 8,151

binary, 8,35

divides, see divides relation

equivalence, see equivalence relation free preorder on, 35,153

function as, 10

linear, see linear relation

preorder, see preorder

subset, 54

relations

composition of, 175

resource, 117

theory, 39-52

restriction map, see presheaf, restriction map

retraction, 89

reverse icon, see transpose

rig, 159-160

matrices as, 160

matrices over, 164

vs. ring, 160

ring

free, 104

safety proof, 255

schema, see dtabase schema89

semantics, 223, 251

sound, 251

semiring, see rig

set, $7-8$

$n^{\text {th }}$ ordinal as, 7

as sheaf on one-point space, 242

booleans as, 7

empty, 7,11

integers as, 7

natural numbers as, 7

one element, 7

real numbers as, 7

sets

category of, 87

sheaf, 223, 232-242

condition, 237

constant, 253

of local functions, 254

of sections of a function, 237,240

of vector fields, 241
on $\mathbb{R}$ as semantics of behavior, 253 sheaves

morphism of, 237

topos of, 242

Sierpinski space, 235, 242

signal flow graph, 148-179

and linear algebra, 176

as morphism, 162

general, 175

semantics of, 163

simplified, 160

site, 232

database schema as, 232

topological space as, 234

skeleton, 13, 66

snake equations, 141

soundness

of proof system, 170

spider, 183, 199

as iconography, 184

spiderable wire

Frobenius structure as, 199

stacking, see monoidal product

subobject classifier, 225, 228-230, 243251

and logic, 230

as superdense nugget from outer space, 228

for behavior types, 254

in Set, 229

subset, 7 , see also power set

summaries

limits and colimits as, 106

supremum, 25

symmetry, $9,13,42,149$

and dagger, 21

as required for enriched products, 67

in wiring diagrams, 46

lack of for effort, 60 of monoid operation, wiring diagram for, 198

of monoidal product, 42

system

component, 222

property, 223

tangent bundle, 241

vector fields as sections of, 241

terminal object, 108

as limit, 107

limit as, 111

universal property of, 108

theory

of hypergraph props, 218

of monoids, 173

top element, see terminal object

topological space, 234-240, 252

topology

codiscrete, 104

discrete, 104, 235

subspace, 236

topos, 223, 242-257

as category of sheaves, 237, 242

database instances as, 231

of sets, 224

properties of, 225-230

total order, 15,28

transitive closure, 29

transitivity, 9,12

as composition in a preorder, 85

transpose, 174

tree of life, 19

triangle inequality, 60

trivial path, $82,91,151$

type theory, 251

union, $4,8,64$

and data migration, 105

as join, 25

disjoint, 8 , 107, 133, 186
unique up to unique isomorphism, 24 , 108

unit, 141

monoidal, 41 , see also monoidal unit profunctor, 128

unitality, $81,83,125,135$

as coherence condition, 115

in enriched categories, 139

of identity functions, 87

of monoid operation, wiring diagram for, 198

of monoidal product, 42

weak, 130

universal property, 23, 108, 115, 153159,184

upper set, 17,22

$\mathcal{V}$-category, see enrichment

$\mathcal{V}$-profunctor, see profunctor, enriched

vector, $164,165,173$

tangent, 241

vector field, 241

vector space, 212

free, 104

weighted graph, see graph, weighted

wiring diagram, iv, 40, 43-48, 134-136, 140

as graphical proof, 47

for categories, 134

for hypergraph categories, 200

for monoidal categories, 136

for monoidal preorders, 44

icon of, $44,46,51$

styles of, 44, 146, 183

Yoneda lemma

for preorders, 20