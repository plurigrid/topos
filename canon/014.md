\title{
DRAFT
}

September 3, 2023

\title{
Categorical Systems Theory
}

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-002.jpg?height=266&width=312&top_left_y=718&top_left_x=901)

\author{
David Jaz Myers
}

(Last updated: September 3, 2023)

This page intentionally left blank.

\section*{Preface}

This book is a work in progress - including the acknowledgements below! Use at your own peril!

Categorical systems theory is an emerging field of mathematics which seeks to apply the methods of category theory to general systems theory. General systems theory is the study of systems - ways things can be and change, and models thereof - in full generality. The difficulty is that there doesn't seem to be a single core idea of what it means to be a "system". Different people have, for different purposes, come up with a vast array of different modeling techniques and definitions that could be called "systems". There is often little the same in the precise content of these definitions, though there are still strong, if informal, analogies to be made accross these different fields. This makes coming up with a mathematical theory of general systems tantalizing but difficult: what, after all, is a system in general?

Category theory has been describe as the mathematics of formal analogy making. It allows us to make analogies between fields by focusing not on content of the objects of those fields, but by the ways that the objects of those fields relate to one another. Categorical systems theory applies this idea to general systems theory, avoiding the issue of not having a contentful definition of system by instead focusing on the ways that systems interact with eachother and their environment.

These are the main ideas of categorical systems theory:

1. Any system interacts with its environment through an interface, which can be described separately from the system itself.

2. All interactions of a system with its environment take place through its interface, so that from the point of view of the environment, all we need to know about a system is what is going on at the interface.

3. Systems interact with other systems through their respective interfaces. So, to understand complex systems in terms of their component subsystems, we need to understand the ways that interfaces can be connected. We call these ways that interfaces can be connected composition patterns.

4. Given a composition pattern describing how some interface are to be connected, and some systems with those interfaces, we should have a composite system which
consists of those subsystems interacting according to the composition pattern. The ability to form composite systems of interacting component systems is called modularity, and is a well known boon in the design of complex systems.

In a sense, the definitions of categorical systems theory are all about modularity how can systems be composed of subsystems. On the other hand, the theorems of categorical systems theory often take the form of compositionality results. These say that certain facts and features of composite systems can be understood or calculated in terms of their component systems and the composition pattern.

This book will follow this general paradigm. We will see definitions of systems which foreground modularity - the ways that systems can be composed to form more complex systems. And then we will prove a general compositionality theorem, showing that a large class of behaviors of composite systems can be calculated in terms of their components and the composition pattern.

This abstract overview leaves a lot of questions to be answered. What is, or what can be a system? What is an interface? What is a composition pattern? How do we compose systems using composition patterns? What is a behavior of a system, and how do we study it categorically? There is no single answer to this suite of questions. Different people working with different aims will answer these questions differently. But we can package this suite of questions into an informal definition of a doctrine of dynamical systems.

Informal Definition 0.0.0.1. A doctrine of dynamical systems is a particular way to answer the following questions about it means to be a systems theory:
- What does it mean to be a system? Does it have a notion of states, or of behaviors? Or is it a diagram describing the way some primitive parts are organized?
- What should the interface of a system be?
- How can interfaces be connected in composition patterns?
- How are systems composed through composition patterns between their interfaces.
- What is a map between systems, and how does it affect their interfaces?
- When can maps between systems be composed along the same composition patterns as the systems.

We will give a semi-formal ${ }^{1}$ definition of dynamical systems doctrine in Chapter 6. For the first five chapters of this book on the other hand, we will work within a fixed doctrine of dynamical systems which we might call the parameter-setting doctrine. This doctrine gives a particular answer to the above questions, based around the following defintion of a system.
\footnotetext{
${ }^{1}$ And for experts, a formal definition, though we won't fully justify it.
}

Informal Definition 0.0.0.2. A dynamical system consists of:
- a notion of how things can be, called the states, and
- a notion of how things will change given how they are, called the dynamics.

The dynamics of a system might also depend on some free parameters or inputs that are imported from the environment, and we will often be interested in some particular variables of the state that are exposed or output to the environment.

In the first two chapters, we will see a variety of examples of such systems, including discrete-time deterministic systems, systems of differential equations, and non-deterministic systems such as Markov decision processes. We will also see what composition patterns can be in the parameter-setting doctrine; they can be drawn as wiring diagrams like this:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-007.jpg?height=253&width=306&top_left_y=936&top_left_x=904)

But Informal Definition 1.1.0.1 is not so precise. Deterministic systems, systems of differential equations, Markov decision processes, and many more sorts of systems fit the mold, but they also differ in many important ways. Informal Definition 1.1.0.1 doesn't tell us what the states should be (a set? a topological space? a manifold? a graph? something else?), and it doesn't tell us what it means to specify how things change given how they are. We can package this suite of questions into the notion of a theory of dynamical systems, or systems theory for short.

Informal Definition 0.0.0.3. A theory of dynamical systems â€” or a systems theory for short - is a particular way to answer the following questions about what it means to be a dynamical system:
- What does it mean to be a state?
- How should the output vary with the state - discretely, continuously, linearly?
- Can the kinds of input a system takes in depend on what it's putting out, and how do they depend on it?
- What sorts of changes are possible in a given state?
- What does it mean for states to change.
- How should the way the state changes vary with the input?

We will make this definition fully formal in Chapter 3, after introducing enough category theory to state it. Once we have made the definition of systems theory formal, we can make the definition of system. But what is interesting about dynamical systems is how they behave.

Informal Definition 0.0.0.4. A behavior of a dynamical system is a particular way its states can change according to its dynamics.

There are different kinds of behavior corresponding to the different sorts of ways that the states of a system could evolve. Perhaps they eventually repeat, or they stay the same despite changing conditions.

In Chapter 3, we will formalize this definition of behavior for each systems theory by noticing that for any given kind of behavior, there is almost always a system that represents that behavior, in that it does exactly that behavior and nothing more. For example, a point moving uniformly on a line represents a trajectory, and a point moving on a circle represents a periodic orbit. We will also note that a particular behavior of a system will alway requires a particular choice of parameters, which we call the chart of the behavior.

Using this observation, we will prove our main compositionality theorem in Chapter 5. This theorem states, informally, the following facts concerning the composition of systems.
- Suppose that we are wiring our systems together in two stages. If we take a bunch of behaviors whose charts are compatible for the total wiring pattern and wire them together into a behavior of the whole system, this is the same behavior we get if we first noticed that they were compatible for the first wiring pattern, wired them together, then noticed that the result was compatible for the second wiring pattern, and wired that together. This means that nesting of wiring diagrams commutes with finding behaviors of our systems.
- Suppose that we have two charts and a behavior of each. Then composing a behavior with the composite of those behaviors is the same as composing it with the first one and then with the second one.
- Suppose that we have a pair of wiring patterns and compatible charts between them. If we take a bunch of behaviors whose charts are compatable according to the first wiring pattern, wire them together, and then compose with a behavior of the second chart, we get the same thing as if we compose them all with behaviors of the first chart, noted that they were compatible with the second wiring pattern, and then wired them together.

These basic principles show us how the problem of understanding the behaviors of composite systems can be broken down consistently into the hopefully smaller problems of understanding the behaviors of their components, and the pattern of composition.

This theorem comes down to some fully abstract category theory: the construction of representable lax doubly indexed functors. Since the theorem is abstract, it can be applied not only to any systems theory as in Informal Definition 1.1.0.2, but any systems theory in any doctrine (Informal Definition 6.1.0.1). In Chapter 6, we will see two other doctrines which give us substantially different ways to think about systems theory. But
the compositionality theorem proven in Chapter 5 will apply to them as well.

This book is intended as a first guide to the rapidly growing field of categorical systems theory. While the book does presume a knowledge of basic category theory (which can be gained from any one of the many wonderful introductions to the subject - see Section 1.1.1), the special topics needed for the definitions and theorems indexed categories, double categories, doubly indexed categories and their functors will be introduced as they become necessary.

My hope is that this book can inspire you to use categorical methods in systems theory in your work, whenever they are useful, and to demand more from these tools where they are not yet useful.

\section*{Acknowledgments}

David Spivak has been a friend and mentor to me as I write this book and beyond. In many ways, I see this book as my take on David's research in lens based systems in recent years. David and I began writing a book together, of which this book was to be the first half and David's book on polynomial functors (now co-authored with Nelson Niu) was to be the second. But as we were writing, we realized that these weren't two halves of the same book, but rather two books in the same genre. It was a great pleasure writing with David during the summer of 2020, and I owe him endless thanks for ideas, suggestions, and great conversation. This book wouldn't exist without him.

Emily Riehl has been a better advisor than I could have thought to have asked for. I want to thank her for her financial support (through grant ????) during the development of much of the mathematics in this book. I'll write more in my thesis, but as far as this book goes, I would like to thank her for her careful reading, her advice on logistics, and her patience.

Thanks go to Emily Riehl, tslil clingman, Sophie Libkind, John Baez, Geoff Cruttwell, Brendan Fong, Christian Williams.

Thanks to Henry Story for pointing out typos.

This book was written with support from the Topos Institute.

\section*{Contents}
Preface ..... $\mathbf{v}$
1 Wiring together dynamical systems ..... 1
1.1 Introduction ..... 1
1.1.1 Category Theory ..... 3
1.2 Deterministic and differential systems theories ..... 4
1.2.1 Deterministic systems ..... 5
1.2.2 Differential systems ..... 14
1.3 Wiring together systems with lenses ..... 19
1.3.1 Lenses and lens composition ..... 19
1.3.2 Deterministic and differential systems as lenses ..... 22
1.3.3 Wiring diagrams as lenses in categories of arities ..... 31
1.3.4 Wiring diagrams with operations as lenses in Lawvere theories ..... 39
1.4 Summary and Futher Reading ..... 43
2 Non-deterministic systems theories ..... 45
2.1 Possibilistic systems ..... 45
2.2 Stochastic systems ..... 53
2.3 Monadic systems theories and the Kleisli category ..... 61
2.4 Adding rewards to non-deterministic systems ..... 68
2.5 Changing the flavor of non-determinism: Monad maps ..... 70
2.6 Wiring together non-deterministic systems ..... 75
2.6.1 Indexed categories and the Grothendieck construction ..... 76
2.6.2 Maps with context and lenses ..... 80
2.6.3 Monoidal indexed categories and the product of lenses ..... 84
2.6.4 Monadic lenses as generalized lenses ..... 86
2.7 Changing the Flavor of Non-determinism ..... 92
2.8 Summary and Further Reading ..... 97
3 How systems behave ..... 99
3.1 Introduction ..... 99
3.2 Kinds of behavior ..... 100
3.2.1 Trajectories ..... 100
3.2.2 Steady states ..... 105
3.2.3 Periodic orbits ..... 108
3.3 Behaviors of systems in the deterministic theory ..... 110
3.3.1 Simulations ..... 120
3.4 Dealing with two kinds of composition: Double categories ..... 124
3.4.1 The double category of arenas in the deterministic systems theory ..... 127
3.4.2 The double category of sets, functions, and matrices ..... 130
3.4.3 The double category of categories, profunctors, and functors ..... 133
3.5 Theories of Dynamical Systems ..... 139
3.5.1 The deterministic systems theories ..... 147
3.5.2 The differential systems theories ..... 148
3.5.3 Dependent deterministic systems theory ..... 160
3.5.4 Non-deterministic systems theories ..... 160
3.6 Restriction of systems theories ..... 162
3.7 Summary and Futher Reading . ..... 164
4 Change of Systems Theory ..... 165
4.1 Introduction ..... 165
4.2 Composing behaviors in general ..... 170
4.3 Arranging categories along two kinds of composition: Doubly indexed categories ..... 177
4.4 Vertical Slice Construction ..... 183
4.4.1 Double Functors ..... 184
4.4.2 The Vertical Slice Construction: Definition ..... 186
4.4.3 Natural Transformations of Double Functors ..... 189
4.4.4 Vertical Slice Construction: Functoriality ..... 194
4.5 Change of systems theory ..... 203
4.5.1 Definition ..... 204
4.5.2 Functoriality ..... 210
4.6 Summary and Further Reading ..... 216
5 Behaviors of the whole from behaviors of the parts ..... 217
5.1 Introduction ..... 217
5.2 Steady states compose according to the laws of matrix arithmetic ..... 218
5.3 The big theorem: representable doubly indexed functors ..... 226
5.3.1 Turning lenses into matrices: Representable double Functors ..... 228
5.3.2 How behaviors of systems wire together: representable doubly indexed functors ..... 238
5.3.3 Is the whole always more than the composite of its parts? ..... 245
5.4 Summary and Further Reading ..... 250
6 Dynamical System Doctrines ..... 251
6.1 Introduction ..... 251
6.2 The Behavioral Approach to Systems Theory ..... 254
6.2.1 The idea of the behavioral approach ..... 256
6.2.2 Bubble diagrams as spans in categories of arities ..... 265
6.2.3 The behavioral doctrine of interval sheaves ..... 274
6.2.4 Further Reading in the Behavioral Doctrine ..... 281
6.3 Drawing Systems: The Port Plugging Doctrine ..... 281
6.3.1 Port-plugging systems theories: Labelled graphs ..... 285
6.3.2 Bubble diagrams for the port-plugging doctrine ..... 290
6.3.3 Further Reading in the port-plugging doctrine ..... 293
Bibliography ..... 295

\section*{Chapter 1}

\section*{Wiring together dynamical systems}

\subsection*{1.1 Introduction}

Here's a basic fact of life: things change. And how things change most often depends on how they currently are. This is the fundamental idea underlying all the various notions of dynamical system that we will see in this book.

Informal Definition 1.1.0.1. A dynamical system consists of:
- a notion of how things can be, called the states, and
- a notion of how things will change given how they are, called the dynamics.

The dynamics of a system might also depend on some free parameters or inputs that are imported from the environment, and we will often be interested in some particular variables of the state that are exposed or output to the environment.

You and I are big, complicated dynamical systems. Our bodies and minds are in some particular configuration, and over time this configuration changes. We can sense things - seeing, touching, tasting - and what we sense affects how our bodies and minds change. Seeing a scary snake can make me recoil and feel fear, but seeing a cute snake plushie can make me go over and start to pet it. Some parts of me are also put back into the environment, like the expression on my face. But not all of me is exposed in that way - some things just go on in my head.

This is the basic model of a dynamical system we will be working with in this book. ${ }^{1}$ But to make the above informal definition precise, we need to answer a number of questions:
- What should a state be, really? Do we just have an abstract set of states, or could there be a continuum of states? Maybe there are some other structures that states can enter into which have to be respected by the dynamics, but aren't determined by them? Jaz: With this last sentence, I'm thinking of "states as polynomial comonad aka category". Not sure how to phrase it right.
\footnotetext{
${ }^{1}$ At least until Chapter 6, where we will encounter other doctrines of dynamical systems.
}
- What does it mean to change? Do we want to know precisely which state will be next if we know how things are? Or, maybe we will only have a guess at which state will come next? Or, maybe we'll just say how a state is tending to change, but not where it will end up?
- Do we always take in the same sort of parameters, or does it depend on how our system is placed in its environment? Should the dynamics vary continuously (or linearly, or some other way) in the choice of parameters?

Different people have decided on different answers to these questions for different purposes. Here are three of the most widespread different ways to answer those questions:

1. We'll assume the states form a discrete set, and that if we know the current state and our parameters, we know exactly what the next state will be. Such a system generally called a Moore machine or deterministic automaton.

2. We'll assume the states form a continuum, but that we only know how a state is tending to change, not what the "next" state will be. Such a system is generally called a system of differential equations - the differential equations tells us the derivatives of the state variables: the way they are tending.

3. We'll assume the states form a discrete set, but that we only have a guess at which state will follow from the current state. Such a system is generally called a Markov process, or a Markov decision process.

We will call a way of answering these questions the theory of dynamical systems we are working in.

Informal Definition 1.1.0.2. A theory of dynamical systems - or a systems theory for short - is a particular way to answer the following questions about what it means to be a dynamical system:
- What does it mean to be a state?
- How should the output vary with the state - discretely, continuously, linearly?
- Can the kinds of input a system takes in depend on what it's putting out, and how do they depend on it?
- What sorts of changes are possible in a given state?
- What does it mean for states to change.
- How should the way the state changes vary with the input?

Moore machines, differential equations, and Markov decision processes are each dynamical systems understood in a different theory.

1. A Moore machine is a dynamical system in a discrete and deterministic systems theory.

2. A system of differential equations is a dynamical system in a differential systems theory.

3. A Markov decision process is a dynamical system in a stochastic systems theory.

In most cases, mathematicians have assumed that that the kinds of parameters our systems take in never change - that our system will always interface with its environment in the same way. However, this assumption is quite restrictive; after all, I change the way I interface with my environment all the time. Every time I turn and face a new direction, I open myself up to new inputs. There are variations on all of the above systems theories which allow for the kinds of input to depend on what the system is putting out, but for most of this book, we will work with systems theories that pick a fixed sort of input.

The dynamical systems we will see in this book are open in the sense that they take in inputs from their environment and expose outputs back to their environment. Because of this, our systems can interact with eachother. One system can take what the other system outputs as part of its input, and the other can take what the first outputs as part of its input. For example, when we have a conversation, I take what I hear from you and use it to change how I feel, and from those feelings I generate some speech which I output to the world. You then take what I've said and do the same thing.

\section*{Jaz: Some wiring diagram of a conversation}

We call this way of putting together dynamical systems to make more complex systems composition.

Informal Definition 1.1.0.3. Composition is the process by which some things are brought together to form bigger things.

Functions can be composed by plugging outputs into inputs, and dynamical systems can be composed by plugging in the variables of the states of some into the parameters of others.

This book is all about composing dynamical systems. Because of this, we will use the abstract language of composition: category theory.

Informal Definition 1.1.0.4. Category theory is the abstract study of composition.

\subsection*{1.1.1 Category Theory}

We'll be using the language of category theory quite freely in this book, and so we'll expect you to know the basics. These are the notions in category theory that you should look up if they are unfamiliar to you:
- What a category is.
- What an isomorphism is.
- What a functor is.
- What a natural transformation is.
- What a terminal and an initial object are.
- What a product and a coproduct are.
- What a monad is, and it will help if you also know what a comonad is.
- What a monoidal category is.

Good introductions to category theory abound. One place to start is An invitation to applied category theory [FS19]. Another is Notes on category theory [Per21]. For more mathematically inclined readers, see [Rie17].

We will be using cartesian categories quite a bit in the first few chapters.

Definition 1.1.1.1. A category $C$ is cartesian if every two objects $A$ and $B$ in $C$ have a product $A \times B$, and $C$ has a terminal object 1 . Equivalently, $C$ is cartesian if for any finite set $I$ and $I$-indexed family $A_{(-)}: I \rightarrow C$ of objects, there is a product $\prod_{i \in I} A_{i}$ in $C$.

A functor $F: C \rightarrow D$ between cartesian categories is said to be cartesian if it preserves products and terminal objects, i.e. the map $\left(F \pi_{A}, F \pi_{B}\right): F(A \times B) \rightarrow F A \times F B$ is an isomorphism for all $A$ and $B$, and the terminal morphism $F 1 \rightarrow 1$ is an isomorphism.

We will also use some more advanced category theory, like indexed categories and double categories. However, you don't need to know them up front; we will introduce these concepts as we use them.

While we're at it, here's some notation we'll use repeatedly throughout the book. The $n$th ordinal is denoted $\mathrm{n}$. It is defined to be the set

$$
\mathrm{n}:=\{1,2, \ldots, n\}
$$

So 0 is the empty set, 1 is a one-element set, etc. We will also use

$$
A+B
$$

to mean the disjoint union (or coproduct) of sets.

\subsection*{1.2 Deterministic and differential systems theories}

In this chapter, we will see how to wire together dynamical systems of all different sorts. First, however, we start with two exemplary systems theories:

1. First, systems which we will call (discrete-time) deterministic systems, which specify exactly which state the system will transition into given its current state and input parameters.

2. Second, systems which we will call differential systems, which do not specify a "next state" but rather specify exactly how the state is tending to change in the moment, given the current state and input parameters.

\subsection*{1.2.1 Deterministic systems}

A paradigmatic example of this sort of dynamical system is a clock.

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-017.jpg?height=439&width=439&top_left_y=390&top_left_x=840)

Suppose that our clock has just an hour hand for now. Then we may collect all the ways things can be for the clock into a set of hours:

$$
\text { Hour }:=\{1,2,3,4,5,6,7,8,9,10,11,12\}
$$

This set Hour is the set of states of our clock system.

If we know what hour it is, we also know what hour is coming next. So, this system has the following dynamics:

$$
\begin{align*}
& \text { tick: Hour } \rightarrow \text { Hour }  \tag{1.1}\\
& \qquad t \mapsto \begin{cases}t+1 & \text { if } t<12 \\
1 & \text { if } t=12\end{cases}
\end{align*}
$$

By saying that the function tick is the dynamics for this system, what we mean is that this function sends the current state of the system to the next state it will have. Here's a sample of the dynamics of the clock. Say we started at the 10 o'clock state:

$$
10 \stackrel{\text { tick }}{\longmapsto} 11 \stackrel{\text { tick }}{\longmapsto} 12 \stackrel{\text { tick }}{\longmapsto} 1 \xrightarrow{\text { tick }} 2 \stackrel{\text { tick }}{\longmapsto} \ldots
$$

Ok, it's not the most dynamic of systems, but we have to start somewhere. If we want to refer to the whole system at once, we can box it up and draw it like this:

$$
\begin{equation*}
\text { Clock - Hour } \tag{1.2}
\end{equation*}
$$

We imagine that the clock is going about its business inside the box, and that is shows the hour it is currently displaying on the outgoing wire. This outgoing wire constitutes the clock's exposed variable, but we'll explain that more later.

One issue with our clock is that it doesn't tell us whether it is morning or evening. Being morning or evening and going back and forth between them is another way that things might be and change, and hence we can see it as its own two-state dynamical system with states

$$
\text { a.m./p.m. }=\{\text { a.m., p.m. }\}
$$

However, rather than have this be an independent system, we want to consider it as a little addition to our clock system, one that reads a.m. or p.m.:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-018.jpg?height=425&width=434&top_left_y=389&top_left_x=843)

To connect the meridiem to the clock means that the way the meridiem changes should be based on the hour:

$$
\begin{align*}
\text { next : a.m./p.m. } \times \text { Hour } & \rightarrow \text { a.m./p.m. }  \tag{1.4}\\
(\text { a.m., } t) & \mapsto \begin{cases}\text { p.m. } & \text { if } t=11 \\
\text { a.m. } & \text { otherwise }\end{cases} \\
(\text { p.m., } t) & \mapsto \begin{cases}\text { a.m. } & \text { if } t=11 \\
\text { p.m. } & \text { otherwise }\end{cases}
\end{align*}
$$

If it is a.m. and the clock reads 8, then it will still be a.m. at the next tick; but if it is a.m. and the clock reads 11 , then the next tick will switch the meridiem to p.m..

Again, the thing to note about the dynamics of the a.m./p.m. system is that they depend on what hour it is. The hour is imported as a parameter for the dynamics of the meridiem system. We can draw the meridiem system as a box like this:

$$
\begin{equation*}
\text { Hour Meridiem -a.m./p.m. } \tag{1.5}
\end{equation*}
$$

We have the a.m./p.m. wire coming out, which carries the information of whether it is a.m. or p.m., just like the clock. But we also have a wire coming in, which carries the hour that we need as a parameter for our dynamics.

We can now express our whole clock (1.3) by wiring together our bare clock (1.2) and the a.m./p.m. system:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-018.jpg?height=379&width=507&top_left_y=2147&top_left_x=798)

We've put both our systems Meridiem and Clock into this bigger box with two outgoing wires. We didn't just dump these systems in the bigger box; we connected them up to each other and the outgoing wires. The resulting system has states

$$
\text { HoursWithDisplay := Hour Ã— a.m./p.m. }
$$

each of which is a pair, e.g. (11, a.m.), consisting of an hour and a meridiem reading. They update in a combined way, by using the hour shown on the clock face as the parameter we need for the Meridiem system; this is expressed by having a wire from the output of Clock to the input of Meridiem. In full, the dynamics looks like this:

$$
\begin{aligned}
\text { tick }^{\prime}: \text { HoursWithDisplay } & \rightarrow \text { HoursWithDisplay } \\
(t, m) & \mapsto(\operatorname{tick}(t), \operatorname{next}(t, m))
\end{aligned}
$$

where tick and next are as in (1.1) and (1.4).

Exercise 1.2.1.1. Convince yourself that the combined system really does behave like the clock with a.m./p.m. display should.

Now that we have a working clock, we can use it for systems that need to know the time. For example, consider a diner that opens at 7a.m. and closes at 10p.m.. The states of this diner are

$$
\text { DinerState }=\{\text { open }, \text { closed }\} .
$$

The diner's dynamics are then

$$
\begin{aligned}
\text { dinerDynamics : DinerState } \times \text { HoursWithDisplay } & \rightarrow \text { DinerState } \\
(\text { open, }(10, \text { p.m. })) & \mapsto \text { closed } \\
(\text { closed, }(7, \text { a.m. })) & \mapsto \text { open } \\
(s,(t, m)) & \mapsto s \text { otherwise. }
\end{aligned}
$$

Again, we can represent the diner by this box:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-019.jpg?height=170&width=434&top_left_y=1796&top_left_x=843)

This time, we have two wires coming in, corresponding to the two parameters we need for the diner system: the hour and the meridiem.

Assuming that the diner has a clock on its wall which it uses to decide whether to open or close, the full diner system would be given by wiring the clock with display into those input wires:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-019.jpg?height=231&width=794&top_left_y=2297&top_left_x=709)

If we want to, we can peak into the clock with display and see that it is itself made out of a clock wired to a display:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-020.jpg?height=433&width=954&top_left_y=388&top_left_x=583)

These examples are simple, but it doesn't take much more to get to some truly amazing phenomena. Consider this system: we have an infinite tape with a read-head at some integer position. On this infinite tape, we will write the symbols $a, b, c$, or $d$, or we will leave it blank: _. Together, the state of the tape and the position of the read-head have states pairs $(T, n)$ consisting of a function $T: \mathbb{Z} \rightarrow\{a, b, c, d,-\}$, telling us what symbol $\mathrm{T}(i)$ is found at position $i$ of the tape, and a position $n$ of the read-head:

$$
\begin{aligned}
\text { Symbol } & =\{a, b, c, d,-\} \\
\text { Tape } & =\text { Symbol }^{\mathbb{Z}} \\
\text { Head } & =\mathbb{Z}
\end{aligned}
$$

The parameters that this system needs in order to change are a move-command and a write-command. The move-command will be either move left or move right, encoded as -1 or 1 respectively, and the write command will be one of the symbols that can be written on the tape:

$$
\text { Move }=\{-1,1\} \quad \text { and } \quad \text { Write }=\left\{a, b, c, d, \_\right\} .
$$

The way this system changes is by writing the write command to the tape at the current position, and then moving according to the move command. As a function, this is:

$$
\begin{aligned}
& \text { execute : Head } \times \text { Tape } \times \text { Move } \times \text { Write } \rightarrow \text { Head } \times \text { Tape } \\
& \qquad(n, i \mapsto \mathrm{T}(i), d, s) \mapsto\left(n+d, i \mapsto\left\{\begin{array}{ll}
\mathrm{T}(i) & \text { if } i \neq n \\
s & \text { if } i=n
\end{array}\right) .\right.
\end{aligned}
$$

We can imagine that the system exposes the tape and the symbol under its read head. We can box this system up and draw it like so:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-020.jpg?height=168&width=461&top_left_y=2361&top_left_x=824)

Now, we need one more simple ingredient to get our system going; a mysterious system of the form:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-021.jpg?height=168&width=439&top_left_y=347&top_left_x=840)

We can see that our mystery box will take in a symbol and put out a move command and a write command. The way our mystery box behaves is rather mysterious. It has seven states $S=\{1,2,3,4,5,6$, end $\}$, and its update rule is given by the following table, where the entry in the row $i$ and the column $s$ is written $(m, w): s^{\prime}$ to express the move command $m$, the write command $w$, and the next state $s^{\prime}$ that our mysterious system transitions to when input the symbol $i$ in state $s$ :

\begin{tabular}{|c|c|c|c|c|c|c|}
\hline & 1 & 2 & 3 & 4 & 5 & 6 \\
\hline a & $(-1, b): 1$ & $(1, \mathrm{a}): 1$ & $(-1, \mathrm{~b}): 3$ & $(1, \mathrm{~b}): 2$ & $(-1, \mathrm{~b}): 6$ & $(-1, \mathrm{~b}): 4$ \\
\hline b & $(-1, \mathrm{a}): 1$ & $(1, \mathrm{a}): 2$ & $(-1, \mathrm{~b}): 5$ & $(1, \mathrm{a}): 4$ & $(1, \mathrm{a}): 6$ & $(1, \mathrm{a}): 5$ \\
\hline c & $(1, \mathrm{~d}): 2$ & $(1, \mathrm{~d}): 2$ & $(-1, \mathrm{c}): 5$ & $(1, \mathrm{~d}): 4$ & $(1, \mathrm{c}): 5$ & $(1, \mathrm{a}): 1$ \\
\hline d & $(-1, \mathrm{c}): 1$ & $(1, \mathrm{a}): 5$ & $(-1, \mathrm{c}): 3$ & $(1, \mathrm{~d}): 5$ & $(-1, \mathrm{~b}): 3$ & end \\
\hline
\end{tabular}

The end state always transitions to itself. Mysterious indeed. But when we wire the two together, magic happens!

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-021.jpg?height=320&width=724&top_left_y=1382&top_left_x=711)

This is a universal Turing machine, i.e. when we encode everything into this strange alphabet, it is capable of arbitrarily complex calculation!

\section*{Even simple systems can have very interesting behavior when plugged in to the right environment.}

That's a lot of informal definitions, we are ready for something precise:

Definition 1.2.1.2. A deterministic system $\mathrm{S}$, also written as

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-021.jpg?height=137&width=557&top_left_y=2143&top_left_x=776)

consists of:
- a set States of states;
- a set Outs of values for exposed variables, or outputs for short;
- a set Ins of parameter values, or inputs for short;
- a function expose ${ }_{\mathrm{S}}:$ State $_{\mathrm{S}} \rightarrow$ Outs $_{\mathrm{s}}$, the exposed variable of state or expose function, which takes a state to the output it yields; and
- a function update ${ }_{S}$ : States $X \ln _{S} \rightarrow$ States , the dynamics or update function which takes a state and a parameter and gives the next state.

We refer to the pair $\left(\begin{array}{c}\text { Ins } \\ \text { Outs }\end{array}\right)$ of exposed variable and parameter values as the interface of the system.

We can interpret this definition in any cartesian category $C$ by taking States, Outs and Ins to be objects of $C$ and update ${ }_{S}$ and expose ${ }_{S}$ to be maps in $C$; here, we have have used the cartesian category Set of sets.

Remark 1.2.1.3. Deterministic systems are also known as Moore machines in the literature. If the output set is taken to be $\{$ true, $f a l s e\}$, then they are known as deterministic automata.

Often, these definitions also include a start state $s_{0} \in$ States as part of the data. We don't do this.

Example 1.2.1.4. The Clock system can be seen as a deterministic system with:

$$
\left(\begin{array}{c}
\text { tick } \\
\text { id }
\end{array}\right):\left(\begin{array}{l}
\text { Hour } \\
\text { Hour }
\end{array}\right) \leftrightarrows\left(\begin{array}{c}
\{*\} \\
\text { Hour }
\end{array}\right)
$$

In other words, it consists of
- State set State Clock $=$ Hour $=\{1,2, \ldots, 12\}$.
- Output set Out Clock $=$ Hour.
- Input set $\ln _{\text {Clock }}=\{*\}$, a one element set.

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-022.jpg?height=54&width=691&top_left_y=1618&top_left_x=389)

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-022.jpg?height=62&width=1390&top_left_y=1671&top_left_x=389)
tick $(t)$.

Example 1.2.1.5. Not only is the term Moore machine is used for the mathematical notion of deterministic system we've just presented, but it is also used for actual, real-life circuits which are designed on that principle.

For example, suppose that a wire carries the signals Wire $=\{$ high, low $\}$. We can see a deterministic system $M$ with input $\operatorname{In}_{M}=$ Wire $^{n}$ and Out ${ }_{M}=$ Wire $^{k}$ as a circuit with $n$ incoming wires and $k$ outgoing wires. ${ }^{a}$ The state then describes the state of all the internal wires (and capacitors, etc.) in the circuit. We would wire up these systems by literally wiring them together.

Jaz: I would like to add an example of an implementation of a Moore machine into a circuit.
\footnotetext{
"Of course, the notion of "incoming" and "outgoing" wires are ways we think about the circuit in
}
design terms. Circuits aren't actually directed in this way. We'll think about undirected notions of system in Chapter 2.

Note that when we say that a system doesn't have any parameters, as in Example 1.2.1.4, we don't take the parameter set to be empty but instead take it to have a single dummy value $\{*\}$, the one-element "hum of existence". In other words, having "no parameters" really means that the parameters are unchanging, or that there is no way to change the value of the parameters.

Also, we are just exposing the whole state with the system in Example 1.2.1.4. There is nothing preventing our systems from exposing their whole state (which means States $=$ Outs and expose $\left._{S}=\mathrm{id}\right)$, but often some aspects of the state are private, i.e. not exposed for use by other systems.

Exercise 1.2.1.6. Write out the clock and meridiem systems from (1.1) and (1.4) in terms of Definition 1.2.1.2. Really, this amounts to noticing which sets are the sets of states, which are the sets of inputs, and what (implicitly) are the sets of outputs.

Example 1.2.1.7 (SIR model). The set of states for a deterministic system doesn't need to be finite. The SIR model is an epimediological model used to study how a disease spreads through a population. "SIR" stands for "susceptible", "infected", and, rather ominously, "removed". This model is usually presented as a system of differential equations - what we will call a differential system - and we will see it in that form in Example 1.2.2.5. But we can see a discrete approximation to this continuous model as a deterministic system.

A state of the SIR model is a choice of how many people are susceptible, how many are infected, and how many are removed. That is,

$$
\text { StatesIR }=\left\{\left.\left[\begin{array}{c}
s \\
i \\
r
\end{array}\right] \right\rvert\, s, i, r \in \mathbb{R}\right\} \cong \mathbb{R}^{3} \text {. }
$$

is a 3-place vector of real numbers. We will again expose the whole state, so Out ${ }_{\text {SIR }}=$ StatesIR $^{\text {and }}$ expose $_{\text {SIR }}=$ id.

The idea behind the SIR model is that if a susceptible person comes in contact with an infected person, then they have a chance of becoming infected too. And, eventually, infected persons will be removed from the model, either by recovering (a gentler way to read the " $\mathrm{R}$ ") or by dying. So we need two parameters: the rate $a$ of infection and the rate $b$ of removal:

$$
\operatorname{InSIR}=\left\{\left.\left[\begin{array}{l}
a \\
b
\end{array}\right] \right\rvert\, a, b \in \mathbb{R}\right\}=\mathbb{R}^{2}
$$

Now, we can show how a population will develop according to this model by
defining the update function:

$$
\begin{aligned}
& \text { update }_{\text {SIR }}: \text { State }_{\text {SIR }} \times \ln _{\text {SIR }} \rightarrow \text { State }_{\text {SIR }} \\
& \left(\left[\begin{array}{l}
s \\
i \\
r
\end{array}\right],\left[\begin{array}{l}
a \\
b
\end{array}\right]\right) \mapsto\left[\begin{array}{c}
s-a s i \\
i+a s i-b i \\
r+b i
\end{array}\right]
\end{aligned}
$$

Example 1.2.1.8. If a deterministic system has a small finite set of states, then we can draw it entirely as a transition diagram:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-024.jpg?height=306&width=398&top_left_y=823&top_left_x=858)

Note that every node has an orange and a green arrow emanating from it, but that there are no rules on how many arrows point to it.

This diagram describes the following system S:

$$
\left(\begin{array}{c}
\text { update }_{\mathrm{S}} \\
\text { expose }_{\mathrm{S}}
\end{array}\right):\left(\begin{array}{c}
\{1,2,3\} \\
\{1,2,3\}
\end{array}\right) \leftrightarrows\left(\begin{array}{c}
\{\text { green, orange }\} \\
\{a, b\}
\end{array}\right)
$$

That is, we have
- States $_{S}=\{1,2,3\}$.
- In $_{\mathrm{S}}=\{$ green, orange $\}$,
- Out $_{\mathrm{S}}=\{a, b\}$,

\section*{-}

$$
\begin{aligned}
& \text { expose }_{S}: \text { State }_{S} \rightarrow \text { Out }_{S} \quad \text { update }_{S}: \text { State }_{S} \times \ln _{S} \rightarrow \ln _{S} \\
& 1 \mapsto a \quad(1, \text { green }) \mapsto 2 \\
& 2 \mapsto b \quad(1, \text { orange }) \mapsto 1 \\
& 3 \mapsto b \quad(2, \text { green }) \mapsto 3 \\
& (2, \text { orange }) \mapsto 1 \\
& (3, \text { green }) \mapsto 3 \\
& (3, \text { orange }) \mapsto 1
\end{aligned}
$$

To draw a transition diagram of a system $S$, we draw each state $s \in$ States as a bubble filled with the label $\operatorname{expose}_{S}(s)$, and for each parameter $i \in \operatorname{In}_{S}$ we draw an arrow from $s$ to update $S_{S}(s, i)$ and label it by $i$. For a diagram like this to be a transition diagram, every node must have exactly one edge leaving it for each parameter.

Exercise 1.2.1.9. Draw the Clock system (Example 1.2.1.4) as a transition diagram.

Example 1.2.1.10 (Deterministic Finite Automata). A deterministic finite automaton (DFA) is a simple model of computation. Given our definition of deterministic system, DFAs are easy enough to define: they are just the deterministic systems with finitely many states whose output values are either accept or reject.

This means that the exposed variable of state expose $_{S}:$ State $_{S} \rightarrow$ \{accept, reject $\}$ is a boolean valued function. We say a state $s$ is an accept state if $\operatorname{expose}_{S}(s)=\operatorname{accept}^{\text {, }}$ and a reject state if $\operatorname{expose}_{S}(s)=$ reject.

The idea is that a DFA is a question answering machine. Given a starting state $s_{0}$ and a sequence of input values $i_{1}, \ldots, i_{n}$, we get a sequence of states by $s_{t+1}:=$ update $_{\mathrm{S}}\left(s_{t}, i_{t}\right)$. The answer to the question is "yes" if $s_{n}$ is an accept state, and "no" if $s_{n}$ is a reject state.

There is an important special case of deterministic systems which appear very commonly in the literature: the closed systems. These are the systems which have no parameters, and which expose no variables. They are closed off from their environment, and can't be wired into any other systems.

As mentioned after Example 1.2.1.4, when we say "no" in this way â€” no parameters, no variables - we should be careful with what we mean exactly. We mean that there is no variation in the parameters or variables, that they are trivial. That is, we make the following definition.

Definition 1.2.1.11. We say that a deterministic system $S$ as "no inputs" if $\ln _{S}$ has a single element, and has "no outputs" if Outs has a single element. We say that S is closed if it has no inputs and no outputs: both Ins and Outs have only one element

$$
\text { Ins } \cong\{*\} \cong \text { Outs }
$$

Exercise 1.2.1.12. Show that to give a closed system

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-025.jpg?height=164&width=542&top_left_y=1959&top_left_x=786)

one just needs to choose a set States and an update function update S $_{S}$ States $\rightarrow$ States.

Given that we are mostly interested in how systems wire together, it may seem strange to draw attention to the closed systems that can't be wired into anything else. But we will often end up with a closed system as the result of wiring together some systems.

For example, suppose we have an Agent acting within a Environment. The agent will take an action, and the evironment will respond to that action. Depending on the action taken and response given, the agent and the environment will update their states. We can model this by the following wiring diagram:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-026.jpg?height=353&width=650&top_left_y=504&top_left_x=735)

To model this as a closed system is to think â€” or pretend â€” that the our model of the Agent and the Environment includes all possible external parameters, that it is well isolated from its own environment.

Exercise 1.2.1.13. What would happen to a system $\mathrm{S}$ if its set of parameters or output values were actually empty sets? Let's find out.

1. Suppose $\ln _{S}=\varnothing$. Explain the content of a deterministic system

$$
\left(\begin{array}{l}
\text { update }_{S} \\
\text { expose }_{S}
\end{array}\right):\left(\begin{array}{l}
\text { States } \\
\text { State }_{S}
\end{array}\right) \leftrightarrows\left(\begin{array}{c}
\varnothing \\
\{*\}
\end{array}\right)
$$

2. Suppose Out $\mathrm{S}=\varnothing$. Explain the content of a deterministic system

$$
\left(\begin{array}{l}
\text { update }_{S} \\
\text { expose }_{S}
\end{array}\right):\left(\begin{array}{l}
\text { States }_{S} \\
\text { States }^{2}
\end{array}\right) \leftrightarrows\left(\begin{array}{c}
\{*\} \\
\varnothing
\end{array}\right) .
$$

\subsection*{1.2.2 Differential systems}

La nature ne fait jamais des sauts - Leibniz

A quirk of modeling dynamical systems as deterministic systems is that deterministic systems lurch from one state to the next. In life, there are no next moments. Time, at least at human scales and to a first approximation, flows continuously.

Instead of modelling the "next" state a system will be in, we can model how the system is tending to change, in the moment. In order to do this, we need to make concession in the way we model the states of our system: we must assume they form a continuum themselves.

For example, suppose we are studying a population of Rabbits. We can measure the rate at which rabbits are born, and the rate they die. Then the population changes according to these rates. We can express this dependency of the change in population
on certain rates with a differential equation:

$$
\frac{d r}{d t}=\mathrm{b}_{\text {Rabbits }} \cdot r-\mathrm{d}_{\text {Rabbits }} \cdot r
$$

where $r \in \mathbb{R}$ is the population of rabbits (considered as a real number for convenience), and the rates $b_{\text {Rabbits }}$ and $d_{\text {Rabbits }}$. The state of our system of Rabbits is the current

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-027.jpg?height=60&width=1450&top_left_y=599&top_left_x=327)
parameters, so that $\operatorname{In}_{\text {Rabbits }}=\mathbb{R} \times \mathbb{R}$. Accordingly, we can box the rabbit system up like so:

$$
\begin{equation*}
\mathbb{R}-{ }^{R}-\text { Rabbits }^{-} \tag{1.16}
\end{equation*}
$$

Now, rabbits are prey; they are eaten by other animals. That means that the rate at which rabbits die will depend on how often they are being eaten, and how often they are being eaten will depend on how many predators there are out there.

The population of any predator will also change according to a birth rate and death rate. Suppose we have a similarly defined system of Foxes whose population is governed by the differential equation

$$
\frac{d f}{d t}=\mathrm{b}_{\text {Foxes }} \cdot f-\mathrm{d}_{\text {Foxes }} \cdot f \text {. }
$$

We can box up this system like so:

$$
\begin{align*}
& \mathbb{R}-  \tag{1.17}\\
& \mathbb{R}- \\
&
\end{align*}
$$

Now, we want the death rate of rabbits to depend on the number of foxes. But we also need the birth rate of the foxes to depend on the number of rabbits; after all, if a fox has nothing to eat, it has no energy for hanky-panky. So we will add the following system of equations to the mix:

$$
\left\{\begin{array}{l}
\mathrm{d}_{\text {rabbits }}=c_{1} f \\
\mathrm{~b}_{\text {Foxes }}=c_{2} r
\end{array}\right.
$$

Making these substitutions, we get the following system of differential equations:

$$
\left\{\begin{array}{l}
\frac{d r}{d t}=\mathrm{b}_{\text {Rabbits }} \cdot r-c_{1} f r \\
\frac{d f}{d t}=c_{2} r f-\mathrm{d}_{\text {Foxes }} \cdot f
\end{array}\right.
$$

We are setting the parameters of the systems of Rabbits and Foxes according to the states of the other system. That is, we are wiring up the systems of Rabbits and Foxes:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-028.jpg?height=363&width=529&top_left_y=385&top_left_x=798)

The resulting system is called the Lotka-Volterra predator-prey model, and it is a simple differential model of the ways that the population of a predator species depends on the population of a prey species, and vice-versa.

Where before our boxes were filled with deterministic systems, now they are filled with systems of (first order, ordinary) differential equations. We call these differential systems.

Definition 1.2.2.1. A (first order, ordinary) differential system $\mathrm{S}$ with $n$ state variables, $m$ parameters, and $k$ exposed variables

$$
\left(\begin{array}{c}
\text { update }_{\mathrm{S}} \\
\text { expose }_{\mathrm{S}}
\end{array}\right):\left(\begin{array}{c}
\mathbb{R}^{n} \\
\mathbb{R}^{n}
\end{array}\right) \leftrightarrows\left(\begin{array}{c}
\mathbb{R}^{m} \\
\mathbb{R}^{k}
\end{array}\right)
$$

consists of:
- An $n$-dimensional state space States $=\mathbb{R}^{n}$.
- An m-dimensional parameter space $\ln _{\mathrm{S}}=\mathbb{R}^{m}$.
- A $k$-dimensional space of exposed variable values Out $s=\mathbb{R}^{k}$.
- A smooth function update $\mathbb{R}^{n} \times \mathbb{R}^{m} \rightarrow \mathbb{R}^{n}$ - or equivalently $n$ smooth functions update $_{S_{k}}: \mathbb{R}^{n} \times \mathbb{R}^{m} \rightarrow \mathbb{R}$ - which gives us the derivative of each state variable at each time, so that the defining system of differential equations of $S$ reads

$$
\left\{\begin{aligned}
\frac{d s_{1}}{d t} & =\text { update }_{\mathrm{S} 1}(s, i) \\
& \vdots \\
\frac{d s_{n}}{d t} & =\text { update }_{\mathrm{S} n}(s, i) .
\end{aligned}\right.
$$
- $k$ exposed variables expose $\mathrm{S}_{i}: \mathbb{R}^{n} \rightarrow \mathbb{R}$, which organize into a single smooth function $\operatorname{expose}_{\mathrm{S}}: \mathbb{R}^{n} \rightarrow \mathbb{R}^{k}$.

Remark 1.2.2.2. Definition 1.2.2.1 looks remarkably similar to Definition 1.2.1.2. As we mentioned, Definition 1.2.1.2 can be interpreted in any cartesian category, including the category Euc of Euclidean spaces and smooth maps (Definition 1.2.2.7). It appears that a differential system is the same thing as a deterministic system in the cartesian category Euc. But while the $\mathbb{R}^{n}$ occuring in update ${ }_{S}: \mathbb{R}^{n} \times \mathbb{R}^{m} \rightarrow \mathbb{R}^{n}$ look the same,
they are in fact playing very different roles. The $\mathbb{R}^{n}$ on the left is playing the role of the state space, while the $\mathbb{R}^{n}$ on the right is playing the role of the tangent space at $s$ for some state $s \in \mathbb{R}^{n}$. The difference will be felt in Chapter 3 when we study behaviors of systems: the way a trajectory is defined is different for differential systems and deterministic systems. For differential systems, a trajectory will be a solution to the system of differential equations, that is, a function $s: \mathbb{R} \rightarrow \mathbb{R}^{n}$ which satisfies

$$
\frac{d s}{d t}(t)=\operatorname{update}_{s}(s(t), i(t))
$$

for all choice of times $t$, while for a deterministic system a trajectory would be a sequence $s_{j}$ of states so that $s_{j+1}=$ update $_{\mathrm{S}}\left(s_{j}, i_{j}\right)$.

We will see precisely how this difference is made manifest in the formal definition of a systems theory as the choice of section in Section 3.5.

Remark 1.2.2.3. There are other theories of differential systems that one can define (for example, allowing the state space to be a manifold), but in this book we will work with this simpler systems theory.

Example 1.2.2.4. The system of Rabbits has 1 state variable (the population of rabbits), 2 parameters (the birth and death rates of the rabbits), and 1 exposed variable. It exposes its whole state, so that $\operatorname{expose}_{S}=\mathrm{id}$, and its update is given by

$$
\operatorname{update}_{\text {Rabbits }}\left(r,\left(\mathrm{~b}_{\text {Rabbits }}, \mathrm{d}_{\text {Rabbits }}\right)\right)=\mathrm{b}_{\text {Rabbits }} \cdot r-\mathrm{d}_{\text {Rabbits }} \cdot r \text {. }
$$

The whole Lotka-Voltera model of Eq. (1.18) has 2 state variables (the populations of rabbits and of foxes), 2 parameters (the birth rate of rabbits and the death rate of foxes), and 2 exposed variables. It exposes its whole state, and its update is given by

$$
\text { update }_{\mathrm{LK}}\left(\left[\begin{array}{l}
r \\
f
\end{array}\right],\left(\mathrm{b}_{\text {Rabbits }}, \mathrm{d}_{\text {Foxes }}\right)\right)=\left[\begin{array}{l}
\mathrm{b}_{\text {Rabbits }} \cdot r-c_{1} r f \\
c_{2} f r-\mathrm{d}_{\text {Foxes }} \cdot f
\end{array}\right]
$$

One might wonder why we said this system has 2 parameters when there are also the rate constants $c_{1}$ and $c_{2}$ involved - aren't they also parameters? We chose them to be constant, where our parameters might vary over time. We could have made them parameters instead - it was an arbitrary choice in how to make the model.

Example 1.2.2.5. The most basic epidemiological model is the SIR model. We saw the discrete version of this model in Example 1.2.1.7. Here, let's see the differential version.

The SIR equations model the spread of disease through a population. People are either susceptible $(\mathrm{S})$, infected $(\mathrm{I})$, recovered or more ominously removed $(\mathrm{R})$ from the model. When a susceptible person comes in contact with an infected person, they have a chance to become infected; this means that the population of susceptible people tends downwards in proportion to the number of susceptible and the number of infected
people, and the population of infected people tends up by the same amount. On the other hand, infected people will eventually be removed from the model, either by recovering or dying; this means that the population of infected people tends downwards proportional to the current infected population, while the removed population tends upwards by the same amount. Said as a system of differential equations, this means:

$$
\left\{\begin{array}{l}
\frac{d S}{d t}=-\alpha S I  \tag{1.19}\\
\frac{d I}{d t}=\alpha S I-\beta I \\
\frac{d R}{d t}=\beta I
\end{array}\right.
$$

The SIR model is a differential system with 3 state variables $(S, I$, and $R$ ) and 2 parameters $(\alpha$ and $\beta)$. We will suppose that it exposes its whole state: $\operatorname{expose}_{\text {SIR }}=\mathrm{id}$. The update is given by

$$
\text { update }_{\text {SIR }}\left(\left[\begin{array}{l}
S \\
I \\
R
\end{array}\right],(\alpha, \beta)\right)=\left[\begin{array}{c}
-\alpha S I \\
\alpha S I-\beta I \\
\beta I
\end{array}\right] \text {. }
$$

In order to model higher order systems of ordinary differential equations, we will resort to the standard trick of encoding them as larger systems of first order systems. For example, to encode a second order differential equation in $n$ variables, we would set the state space to be $\mathbb{R}^{2 n}$ with state variables $(s, \dot{s}$ ) (the first $n$ being $s$, the second $n$ being $\dot{s}$ ). We think of $s$ as the actual state variable, and $\dot{s}$ as its formal derivative. We can make this formal derivative an actual derivative by adding the equations update ${ }_{S k}((s, \dot{s}), i):=\dot{s}$ for $1 \leq k \leq n$ to the system $\frac{d \dot{s}}{d t}=$ update $_{S_{k}}((s, \dot{s}), i)$ for $n+1 \leq k \leq 2 n$ of second order differential equations we were trying to model.

Often, we want to think of the state variables $\dot{s}$ as hidden technical tricks. For this reason, we will often only expose the "actual" state variables $s$. This is one use for the function expose ${ }_{S}$.

Example 1.2.2.6. Consider a mass $m$ on a spring with a spring constant of $c$, taking position $s(t)$ at time $t$. Newton's second law then says that the acceleration of the mass is proportional to the force exerted upon it:

$$
\begin{equation*}
m \frac{d^{2} s}{d t}=-c s \tag{1.20}
\end{equation*}
$$

We can express this as a differential system in the following way. We take the state variables to be $s$ and $\dot{s}$ : Statespring $:=\mathbb{R}^{2}$. We will suppose that the mass and the spring constant are constant, so that this system takes no parameters: In ${ }_{\text {Spring }}:=\mathbb{R}^{0}=\{*\}$. We will only expose the position of the spring, and not its velocity: Out ${ }_{\text {Spring }}:=\mathbb{R}$ and
$\operatorname{expose}_{\text {Spring }}(s, \dot{s}):=s$. Finally, the dynamics of the system are given by:

$$
\text { update }_{\text {Spring }}\left(\left[\begin{array}{l}
S \\
\dot{S}
\end{array}\right]\right):=\left[\begin{array}{c}
\dot{S} \\
-\frac{c S}{m}
\end{array}\right]
$$

This is a way of re-writing Eq. (1.20) as a system of first order differential equations:

$$
\left\{\begin{array}{l}
\frac{d s}{d t}=\dot{s} \\
\frac{d \dot{s}}{d t}=-\frac{c s}{m}
\end{array}\right.
$$

Before we go on, we should clarify the category that we are working in when we work with our differential systems.

Definition 1.2.2.7. The category Euc is the category of Euclidean spaces and smooth maps between them. The objects of Euc are $\mathbb{R}^{n}$ for all $n \in \mathbb{N}$, and a morphism $f: \mathbb{R}^{n} \rightarrow \mathbb{R}^{m}$ is a smooth map.

We note that Euc is a cartesian category with $\mathbb{R}^{n} \times \mathbb{R}^{m}=\mathbb{R}^{n+m}$ and $1=\mathbb{R}^{0}$.

\subsection*{1.3 Wiring together systems with lenses}

In the last section, we saw the formal definition of deterministic and differential systems and a few examples of them. In this section, we'll see how to wire systems together - as we did in Section 1.1 for the clock and the universal Turing machine, and in Section 1.2.2 for the Lotka-Volterra predator prey model â€” to make more complex systems. We will do this using an interesting notion coming from the world of functional programming: a lens.

\subsection*{1.3.1 Lenses and lens composition}

A lens is a framework for bi-directional information passing. We will see that lenses are a common generalization of systems and of wiring diagrams.

Definition 1.3.1.1. A lens

$$
\left(\begin{array}{c}
f^{\sharp} \\
f
\end{array}\right):\left(\begin{array}{c}
A^{-} \\
A^{+}
\end{array}\right) \leftrightarrows\left(\begin{array}{l}
B^{-} \\
B^{+}
\end{array}\right)
$$

in a cartesian category $C$ consists of:
- A passforward map $f: A^{+} \rightarrow B^{+}$, and
- a passback map $f^{\sharp}: A^{+} \times B^{-} \rightarrow A^{-}$.

We think of the passforward $f: A^{+} \rightarrow B^{+}$as sending information "downstream", while the passback $f^{\sharp}: A^{+} \times B^{-} \rightarrow A^{-}$sends information back "upstream". But the passback is allowed to use the value in $A^{+}$which is about to flow downstream to calculate how to pass information back upstream.

The most useful thing about lenses is that they compose.

Definition 1.3.1.2. Let $\left(\begin{array}{c}f^{\sharp} \\ f\end{array}\right):\left(\begin{array}{c}A^{-} \\ A^{+}\end{array}\right) \leftrightarrows\left(\begin{array}{c}B^{-} \\ B^{+}\end{array}\right)$and $\left(\begin{array}{c}g^{\sharp} \\ g\end{array}\right):\left(\begin{array}{c}B^{-} \\ B^{+}\end{array}\right) \leftrightarrows\left(\begin{array}{l}C^{-} \\ C^{+}\end{array}\right)$be lenses in a cartesian category $C$. We define their composite

$$
\left(\begin{array}{l}
g^{\sharp} \\
g
\end{array}\right) \circ\left(\begin{array}{l}
f^{\sharp} \\
f
\end{array}\right)
$$

to have passforward $g \circ f$ and passback

$$
\left(a^{+}, c^{-}\right) \mapsto f^{\sharp}\left(a^{+}, g^{\sharp}\left(f\left(a^{+}\right), c^{-}\right)\right) .
$$

Here's a picture so that you can see the information flow for the composite of lenses:2
![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-032.jpg?height=182&width=938&top_left_y=1031&top_left_x=584)

Remark 1.3.1.3. Even though our definition of lens was given in an arbitrary cartesian category $C$, we felt comfortable defining it in terms of elements. Going forward, we will also reason with it using elements. This trick works for any cartesian category by using "generalized elements". We interpret an "element" $x$ in an object $X$ as a map $x: Z \rightarrow X$. If we do work with $x$ to get a new element $f(x)$ of $Y$, then by the Yoneda lemma there is a map $f: X \rightarrow Y$ in the category which does that work by post-composition: $f(x)=f \circ x$. At least, so long as that work we do is natural in $x$, which means that it could be done just as well if we substituted $x$ for anything else.

The take-away is that even in a totally arbitrary cartesian category whose objects are not sets of any kind, we can still reason about them as if they were - at least when it comes to pairing elements and applying functions.

This gives us a category of lenses in any cartesian category $C$.

Definition 1.3.1.4. Let $C$ be a cartesian category. Then the category Lens $_{C}$ has:
- as objects, the pairs $\left(\begin{array}{l}A^{-} \\ A^{+}\end{array}\right)$of objects in $C$, which we will call arenas.
- as morphisms, the lenses $\left(\begin{array}{c}f^{\sharp} \\ f\end{array}\right):\left(\begin{array}{c}A^{-} \\ A^{+}\end{array}\right) \leftrightarrows\left(\begin{array}{c}B^{-} \\ B^{+}\end{array}\right)$.
- The identity lens is $\left(\begin{array}{c}\pi_{2} \\ \mathrm{id}\end{array}\right):\left(\begin{array}{l}A^{-} \\ A^{+}\end{array}\right) \leftrightarrows\left(\begin{array}{l}A^{-} \\ A^{+}\end{array}\right)$, where $\pi_{2}: A^{+} \times A^{-} \rightarrow A^{-}$is the projection.

Composition is given by lens composition as in Definition 1.3.1.2.
\footnotetext{
${ }^{2}$ We draw this with a different style-green boxes, etc.-so that the reader will not confuse it with our usual wiring diagrams for systems. These are not dynamic in any way; every wire is a set and every bead on that wire is a function.
}

Remark 1.3.1.5. The category of lenses is special among categories because it is named for its maps (which are the lenses), rather than its objects (which are the arenas). This is because we will later meet another category, the category of charts (See Definition 3.3.0.13), whose objects are the arenas but whose maps are not lenses. Finally, in Definition 3.4.1.1 we will meet a double category ${ }^{3}$ Arena $_{C}$ which combines these two categories whose objects are arenas and which is named after its objects. In Section 3.4.1, we will explain the name "arena" and its role in the theory of dynamical systems.

\section*{Exercise 1.3.1.6.}

1. Draw the composite of two lenses in the style of (1.21) â€” that is, with the sets as wires and the functions as beads on those wires.

2. Check that Lens $_{C}$ is actually a category. That is, check that lens composition is associative, and that the identity lens is an identity for it. (Hint: You can use your drawing for this. You can slide the function beads around on the strings; if you pull a function bead past a split in the string, you have to duplicate it (since that split represents the duplication function).)

Like any good categorical construction, Lens ${ }_{C}$ varies functorially in its variable cartesian category $C$.

Proposition 1.3.1.7 (Functoriality of Lens). Every cartesian functor $F: C \rightarrow \mathscr{D}$ induces a functor $\left(\begin{array}{l}F \\ F\end{array}\right):$ Lens $_{C} \rightarrow$ Lens $_{\mathscr{D}}$ given by

$$
\left(\begin{array}{c}
F \\
F
\end{array}\right)\left(\begin{array}{c}
f^{\sharp} \\
f
\end{array}\right)=\left(\begin{array}{c}
F f^{\sharp} \circ \mu^{-1} \\
F f
\end{array}\right)
$$

where $\mu=\left(F \pi_{1}, F \pi_{2}\right): F(X \times Y) \xrightarrow{\sim} F X \times F Y$ is the isomorphism witnessing that $F$ preserves products.

Proof Sketch. Because lenses are defined just using the cartesian product, and $F$ preserves these products, it commutes with everything in sight.

\section*{Exercise 1.3.1.8.}

1. What does the functor $\left(\begin{array}{l}F \\ F\end{array}\right):$ Lens $_{C} \rightarrow$ Lens $_{\perp}$ do on objects?

2. Complete the proof of Proposition 1.3.1.7, by showing that $\left(\begin{array}{l}F \\ F\end{array}\right)$ really is a functor.
\footnotetext{
${ }^{3}$ A double category is like a category with two different kinds of morphisms and a way for them to commute. See Definition 3.4.0.1 for the precise definition and the accompanying discussion.
}

\subsection*{1.3.2 Deterministic and differential systems as lenses}

The reason we are interested in lenses and lens composition is because dynamical systems of various sorts are themselves lenses. As written in Definition 1.2.1.2, a system $\mathrm{S}$ is a lens in the category of sets of the form

$$
\left(\begin{array}{c}
\text { update }_{\mathrm{S}} \\
\text { expose }_{\mathrm{S}}
\end{array}\right):\left(\begin{array}{l}
\text { State }_{\mathrm{S}} \\
\text { State }_{\mathrm{S}}
\end{array}\right) \leftrightarrows\left(\begin{array}{c}
\ln _{\mathrm{S}} \\
\text { Out }_{\mathrm{S}}
\end{array}\right)
$$

In fact, the deterministic systems are precisely the lenses whose input arena is of the form $\left(\begin{array}{l}S \\ S\end{array}\right)$. This means that we can compose a system $S$ with a lens $\left(\begin{array}{c}f^{\sharp} \\ f\end{array}\right):\left(\begin{array}{c}\ln s \\ \text { Outs }\end{array}\right) \leftrightarrows\left(\begin{array}{c}I \\ O\end{array}\right)$ to get a new dynamical system

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-034.jpg?height=145&width=601&top_left_y=841&top_left_x=754)

with a new interface! We will see that wiring diagrams are a special sort of lenses too in the upcoming Section 1.3.3, so that wiring together systems will be an instance of lens composition.

Similarly, a differential system is a lens in the category Euc (Definition 1.2.2.7) of the form

$$
\left(\begin{array}{c}
\text { update }_{\mathrm{S}} \\
\text { expose }_{\mathrm{S}}
\end{array}\right):\left(\begin{array}{l}
\mathbb{R}^{n} \\
\mathbb{R}^{n}
\end{array}\right) \leftrightarrows\left(\begin{array}{l}
\mathbb{R}^{m} \\
\mathbb{R}^{k}
\end{array}\right)
$$

We can then compose this with lenses in Euc to get new differential systems!

We can use this observation to wire together different systems. We separate this into two phases: first we put two systems in parallel, then we wire them together using a lens. It's far from obvious that wiring diagrams are lenses, but we'll see precisely how they are in Section 1.3.3 and describe the second phase there.

The first phase â€” combine two systems without having them interact â€” is achieved through what we call the parallel product and denote $\otimes$. To put two arenas $\left(\begin{array}{l}A_{1} \\ B_{1}\end{array}\right)$ and $\left(\begin{array}{l}A_{2} \\ B_{2}\end{array}\right)$ in parallel we just take their product in our cartesian category $C$ :

$$
\left(\begin{array}{l}
A_{1} \\
B_{1}
\end{array}\right) \otimes\left(\begin{array}{l}
A_{2} \\
B_{2}
\end{array}\right):=\left(\begin{array}{c}
A_{1} \times A_{2} \\
B_{1} \times B_{2}
\end{array}\right)
$$

In Definition 1.3.2.1 we define parallel product for morphisms in Lens, i.e. for general lenses.

Definition 1.3.2.1. For lenses $\left(\begin{array}{c}f^{\sharp} \\ f\end{array}\right):\left(\begin{array}{c}A_{1} \\ B_{2}\end{array}\right) \leftrightarrows\left(\begin{array}{c}C_{1} \\ D_{1}\end{array}\right)$ and $\left(\begin{array}{c}g^{\sharp} \\ g\end{array}\right):\left(\begin{array}{c}A_{2} \\ B_{2}\end{array}\right) \leftrightarrows\left(\begin{array}{c}C_{2} \\ D_{2}\end{array}\right)$, we define their parallel product

$$
\left(\begin{array}{c}
f^{\sharp} \\
f
\end{array}\right) \otimes\left(\begin{array}{c}
g^{\sharp} \\
g
\end{array}\right):\left(\begin{array}{c}
A_{1} \times A_{2} \\
B_{1} \times B_{2}
\end{array}\right) \leftrightarrows\left(\begin{array}{c}
C_{1} \times C_{2} \\
D_{1} \times D_{2}
\end{array}\right)
$$
to have passforward $f \times g$ and passback

$$
\left(\left(b_{1}, b_{2}\right),\left(c_{1}, c_{2}\right)\right) \mapsto\left(f^{\sharp}\left(b_{1}, c_{1}\right), g^{\sharp}\left(b_{2}, c_{2}\right)\right) .
$$

In terms of morphisms, this is

$$
\left(B_{1} \times B_{2}\right) \times\left(C_{1} \times C_{2}\right) \xrightarrow{\sim}\left(B_{1} \times C_{1}\right) \times\left(B_{2} \times C_{2}\right) \xrightarrow{f^{\sharp} \times g^{\sharp}} A_{1} \times A_{2} .
$$

Together with $\left(\begin{array}{l}1 \\ 1\end{array}\right)$, this gives Lens $_{C}$ the structure of a monoidal category.

Remark 1.3.2.2. We will show a slick way to prove that the parallel product does indeed make Lens $_{C}$ into a monoidal category in Section 4.3.

Exercise 1.3.2.3. Show the parallel product of morphisms as in Definition 1.3.2.1 using the string diagram notation from (1.21).

Proposition 1.3.2.4. Let $F: C \rightarrow \mathscr{D}$ be a cartesian functor. The induced functor $\left(\begin{array}{l}F \\ F\end{array}\right):$ Lens $_{C} \rightarrow$ Lens $_{\mathscr{D}}$ is strong monoidal with respect to the parallel product - it preserves the monoidal product $\otimes$.

Proof. Since $F$ preserves products, we have that

$$
\begin{aligned}
F\left(\left(\begin{array}{l}
A^{-} \\
A^{+}
\end{array}\right) \otimes\left(\begin{array}{l}
B^{-} \\
B^{+}
\end{array}\right)\right) & =F\left(\begin{array}{l}
A^{-} \times B^{-} \\
A^{+} \times B^{+}
\end{array}\right) \\
& =\left(\begin{array}{l}
F\left(A^{-} \times B^{-}\right) \\
F\left(A^{+} \times B^{+}\right)
\end{array}\right) \\
& \cong\left(\begin{array}{l}
F A^{-} \times F B^{-} \\
F A^{+} \times F B^{+}
\end{array}\right) \\
& =F\left(\begin{array}{l}
A^{-} \\
A^{+}
\end{array}\right) \otimes F\left(\begin{array}{l}
B^{-} \\
B^{+}
\end{array}\right) .
\end{aligned}
$$

Given two dynamical systems $S_{1}$ and $S_{2}$, their parallel product $S_{1} \otimes S_{2}$ is defined explicitly as follows:
- State ${ }_{1} \otimes \mathrm{S}_{2}:=$ State $_{1} \times$ State $_{2}$.
- Out $_{S_{1} \otimes S_{2}}:=$ Out $_{S_{1}} \times$ Out $_{S_{2}}$.
- $\ln _{\mathrm{S}_{1} \otimes \mathrm{S}_{2}}:=\ln _{\mathrm{S}_{1}} \times \ln _{\mathrm{S}_{2}}$.
- $\operatorname{expose}_{\mathrm{S}_{1} \otimes \mathrm{S}_{2}}\left(\left(s_{1}, s_{2}\right)\right)=\left(\operatorname{expose}_{\mathrm{S}_{1}}\left(s_{1}\right), \operatorname{expose}_{\mathrm{S}_{2}}\left(s_{2}\right)\right)$.
- update $_{\mathrm{S}_{1} \otimes \mathrm{S}_{2}}\left(\left(s_{1}, s_{2}\right),\left(i_{1}, i_{2}\right)\right)=\left(\right.$ update $_{\mathrm{S}_{1}}\left(s_{1}, i_{1}\right)$, update $\left.\mathrm{S}_{\mathrm{S}_{2}}\left(s_{2}, i_{2}\right)\right)$.

This can be expressed as the following wiring diagram:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-036.jpg?height=334&width=285&top_left_y=321&top_left_x=909)

If we imagine physically wiring together our boxes, the first thing we would need to do is collect them together like this; then we can proceed to wire them. We will do exactly this with our systems: first we will take their parallel product, and then we compose it with a lens that represents the wiring diagram.

Example 1.3.2.5. We can describe the ClockWithDisplay system (reproduced below) as a composite of lenses.

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-036.jpg?height=383&width=512&top_left_y=1039&top_left_x=796)

First, we take the parallel product of Meridiem and Clock (see Exercise 1.2.1.6) to get the system

$$
\text { Meridiem } \otimes \text { Clock : }\left(\begin{array}{l}
\text { a.m./p.m. } \times \text { Hour } \\
\text { a.m./p.m. } \times \text { Hour }
\end{array}\right) \leftrightarrows\left(\begin{array}{c}
1 \times \text { Hour } \\
\text { a.m. } / \text { p.m. } \times \text { Hour }
\end{array}\right)
$$

Now, we will express the wiring pattern in Eq. (1.23) as a lens

$$
\left(\begin{array}{c}
w^{\sharp} \\
w
\end{array}\right):\left(\begin{array}{c}
1 \times \text { Hour } \\
\text { a.m./p.m. } \times \text { Hour }
\end{array}\right) \leftrightarrows\left(\begin{array}{c}
1 \\
\text { a.m./p.m. } \times \text { Hour }
\end{array}\right)
$$

We do this by setting

$$
\begin{aligned}
w(m, h) & :=(m, h), \text { and } \\
w^{\sharp}((m, h), *) & :=(*, h) .
\end{aligned}
$$

Seen as a wiring diagram on its own, $\left(\begin{array}{c}w^{\sharp} \\ w\end{array}\right)$ looks like this:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-037.jpg?height=442&width=550&top_left_y=348&top_left_x=777)

We can then see that

$$
\text { ClockWithDisplay }=\left(\begin{array}{c}
w^{\sharp} \\
w
\end{array}\right) \circ(\text { Meridiem } \otimes \text { Clock })
$$

just like we wanted! In terms of wiring diagrams, this looks like:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-037.jpg?height=626&width=1328&top_left_y=1167&top_left_x=344)

Example 1.3.2.6. We can describe the Lotka-Volterra predator prey model (reproduced below) as a composite of lenses.

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-037.jpg?height=374&width=533&top_left_y=2019&top_left_x=796)

We can express the wiring pattern in Eq. (1.26) as a lens

$$
\left(\begin{array}{c}
w^{\sharp} \\
w
\end{array}\right):\left(\begin{array}{c}
\mathbb{R}^{2} \\
\mathbb{R}
\end{array}\right) \otimes\left(\begin{array}{l}
\mathbb{R}^{2} \\
\mathbb{R}
\end{array}\right) \leftrightarrows\left(\begin{array}{l}
\mathbb{R}^{2} \\
\mathbb{R}^{2}
\end{array}\right)
$$

We do this by setting

$$
\begin{aligned}
w(r, f) & :=(r, f) \\
w^{\sharp}((r, f),(a, b)) & :=\left(a, c_{2} f, c_{1} r, b\right)
\end{aligned}
$$

We can draw $\left(\begin{array}{c}w^{\sharp} \\ w\end{array}\right)$ as a wiring diagram on its own like this:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-038.jpg?height=374&width=533&top_left_y=913&top_left_x=796)

Filling those boxes with the systems of Rabbits and Foxes corresponds to taking the composite

$$
(\text { Rabbits } \otimes \text { Foxes }) \stackrel{\circ}{ }\left(\begin{array}{c}
w^{\sharp} \\
w
\end{array}\right)
$$

of lenses.

Wiring together transition diagrams. When a deterministic system is presented as a transition diagram (See Example 1.2.1.8), its dynamics are given by reading the input and following the arrow with that label, and then outputting the label on the resulting node. When we wire together systems presented as transition diagrams, the dynamics then involve reading the input labels of all inner systems, moving along all the arrows with those labels, and then outputing the labels at each state, possible into the input of another system.

Exercise 1.3.2.7. Here are two systems, $\mathrm{S}_{1}$ and $\mathrm{S}_{2}$ presented in terms of transition diagrams. The task is calculate the transition diagram of a system made by wiring them together.

First, let Colors $=\{$ red, blue, green $\}$ and let Bool $=\{$ true, false $\}$. Here is our first system $\mathrm{S}_{1}$, which has interface $\left(\begin{array}{c}\text { Bool } \\ \text { Colors }\end{array}\right)$ :

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-039.jpg?height=421&width=767&top_left_y=272&top_left_x=668)

Our second system $\mathrm{S}_{2}$ will have interface $\left(\begin{array}{c}\text { Colors } \\ \text { Bool }\end{array}\right)$ :

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-039.jpg?height=515&width=811&top_left_y=808&top_left_x=646)

1. Write down the transition diagram of the system obtained by connecting the above systems according to the following wiring diagram:

$$
S:=5 \mathrm{~S}_{1}
$$

2. Explain how to understand the dynamics of this $S$ in terms of the component systems $\mathrm{S}_{1}$ and $\mathrm{S}_{2}$.

Multi-city SIR models In Examples 1.2.1.7 and 1.2.2.5, we saw deterministic and differential SIR models. Each models the spread of a disease through a single population. But what about a global pandemic where the disease is spreading through many local populations?

To model the spread of a disease through many different populations, we can use what is called a multi-city SIR model. We call each population a "city", and for now we will take flow of population between each city to be known constants. We can define a city as a differential system; then certain wiring diagrams of cities will correspond to multi-city models!

Definition 1.3.2.8. A City in a multi-city SIR model is a differential system

$$
\begin{equation*}
\mathbb{R}^{3}-\text { City } \mathbb{R}^{3} \tag{1.30}
\end{equation*}
$$

A city is defined by:
- State $_{\text {city }}:=\left\{\left.\left[\begin{array}{l}S \\ I \\ R\end{array}\right] \right\rvert\, S, I, R \in \mathbb{R}\right\}=\mathbb{R}^{3}$.
- $\operatorname{In}_{\text {City }}=\left\{(\right.$ inflow, outflow $) \mid$ inflow, outflow $\left.\in \mathbb{R}^{3}\right\}=\mathbb{R}^{3} \times \mathbb{R}^{3}$
- Out $_{\text {City }}=$ State $_{\text {City }}=\mathbb{R}^{3}$.
- expose $_{\mathrm{S}}=\mathrm{id}$.

$$
\operatorname{update}_{\mathrm{S}}\left(\left[\begin{array}{l}
S \\
I \\
R
\end{array}\right],(\text { inflow, outflow })\right):=\left[\begin{array}{c}
-k_{1} S I+\text { inflow } \text { outflow }_{1} \\
k_{1} S I-k_{2} I+\text { inflow }- \text { outflow } 2 \\
k_{1} I+\text { inflow } \text { outflow }_{3}
\end{array}\right]
$$

for some choice of constants $k_{1}$ and $k_{2}$.

That is, each city will run its own SIR model, and each of the three populations can flow between cities.

Now, to define a multi-city SIR model, we need to know what cities we are dealing with and how population flows between them. We'll call this a population flow graph.

Definition 1.3.2.9. A population-flow graph (for a multi-city SIR model) is a graph whose nodes are labeled by cities and whose edges City $_{1} \rightarrow$ City $_{2}$ are labeled by $3 \times 3$ real diagonal matrices Flow $_{1} \rightarrow 2$ of the following form:

$$
\left[\begin{array}{ccc}
r_{S} & 0 & 0 \\
0 & r_{I} & 0 \\
0 & 0 & r_{R}
\end{array}\right]
$$

Example 1.3.2.10. Let's take a minute to understand Definition 1.3.2.9. Here is an example of a network of cities, represented in a graph:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-040.jpg?height=230&width=610&top_left_y=2086&top_left_x=752)

This map contains three cities, Boston, NYC, and Tallahassee. As we can see, Boston and NYC have restricted access to travellers from Tallahassee, but otherwise people can
travel freely. Let's focus in on one of these ways to travel, say Boston $\rightarrow$ NYC. This is associated to a matrix

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-041.jpg?height=197&width=563&top_left_y=324&top_left_x=773)

per the definition of a population flow graph. Here's how to understand this matrix. If the current population of Boston (split into susceptible, infected, and removed) is $s=\left[\begin{array}{l}S \\ I \\ R\end{array}\right]$, then

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-041.jpg?height=205&width=769&top_left_y=781&top_left_x=667)

is the population that will leave Boston and arrive in NYC. Of course, this assumes that people do not become sick in transit, a temporary assumption that a more robust model would have to address.

Given a population flow graph, we can form a multi-city SIR model by wiring together the cities in a particular way. Namely, to every city we will first add sums to its inputs for every city it is flowing to and every that flows to it. That is, we will prepare each city like so:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-041.jpg?height=339&width=387&top_left_y=1381&top_left_x=869)

Specifically, we need to add together all the inflows from all other cities, and then record all the outflows to all other cities. We also need to copy the state enough times so that it can be passed to all other cities that our city flows to. So we need to add together inputs for all incoming edges in the population flow graph to the inflow port, and add together inputs for all outgoing edges in the population flow graph to the outflow port. And we also need to copy the output port to for all outgoing edges.

Example 1.3.2.11. For example, here is the preparation necessary for Boston in Eq. (1.31):

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-042.jpg?height=388&width=401&top_left_y=321&top_left_x=859)

As you can see, there is only one incoming edge, and so the inflow input port doesn't need to anything to be added. But there are two outgoing edges, so we need to copy the output so they can be passed to NYC and Tallahassee and add together the two outflows into the outflow input port of Boston.

Exercise 1.3.2.12. Prepare the cities of NYC and Tallahassee from Eq. (1.31) in the same way Boston was prepared in Example 1.3.2.11.

Next, we wire together these prepared cities (from Eq. (1.32)). For each edge City ${ }_{1} \rightarrow$ $\mathrm{City}_{2}$ in our population flow graph, we will put the matrix Flow $\mathrm{City}_{1} \rightarrow \mathrm{City}_{2}$ on the wire leaving the prepared City $_{1}$ corresponding to the edge, then split the wire and plug one end into the corresponding outflow input port of City $_{1}$ and the corresponding inflow input port of City $_{2}$.

Example 1.3.2.13. Here is what it looks like to wire Boston to NYC along the edge

Boston $\rightarrow$ NYC in the population flow graph Eq. (1.31):

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-043.jpg?height=854&width=444&top_left_y=321&top_left_x=838)

This wiring diagram says to take the population of Boston, take the proportion given

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-043.jpg?height=51&width=1453&top_left_y=1254&top_left_x=325)
and the inflow parameter of NYC.

\subsection*{1.3.3 Wiring diagrams as lenses in categories of arities}

We have been drawing a bunch of wiring diagrams so far, and we will continue to do so throughout the rest of the book. Its about time we explicitly described the rules one uses to draw these diagrams, and give a formal mathematical definition of them. The motto of this section is:

A wiring diagram is a lens in a free cartesian category - a category of arities.

We'll begin by describing wiring diagrams and their category in informal terms. Then, we will see how diagrams relate to lenses in a particular category - which we call the category of arities - and finally give a formal definition of the category of wiring diagrams.

Informal Definition 1.3.3.1. A wiring diagram is a diagram which consists of a number of inner boxes, each with some input ports and some output ports, that are wired together inside an outer box, which also has input and output ports. This gives four types of ports: inner (box) input (port), inner output, outer input, and outer output.

We can wire in the following ways:

1. Every outer output port is wired to exactly one inner output port.

2. Every inner input port is wired to exactly one inner output port or an outer input
port.

The category of wiring diagrams has boxes as its objects and wiring diagrams as its morphisms. Wiring diagrams are composed by filling the inner boxes with other wiring diagrams, and then erasing the middle layer of boxes.

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-044.jpg?height=458&width=1022&top_left_y=492&top_left_x=541)

Wiring diagrams are designed to express the flow of variables through the system; how they are to be copied from one port to another, how they are to be shuffled about, and (though we haven't had need for this yet) how they are to be deleted or forgotton.

In order to capture this idea of copying, deleting, and shuffling around variables, we will work with the category of arities (and variations on it). The category of arities is extremely important since it captures precisely the algebra of copying, deleting, and shuffling around variables. In this section, we will interpret various sorts of wiring diagrams as lenses in categories of arities, which are the free cartesian categories.

Definition 1.3.3.2. The category Arity of arities is the free cartesian category generated by a single object $X$. That is, Arity constains an object $X$, called the generic object, and for any finite set $I$, there is an $I$-fold power $X^{I}$ of $X$. The only maps are those that can be defined from the product structure by pairing and projection.

Explicitly, Arity is has:
- Objects $\left\{X^{I} \mid I\right.$ a finite set $\}$.
- Maps $f^{*}: X^{I} \rightarrow X^{J}$ for any function $f: J \rightarrow I$.
- Composition defined by $g^{*} \circ f^{*}:=(f \circ g)^{*}$ and $\mathrm{id}:=\mathrm{id}^{*}$.

The cartesian product in Arity is given, in terms of index sets, by the following familiar formula:

$$
X^{I} \times X^{J}=X^{I+J} .
$$

If you like opposite categories, this might clarify things a bit.

Proposition 1.3.3.3. Arity is isomorphic to the opposite of the category finite sets

$$
\text { Arity } \cong \text { FinSet }^{\mathrm{op}}
$$

Now, $\mathrm{X}$ is just a formal object, so it doesn't have elements. But we can give a language for writing down the objects and arrows of Arity that makes it look like it does. Think
of the elements of $X^{I}$ as finite lists of variables $X^{I}=\left(x_{i} \mid i \in I\right)$ indexed by the set $I$. Then for any reindexing function $f: J \rightarrow I$, we can see $f^{*}$ as telling us how $J$-variables are assigned $I$-variables. We can see this as a $J$-indexed list of the variables $x_{i}$. For example, consider the function $f: 3 \rightarrow 2$ given by $1 \mapsto 1,2 \mapsto 1$, and $3 \mapsto 2$

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-045.jpg?height=211&width=347&top_left_y=480&top_left_x=886)

In other words, $f$ says that the first slot of the resulting list will be filled by the second variable of the first, and the second slot will be filled by the first variable, and the third slot will be filled by the second variable. We could write these lists of variables as $\left(x_{1}, x_{2}\right) \mapsto\left(x_{1}, x_{1}, x_{2}\right)$ to make it look like a function. We'll call this the function notation.

Composition is just given by composing functions in the opposite direction. For example, given some $g: 4 \rightarrow 3$, we just compose to get our map $X^{2} \rightarrow X^{4}$.
![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-045.jpg?height=274&width=984&top_left_y=1039&top_left_x=558)

If we write both of these in function notation, then we can see that the composite can be calculated by just "composing the functions". The map $f^{*}: X^{3} \rightarrow X^{2}$ looks like $\left(x_{1}, x_{2}\right) \mapsto\left(x_{1}, x_{1}, x_{2}\right)$ in function notation, and the map $g^{*}: X^{4} \rightarrow X^{3}$ looks like $\left(y_{1}, y_{2}, y_{3}\right) \mapsto\left(y_{1}, y_{1}, y_{3}, y_{2}\right)$. Their composite would look like $\left(x_{1}, x_{2}\right) \mapsto\left(x_{1}, x_{1}, x_{2}, x_{1}\right)$, and this is precisely the composite $(g \circ f)^{*}$.

Exercise 1.3.3.4. Express the following morphisms in Arity in terms of lists of variables:

1. The terminal morphism $X^{2} \rightarrow X^{0}$, given by the initial function !' : $0 \rightarrow 2$ which includes empty set into the set with two elements (hint, there's nothing on one side).

2. The duplication morphism $!^{*}: X \rightarrow X^{2}$ given by $!: 2 \rightarrow 1$.

3. The swap morphisms swap* : $X^{2} \rightarrow X^{2}$ given by swap : $2 \rightarrow 2$ defined by $0 \mapsto 1$ and $1 \mapsto 0$.

4. What map corresponds to the map $1: 1 \rightarrow 2$ picking out $1 \in 2=\{1,2\}$ ? What about $2: 1 \rightarrow 2$.

5. Convince yourself that any map $X^{I} \rightarrow X^{J}$ you can express with the universal property of products can be expressed by choosing an appropriate $f: J \rightarrow I$.

Because Arity expresses the algebra of shuffling, copying, and deleting variables in the abstract, we can use it to define wiring diagrams. Recall from Definition 1.3.1.4 the definition of lens in an arbitrary cartesian category.

Definition 1.3.3.5. The category WD of wiring diagrams is defined to be the category of lenses in the category of arities Arity.

$$
\text { WD := Lens Arity }
$$

We consider WD as a monoidal category in the same way we consider Lens $_{\text {Arity }}$ as a monoidal category.

This definition shows us that the wiring diagrams we have been using are precisely the lenses you can express if you only copy, delete, and shuffle around your variables.

We can read any wiring diagram as a lens in Arity in the following way:
![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-046.jpg?height=396&width=1088&top_left_y=856&top_left_x=386)

Here's how we interpret a lens $\left(\begin{array}{c}w^{\sharp *} \\ w^{*}\end{array}\right):\left(\begin{array}{c}X^{A^{-}} \\ X^{A^{+}}\end{array}\right) \leftrightarrows\left(\begin{array}{c}X^{B^{-}} \\ X^{B^{+}}\end{array}\right)$in Arity as a wiring diagram:
- First, we interpret the index set $A^{-}$as the set of input ports of the inner boxes, and the set $A^{+}$as the set of output ports of the inner boxes. Similarly, we see $B^{-}$ as the set of input ports of the outer box, and $B^{+}$as the set of output ports of the outer box.
- Then we remember that $w^{*}: X^{A^{+}} \rightarrow X^{B^{+}}$comes from a reindexing function $\left.w: B^{+} \rightarrow A^{+}\right)$, which we interpret as selecting for each outer output port $p \in B^{+}$, the unique inner output port $w(p)$ it will be wired to.
- Finally, we note that $w^{\sharp *}: X^{A^{+}} \times X^{B^{-}} \rightarrow X^{A^{-}}$comes from a function $w^{\sharp}: A^{-} \rightarrow$ $A^{+}+B^{-}$(because $\mathrm{X}^{A^{+}} \times \mathrm{X}^{B^{-}}=\mathrm{X}^{A^{+}+B^{-}}$, where $A^{+}+B^{-}$is the disjoint union of $A^{+}$ and $B^{-}$), and we interpret this as selecting for each inner input port $p \in A^{-}$either the inner output port $w^{\sharp}(p) \in A^{+}$or the outer input port $w^{\sharp}(p) \in B^{-}$which $p$ will be wired to.

Exercise 1.3.3.6. Translate the following wiring diagrams into lenses in the category of arities, and vice versa:

1.

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-046.jpg?height=295&width=764&top_left_y=2178&top_left_x=716)
2. $\left(\begin{array}{c}w^{\sharp} \\ w\end{array}\right):\left(\begin{array}{c}\mathrm{X}^{2} \times \mathrm{X}^{1} \times \mathrm{X}^{2} \\ \mathrm{X} \times \mathrm{X} \times \mathrm{X}^{2}\end{array}\right) \leftrightarrows\left(\begin{array}{c}\mathrm{X}^{2} \\ \mathrm{X}^{1}\end{array}\right)$
![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-047.jpg?height=592&width=464&top_left_y=346&top_left_x=863)

$\mathrm{Ok}$, so the wiring diagrams correpond to the lenses in the category of arities. But do they compose in the same way? Composition of wiring diagrams is given by nesting: to compute $\left(\begin{array}{c}w^{\sharp} \\ w\end{array}\right):\left(\begin{array}{c}u^{\sharp} \\ u\end{array}\right)$, we fill in the inner box of $\left(\begin{array}{c}u^{\sharp} \\ u\end{array}\right)$ with the outer box of $\left(\begin{array}{c}w^{\sharp} \\ w\end{array}\right)$, and then remove this middle layer of boxes.
![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-047.jpg?height=562&width=1646&top_left_y=1296&top_left_x=324)

Let's say in prose how to compose two wiring diagrams. Then, we can check that this matches the formula given to us by lens composition in Arity.
- An outer output port is wired to a middle output port, and this middle output port is wired to an inner input port. So, to compose, we wire the outer output port to this inner output port.
- A inner input port is either wired to an inner input port or a middle input port. If it is wired to an inner input port, we leave it that way. Suppose that it was instead wired to a middle input port. This middle input port is wired either to a middle output port or an outer input port. If it is wired to an outer input port, we then wire the inner input port to this outer input port. But if it was wired to a middle output port, we need to follow along to the inner output port that it is wired to; then we wire the inner input port to this inner output port.

Phew. After that block of text, I hope the mathematics will feel refreshingly crisp. Let's see what the lens composition looks like in Arity:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-048.jpg?height=539&width=770&top_left_y=392&top_left_x=667)

It's worth going through and seeing exactly how lens composition expresses the description we gave of nesting wiring diagrams above.

That Arity is the free cartesian category generated by a single object means that it satisfies a very useful universal property.

Proposition 1.3.3.7 (Universal property of Arity). For any cartesian category $C$ and object $C \in C$, there is a cartesian functor $\mathrm{ev}_{C}$ : Arity $\rightarrow C$ which sends $X$ to $C$. This functor is the unique such functor up to a unique natural isomorphism.

Proof Sketch. The functor $\mathrm{ev}_{C}$ can be defined by "just substitute $C$ for $X^{\prime}$. Namely, we send

$$
X^{I} \mapsto C^{I}
$$

and for every function $f^{*}: \mathrm{X}^{I} \rightarrow \mathrm{X}^{J}$, we send it to $f^{*}: C^{I} \rightarrow C^{J}$ defined by the universal property of the product in $C$. This is cartesian because $C^{I+J} \cong C^{I} \times C^{J}$ in any cartesian category. It is unique up to a unique natural isomorphism because $X^{I}$ is the $I$-fold product of $X$, and so if $X \mapsto C$, then universal comparison maps between the image of $X^{I}$ and $C^{I}$ must be isomorphisms.

We can think of the functor $\mathrm{ev}_{C}$ : Arity $\rightarrow C$ as the functor which tells us how to interpret the abstract variables in Arity as variables of type $C$. For example, the functor $\mathrm{ev}_{\mathbb{R}}:$ Arity $\rightarrow$ Set tells us how to interpret the abstract variables $\left(x_{i} \mid i \in I\right)$ in Set as variable real numbers $\left\{x_{i} \in \mathbb{R} \mid i \in I\right\}$. Under $\mathrm{ev}_{C}$, the map of arities $\left(x_{1}, x_{2}, x_{3} \mapsto x_{2}, x_{2}\right)$ gets sent to the actual map $C^{3} \rightarrow C^{2}$ given by sending $\left(c_{1}, c_{2}, c_{3}\right)$ to $\left(c_{2}, c_{2}\right)$.

By the functoriality of the lens construction, this means that given an object $C \in$ $C$ of a cartesian category of "values that should be flowing on our wires", we can interpret a wiring diagram as a lens in $C$ ! We record this observation in the following proposition.

Proposition 1.3.3.8. Let $C \in C$ be an object of a cartesian category. Then there is a strong monoidal functor

$$
\left(\begin{array}{l}
\mathrm{ev}_{C} \\
\mathrm{ev}_{C}
\end{array}\right): \mathbf{W D} \rightarrow \text { Lens }_{C}
$$

which interprets a wiring diagram as a lens in $C$ with values in $C$ flowing along its wires.

Proof. This is just Proposition 1.3.1.7 (and Proposition 1.3.2.4) applied to $\mathrm{ev}_{C}:$ Arity $\rightarrow$ e from Proposition 1.3.3.7.

The upshot of Proposition 1.3.3.8 is that we may interpret a wiring diagram as a lens in whatever cartesian category we are working in. There is, however, a slight issue; in most of our previous examples, there have been many different types of signals flowing along the wires. We can fix this by using typed arities. We will keep track of what type of signal is flowing along each wire, and only allow ourselves to connect wires that carry the same type of signal.

Definition 1.3.3.9. Let $\mathfrak{T}$ be a set, elements of which we call types. The category Arity $\mathcal{T}_{\mathcal{T}}$ is the free cartesian category generated by objects $\mathrm{X}_{\tau}$ for each type $\tau \in \mathfrak{T}$. Explicitly, Arity $_{\mathfrak{T}}$ has:
- Objects $\prod_{i \in I} X_{\tau_{i}}$ for any finite set $I$ and typing function $\tau_{(-)}: I \rightarrow \mathcal{T}$. We interpret $\tau_{i} \in \mathscr{T}$ as the type of index $i \in I$.
- Maps $f^{*}: \prod_{j \in J} \mathrm{X}_{\tau_{j}} \rightarrow \prod_{i \in I} \mathrm{X}_{\tau_{i}}$ for any function $f: I \rightarrow J$ which preserves the typing: $\tau_{f i}=\tau_{i}$.
- Composition is given by $g^{*} \circ f^{*}=(f \circ g)^{*}$ and the identity is given by $\mathrm{id}:=\mathrm{id}^{*}$. That is, Arity $_{\mathscr{T}} \cong(\text { Fin } \downarrow \mathfrak{T})^{\text {op }}$ is dual to the category Fin $\downarrow \mathfrak{T}$ of $\mathfrak{T}$-typed finite sets, the slice category (a.k.a. comma category) of the inclusion Fin $\hookrightarrow$ Set over the set $\mathfrak{T}$ of types.

Exercise 1.3.3.10. We blew through that isomorphism Arity $\mathscr{T}_{\mathfrak{T}} \cong(\boldsymbol{F i n} \downarrow \mathfrak{T})^{\mathrm{op}}$ quickly, but its not entirely trivial. The category Fin $\downarrow \mathfrak{T}$ has objects functions $\tau: I \rightarrow \mathscr{J}$ where $I$ is a finite set, and a morphism is a commuting triangle like this:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-049.jpg?height=206&width=336&top_left_y=2136&top_left_x=886)

This is a function $f: I \rightarrow J$ so that $\tau_{f i}=\tau_{i}$ for all $i \in I$.

Expand the isomorphism out in full and check that you understand it.

Note that Arity $=$ Arity $_{1}$ is the special case where we have a single type. Just as we wrote the morphisms in Arity as $\left(x_{1}, x_{2} \mapsto x_{2}, x_{1}, x_{2}\right)$, we can write the morphisms in Arity $_{\mathfrak{T}}$ as

$$
\left(x_{1}: \tau_{1}, x_{2}: \tau_{2}, x_{3}: \tau_{2} \mapsto x_{2}: \tau_{2}, x_{1}: \tau_{1}, x_{2}: \tau_{1}\right)
$$

where $\tau_{1}, \tau_{2}, \tau_{3} \in \mathcal{T}$ are all (fixed, not variable) types.

We check that Arity $\boldsymbol{T}_{\mathcal{T}}$ as we defined it does indeed have the correct universal property.

Proposition 1.3.3.11. For any $\mathcal{T}$-indexed family of elements $C_{(-)}: \mathfrak{T} \rightarrow C$ in a cartesian category $C$, there is a cartesian functor $\mathrm{ev}_{C}:$ Arity $_{\mathcal{T}} \rightarrow C$ sending $\mathrm{X}_{\tau}$ to $C_{\tau}$. The functor $\mathrm{ev}_{C}$ is the unique such functor up to a unique natural isomorphism.

Proof Sketch. Just like in Proposition 1.3.3.7, we define

$$
\operatorname{ev}_{C}\left(\prod_{i \in I} \mathrm{X}_{\tau_{i}}\right):=\prod_{i \in I} C_{\tau_{i}}
$$

Exercise 1.3.3.12. Complete the proof of Proposition 1.3.3.11.

As before, we note that this functor sends a map in Arity $_{\mathscr{T}}$ to the function that does exactly that. For example,

$$
\left(x_{1}: \tau_{1}, x_{2}: \tau_{2}, x_{3}: \tau_{2} \mapsto x_{2}: \tau_{2}, x_{1}: \tau_{1}, x_{2}: \tau_{1}\right)
$$

gets sent by $\mathrm{ev}_{C}$ to the function $C_{\tau_{1}} \times C_{\tau_{2}} \times C_{\tau_{3}} \rightarrow C_{\tau_{2}} \times C_{\tau_{1}} \times C_{\tau_{2}}$ which sends $\left(c_{1}, c_{2}, c_{3}\right)$ to $\left(c_{2}, c_{1}, c_{2}\right)$

Corollary 1.3.3.13. For any function $f: \mathfrak{T} \rightarrow \mathfrak{T}^{\prime}$, there is a change of type functor $\mathrm{ev}_{\mathrm{X}_{f}}:$ Arity $_{\mathfrak{T}} \rightarrow$ Arity $_{\mathfrak{T}}$.

Proof. We apply Proposition 1.3.3.11 to the family $\mathrm{X}_{f(-)}: \mathfrak{T} \rightarrow$ Arity $_{\mathcal{T}^{\prime}}$ of objects of Arity $_{\mathfrak{J}}$. That is, we send

$$
\prod_{i \in I} X_{\tau_{i}} \mapsto \prod_{i \in I} X_{\tau(f(i))}
$$

We can now define the category of typed wiring diagrams to be the category of lenses in the category of typed arities.

Definition 1.3.3.14. For a set $\mathfrak{T}$ of types, the category $\mathbf{W D}_{\mathfrak{T}}$ of $\mathfrak{T}$-typed wiring diagrams is the category of lenses in the category of $\mathfrak{T}$-typed arities:

$$
\mathbf{W D}_{\mathfrak{T}}:=\operatorname{Lens}_{\mathfrak{T}}
$$

As with the singly-typed case, we can interpret any typed wiring diagram as a lens in a cartesian category of our choosing.

Proposition 1.3.3.15. For any family $C_{(-)}: \mathfrak{T} \rightarrow C$ of objects in a cartesian category $C$, indexed by a set $\mathcal{T}$ of types, there is a strong monoidal functor

$$
\left(\begin{array}{l}
\mathrm{ev}_{C} \\
\mathrm{ev}_{C}
\end{array}\right): \mathbf{W D}_{\mathscr{T}} \rightarrow \text { Lens }_{C}
$$

which interprets a typed wiring diagram as a lens in $C$ with appropriately typed values flowing along its wires.

Proof. Combine Proposition 1.3.3.7 with Proposition 1.3.1.7.

Remark 1.3.3.16. Because the action of $\mathrm{ev}_{C}$ is so simple, we will often just equate the typed wiring diagram with the lens it gives when interpreted in our category of choice.

Example 1.3.3.17. We can describe the wiring diagram

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-051.jpg?height=444&width=550&top_left_y=1090&top_left_x=777)

from Example 1.3.2.5 as a lens in a category of typed arities using Proposition 1.3.3.15. We have two types: a.m./p.m. and Hour. So, $\mathfrak{T}=\{$ a.m./p.m., Hour $\}$. Then

$$
\begin{aligned}
w & =(t: \text { Hour, } m: \text { a.m. } / \text { p.m. } \mapsto t: \text { Hour, } m: \text { a.m./p.m. }) \\
w^{\sharp} & =(t: \text { Hour, } m: \text { a.m./p.m. } \mapsto t: \text { Hour })
\end{aligned}
$$

giving us a wiring diagram in $\mathbf{W} \mathbf{D}_{\mathcal{J}}$. We can then interpret this wiring diagram as the lens from Example 1.3.2.5 by sending the types a.m./p.m. and Hour to the actual sets \{a.m., p.m. $\}$ and $\{1,2, \ldots, 12\}$. That is, we define the function $C_{-}: \mathfrak{T} \rightarrow$ Set used in Proposition 1.3.3.15 to send a.m./p.m. and Hour to the sets $\{$ a.m., p.m. $\}$ and $\{1,2, \ldots, 12\}$ respectively.

\subsection*{1.3.4 Wiring diagrams with operations as lenses in Lawvere theories}

The wiring diagrams we have described as lenses in categories of arities are pure wiring diagrams. But in Example 1.3.2.6, we used a wiring diagram (Eq. (1.27)) with little green beads representing multiplication by a constant scalar, and in Section 1.3.2 we used a wiring diagram with little green beads representing multiplication by a
matrix (Eq. (1.33)). It is very useful to be able to perform operations on the exposed variables we are passing to parameters.

In this section, we will see that if we have an algebraic theory of the kinds of operations we want to perform on our variables while we wire them, we can describe wiring diagrams with green beads representing those adjustments as lenses in the Lawvere theory of that algebraic theory.

Algebraic theories are theories of operations that are subject to certain equational laws.

Informal Definition 1.3.4.1. A algebraic theory $\mathbb{T}$ consists:
- A set $\mathbb{T}_{n}$ of $n$-ary operations for each $n \in \mathbb{N}$.
- A set of laws setting some composites of operations equal to others.

Example 1.3.4.2. The algebraic theory of real vector spaces can be described like this:
- There is a binary operation (-) + (-) of vector addition, and for every $r \in \mathbb{R}$ a unary operation $r \cdot(-)$ of scalar multiplication, and a nullary operation (a.k.a. constant) 0 .
- These satisfy the laws that make + and 0 into an abelian group with addition inverses given by $-1 \cdot(-)$, and which satisfy associativity and distributivity with regards to scalar multiplication.

$$
\begin{array}{rlrl}
(a+b)+c & =a+(b+c) & r \cdot(s \cdot a) & =(r s) \cdot a \\
0+a & =a & (r+s) \cdot a & =r \cdot a+s \cdot a \\
a+b & =b+a & 1 \cdot a & =a \\
a+(-1 \cdot a) & =0 & 0 \cdot a & =0
\end{array}
$$

We can use an algebraic theory to organize the sorts of operations we are willing or able to perform on the values flowing through the wires of our wiring diagrams.

Informal Definition 1.3.4.3. A wiring diagram with operations from an algebraic theory $\mathbb{T}$ is a wiring diagram where operations from the theory $\mathbb{T}$ can be drawn in little green beads on the wires.

Example 1.3.4.4. The wiring diagram (1.27) (reproduced below) is a wiring diagram in the algebraic theory of real vector spaces. The little green beads have scalar multipli-
cations drawn in them.

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-053.jpg?height=374&width=534&top_left_y=314&top_left_x=793)

We want to make these informal definitions precise. Ultimately, we want to be able to say that "wiring diagrams with operations from $\mathbb{T}$ are lenses in such and such cartesian category". We can do this with the notion of Lawvere theory.

Lawvere introduced his theories in his 1963 thesis "Functorial Semantics of Algebraic Theories" [Law04] as the invariant concepts of algebraic theories, freed from any particular presentation by symbols and their relations. In Example 1.3.4.2, we presented the algebraic theory of real vector spaces in a particular way; but we could have done it differently, say by avoiding the vector 0 entirely and adding the law $(0 \cdot a)+b=b$. Lawvere wanted to avoid these petty differences in presentation. He focuses instead on the cartesian category freely containing the operations of the theory (satisfying their laws). This gives an invariant of the concept of real vector space that is independent of how that concept is presented axiomatically.

A Lawvere theory is, in some sense, a category of arities "with extra maps". We think of these extra maps as coming from the operations of some theory.

Definition 1.3.4.5. A $\mathfrak{T}$-sorted Lawvere theory $\mathcal{L}$ is a cartesian category equipped with a bijective-on-objects functor Arity $_{\mathcal{T}} \hookrightarrow \mathcal{L}$.

If $\mathfrak{T}$ has a single element, we refer to this as a single sorted Lawvere theory.

Where we wrote the objects of Arity as $X^{I}$ to suggest the genericness of the generating object X, we will see that the objects of Lawvere theories are often $A^{I}$ for some "actual" object $A$ in some cartesian category.

Example 1.3.4.6. The single sorted Lawvere theory Vect of real vector spaces is the category of finite dimensional vector spaces, which can be defined as follows:
- For every finite set $I$, it has an object $\mathbb{R}^{I} \in$ Vect.
- A map $f: \mathbb{R}^{I} \rightarrow \mathbb{R}^{J}$ is a linear map, or equivalently a $J \times I$ matrix.
- The cartesian product $\mathbb{R}^{I} \times \mathbb{R}^{I}$ is $\mathbb{R}^{I+J}$.

Since Vect is a cartesian category, it admits a functor $X \mapsto \mathbb{R}$ from Arity. By construction, this functor is bijective on objects; we just need to show that it is faithful. If $g^{*}, f^{*}: X^{I} \rightarrow X^{J}$ are such that $g^{*}=f^{*}$ as maps $\mathbb{R}^{I} \rightarrow \mathbb{R}^{I}$, then in particular $g^{*}\left(e_{i}\right)=f^{*}\left(e_{i}\right)$
for all standard basis vectors $e_{i}$ defined by

$$
e_{i}(j):= \begin{cases}1 & \text { if } i=j \\ 0 & \text { otherwise. }\end{cases}
$$

But $g^{*}\left(e_{i}\right)(j):=e_{i}(g(j))$ and $f^{*}\left(e_{i}\right)(j):=e_{i}(f(j))$, so by varying $i$ we can test that $g(j)=$ $f(j)$ for all $j \in J$, and therefore that $g^{*}=f^{*}$ as maps $X^{I} \rightarrow X^{J}$.

How do we know that the extra maps in a Lawvere theory really do come from the operations of an algebraic theory? We show that the Lawvere theory satisfies a certain universal property: cartesian functors out of it correpond to models of the theory. If this is the case, we say that the Lawvere theory is presented by the algebraic theory.

Informal Definition 1.3.4.7. Let $\mathbb{T}$ be an algebraic theory. A model of $\mathbb{T}$ in a cartesian category $C$ is an object $C \in C$ together with maps $m(f): C^{n} \rightarrow C$ for each $n$-ary operation $f \in \mathbb{T}_{n}$ such that the maps $m(f)$ satisfy the laws of the theory.

Definition 1.3.4.8. A model of a Lawvere theory $\mathcal{L}$ in a cartesian category $C$ is a cartesian functor $M: \mathcal{L} \rightarrow C$.

We say that a Lawvere theory is presented by an algebraic theory if they have the same models in any cartesian category. We can show that our Lawvere theory Vect of vector spaces is presented by the theory of vector spaces of Example 1.3.4.2.

Proposition 1.3.4.9. Let $C$ be a cartesian category. Then for every real vector space in $C$, by which we mean an object $V \in C$ with a binary addition $+: V^{2} \rightarrow V$, a unary scalar multiplication $r \cdot: \rightarrow V$ for each $r \in \mathbb{R}$, and a nullary $0: 1 \rightarrow V$ which satisfy the laws of a vector space, there is a cartesian functor $\hat{V}$ : Vect $\rightarrow C$ sending $\mathbb{R}$ to $V$. Moreover, this functor is unique up to a unique isomorphism among functors sending $\mathbb{R}$ to $V$.

Proof Sketch. We define the functor $\hat{V}$ by sending $\mathbb{R}^{I}$ to $V^{I}$, and sending the operations $+: \mathbb{R}^{2} \rightarrow \mathbb{R}, r \cdot: \mathbb{R} \rightarrow \mathbb{R}$, and $0: \mathbb{R}^{0} \rightarrow \mathbb{R}$ to the corresponding operations on $V$. Given a general linear map $f: \mathbb{R}^{I} \rightarrow \mathbb{R}^{I}, f$ can be expressed as a composite of these operations; therefore, we can define $\hat{V}(f)$ to be the corresponding composite of the operations on $V$.

Definition 1.3.4.10. Let $\mathcal{L}$ be a Lawvere theory. The category $\mathbf{W D}_{\mathscr{L}}$ of wiring diagrams with operations from $\mathcal{L}$ is the category of lenses in $\mathcal{L}$ :

$$
\mathbf{W D}_{\mathscr{L}}:=\text { Lens }_{\mathscr{L}} .
$$

Remark 1.3.4.11. The bijective-on-objects functor Arity $_{\mathscr{T}} \rightarrow \mathcal{L}$ lets us interpret every $\mathcal{T}$-typed wiring diagram as a wiring diagram with operations from $\mathcal{L}$ by Proposition 1.3.3.15.

In order to interpret a wiring diagram with operations from $\mathcal{L}$ as a lens in a cartesian category $C$, we need a cartesian functor $\mathcal{L} \rightarrow C$. These are precisely the models of the Lawvere theory. So, if our interpretation of the wires of our diagrams are models of our Lawvere theory $\mathcal{L}$, we can interpret diagrams with operations from $\mathcal{L}$.

Example 1.3.4.12. The wiring diagram Eq. (1.33) is a wiring diagram with operations from Vect, the theory of vector spaces. This is why we are able to put matrices in the beads.

\subsection*{1.4 Summary and Futher Reading}

In this first chapter, we introduced deterministic and differential systems and saw how they could be composed using wiring diagrams. The trick is that both systems and wiring diagrams are examples of lenses â€” systems are lenses with a special domain, and wiring diagrams are lenses in free cartesian categories.

We will build on these ideas through the rest of the book. Most directly, in Chapter 2, we will see how non-deterministic systems can be seen as a variant of lenses: monadic lenses.

Our notion of deterministic system is commonly known as a Moore machine [Chu58]. The idea of composing dynamical systems - deterministic and differential - using lenses can be found in [VSL15]. Further exploration of this idea for both deterministic and differential systems can be found in the work of Bakirtzis and collaborators: [Bak21][BVF21][BSF21].

Lenses were first defined by Oles in Chapter 6 of his thesis [Ole83] as a "category of store shapes". These lenses are the "lawful lenses" of [Fos+07], used to solve the viewupdate problem in program design. A group of Haskell programmers including but not limited to Palmer, O'Connor, Van Laarhoven, and Kmett then generalized lawful lenses to the sorts of lenses used in this section. See this blog post for more on the history of lenses: [Hed].

\section*{Chapter 2}

\section*{Non-deterministic systems theories}

So far, we have seen how deterministic systems of the discrete- and continuous-time variety can be wired together. But modelling a system deterministically can be a bit hubristic: it assumes we have taken account of all variables that act on the state of the system, so that we can know exactly what will happen next or exactly how the system is tending to change. Often we know that the way we've modeled state is incomplete, and so knowing the state in our model might not tell us exactly what will happen next.

As an example, consider a person typing out an email. We know that the output of this system over time will be a stream of ASCII characters, and we won't model the various sorts of inputs that might be affecting the person as they write the email. The particular email written will depend on the person's state, but this state is extraordinarily complex and modelling it to the point that we would know exactly which email they will write is nigh impossible.

So, instead, we could use what we know about how this person usually writes emails to predict what the next character will be. This would give us a stochastic model of the email-writer system.

In this section, we will see a variety of non-deterministic (discrete-time) systems theories. The kind of non-deterministism - possibilistic, stochastic, etc. - will be encoded in a commutative monad (Definition 2.1.0.5).

\subsection*{2.1 Possibilistic systems}

Suppose that we are observing a deterministic system $\mathrm{S}$ from the outside. We can choose what input $i \in \operatorname{In}_{\mathrm{S}}$ to put into the system, and we observe from that what output $o \in$ Outs $^{2}$ comes out as a result. Can we understand how the system works from knowing this alone? In other words, can we construct a new system $S^{\prime}$ just from knowing how inputs relate to outputs in S?

In full generality, the answer is of course "no"; if there was only one possible output, for example, we have no chance to understand what's going on inside the system. But
if we do observe a bunch of different changing outputs, we can give it a shot.

As a first guess, we might try to model how an input $i \in \operatorname{In}_{\mathrm{S}}$ changes the output $o \in$ Outs that we are seeing. That is, we might try and make States' = Outs, and then define the new dynamics update ${ }_{S^{\prime}}(o, i)$ be the new output $S$ gives when fed input $i$ while it is exposing output $o$. There's just one problem with this idea: we won't always get the same output when we feed $i$ in to $S$ while it's exposing $o$.

For example, consider the following transition system:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-058.jpg?height=437&width=746&top_left_y=649&top_left_x=684)

The inputs to this system are from the set Ins $=\{$ true, false $\}$, and the outputs are from the set Outs $=\{$ red, blue, green $\}$. Suppose that we can only see what the system is outputting, and that it is outputing blue. If we feed the system false, we will see blue. But, if we feed the system true, what happens depends on whether the system was in state 1 or state 3 ; if we were in state 1 , then we will see red, but if were were in state 3 , we will see blue. So, the next output is not uniquely determined by the current output and current input - there are many possibilities. We are tempted to say that blue will transition to either red or blue in our model $S^{\prime}$ of the system $\mathrm{S}$. That is, we want the update of $S^{\prime}$ to tell us what is possible, since we can't know just from the outputs of $S$ what is determined to happen. We can do that by having the update of $S^{\prime}$ give us the set of possibilities:

$$
\text { update }_{\mathrm{S}^{\prime}}(\text { blue }, \text { true })=\{\text { blue }, \text { red }\} .
$$

In this section, we will see two systems theories which, instead of telling us the next state, tell us which states are possible or which are probable. Both are examples of non-deterministic systems theories, since the current state doesn't determine precisely the next state.

Definition 2.1.0.1. A possibilistic system S, also written as

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-058.jpg?height=143&width=560&top_left_y=2140&top_left_x=777)

consists of:
- a set States of states;
- a set Outs of values for exposed variables, or outputs for short;
- a set $\operatorname{In}_{S}$ of parameter values, or inputs for short;
- a function expose $\mathrm{S}_{\mathrm{S}}$ : States $\rightarrow$ Outs $_{\mathrm{S}}$, the exposed variable of state or expose function, which takes a state to the output it yields; and
- a function update ${ }_{S}$ : State $X \ln _{S} \rightarrow$ PStates, where PState $_{S}$ is the set of subsets of States. This is the dynamics or update function which takes a state and a parameter and gives the set of possible next states.

Remark 2.1.0.2. While Definition 1.2.1.2 can be interepreted in any cartesian category because it only used maps and the cartesian product, Definition 2.1.0.1 makes use of the power set operation $\mathrm{P}$ which sends a set to its set of subsets. This can't be interpreted in any cartesian category - we need something resembling $\mathrm{P}$ in order for it to make sense.

Example 2.1.0.3. A possibilistic automaton can be presented as a transition diagram as well. Consider, for example, the following diagram:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-059.jpg?height=404&width=761&top_left_y=1056&top_left_x=671)

This system resembles system $S$ of Eq. (2.1), except that it has a single state for each output. We can tell that this transition diagram represents a possibilistic system because there are two arrows leaving blue both labeled true. Since the dynamics of a transition diagram are given by following the arrow labeled by the input along to a new state, we see that here we will end up at a set of states:

$$
\text { update }_{\mathrm{S}^{\prime}}(\text { blue }, \text { true })=\{\text { blue }, \text { red }\} .
$$

Example 2.1.0.4. In Example 1.2.1.10, we saw that deterministic finite automata (DFAs) are examples of deterministic systems. There is another common notion in automata theory: non-deterministic finite automata (NFAs).

An NFA is a possibilistic system $S$ with finitely many states whose output values are Booleans: Outs $=\{$ true, false $\}$. As with DFAs, the exposed variable expose ${ }_{S}$ : States $\rightarrow$ \{true,false $\}$ tells us whether or not a state is an accept state.

Again, NFAs are question answering machines. But this time, since they are nondeterministic, we ask whether or not it is possible to accept a given sequence of inputs. Suppose we have a sequence of inputs $i_{0}, \ldots, i_{n}$, and we start in a state $s_{0}$. Now, because an NFA is possibilistic, we don't have a "next state" $s_{1}$. Rather, we have a set of states
$S_{1}:=\operatorname{update}_{\mathrm{S}}\left(s_{0}, i_{0}\right)$. Now, we need to interatively define the next evolution: $S_{2}$ should be the set of states that are possible to get to from any state in $S_{1}$. Generally,

$$
S_{j+1}:=\left\{s^{\prime} \mid s \in S_{j}, s^{\prime} \in \operatorname{update}_{S}\left(s, i_{j}\right)\right\}=\bigcup_{s \in S_{j}} \text { update }_{S}\left(s, i_{j}\right)
$$

We then say that the machine accepts the input sequence if there is any accept state in $S_{n}$.

Example 2.1.0.4 contains an answer to an interesting question: how do we iterate the behavior of a possibilistic system? For a deterministic system whose update has the signature update ${ }_{S}:$ State $_{S} \times \ln _{S} \rightarrow$ States, we can compose to get $^{2}$

$$
\text { State }_{S} \times \operatorname{In}_{S} \times \ln _{S} \xrightarrow{\text { update }_{S} \times \ln _{S}} \text { State }_{S} \times \ln _{S} \xrightarrow{\text { update }_{S}} \text { States }
$$

which sends $\left(s,\left(i_{0}, i_{1}\right)\right)$ to update (update $\left._{S}\left(s, i_{0}\right), i_{1}\right)$. We can do this as many times as we like to apply an entire sequence of inputs to a state.

But for a possibilistic system, the update has signature update ${ }_{S}$ : States $\times \ln _{S} \rightarrow$ PStates. Now we can't just compose, if we tried the trick above we would go from States $X \ln _{S} X \ln _{S} \rightarrow$ PStates $X \ln _{s}$, and we're stuck.

But from update ${ }_{S}:$ State $_{S} \times \ln _{S} \rightarrow$ PStates we can define a function $U$ : PStates $\times$ In $_{S} \rightarrow$ PStates by

$$
U(S, i):=\left\{s^{\prime} \mid s \in S, s^{\prime} \in \operatorname{update}_{S}(s, i)\right\}=\bigcup_{s \in S} \operatorname{update}_{S}(s, i)
$$

Then we can define the iterated action of the system to be the composite

$$
\text { State }_{S} \times \ln _{S} \times \ln _{S} \xrightarrow{\text { update }_{S}} \text { PState }_{S} \ln _{S} \xrightarrow{U} \text { PStates. }
$$

This process of lifting a function $A \times B \rightarrow \mathrm{PC}$ to a function $\mathrm{P} A \times B \rightarrow \mathrm{PC}$ is fundamental, and worthy of abstraction. This operation comes from the fact that $\mathrm{P}$ is a commutative monad. Take a deep breath, because here comes the definition.

Definition 2.1.0.5. Let $C$ be a cartesian category. A monad $(M, \eta)$ on $C$ consists of:
- An assignment of an object $M A$ to every object $A \in C$.
- For every object $A \in C$, a map $\eta_{A}: A \rightarrow M A$.
- For every map $f: A \rightarrow M B$, a lift $f^{M}: M A \rightarrow M B$.

This data is required to satisfy the following laws:
- (Unit) For any object $A$,

$$
\eta_{A}^{M}=\mathrm{id}_{M A}
$$
- (Identity) For any map $f: A \rightarrow M B$,

$$
f^{M} \circ \eta_{A}=f .
$$
- (Composition) For any $f: A \rightarrow M B$ and $g: B \rightarrow M C$,

$$
g^{M} \circ f^{M}=\left(g^{M} \circ f\right)^{M}
$$

From this data, we note that we can extend $M$ into a functor $M: C \rightarrow C$ by sending $f: A \rightarrow B$ to $M f:=\left(\eta_{B} \circ f\right)^{M}: M A \rightarrow M B$. Then $\eta: A \rightarrow M A$ is natural in $A$, and we get another natural transformation $\mu: M M A \rightarrow M A$ defined by lifting the identity: $\mu:=\mathrm{id}^{M}$. In fact, a monad may be equivalently defined as a functor $M: C \rightarrow C$ with natural transformations $\eta: A \rightarrow M A$ and $\mu: M^{2} A \rightarrow M A$ for which the following diagrams commutes:
![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-061.jpg?height=226&width=1000&top_left_y=782&top_left_x=557)

For $f: A \rightarrow M B$, we can recover $f^{M}: M A \rightarrow M B$ from this definition of the monad $M$ as $M A \xrightarrow{M f} M^{2} B \xrightarrow{\mu} M B$.

A monad $M$ is said to be commutative if there is a natural transformation $\sigma: M A \times$ $M B \rightarrow M(A \times B)$ for which the following diagrams commute:
![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-061.jpg?height=976&width=698&top_left_y=1317&top_left_x=754)

\section*{$\cdot$}

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-062.jpg?height=223&width=534&top_left_y=276&top_left_x=839)

Remark 2.1.0.6. If you are familiar with the programming language Haskell, you will likely be familiar with the notion of monad. What we have called $\eta_{A}$ here (which is traditional in the category theory literature) is called or. What we have called $f^{M}$ for $f: A \rightarrow M B$ would, in haskell, be called and be defined by

What we have called $\mu$ is called. A monad in haskell is commutative if the following two programs have the same results: That is, a monad is commutative when its order of execution doesn't matter.

Proposition 2.1.0.7. The powerset $\mathrm{P}$ is a commutative monad on the category of sets, with the following data:
- $\eta: A \rightarrow \mathrm{P} A$ sends $a \in A$ to the singleton set $\{a\}$.
- For $f: A \rightarrow \mathrm{P} B, f^{\mathrm{P}}: \mathrm{P} A \rightarrow \mathrm{P} B$ is defined by

$$
f^{\mathrm{P}}(X)=\bigcup_{a \in X} f(a)
$$
- $\sigma_{A, B}: \mathrm{P} A \times \mathrm{P} B \rightarrow \mathrm{P}(A \times B)$ is defind by

$$
\sigma_{A, B}(X, Y)=\{(a, b) \mid a \in X, b \in Y\} .
$$

Proof. We just need to check the laws.
- The function $\eta_{A}^{\mathrm{P}}$ takes a set $X \in \mathrm{P} A$ and yields $\bigcup_{x \in X}\{x\}$, which is equal to $X$.
- Let $f: A \rightarrow \mathrm{P} B$ be a function. Then $f^{\mathrm{P}}(\{a\})=\bigcup_{a^{\prime} \in\{a\}} f\left(a^{\prime}\right)=f(a)$ for any element $a \in A$.
- Let $f: A \rightarrow \mathrm{P} B$ and $g: B \rightarrow \mathrm{PC}$. For $X \in \mathrm{P} A$, we have

$$
\begin{aligned}
g^{\mathrm{P}} \circ f^{\mathrm{P}}(X) & =\bigcup_{b \in f^{\mathrm{P}}(X)} g(b) \\
& =\bigcup_{b \in \bigcup_{a \in X} f(a)} g(b) \\
& =\bigcup_{a \in X} \bigcup_{b \in f(a)} g(b) \\
& =\left(g^{\mathrm{P}} \circ f\right)^{\mathrm{P}} .
\end{aligned}
$$

It remains to show that the powerset monad is commutative. We note that $\mathrm{P}$ acts as a functor on $f: A \rightarrow B$ by

$$
\operatorname{Pf}(X)=\left(\eta_{B} \circ f\right)^{\mathrm{P}}(X)=\bigcup_{a \in X}\{f(a)\}=f[X]
$$
sending a subset of $A$ to its image in $B$. We also note that $\mu: \mathrm{P}^{2} A \rightarrow \mathrm{P} A$ defined by $\mu=\mathrm{id}_{\mathrm{P} A}^{\mathrm{P}}$ sends a set $S$ of subsets of $S$ to its union $\cup s \in S s$.
- (Eq. (2.4)) Beginning with $(X, *) \in \mathrm{P} A \times 1$ (taking $1 \cong\{*\}$ ), we need to show that $\mathrm{P} \pi_{1} \circ \sigma_{A, 1}(X,\{*\})=X$. Now, $\sigma_{A, 1}(X,\{*\})=\{(a, b) \mid a \in X, b \in\{*\}\} ;$ since there is just one $b \in\{*\}$, every $a \in X$ is paired with some $b$, so projecting out the first component gives us all of $X$.
- (Eq. (2.5)) This is the same as the above, but on the other side.
- (Eq. (2.6)) If we have $(X, Y, Z) \in P A \times P B \times P C$, both sides of this diagram will give us $\{(a, b, c) \mid a \in X, b \in Y, c \in Z\}$.
- (Eq. (2.7)) For $(a, b) \in A \times B$, we have $\eta(a, b)=\{a, b\}$, and $\sigma(\eta(a), \eta(b))=\{(x, y) \mid$ $x \in\{a\}, y \in\{b\}$.
- (Eq. (2.8)) Let $S$ be a set of subsets of $A$ and $T$ a set of subsets of $B$. The bottom path gives us

$$
\sigma(\mu(S), \mu(T))=\left\{(x, y) \mid x \in \bigcup_{s \in S} s, y \in \bigcup_{t \in T} t\right\}
$$

while taking the top path, we first get $\sigma(S, T)=\{(s, t) \mid s \in S, t \in T\}$ and then $M \sigma$ of that to get

$$
\sigma[\{(s, t) \mid s \in S, t \in T\}]=\{\{(x, y) \mid x \in s, y \in t\} \mid s \in S, t \in T\} .
$$

Finally, we take the union over this to get

$$
\mu(\operatorname{P} \sigma(\sigma(S, T)))=\bigcup_{s \in S, t \in T}\{(x, y) \mid x \in s, y \in t\}
$$

We end out proof by noting that

$$
\left\{(x, y) \mid x \in \bigcup_{s \in S} s, y \in \bigcup_{t \in T} t\right\}=\bigcup_{s \in S, t \in T}\{(x, y) \mid x \in s, y \in t\}
$$

Remark 2.1.0.8. While the powerset monad is commutative, the list monad is not. For the list monad, Eq. (2.8) does not hold since the two lists end up in a different order in the end.

Using the commutative monad structure of $\mathrm{P}$, we can see that $U:$ PStates $\times \ln _{S} \rightarrow$ PStates is the composite

$$
\text { PState }_{S} \times \ln _{S} \xrightarrow{\mathrm{id} \times \eta} \text { PState }_{S} \times \operatorname{Pln}_{S} \xrightarrow{\sigma} \mathrm{P}\left(\text { State }_{S} \times \ln _{S}\right) \xrightarrow{\text { update }_{S}^{\mathrm{P}}} \text { PStates. }
$$

This lets us iteratively apply the update function to a starting state or set of states.

It also lets us get the exposed variable out at the end. If we've been iteratively running a possibilistic system, then we won't know which state we are in but instead have a set $S \in$ PStates of states we could possibly be in. Because of this, we can't
directly apply expose $\mathrm{S}_{\mathrm{S}}$ : States $\rightarrow$ Outs, since it takes in a single state. But the monad structure of $\mathrm{P}$ gives us a function Pexpose $\mathrm{S}_{\mathrm{S}}$ : PStates $\rightarrow$ POuts. Applying this to our current set of possible states gives us a set of possible outputs, which is the best we could hope to know.

Do Notation If we have a function $f: X \rightarrow Y$, we can think of this as mapping $x$ in $X$ to $f(x)$ in $Y$ using "generalized elements" (see Remark 1.3.1.3). The do notation extends this way of writing morphisms in a cartesian category to include the action of a commutative monad $M$. The do notation is based on this simple equation for $f: X \rightarrow M Y:$

$$
\text { do } \begin{array}{ll} 
&  \tag{2.9}\\
& x \leftarrow m \\
& f(x)
\end{array}:=f^{M}(m)
$$

where $m$ is an element of $M X$ and $f: X \rightarrow M Y$. For $M=\mathrm{D}$, we can understand the do notation in this way: $m$ is a subset of $X, f^{M}(m)$ is the subset $\{f(x) \in Y \mid x \in m\}$. We see this reflected in the do notation; we can read it as saying "get an element $x$ from $m$, and then apply $f(x)$ to it; join together all the results." As we see more monads, we will see that a similar story can be told about them using the do notation.

There are a few rules for do notation which correspond to the laws for a monad. We can discover them by using Eq. (2.9) to expand out a few terms. First of all, since $\eta_{M}=\mathrm{id}_{M X}$, if $m$ is an element of $M X$, then

$$
\begin{array}{ll}
\text { do } & \\
& x \leftarrow m \\
& \eta(x)
\end{array}
$$

Next, since $\eta \circ f^{M}=f$, we find that

$$
\begin{array}{ll}
\text { do } & \\
& x^{\prime} \leftarrow \eta(x) \\
& f\left(x^{\prime}\right)
\end{array}=f(x)
$$

Finally, since $f^{M} \circ g^{M}=\left(f \circ g^{M}\right)^{M}$, we find that

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-064.jpg?height=278&width=796&top_left_y=1905&top_left_x=659)

Because these two expressions with nested do's are equal, we can simplify our notation by writing them as:

$$
\begin{array}{|ll}
\hline \text { do } & \\
& x \leftarrow m \\
& y \leftarrow f(x) \\
& g(y)
\end{array}
$$

So far, we haven't used any pairs $(x, y)$ in our do notation. To use pairs, we need our monad to be commutative. We can write down two expressions, assuming $m_{1}$ is an element of $M X$ and $m_{2}$ is an element of $M Y$. A monad is commutative precisely when these two expressions are equal:

$$
\begin{array}{cc}
\text { do } & \\
& x \leftarrow m_{1} \\
& y \leftarrow m_{2} \\
& \eta(x, y)
\end{array}=\begin{array}{cc}
\text { do } & \\
& y \leftarrow m_{2} \\
& x \leftarrow m_{1} \\
& \eta(x, y)
\end{array}
$$

When they are both equal, they are $\sigma\left(m_{1}, m_{2}\right)$, where $\sigma: M X \times M Y \rightarrow M(X \times Y)$ is from the definition of a commutative monad. This lets us describe morphisms quite nicely. For example, given $f: X \rightarrow M Y, g: Z \rightarrow M W$, and $h: Y \times W \rightarrow M Q$, we may define

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-065.jpg?height=232&width=267&top_left_y=938&top_left_x=929)

which desugars to the composite

$$
X \times Z \xrightarrow{f \times g} M Y \times M W \xrightarrow{\sigma} M(Y \times W) \xrightarrow{h^{M}} M Q .
$$

In particular, to iterate a system $S$ with update update ${ }_{S}$ : States $\times \ln _{S} \rightarrow$ DStates, we can define

$$
U(S, i):=\begin{array}{ll}
\mathbf{d o} & \\
& s \leftarrow S \\
& \operatorname{update}_{S}(s, i)
\end{array}
$$

\subsection*{2.2 Stochastic systems}

Possibility is not the only kind of non-determinism. When studying how things change in the world, we often notice that we can only predict how likely some change will be, and not precisely which change will occur. If instead of asking whether a change is possible, we ask how probable it is, we arrive at a notion of probabilistic or stochastic system.

The notion of a stochastic system is based on the idea that there should be a probability of a given change occuring, conditioned upon the current state. A useful way to formulate the notion of conditional probability is the notion of stochastic map. A stochastic map from $A$ to $B$ is a function which takes an $a \in A$ and yields a probability distribution $p(-\mid a)$ on elements of $B$ which we think of as likelyhoods conditioned on $a$. We can make this more precise using the notion of monad.

Definition 2.2.0.1. For a set $A$, the set $\mathrm{D} A$ is the set of finitely supported probability distributions on $A$. A probability distribution on $A$ is a function $p: A \rightarrow[0,1]$ which takes non-zero values at only finitely many elements of $A$, and for which

$$
\sum_{a \in A} p(a)=1
$$

This sum makes sense because only finitely many elements of $A$ give non-zero $p(a)$.

The elements of $\mathrm{D} A$ can be identified with formal convex combinations of elements of A. A formal convex combination

$$
\sum_{a \in X} \lambda_{a} a
$$

of elements of $A$ consists of a finite and inhabited ${ }^{a}$ subset $X \subseteq A$ of elements together with a function $\lambda_{(-)}: X \rightarrow(0,1]$ assigning each $a \in X$ a coefficient $\lambda_{a}$ such that $\sum_{a \in X} \lambda_{a}=1$.

$$
\mathrm{D} A=\left\{\sum_{a \in X} \lambda_{a} a \mid X \subseteq A, X \text { finite and inhabited, } \lambda_{(-)}: X \rightarrow(0,1], \sum_{a \in X} \lambda_{a}=1\right\}
$$
\footnotetext{
${ }^{a}$ That is, there is some $a \in X$.
}

Example 2.2.0.2. Let's see what $\mathrm{D} A$ looks like for a few different sets $A$ :

1. If $A=\{a\}$ has a single element, then there is only one inhabited subset $X \subseteq A$ (namely $\mathrm{X}=A$ ) and since the coefficients of any convex linear combination must sum to 1 , the coefficient of the single element must be 1 . So $\mathrm{D}\{a\}=\{1 \cdot a\}$ contains a single element.

2. If $A=\{a, b\}$, things get more interesting. Now there are three inhabited subsets $X:\{a\},\{b\}$, and $\{a, b\}$. A convex combination with a single element must have coefficient 1 , so we at least have the convex combinations $1 \cdot a$ and $1 \cdot b$. But for the set $\{a, b\}$, we have the convex combination $\lambda_{a} a+\lambda_{b} b$ where $\lambda_{a}+\lambda_{b}=1$ and $\lambda_{a}, \lambda_{b}>0$. If we make the association of $1 \cdot a$ with $1 \cdot a+0 \cdot b$, and similarly for $1 \cdot b$, then we can see that

$$
\mathrm{D}\{a, b\}=\{\lambda a+(1-\lambda) b \mid \lambda \in[0,1]\}
$$

which is bijective with the closed interval $[0,1]$.

3. In general, if $A$ is a finite set with $n+1$ elements, then $\mathrm{D} A$ can be identified with the standard $n$-simplex, that is, the set of solutions to the equation $\sum_{i=1}^{n+1} \lambda_{i}=1$ for $\lambda_{i} \in[0,1]$.

$$
\text { Dn }+1 \cong\left\{\left(\lambda_{1}, \ldots, \lambda_{n+1}\right) \in[0,1]^{n+1} \mid \sum_{i=1}^{n+1} \lambda_{i}=1\right\}
$$

Definition 2.2.0.3. A stochastic map from a set $A$ to a set $B$ is a function $f: A \rightarrow \mathrm{D} B$, assigning each $a \in A$ to a probability distribution $f(a)$ on $B$.

If the sets $A$ and $B$ are finite, then we can write a stochastic map $f: A \rightarrow \mathrm{D} B$ as a stochastic matrix. This is an $B \times A$ matrix whose $b a$-entry is $f(a)(b)$. Any matrix of positive entries where every column sums to 1 arises as the stochastic matrix of a stochastic map.

We think of a stochastic map $f: A \rightarrow \mathrm{D} B$ as giving a bunch of conditional probabilities

$$
p(b \mid a):=f(a)(b)
$$

Example 2.2.0.4. If I see someone enter the office soaking wet, it is likely to have been raining. If they are dry, it may be less likely that it was raining; but, if they have an umbrella, then they might be dry but it is still more likely that it was raining. We can express these various conditional probabilities as a stochastic function

$$
\{\text { wet, } \mathrm{dry}\} \times\{\text { umbrella, no-umbrella }\} \rightarrow \mathrm{D}\{\text { raining, not-raining }\} .
$$

We can describe this stochastic function in full by giving its stochastic matrix:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-067.jpg?height=187&width=1217&top_left_y=1359&top_left_x=449)

A stochastic system is a system whose dynamics is given by a stochastic map.

Definition 2.2.0.5. A stochastic system S, also written as

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-067.jpg?height=154&width=574&top_left_y=1826&top_left_x=773)

consists of:
- a set States of states;
- a set Outs of values for exposed variables, or outputs for short;
- a set Ins of parameter values, or inputs for short;
- a function expose $_{\mathrm{S}}$ : States $\rightarrow$ Outs, the $^{\text {: }}$, exposed variable of state or expose function, which takes a state to the output it yields; and

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-067.jpg?height=66&width=1390&top_left_y=2317&top_left_x=389)
States. This is the dynamics or update function which takes a state and a parameter and gives the set of possible next states.

Remark 2.2.0.6. Note that the exposed variable of a stochastic system is not a stochastic function. This is theoretically important for wiring stochastic systems together, because it is necessary for stochastic lens composition to be well defined. We will return to this point in Section 2.6.

Remark 2.2.0.7. A stochastic system is often called a Markov process.

Example 2.2.0.8. A simple but entertaining example of a stochastic system is a text generator. Suppose we have a big pile of text - say, the plays written by a famous author - and we want to generate some text that looks like it was written by the same author. There are many sophisticated ways to do this, but here's a very bone-headed approach. We will look at the text in 5-character length sequences, and ask: how likely is for a given character to follow this 5-character sequence.

For example, if our text is

To be or not to be, that is the question.

Then we can see that there is a 50\% chance that " " and a 50\% chance that "," follows the 5-character sequence "to be". Of course, such a small sample wouldn't give us very useful statistics, but if we use the combined works of Shakespeare, we might get a better sense of what is likely to occur next.

Now we build a stochastic system $S$ which will generate text. We take States to be length 5 sequences of characters from our alphabet Alphabet: States $=$ Alphabet ${ }^{5}$. We will expose the first character in the sequence: Out $=$ Alphabet and $\operatorname{expose}_{S}(s)=s_{1}$. We don't need any input to the system: $\ln _{S}=\{*\}$. Now, update ${ }_{S}(s)$ will assign to a sequence $\left(s_{2}, s_{3}, s_{4}, s_{5}, c\right)$ the probability that the character $c$ follows the sequence $s=\left(s_{1}, s_{2}, s_{3}, s_{4}, s_{5}\right)$ in our sample text, and assign all other sequences the probability 0 .

If we run our stochastic text generator over time, it will produce a stream of characters that have the statistical properties of our sample text. As simple minded as this approach is, it can produce some fun results:

\section*{HAMLET}

Whose image even but now appear'd again!

\section*{HORATIO}

From top to toe?

\section*{FRANCISCO}

Bernardo, on their promise, as it is a course to any moment leisure, but to persever Than the cock, that this believe Those friend on Denmark Do not dull thy name with a defeated him yesternight.

Example 2.2.0.9. A stochastic source process is a stochastic system $S$ with no inputs $\ln _{S}=1$. Such a stochastic system would be boxed up like this:

Source

These are means by which random streams of outputs can be generated. In Example 2.2.0.8, we described a stochastic source process that produced Shakespearean writing (of a stunted sort). In his seminal paper "A mathematical theory of communication", Claude Shannon imagined communicators as stochastic source processes sending somewhat random language through various communication channels. This point of view is still used today to model communications that have some complicated structure which, not knowing how that structure is generated in particular, are best modeled as somewhat random processes.

Example 2.2.0.10. We can model a faulty wire as a stochastic system of the following sort:

$$
\text { Bit }- \text { FaultyWire }- \text { Bit }
$$

We will define FaultyWire as follows:
- A faulty wire will either have good contact, partial contact, or missing contact, and it will be carrying a high or low charge:

$$
\text { State }_{\text {FaultyWire }}:=\{\text { high, low }\} \times\{\text { good, partial, missing }\}
$$
- The faulty wire will take in either a high or low:

$$
\operatorname{In}_{\text {FaultyWire }}=\text { Out }_{\text {FaultyWire }}=\text { Bit }=\{\text { high, low }\} .
$$
- The faulty wire exposes its current charge:

$$
\operatorname{expose}_{\text {FaultyWire }}(b, s)=b
$$
- The faulty wire will try and set its charge to the charge on the incoming wire, but if it is has bad contact, this won't succeed and it will have low charge. It's contact also has a small chance to decay.

$$
\begin{aligned}
& \text { update }_{\text {FaultyWire }}((b, \text { good }), i)=.99(i, \text { good })+.01(i, \text { partial }), \\
& \text { update }_{\text {FaultyWire }}((b, \text { partial }), i)=.50(i, \text { partial })+.49(\text { low, partial })+.01(\text { low, missing }), \\
& \text { update }_{\text {FaultyWire }}((b, \text { missing }), i)=(\text { low, no }) .
\end{aligned}
$$

When wiring up our systems, if we put a faulty wire in between, we will introduce the probability of the failure of this wire to communicate into the model.

Example 2.2.0.11. We can draw transition diagrams for stochastic systems, just like we do for deterministic and possibilistic systems. This time, we will label each transition with the probability that it occurs. We just have to make sure that the probability labels on all the outgoing transitions with the same input label on any state sum to 1.

For example, here is a stochastic system drawn as a transition diagram:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-070.jpg?height=266&width=401&top_left_y=1363&top_left_x=859)

The set $\mathrm{D}$ of probability distributions is a commutative monad, like the powerset $\mathrm{P}$ monad.

Proposition 2.2.0.12. The assignment of a set $A$ to its set $\mathrm{D} A$ of probability distributions is a commutative monad with the data:
- $\eta_{A}: A \rightarrow \mathrm{D} A$ sends every element $a$ to its Dirac delta distribution $\eta_{A}(a)=1 \cdot a$ which assigns probability 1 to $a$ and probability 0 to everything else. As a convex linear combination, it looks like this:

$$
\eta_{A}(a)=\sum_{a^{\prime} \in\{a\}} 1 \cdot a^{\prime}
$$
- Given a stochastic map $f: A \rightarrow \mathrm{D} B$ sending $a \in A$ to $f(a)=\sum_{b \in Y_{a}} \rho_{b a} b$, we can push forward a probability distribution $p=\sum_{a \in X} \lambda_{a} a$ on $A$ to a probability distribution

$$
f^{\mathrm{D}}(p):=\sum_{b \in \bigcup_{a \in X}}\left(\sum_{a \in X} \rho_{b a} \lambda_{a}\right) b=\sum_{a \in X} \sum_{b \in Y_{a}} \rho_{b a} \lambda_{a} b
$$

on $B$. In classical terms, this says that given conditional probabilities $p(b \mid a):=$ $f(a)(b)$ and any prior distribution $p(a):=\lambda_{a}$, we can form a posterior distribution $p(b):=\sum_{a \in A} p(b \mid a) p(a)$.
- Given a probability distribution $\sum_{a \in X} \lambda_{a} a$ on $A$ and $\sum_{b \in Y} \mu_{b} b$ on $B$, we can form their joint distribution

$$
\sum_{(a, b) \in X \times Y} \lambda_{a} \mu_{b}(a, b)
$$

on $A \times B$. This gives us $\sigma: \mathrm{D} A \times \mathrm{D} B \rightarrow \mathrm{D}(A \times B)$. In classical terms, this says that the probability of two independent events is the product of their probabilities: $p(a, b)=p(a) p(b)$.

Proof. We check the laws:
- If we push forward a distribution $p=\sum_{a \in X} \lambda_{a} a$ along $\eta_{A}: A \rightarrow \mathrm{D} A$, we get

$$
\eta_{A}^{\mathrm{D}}(p)=\sum_{a \in X} \sum_{a^{\prime} \in\{a\}} 1 \cdot \lambda_{a^{\prime}} a^{\prime}=\sum_{a \in X} \lambda_{a} a
$$
- For a stochastic map $f: A \rightarrow \mathrm{D} B$, we aim to show that pushing forward the Dirac delta distribution $\eta_{A}(a)$ along $f$ gives $f(a)=\sum_{b \in Y_{a}} \lambda_{b a} b$. The definition of push forward gives us

$$
f^{\mathrm{D}}\left(\eta_{A}(a)\right)=\sum_{a^{\prime} \in\{a\}} \sum_{b \in Y_{a^{\prime}}} \lambda_{b a} \cdot 1 \cdot b=\sum_{b \in Y_{a}} \lambda_{b a} b
$$
- Given stochastic functions $f: A \rightarrow \mathrm{D} B$ and $g: B \rightarrow \mathrm{DC}$, we need to show that $g^{\mathrm{D}}\left(f^{\mathrm{D}}(p)\right)=\left(g^{\mathrm{D}} \circ f\right)^{\mathrm{D}}(p)$. Let

$$
p=\sum_{a \in X} \lambda_{a}, \quad f(a)=\sum_{b \in Y_{a}} \rho_{b a} b, \quad g(b)=\sum_{c \in Z_{b}} \gamma_{c b} c
$$

Then we see that

$$
g^{\mathrm{D}}(f(a))=\sum_{c \in \bigcup_{b \in \cup_{a} \in X} Y_{a}} \gamma_{c b} \rho_{b a} c
$$

so that, finally

$$
\begin{aligned}
g^{\mathrm{D}}\left(f^{\mathrm{D}}(p)\right) & =g^{\mathrm{D}}\left(\sum_{a \in X} \sum_{b \in Y_{a}} \rho_{b a} \lambda_{a} c\right) \\
& =\sum_{a \in X} \sum_{b \in Y_{a}} \sum_{c \in Z_{b}} \gamma_{c b} \rho_{b a} \lambda_{a} c \\
& =\left(g^{\mathrm{D}} \circ f\right)^{\mathrm{D}}(p) .
\end{aligned}
$$

Next, we check that the laws of a commutative monad hold. We note that for a function $f: A \rightarrow B$, the function $\mathrm{D} f=\left(\eta_{B} \circ f\right)^{\mathrm{D}}$ is defined by

$$
\mathrm{D} f\left(\sum_{a \in X} \lambda_{a} a\right)=\sum_{a \in X} \sum_{b \in\{f(a)\}} \lambda_{a} b=\sum_{a \in X} \lambda_{a} f(a)
$$

Furthermore, $\mu: \mathrm{D}^{2} A \rightarrow \mathrm{D} A$ sends a formal convex combination $\sum_{i} \lambda_{i} p_{i}$ of probability distributions to the actual convex combination of those probability distributions, namely the distribution

$$
\mu\left(\sum_{i} \lambda_{i} p_{i}\right)(a):=\sum_{i} \lambda_{i} p_{i}(a)
$$
- (Eq. (2.4)) The unit on $1 \cong\{*\}$ sends $*$ to the distribution $1 \cdot *$. So, $\sigma(p, 1)=$ $\sum_{(a, *) \in X \times 1} \lambda_{a} \cdot 1 \cdot(a, *)$, and projecting out again gives us $p=\sum_{a \in X} \lambda_{a} a$.
- (Eq. (2.5)) The same, but on the other side.
- (Eq. (2.6)) Suppose that we have

$$
p=\sum_{a \in X} p_{a} a, \quad q=\sum_{b \in Y} q_{b} b, \quad r=\sum_{c \in Z} r_{c} c
$$

The both paths of Eq. (2.6) give us the distribution

$$
\sum_{(a, b, c) \in X \times Y \times Z} p_{a} q_{b} r_{c}(a, b, c)
$$
- (Eq. (2.7)) This is asking whether $\delta_{(a, b)}=\delta_{a} \delta_{b}$ as distributions on $A \times B$, which they are.
- (Eq. (2.8)) Let $\sum_{i} \lambda_{i} p_{i}$ be an element of DDA, and similarly let $\sum_{j} \rho_{j} q_{j}$ be an element of DDB. Following the bottom path around, we get

$$
\sigma\left(\mu\left(\sum_{i} \lambda_{i} p_{i}\right), \mu\left(\sum_{j} \rho_{j} q_{j}\right)\right)(a, b)=\left(\sum_{i} \lambda_{i} p_{i}(a)\right)\left(\sum_{j} \rho_{j} q_{j}(b)\right)=\sum_{i} \sum_{j} \lambda_{i} \rho_{j} p_{i}(a) q_{j}(b) .
$$

Meanwhile,

$$
\sigma\left(\sum_{i} \lambda_{i} p_{i}, \sum_{j} \rho_{j} q_{j}\right)=\sum_{i} \sum_{j} \lambda_{i} \rho_{j}\left(p_{i}, q_{j}\right)
$$

and taking $\mathrm{D} \sigma$ of that gives

$$
\sum_{i} \sum_{j} \lambda_{i} \rho_{j} p_{i} q_{j}
$$

which means that finally

$$
\mu\left(\mathrm{D} \sigma\left(\sigma\left(\sum_{i} \lambda_{i} p_{i}, \sum_{j} \rho_{j} q_{j}\right)\right)\right)(a, b)=\sum_{i} \sum_{j} \lambda_{i} \rho_{j} p_{i}(a) q_{j}(b) .
$$

Exercise 2.2.0.13. Let $f: \mathrm{n} \rightarrow$ Dm and $g: \mathrm{m} \rightarrow$ Dk be stochastic maps. Note that we can interpret $f$ as an $m \times n$ stochastic matrix $F$, and similarly $g$ as a $k \times m$ stochastic matrix G. Show that the stochastic map $g^{\mathrm{D}} \circ f$ is associated to the stochastic matrix GF.

Just as the commutative monad structure of $\mathrm{P}$ helped us iterate possibilistic systems and get the set of possible output values from them, so the commutative monad structure of D helps us iterate stochastic systems and get a probability distribution of likely output values from them.

Given a stochastic system $S$, we have update ${ }_{S}$ : State $X \ln _{S} \rightarrow$ DStates. From this, we can get a stochastic map:

$$
\text { DState }_{S} \times \ln _{S} \xrightarrow{\mathrm{id} \times \eta} \text { DState }_{S} \times \operatorname{Dln}_{S} \xrightarrow{\sigma} D\left(\text { State }_{S} \times \ln _{S}\right) \xrightarrow{\text { update }} \text { DStates }
$$

which will let us iterate. We can see that this sends a probability distribution $p$ on states and an input $i$ to the distribution

$$
s \mapsto \sum_{s^{\prime} \in \text { States }} p\left(s^{\prime}\right) \text { update }_{\mathrm{S}}\left(s^{\prime}, i\right)(s)
$$

\subsection*{2.3 Monadic systems theories and the Kleisli category}

We have now seen two sorts of non-determinism expressed by commutative monads. To each of these we associated a systems theories:
- To the powerset monad $\mathrm{P}$, we associated the systems theory of possibilistic systems. This is because a map $f: A \rightarrow \mathrm{PB}$ is a possibilistic map - it assigns a set of possible images to each element $a \in A$.
- To the probability distribution monad $D$, we associated the theory of stochastic systems. This is because a map $f: A \rightarrow \mathrm{D} B$ is a stochastic map.

In general, for any commutative monad $M$ we call a map of the form $f: A \rightarrow M B$ a Kleisli map. The structure of a monad on $M$ lets us compose Kleisli maps, giving us the Kleisli category of the monad. The commutativity then makes the Kleisli category into a symmetric monoidal category.

Definition 2.3.0.1. Let $M: C \rightarrow C$ be a commutative monad on a cartesian category. The Kleisli category $\mathbf{K l}(M)$ is defined as follows:
- The objects of $\mathrm{Kl}(M)$ are the same as those of $C$.
- A map $f: A \leadsto B$ in $\mathbf{K l}(M)$ is a map $f: A \rightarrow M B$ in $C$.
- The identity $\operatorname{id}_{A}: A \leadsto A$ is $\eta_{A}: A \rightarrow M A$.
- For $f: A \leadsto B$ and $g: B \leadsto C$, their composite is $f ; g^{M}: A \rightarrow M C$. In do notation, the Kleisli composite is given by

$$
(f ; g)(a):=\begin{array}{cc}
\text { do } & \\
& b \leftarrow f(a) \\
& g(b)
\end{array} .
$$

Since $g^{M}=M g \circ \mu$, the Kleisli composite may be equivalently defined as $f \circ M g \circ \mu$. The Kleisli category of $M$ becomes a symmetric monoidal structure with with the tensor $A \times B$ and 1 . Note that although $A \times B$ is cartesian in $C$, it will rarely be cartesian in $\mathrm{Kl}(M)$.

We can understand Kleisli composition a bit better if we introduce a graphical language for monads. ${ }^{1}$ This will also help us later in Section 2.6.4 when we learn about biKleisli composition. We will draw an object of our category $X \in C$ as a string:

and a map $f: X \rightarrow Y$ as a bead:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-074.jpg?height=92&width=220&top_left_y=1710&top_left_x=947)

Composition is drawn by connecting strings, and the identity map on $X$ is represented by the same string which represents $X$. We will draw our monad $M: C \rightarrow C$ as a red string:

We can draw the natural transformations $\eta: \operatorname{id}_{C} \Rightarrow M$ and $\mu: M^{2} \Rightarrow M$ as

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-074.jpg?height=122&width=466&top_left_y=2124&top_left_x=821)

respectively. The laws Eq. (2.3) can be written as:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-074.jpg?height=130&width=661&top_left_y=2328&top_left_x=729)
\footnotetext{
${ }^{1}$ If you know of it, this is just the usual string diagram language for 2-categories.
}

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-075.jpg?height=184&width=661&top_left_y=236&top_left_x=729)

The $\operatorname{map} M f: M X \rightarrow M Y$ on objects is written:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-075.jpg?height=97&width=222&top_left_y=499&top_left_x=946)

Note that functoriality is baked in to this string diagram notation; the following diagram could either be interpreted as $M f ; M g$ or $M(f ; g)$, which are equal by the functoriality of $M$ :

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-075.jpg?height=90&width=415&top_left_y=768&top_left_x=844)

The naturality of $\eta$ and $\mu$ is also baked into this notation; it just means we can move them independently of the beads representing functions:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-075.jpg?height=200&width=675&top_left_y=1014&top_left_x=714)

With these conventions in hand, we can now represent a Kleisli map $f: X \rightarrow M Y$ as

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-075.jpg?height=97&width=222&top_left_y=1315&top_left_x=949)

The unit $\eta: X \rightarrow M X$ is written

The composition of Kleisli maps $f: X \rightarrow M Y$ and $g: Y \rightarrow M Z$ is then given by

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-075.jpg?height=203&width=571&top_left_y=1660&top_left_x=777)

We can use these string diagrams to easily check that $\mathbf{K l}(M)$ is actually a category. We use the monad laws Eq. (2.3):

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-075.jpg?height=439&width=1596&top_left_y=2014&top_left_x=335)

Example 2.3.0.2. The Kleisli category $\mathrm{Kl}(\mathrm{P})$ of the powerset monad $\mathrm{P}$ is the category of multi-valued maps. A Kleisli map $f: A \rightarrow \mathrm{P} B$ assigns to each $a \in A$ a subset $f(a) \subseteq B$ of possible images of $a$. Given another Kleisli map $g: B \rightarrow \mathrm{PC}$, their composite in the Kleisli category $g^{\mathrm{P}} \circ f: A \rightarrow \mathrm{PC}$ sends $a \in A$ to the union $\bigcup_{b \in f(a)} g(b)$. In other words, a possible image of $g \circ f$ is any possible image of $g$ of any possible image of $f$.

Example 2.3.0.3. The Kleisli category $\mathrm{Kl}(\mathrm{D})$ of the probability monad $\mathrm{D}$ is the category of stochastic maps. A Kleisli map $f: A \rightarrow \mathrm{D} B$ assigns to each $a \in A$ a probability distribution $f(a)$ on $B$. Given another Kleisli map $g: B \rightarrow \mathrm{DC}$, their composite $g^{\mathrm{D}} \circ f: A \rightarrow \mathrm{DC}$ in the Kleisli category sends $a$ to the probability distribution $c \mapsto$ $\sum_{b \in B} f(a)(b) \cdot g(b)(c)$. That is, since $c$ is the image of $a$ under $g \circ f$ if there is a $b$ which is the image of $a$ under $f$ and $c$ is the image of $b$ under $g$, the probability that $c$ is the image of $a$ is the probability of their being such a $b$.

Thinking of stochastic maps as conditional probabilities, where $f: A \rightarrow \mathrm{D} B$ expresses the conditional probability $p(b \mid a)=f(a)(b)$, then we see that $p(c \mid a)=$ $\sum_{b \in B} p(b \mid a) p(c \mid b)$ as we expect from conditional probabilities.

Now we encompass all our non-deterministic examples in a single definition.

Definition 2.3.0.4. Let $M: C \rightarrow C$ be a commutative monad. A (discrete-time) $M$ system $\mathrm{S}$, also written as

$$
\left(\begin{array}{c}
\text { update }_{S} \\
\text { expose }_{S}
\end{array}\right):\left(\begin{array}{c}
\text { States }_{S} \\
\text { State }_{S}
\end{array}\right) \leftrightarrows\left(\begin{array}{c}
\ln _{S} \\
\text { Out }_{S}
\end{array}\right)
$$

is a system whose dynamics is given by a Kleisli map for $M$. It consists of:
- an object States of states;
- an object Outs of values for exposed variables, or outputs for short;
- an object Ins of parameter values, or inputs for short;
- a map expose ${ }_{\mathrm{S}}$ : States $\rightarrow$ Outs, the $^{\text {, the }}$, exposed variable of state or expose map, which takes a state to the output it yields; and
- a Kleisli map update ${ }_{S}:$ States $\times \operatorname{In}_{S} \rightarrow$ MStates. This is the dynamics or update map which takes a state and a parameter and gives the next state in a non-deterministic way determined by $M$.

This will let us more swiftly describe new non-deterministic systems theories.

For example, suppose that our system is free to choose which state it transitions to next, but there's a catch. For any state $s$ and input parameter $i$, there will be a cost update $_{s}(s, i)\left(s^{\prime}\right) \in[0, \infty]$ associated to each other state $s^{\prime}-$ the cost of transitioning
from $s$ to $s^{\prime}$ given the parameter $i$. A cost of 0 means that this transition is free; a cost of $\infty$ means it is prohibitively expensive, or impossible.

Definition 2.3.0.5. We will define a monad Cost on the category of sets. We think of a Kleisli map $f: A \rightarrow \operatorname{Cost}(B)$ as assiging the best-case cost of producing a $b \in B$ from a given $a \in A$. For practical reasons, we assume that only finitely many $b \in B$ are possible (that is, have finite cost) to produce from an $a \in A$.
- For a set $A$,

$$
\operatorname{Cost}(A):=\{c: A \rightarrow[0, \infty] \mid\{a \in A \mid c(a)<\infty\} \text { is finite }\}
$$

is the set of cost functions $c: A \rightarrow[0, \infty]$ which assign finite values to only finitely many elements of $A$.
- For a set $A, \eta_{\text {Cost }}: A \rightarrow \operatorname{Cost}(A)$ assumes that we can only produce what we have, but that if we already have it, it's free. Formally:

$$
\eta_{\text {Cost }}(a)\left(a^{\prime}\right):= \begin{cases}0 & \text { if } a=a^{\prime} \\ \infty & \text { otherwise }\end{cases}
$$
- For a map with $\operatorname{cost} f: A \rightarrow \operatorname{Cost}(B)$, we define $f^{\operatorname{Cost}}: \operatorname{Cost}(A) \rightarrow \operatorname{Cost}(B)$ by

$$
f^{\text {Cost }}(c)(b):=\min _{a \in A} c(a)+f(a)(b)
$$

That is, given costs on elements of $A$ and conditional costs on elements of $B$ given by $f$, the cost of an element of $B$ is the cost of getting an $a \in A$ together with the cost of producing $b$ from that $a$. So, the best case cost of such a $b$ is the minimum over all $a \in A$ of the total cost of producing $b$ from $a$. We note that the minimum is achieved because only finitely many of the costs are finite.
- Given sets $A$ and $B$, the cost of having an element of $A$ and an element of $B$ is the sum of their costs.

$$
\sigma\left(c, c^{\prime}\right)(a, b):=c(a)+c^{\prime}(b)
$$

Remark 2.3.0.6. We will prove that Definition 2.3.0.5 does indeed give a commutative monad in the upcoming Proposition 2.3.0.11.

Now we can quickly define our new sort of non-determinism.

Definition 2.3.0.7. A (discrete-time) system with costs is a Cost-system.

Example 2.3.0.8. Suppose we are trying to complete a project Proj that involves a number of steps. Let Steps be the set of steps involved. The state of our project at any given time is the set of steps we have completed so far: State Proj $^{\text {:= PSteps. Now, we }}$ may not want to show everyone exactly how our project is going, just that it has hit
certain milestones. So we can let Outproj $:=$ Milestones be our set of milestones and

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-078.jpg?height=59&width=1440&top_left_y=301&top_left_x=337)
completed.

Now, in any project, there are some external conditions to be dealt with. Let In $_{\text {Proj }}=$ Externalities be the set of these externalities. We can assume that there is a cost associated to choosing a next step to take which depends not only on what steps have been completed so far but also on the current external conditions: that is, we can assume we have a function cost : StateProj $\times \operatorname{In}_{\text {Proj }} \rightarrow \operatorname{Cost}($ Steps), and that $\operatorname{cost}(s, i)(x)=0$ whenever $x \in s$ is a step we have already completed. ${ }^{a}$ Given this, we can define the update of our project system as

$$
\text { update }_{\text {Proj }}(s, i)\left(s^{\prime}\right):=\sum_{x \in s^{\prime}} \operatorname{cost}(s, i)(x)
$$

This tells us that the cost moving from having completed the steps $s$ to having completed the steps $s^{\prime}$ given external conditions $i$ is the sum of the cost of completing each step in $s^{\prime}$ which is not in $s$.

The crucial question we want to ask of this model is: how much will the project cost in the best case scenario, given a sequence of external conditions? That is, we will iterate the action of the system through the sequence of paramters starting at

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-078.jpg?height=52&width=1089&top_left_y=1275&top_left_x=339)
\footnotetext{
${ }^{a}$ Although one could imagine this instead as a "maintenance" cost of maintaining the completion of that step.
}

We took Cost to be the monad of best case costs. Let's show that there is also a monad Cost ${ }^{\max }$ of worst case costs. Everything will be the same, but instead of

$$
f^{\text {Cost }}(c)(b):=\min _{a \in A} c(a)+f(a)(b),
$$

we will have

$$
f^{\operatorname{Cost}^{\max }}(c)(b):=\max _{a \in A} c(a)+f(a)(b) .
$$

It is worth noting that this formula has a formal similarity to the following formula:

$$
f^{R}(c)(b):=\sum_{a \in A} c(a) \cdot f(a)(b)
$$

which resembles matrix multiplication. This is indeed the case; for any sort of (commutative) scalars, we get a monad that reproduces matrix arithmetic with those scalars. An appropriate set of scalars is called a commutative rig.

Definition 2.3.0.9. A commutative rig (for "ring without negatives" ${ }^{\prime \prime}$ ) is a set $R$ equipped with a commutative monoid structure $(R,+, 0)$ and a commutative monoid structure $(R, \cdot, 1)$ such that

$$
a \cdot(b+c)=a \cdot b+a \cdot c
$$
for all $a, b, c \in R$.

${ }^{a}$ Rigs are also sometimes referred to as "semirings".

Example 2.3.0.10. The following are important examples of rigs:

1. The natural numbers $\mathbb{N}$ with their usual addition and multiplication form a rig. Similarly, the non-negative rationals and reals form rigs with their usual addition and multiplication.

2. Any ring is a rig. In particular, $\mathbb{Z}, \mathbb{Q}$, and $\mathbb{R}$ are all rigs with their usual addition and multiplication.

3. The tropical rigs are rigs where "addition" is actually minimum or maximum, and "multiplication" is actually addition. In particular, the rig of best-case costs $[0, \infty]$ is a rig with $\mathrm{min}$ as its addition and + as its multiplication. In this rig, distributivity looks like

$$
a+\min \{b, c\}=\min \{a+b, a+c\}
$$

and a linear combination looks like

$$
\min _{i \in I} c_{i}+x_{i}
$$

The additive unit is $\infty$, and the multiplicative unit is 0 .

Similarly, there is a rig of worst-case costs on $[0, \infty]$ with max as addition and + as multiplication. This rig is remarkable in that its additive and multiplicative unit are the same; they are both 0 .

4. In fact, any ordered commutative monoid $(M,+, 0, \leq)$ (where if $a \leq b$, then $c+a \leq c+b$ ) which admits joins $a \vee b$ (that is, least upper bounds) can be made into a commutative rig with addition given by $\vee$ and multiplication given by + .

Proposition 2.3.0.11. For any commutative rig $R$, there is a commutative monad $R \otimes-$ : Set $\rightarrow$ Set defined by
- $R \otimes X$ is the set of $R$-linear combinations of elements of $X$.
- $\eta: X \rightarrow R \otimes X$ sends $x$ to the linear combination $\cdot x$.
- For $f: X \rightarrow R \otimes Y$, we have $f^{R}: R \otimes X \rightarrow R \otimes Y$ defined by

$$
f^{R}\left(\sum_{i} r_{i} x_{i}\right)=\sum_{i} r_{i} f\left(x_{i}\right)
$$
- For sets $X$ and $Y$, we have $\sigma:(R \otimes X) \times(R \otimes Y) \rightarrow R \otimes(X \times Y)$ defined by

$$
\sigma\left(\sum_{i} r_{i} x_{i}, \sum_{j} s_{j} y_{j}\right)=\sum_{i} \sum_{j} r_{i} s_{j}\left(x_{i}, y_{j}\right)
$$

\subsection*{2.4 Adding rewards to non-deterministic systems}

A common way to think of a discrete-time system is as a decision process. We think of the system $\mathrm{A}$ as an agent who needs to make a decision. The agent can choose an action, an element of $\ln _{\mathrm{A}}$, and will then transition into a new state - although it may not know precisely which. We then ask the question: what is the best action for the agent to take in a given situation?

Clearly, an answer to this question will depend on what it means for one action to be better than another. The most common way to model this is by associating each action with a real number reward. The bigger the reward, the better the action (and negative rewards are harmful actions). If the agent is going to take a sequence of actions, we want the rewards to accumulate so that the total reward of a sequence of actions is the sum of each reward.

We can handle this accumulation of rewards, even in a deterministic system, with a commutative monad.

Definition 2.4.0.1. Let $(R,+, 0)$ be a commutative monoid (such as the real numbers). The $R$-valued reward monad or monad of $R$-actions is defined by the following data:
- To each set $A$, we associate the set $R \times A$ of pairs of a reward and an element of $A$.
- For each set $A$, we have $\eta_{A}: A \rightarrow R \times A$ given by yielding no reward: $\eta_{A}(a)=(0, a)$.
- For a function $f: A \rightarrow R \times B$ which yields an element of $B$ and a reward, we give the function

$$
f^{R}: R \times A \rightarrow R \times B
$$

defined by $f^{R}(r, a)=\left(r+\pi_{1} f(a), \pi_{2} f(a)\right)$. This accumulates the reward $\pi_{1} f(a)$ from applying $f$ to $a$ onto a current reward $r$
- For sets $A$ and $B$, we have

$$
\sigma:(R \times A) \times(R \times B) \rightarrow R \times(A \times B)
$$

given by $\sigma\left((r, a),\left(r^{\prime}, b\right)\right)=\left(r+r^{\prime},(a, b)\right)$. The reward for doing two actions simultaneously is the sum of their rewards.

We remark that this works not only in the category of sets, but in any cartesian category.

Exercise 2.4.0.2. Show that the monad of $R$-valued rewards is really a commuativative monad. That is, show that the above data satisfies all each of the laws in Definition 2.1.0.5. Do you see where the commutativity comes into the mix?

We can then describe a system with reward as having an update update ${ }_{S}$ : States $x$ In $_{S} \rightarrow R \times$ States $_{S}$ which sends the current state and action to the next state together with the reward for taking that action (in that state).

Definition 2.4.0.3. A deterministic system with $R$-valued rewards is an $(R \times-)$-system in the sense of Definition 2.3.0.4

We would really like to mix our rewards with non-determinism. In particular, when thinking of a system as an agent making decisions with imperfect information of its environment, we would like to use stochastic systems to model this lack of perfect information. The agent doesn't know exactly what will happen when it performs an action, but it has a good idea of what will probably happen.

The reward our agent gets should depend on what state the agent actually ends up in, and not just the action it takes. Therefore, we want to know the probability of transitioning to a next state and getting a certain reward. This has signature

$$
\text { States } \times \ln _{S} \rightarrow \mathrm{D}(\mathbb{R} \times \text { States })
$$

We will show that the assignment $A \mapsto \mathrm{D}(\mathbb{R} \times A)$ forms a commutative monad. We will show that more generally, if $M$ is any commutative monad and $R$ any commutative monoid, then $M(R \times-)$ is a commutative monad again. We say that we can "put the rewards $R$ into the monad $M^{\prime \prime}$. We can do this explicitly using the map $\lambda: R \times M A \rightarrow$ $M(R \times A)$ defined to be the composite

$$
\lambda:=R \times M A \xrightarrow{\eta_{M} \times \mathrm{id}} M R \times M A \xrightarrow{\sigma^{M}} M(R \times A)
$$

Intuitively, this takes a reward $r \in R$ and a non-deterministic $a \in M A$ and gives us the non-deterministic pair $(r, a)$.

Proposition 2.4.0.4 Let $M$ be a commutative monad and $(R,+, 0)$ a commutative monoid. Then the assignment $A \mapsto M(R \times A)$ is a commutative monad with the following structure:
- $\eta_{M(R \times-)}: A \rightarrow M(R \times A)$ is the composite $A \xrightarrow{\eta_{R}} R \times A \xrightarrow{\eta_{M}} M(R \times A)$.
- Given $f: A \rightarrow M(R \times B)$, we define $f^{M(R \times-)}$ to be the following composite:

$$
\begin{aligned}
M(R \times A) \xrightarrow{M(R \times f)} M(R \times M(R \times B)) \xrightarrow{M \lambda} & M M(R \times R \times B) \\
& \xrightarrow{\mu^{M}} M(R \times R \times B) \xrightarrow{M \mu^{R}} M(R \times B) .
\end{aligned}
$$

Intuitively, this takes a non-deterministic pair $(r, a)$ and, gets the non-deterministic pair $f(a)=\left(f_{1}(a), f_{2}(a)\right)$, and then returns the non-deterministic pair $\left(r+f_{1}(a), f_{2}(a)\right)$.
- Given sets $A$ and $B$, we define $\sigma^{M(R \times-)}: M(R \times A) \rightarrow M(R \times B)$ to be the composite

$$
M(R \times A) \xrightarrow{M}(R \times B) \xrightarrow{\sigma^{M}} M((R \times A) \times(R \times B)) \xrightarrow{M \sigma^{R}} M(R \times A \times B) .
$$

Proof. It is not obvious that this will satsify the monad laws, but it is a rather straightforward check using the laws of $M$ and $R \times-$. We will not prove this result explicitly. However, we will give a slick proof for experts.

A monad structure on $M(R \times A)$ arising via a distributive law such as $\lambda: R \times M A \rightarrow$ $M(R \times A)$ is equivalent to a lift of the monad $M$ to the category of $R \times$-algebras - that is, the category of $R$-actions. But $M: C \rightarrow C$ is a commutative monad, and so in particular it is a symmetric monoidal functor; therefore, it preserves commutative monoids and their actions. For this reason, $M$ extends to the category of $(R \times-)$-algebras, giving us the desired monad structure on $M(R \times-)$. This is again commutative as it is the composite of monoidal functors and so also monoidal.

Example 2.4.0.5. Let's see what this general theorem looks like in the case that $R=\mathbb{R}$ and $M=\mathrm{D}$. In this case, $\lambda: \mathbb{R} \times \mathrm{D} A \rightarrow \mathrm{D}(\mathbb{R} \times A)$ sends the pair $(r, p)$ of a reward and a probability distribution and yields the probability distribution $\delta_{r} p$. Let's see how this lets us iterate the dynamics of a $D(\mathbb{R} \times-)$-system $S$. We have update ${ }_{S}$ : States $X \ln _{S} \rightarrow$ $\mathrm{D}(\mathbb{R} \times$ States $)$, giving us a probabilities update ${ }_{S}(s, i)\left(r, s^{\prime}\right)$ of transitioning from state $s$ on action $i$ into state $s^{\prime}$ and receiving reward $r$. To iterate this, we form the composite

$$
\mathrm{D}\left(\mathbb{R} \times \text { State }_{S}\right) \times \ln _{\mathrm{S}} \xrightarrow{\sigma \circ(\mathrm{id} \times \eta)} \mathrm{D}\left(\mathbb{R} \times \text { State }_{\mathrm{S}} \times \ln _{\mathrm{S}}\right) \xrightarrow{\text { update }_{\mathrm{S}}^{\mathrm{D}(\mathbb{R} \times-)}} \mathrm{D}\left(\mathbb{R} \times \text { State }_{\mathrm{S}}\right)
$$

which sends a pair $(p, i)$ of a prior probability distribution on states and an action to the distribution $(r, s) \mapsto \sum_{s^{\prime} \in \text { States }} p\left(s^{\prime}\right)$ update $_{S}\left(s^{\prime}, i\right)(r, s)$ which gives the probability of receiving the reward $r$ and transitioning into the state $s$ conditioned upon the prior $p$. To iterate, we can continually apply this map to many inputs; let's just do $i$ and $j$. Then we end up with the distribution

$$
(r, s) \mapsto \sum_{s^{\prime \prime} \in \text { States }} \sum_{s^{\prime} \in \text { States }} \sum_{r^{\prime \prime}+r^{\prime}=r} p(s) \cdot \operatorname{update}_{\mathrm{S}}\left(s^{\prime \prime}, i\right)\left(r^{\prime \prime}, s^{\prime}\right) \cdot \text { update }_{\mathrm{S}}\left(s^{\prime}, j\right)\left(r^{\prime}, s\right)
$$

which is the probability that we transition to $s$ in two steps and receive a cumulative reward of $r$.

\subsection*{2.5 Changing the flavor of non-determinism: Monad maps}

In the same way that 0 is a number - or that commutative rings are non-commutative rings - deterministic systems are non-deterministic systems, just with a trivial sort of non-determinism. Deterministic systems are $M$-systems for the identity monad $\mathrm{id}(X)=$ $X$. No matter what kind of non-determinism we are considering, we can always consider a deterministic system as a non-deterministic system, because we can take the update: State $x \operatorname{In} \rightarrow$ State and post compose by $\eta:$ State $\rightarrow$ MState. This operation of turning a deterministic system into an $M$-system has a few nice properties; for example, if we iterate the system and then turn it into an $M$-system, we get the same result as if we had iterated it as an $M$-system.

In general, if we have a commutative monad map $M \rightarrow N$, then we can turn $M$-systems into $N$-systems.

Definition 2.5.0.1. A commutative monad map $\phi: M \rightarrow N$ is a natural transformation for which the following diagrams commute:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-083.jpg?height=201&width=266&top_left_y=398&top_left_x=970)

$\cdot$

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-083.jpg?height=225&width=331&top_left_y=641&top_left_x=932)

$\cdot$

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-083.jpg?height=222&width=485&top_left_y=908&top_left_x=863)

Proposition 2.5.0.2. There is a unique commutative monad map id $\rightarrow M$, and it is given by $\eta_{M}$.

Proof. Let $\phi$ be such a map. Then condition Eq. (2.11) says precisely that $\phi=\eta_{M}$. So it just remains to check that $\eta$ is a commutative monad map. Now, Eq. (2.11) commutes trivially, and Eq. (2.12) is in this case one of the diagrams defining $M$ from Eq. (2.3). Finally, Eq. (2.13) is in this case Eq. (2.7).

We can then turn any deterministic system $S$ into an $M$-system by defining its new update to be $\eta_{M} \circ$ update $_{S}$. For possibilistic systems, this says that only the state that $S$ actually transitions into is possible. For stochastic systems, this says that the probability that the system transitions into the state it actually transitions into is 1.

Intuitively, stochastic non-determinism is a refinement of possibilistic non-determinism: it not only tells us what is possible, but how likely it is. We can package this intuition into a commutative monad morphism $\phi: \mathrm{D} \rightarrow \mathrm{P}$.

Proposition 2.5.0.3. There is a commutative monad morphism $\phi: \mathrm{D} \rightarrow \mathrm{P}$ given by sending a probability distribution to the set of elements with non-zero probability:

$$
\phi(p)=\{a \in A \mid p(a) \neq 0\} .
$$

Proof. We check that this satisfies the laws.
- (Eq. (2.11)) The only element which $\delta_{a}$ assigns a non-zero probability is $a$.
- (Eq. (2.12)) Given a formal convex combination $\sum_{i} \lambda_{i} p_{i}$ of probability distributions $p_{i} \in \mathrm{D} A$, we see that

$$
\phi \mu^{\mathrm{D}}\left(\sum_{i} \lambda_{i} p_{i}\right)=\left\{a \in A \mid \sum_{i} \lambda_{i} p_{i}(a) \neq 0\right\},
$$

while

$$
\mathrm{D} \phi\left(\sum_{i} \lambda_{i} p_{i}\right)=\sum_{i} \lambda_{i}\left\{a \in A \mid p_{i}(a) \neq 0\right\}
$$

and so taking $\phi$ of that yields

$$
\left\{\left\{a \in A \mid p_{i}(a) \neq 0\right\} \mid \lambda_{i} \neq 0\right\}
$$

so, finally

$$
\mu^{\mathrm{P}}\left(\phi \mathrm{D} \phi\left(\sum_{i} \lambda_{i} p_{i}\right)\right)=\bigcup_{\lambda_{i} \neq 0}\left\{a \in A \mid p_{i}(a) \neq 0\right\} .
$$

Both paths around the square are equal since all of the $\lambda_{i}$ and $p_{i}(a)$ are positive.
- (Eq. (2.13)) Let $p$ be a probability distribution on $A$ and $q$ a probability distribution on $B$. Then

$$
\phi(\sigma(p, q))=\{(a, b) \mid p(a) q(b) \neq 0\}
$$

while

$$
\sigma(\phi(p), \phi(q))=\{(a, b) \mid p(a) \neq 0 q(b) \neq 0\} .
$$

These are equal since $p(a) q(b) \neq 0$ if and only if both $p(a)$ and $q(b)$ are not 0 .

This lets us turn a stochastic system into a possibilistic system, saying that a transition is possible if it has non-zero probability.

Exercise 2.5.0.4. Show that $\mathrm{D} \eta_{\mathbb{R}}: \mathrm{D} A \rightarrow \mathrm{D}(\mathbb{R} \times A)$ is a commutative monad morphism. That is, show that the following diagrams commute:

1.

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-084.jpg?height=193&width=331&top_left_y=1920&top_left_x=946)

2.

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-084.jpg?height=239&width=551&top_left_y=2168&top_left_x=825)

3.

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-085.jpg?height=233&width=694&top_left_y=282&top_left_x=756)

This shows that we can always consider a stochastic system as a stochastic system with rewards by assigning every transition the reward 0.

The reason we need all the laws for the monad morphism and not just an arbitrary family of maps $\phi: M A \rightarrow N A$ is that with these laws, we get functors $\mathrm{Kl}(M) \rightarrow$ $\mathbf{K l}(N)$ which tell us that iterating and then changing our non-determinism is the same as changing our non-determinism and then iterating. We begin with a useful lemma.

Lemma 2.5.0.5. In the definition of a commutative monad map $\phi: M \rightarrow N$, the commutativity of diagram Eq. (2.12) can be replaced by the commutativity of the following diagram for any $f: A \rightarrow M B$ :

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-085.jpg?height=220&width=377&top_left_y=1240&top_left_x=863)

That is,

$$
f^{M} \stackrel{ }{\circ} \phi=\phi:(f \circ \phi)^{N} .
$$

In do notation, this reads

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-085.jpg?height=225&width=621&top_left_y=1690&top_left_x=749)

Proof. Before we begin, we note that, by the naturality of $\phi, M \phi ; \phi=\phi ; N \phi$ :

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-085.jpg?height=222&width=401&top_left_y=2120&top_left_x=857)

That is, we can take the top of Eq. (2.12) to be $\phi \circ N \phi$ rather than $M \phi \circ \phi$.

We recall that $f^{M}=M f ; \mu^{M}$, and similarly $(f \circ \phi)^{N}=N(f \circ \phi) ; \mu^{N}$. So we may
rewrite Eq. (2.17) as the solid outer diagram in

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-086.jpg?height=350&width=382&top_left_y=340&top_left_x=861)

Now we are ready to prove our lemma. We note that the top square in this diagram always commutes by the naturality of $\phi$. Eq. (2.12) is the lower square in this diagram; so, if it commutes, then the outer square (which is Eq. (2.17)) commutes. On the other hand, if Eq. (2.17) commmutes for all $f: A \rightarrow M B$, we may take $f=\mathrm{id}: M A \rightarrow M A$ to find that the outer square of Eq. (2.18) becomes just Eq. (2.12).

Proposition 2.5.0.6. Let $\phi: M \rightarrow N$ be a commutative monad morhpisms. Then there is a strict symmetric monoidal functor

$$
\phi_{*}: \mathrm{Kl}(M) \rightarrow \mathbf{K 1}(N)
$$

acting as the identity on objects and sending the Kleisli map $f: A \rightarrow M B$ to the composite

$$
\phi_{*} f:=A \xrightarrow{f} M B \xrightarrow{\phi} N B .
$$

Proof. We will check that this is a functor; that it is strictly symmetric monoidal follows from this and from the fact that it acts as the identity on objects. The identity $\eta_{M}: A \rightarrow$ $M A$ in $\mathbf{K l}(M)$ gets sent to $\phi_{*} \eta_{M}=\eta_{M} ; \phi$. This equals $\eta_{N}: A \rightarrow N A$ by Eq. (2.11).

Given $f: A \rightarrow M B$ and $g: B \rightarrow M C$, their composite is $f: g^{M}: A \rightarrow M C$, so that

$$
\begin{array}{rlr}
\phi_{*}\left(f \circ g^{M}\right) & :=f \circ g^{M} \circ \phi & \\
& =f \circ \phi \circ(g \circ \phi)^{N} & \text { by Lemma 2.5.0.5 } \\
& =\left(\phi_{*} f\right)\left(\phi_{*} g\right)^{N} . &
\end{array}
$$

We can also check that $\phi_{*}$ is a functor using our string diagram notation for monads. In that notation, $\phi: M \rightarrow N$ is written as

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-086.jpg?height=84&width=222&top_left_y=2246&top_left_x=946)

and would satisfy the laws:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-086.jpg?height=87&width=469&top_left_y=2439&top_left_x=820)

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-087.jpg?height=182&width=665&top_left_y=234&top_left_x=730)

(As before, these diagrams are not really equipped to describe the commutativity of monads, and so we are only using the laws concerning the unit and multiplication.) The action of $\phi_{*}$ on a Kleisli map $f: X \rightarrow M Y$ is then written as

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-087.jpg?height=133&width=314&top_left_y=600&top_left_x=903)

We can check that $\phi_{*}$ is functorial quickly and diagrammatically:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-087.jpg?height=328&width=1263&top_left_y=817&top_left_x=428)

\subsection*{2.6 Wiring together non-deterministic systems: the generalized lens construction}

Consider a stochastic source process

$$
\text { Source }
$$

We can imagine, as Claude Shannon did, that this source is an interlocutor communicating over a wire. Suppose we have another interlocutor who reads the signal generated by our source and generates their own signal in repsonse:

$$
\text { Transformer }
$$

Having these two models, we can form a new stochastic source by considering them together:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-087.jpg?height=167&width=738&top_left_y=2009&top_left_x=734)

We imagine that the Transformer listens to the signal generated by the Source, but with noise $\rho$ on the wire. This wiring diagram

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-087.jpg?height=184&width=740&top_left_y=2334&top_left_x=730)
can be described as a monadic lens $\left(\begin{array}{c}f^{\sharp} \\ f\end{array}\right):\left(\begin{array}{c}B \\ A \times C\end{array}\right) \leftrightarrows\left(\begin{array}{l}1 \\ C\end{array}\right)$ :
- $f: A \times C \rightarrow C$ is the projection $\pi_{2}$.
- $f^{\sharp}: A \times C \times 1 \rightarrow D B$ is $\rho \circ \pi_{1}$ where $\rho: A \rightarrow D B$ is the stochastic function describing the noise on the wire.

This new notion of monadic lens which lets us wire together non-deterministic systems will be the focus of this section.

In Section 1.3, we saw how to wire together systems deterministically, and using functions from an algebraic theory on the wire. This worked because wiring diagrams could be interpreted as lenses, and deterministic and differential systems were also lenses; then we could just compose them.

But non-deterministic systems are not lenses in a cartesian category; they have that monad sitting over the states in the codomain of update:

$$
\text { update }_{S}: \text { States } \times \ln \mathrm{S} \rightarrow \text { MStates. }
$$

It may appear that we could consider this as a map in the Kleisli category, and just take lenses in the Kleisli category. But in the Kleisli category, the operation $\times$ is rarely a cartesian product, and we can only describe lenses in cartesian categories. The reason we can only describe lenses in cartesian categories is because in the formula for the passback of a composite of lenses, we use a variable twice; that is, we use the diagonal map $\Delta: A^{+} \rightarrow A^{+} \times A^{+}$, a feature of cartesian categories.

We will need a new perspective on lenses and lens composition which suggests how to change the passback of the lenses. It is worth noting that we only need to duplicate in the passforward direction; we should be free to change the passback direction.

In this section, we will give a new perspective on the category of lenses using the Grothendieck construction. This perspective constructs the category of lenses out of an indexed category $\mathbf{C t x}_{-}: C^{\mathrm{op}} \rightarrow$ Cat of objects of the cartesian category $C$ in context. This construction works for any indxed category $\mathcal{A}: \mathcal{C}^{\text {op }} \rightarrow$ Cat, which lets us define a notion of $\mathcal{A}$-lenses using any indexed category. By choosing an appropriate indexed category, we will arrive at the notion of $M$-lenses for a commutative monad $M$; this will give us the wiring diagram calculus for non-deterministic systems that we wanted.

First, we introduce the abstract categorical notions of indexed category and the Grothendieck construction.

\subsection*{2.6.1 Indexed categories and the Grothendieck construction}

An indexed category $\mathcal{A}: C^{\mathrm{op}} \rightarrow$ Cat is a family of categories $\mathcal{A}(C)$ that varies functorially with an object $C \in C$ of the base category $C$. We will intepret the base category $C$ as the category of passforward maps, and the categories $\mathcal{A}\left(C^{+}\right)$as the categories of passback maps that take $C^{+}$as an extra argument.

Definition 2.6.1.1. A strict indexed category $\mathcal{A}: \mathcal{C}^{\mathrm{op}} \rightarrow$ Cat is a contravariant functor. We call the category $C$ the base of the indexed category $\mathcal{A}$. Explicitly, an indexed category $\mathcal{A}$ has:
- A base category $C$.
- For every object $C \in C$ of the base, a category $\mathcal{A}(C)$.
- For every map $f: C \rightarrow C^{\prime}$ in the base, a pullback functor $f^{*}: \mathcal{A}\left(C^{\prime}\right) \rightarrow \mathcal{A}(C)$, which we think of as "reindexing" the objects of $\mathcal{A}\left(C^{\prime}\right)$ so that they live over $\mathcal{A}(C)$.
- Reindexing is functorial: $(f \circ g)^{*}=g^{*} \circ f^{*}$ and $\mathrm{id}^{*}=\mathrm{id}$.

Remark 2.6.1.2. We have given the definition of a strict indexed category. A general indexed category is a pseudo-functor $\mathcal{A}: \mathrm{C}^{\mathrm{op}} \rightarrow$ Cat, which is like a functor but functoriality only holds up to coherent isomorphism. As in the case of monoidal categories, the coherences in the isomorphisms are often just bookkeeping trivialities.

However, the theory of strict indexed categories is noticeably easier, and most of our examples will be strict. Since we will mostly be using strict indexed categories, we will often refer to them simply as "indexed categories".

Indexed categories are quite common throughout mathematics. We will construct a particular example for our own purposes in Section 2.6.2, and more throughout the book.

Example 2.6.1.3. Recall that a dependent set is a function $X: A \rightarrow$ Set from a set into the category of sets. We have an indexed category of dependent sets

$$
\operatorname{Set}^{(-)}: \text {Set }^{\mathrm{op}} \rightarrow \text { Cat }
$$

which is defined as follows:
- To each set $A$, we assign the category $\operatorname{Set}^{A}$ of sets indexed by $A$. The objects of Set $^{A}$ are the sets $X: A \rightarrow$ Set indexed by $A$, and a map $f: X \rightarrow Y$ is a family of maps $f_{a}: X_{a} \rightarrow Y_{a}$ indexed by the elements $a \in A$. Composition is given componentwise: $(f ; g)_{a}=f_{a} \circ g_{a}$.
- To every function $f: A^{\prime} \rightarrow A$, we get a reindexing functor

$$
f^{*}: \operatorname{Set}^{A} \rightarrow \operatorname{Set}^{A^{\prime}}
$$

Given by precomposition: $X \mapsto X \circ f$. The indexed set $X \circ f: A^{\prime} \rightarrow$ Set is the set $X_{f\left(a^{\prime}\right)}$ on the index $a^{\prime} \in A^{\prime}$. The families of functions get reindexed the same way.
- Since our reindexing is just given by precomposition, it is clearly functorial.

We will return to this example in much greater detail in Chapter 4.

If we have an family of sets $A: I \rightarrow$ Set indexed by a set $I$, we can form the disjoint union $\sum_{i \in I} A_{i}$, together with the projection $\pi: \sum_{i \in I} A_{i} \rightarrow I$ sending each $a \in A_{i}$ to
i. The Grothendieck construction is a generalization of this construction to indexed categories. Namely, we will take an indexed category $\mathcal{A}: C \rightarrow$ Cat and form a new category

$$
\int^{C: C} \mathcal{A}(C)
$$

which we think of as a "union" of all the categories $\mathcal{A}(C)$. But this "union" will not be disjoint since there will be morphisms from objects in $\mathcal{A}(C)$ to objects in $\mathcal{A}\left(C^{\prime}\right)$. This is why we use the integral notation; we want to suggest that the Grothendieck construction is a sort of sum. ${ }^{2}$

Definition 2.6.1.4. Let $\mathcal{A}: e^{\mathrm{op}} \rightarrow$ Cat be an indexed category. The Grothendieck construction of $\mathcal{A}$

$$
\int^{C: C} \mathcal{A}(C)
$$

is the category with:
- Objects pairs $\left(\begin{array}{l}A \\ C\end{array}\right)$ of objects $C \in C$ and $A \in \mathcal{A}(C)$. We say that $A$ "sits over" $C$.
- Maps $\left(\begin{array}{l}f_{b} \\ f\end{array}\right):\left(\begin{array}{l}A \\ C\end{array}\right) \rightrightarrows\left(\begin{array}{l}A^{\prime} \\ C^{\prime}\end{array}\right)$ pairs of $f: C \rightarrow C^{\prime}$ in $C$ and $f_{b}: A \rightarrow f^{*} A^{\prime}$ in $\mathcal{A}(C)$.
- Given $\left(\begin{array}{c}f_{b} \\ f\end{array}\right):\left(\begin{array}{l}A \\ C\end{array}\right) \rightrightarrows\left(\begin{array}{l}A^{\prime} \\ C^{\prime}\end{array}\right)$ and $\left(\begin{array}{c}g_{b} \\ g\end{array}\right):\left(\begin{array}{l}A^{\prime} \\ C^{\prime}\end{array}\right) \rightrightarrows\left(\begin{array}{l}A^{\prime \prime} \\ C^{\prime \prime}\end{array}\right)$, their composite is given by

$$
\left(\begin{array}{c}
f_{b} \\
f
\end{array}\right) \circ\left(\begin{array}{c}
g_{b} \\
g
\end{array}\right):=\left(\begin{array}{c}
f_{b} \circ f^{*} g_{b} \\
f \circ g
\end{array}\right)
$$

Written with the signatures, this looks like

$$
\left(\begin{array}{c}
A \xrightarrow{f_{b}} f^{*} A^{\prime} \xrightarrow{f^{*} g_{b}} f^{*} g^{*} A^{\prime \prime}=(f ; g)^{*} A^{\prime \prime} \\
C \xrightarrow{f} C^{\prime} \xrightarrow{g} C^{\prime \prime}
\end{array}\right)
$$
- The identity is given by $\left(\begin{array}{l}\operatorname{id}_{A} \\ \operatorname{id}_{C}\end{array}\right):\left(\begin{array}{l}A \\ C\end{array}\right) \rightrightarrows\left(\begin{array}{l}A \\ C\end{array}\right)$

Exercise 2.6.1.5. Check that Definition 2.6.1.4 does indeed make $\int^{C: C} \mathcal{A}(C)$ into a category. That is, check that composition as defined above is associative and unital.

Pure and cartesian maps. A map in a Grothendieck construction is a pair $\left(\begin{array}{l}f_{\mathrm{b}} \\ f\end{array}\right)$ : $\left(\begin{array}{l}A \\ C\end{array}\right) \rightrightarrows\left(\begin{array}{l}A^{\prime} \\ C^{\prime}\end{array}\right)$ of maps $f: C \rightarrow C^{\prime}$ and $f_{b}: A \rightarrow f^{*} A^{\prime}$. It is not too hard to see that a map
\footnotetext{
${ }^{2}$ The Grothendieck construction is an example of a lax colimit in 2-category theory, another sense in which it is a 'sort of sum'.
}
is an isomorphism in a Grothendieck construction if and only if both its constituent maps are isomorphisms in their respective categories.

Proposition 2.6.1.6. Let $\mathcal{A}: C^{\mathrm{op}} \rightarrow$ Cat be an indexed category and let $\left(\begin{array}{l}f_{b} \\ f\end{array}\right):\left(\begin{array}{l}A \\ C\end{array}\right) \rightrightarrows$ $\left(\begin{array}{l}A^{\prime} \\ C^{\prime}\end{array}\right)$ be a map in its Grothendieck construction. Then $\left(\begin{array}{l}f_{b} \\ f\end{array}\right)$ is an isomorphism if and only if $f$ is an isomorphism in $C$ and $f_{\mathrm{b}}$ is an isomorphism in $\mathcal{A}(C)$.

Proof. First, let's show that if both $f$ and $f_{\mathrm{b}}$ are isomorphisms, then $\left(\begin{array}{l}f_{\mathrm{b}} \\ f\end{array}\right)$ is an isomorphism. We then have $f^{-1}: C^{\prime} \rightarrow C$ and $f_{\mathrm{b}}^{-1}: f^{*} A^{\prime} \rightarrow A$. From $f_{\mathrm{b}}^{-1}$, we can form $\left(f^{-1}\right)^{*}\left(f_{b}^{-1}\right):\left(f^{-1}\right)^{*} f^{*} A^{\prime} \rightarrow\left(f^{-1}\right)^{*} A$, which has signature $A^{\prime} \rightarrow\left(f^{-1}\right)^{*} A$ because $f^{-1} \stackrel{\circ}{ } f=\mathrm{id}:$

$$
A^{\prime}=\left(f^{-1} \div f\right)^{*} A^{\prime}=\left(f^{-1}\right)^{*} f^{*} A^{\prime} \xrightarrow{\left(f^{-1}\right)^{*}\left(f_{b}^{-1}\right)}\left(f^{-1}\right)^{*} A .
$$

Now, consider the map $\left(\begin{array}{c}\left(f^{-1}\right)^{*} f_{b}^{-1} \\ f^{-1}\end{array}\right):\left(\begin{array}{l}A^{\prime} \\ C^{\prime}\end{array}\right) \rightrightarrows\left(\begin{array}{l}A \\ C\end{array}\right)$. We'll show that this is an inverse to $\left(\begin{array}{l}f_{b} \\ f\end{array}\right)$. Certainly, the bottom components will work out; we just need to worry about the top. That is, we need to show that $f^{*}\left(\left(f^{-1}\right)^{*} f_{\mathrm{b}}^{-1}\right) \circ f_{\mathrm{b}}=\mathrm{id}$ and $\left(f^{-1}\right)^{*}\left(f_{\mathrm{b}}\right) \circ\left(f^{-1}\right)^{*}\left(f_{\mathrm{b}}^{-1}\right)=\mathrm{id}$. Both of these follow quickly by functoriality.

On the other hand, suppose that $\left(\begin{array}{c}f_{\mathrm{b}} \\ f\end{array}\right)$ is an isomorphism with inverse $\left(\begin{array}{c}g_{\mathrm{b}} \\ g\end{array}\right)$. Then $g f=\mathrm{id}$ and $f g=\mathrm{id}$, so $f$ is an isomorphism. We can focus on $f_{\mathrm{b}}$. We know that $f^{*} g_{\mathrm{b}} \circ f_{\mathrm{b}}=\mathrm{id}$ and $g^{*} f_{\mathrm{b}} \circ g_{\mathrm{b}}=\mathrm{id}$. Applying $f^{*}$ to the second equation, we find that $f_{\mathrm{b}} \circ f^{*} g_{\mathrm{b}}=\mathrm{id}$, so that $f_{\mathrm{b}}$ is an isomorphism with inverse $f^{*} g_{\mathrm{b}}$.

This proposition suggests two interesting classes of maps in a Grothendieck construction: the maps $\left(\begin{array}{c}f_{\mathrm{b}} \\ f\end{array}\right)$ for which $f$ is an isomorphism, and those for which $f_{\mathrm{b}}$ is an isomorphism.

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-091.jpg?height=95&width=1453&top_left_y=1687&top_left_x=328)
its Grothendieck construction. We say that $\left(\begin{array}{c}f_{b} \\ f\end{array}\right)$ is
- pure if $f$ is an isomorphism, and
- cartesian if $f_{\mathrm{b}}$ is an isomorphism.

The pure maps correspond essentially to the maps in the categories $\mathcal{A}(C)$ at a given index $C$, while the cartesian maps correspond essentially to the maps in $C$.

Remark 2.6.1.8. The name "pure" is non-standard. The usual name is "vertical". But we are about to talk about "vertical" maps in a technical sense when we come to double categories, so we've renamed the concept here to avoid confusion later.

Example 2.6.1.9. We have often seen systems that expose their entire state, like Time of Example 3.3.0.7. We will soon see that lenses are maps in a Grothendieck construction.

Considered as lenses, these systems are pure in the sense that their expose function is an isomorphism.

Exercise 2.6.1.10. Let $\left(\begin{array}{c}f_{b} \\ f\end{array}\right)$ and $\left(\begin{array}{c}g_{b} \\ g\end{array}\right)$ be composable maps in a Grothendieck construction,

1. Suppose that $\left(\begin{array}{c}g_{b} \\ g\end{array}\right)$ is cartesian. Show that $\left(\begin{array}{c}f_{b} \\ f\end{array}\right)$ is cartesian if and only if their composite is cartesian. Is the same true for pure maps?

2. Suppose that $\left(\begin{array}{l}f_{b} \\ f\end{array}\right)$ is pure. Show that $\left(\begin{array}{c}g_{b} \\ g\end{array}\right)$ is pure if and only if their composite is pure. Is the same true for cartesian maps?

\subsection*{2.6.2 Maps with context and lenses}

In this section, we'll see the category Lens ${ }_{C}$ of lenses in a cartesian category $C$ can be described using the Grothendieck construction. To do this, we need some other categories named after their maps (rather than their objects): category of maps with context $C$ for some a given $C \in C$.

Definition 2.6.2.1. Let $C$ be a cartesian category and let $C \in C$. The category $\operatorname{Ctx}_{C}$ of maps with context $C$ is the category defined by:
- Objects are the objects of $C$.
- Maps $f: X \leadsto Y$ are maps $f: C \times X \rightarrow Y$.
- The composite $g \circ f$ of $f: X \leadsto Y$ and $g: Y \leadsto Z$ is the map

$$
(c, x) \mapsto g(c, f(c, x)): C \times X \rightarrow Z .
$$

Diagrammatically, this is the composite:

$$
C \times X \xrightarrow{\Delta_{C} \times X} C \times C \times X \xrightarrow{C \times f} C \times Y \xrightarrow{g} Z .
$$
- The identity id : $X \leadsto X$ is the second projection $\pi_{2}: C \times X \rightarrow X$.

We can prove that $\mathbf{C t x}_{C}$ is a category using a similar string diagrams to those we used in Section 2.3. We have a functor $X \mapsto C \times X: C \rightarrow C$ which we can draw as a blue string:

If we represent $X \in C$ by the string then we represent $C \times X$ as

We can therefore represent a morphism $f: C \times X \rightarrow Y$ in the context of $C$ as a bead like this:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-092.jpg?height=101&width=225&top_left_y=2427&top_left_x=945)

To compose maps in context, we need the diagonal map $\Delta_{C} \times X: C \times X \rightarrow C \times C \times X$ and the second projection $\pi_{2}: C \times X \rightarrow X$. Since these maps are natural in $X$, we can draw them as

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-093.jpg?height=117&width=477&top_left_y=386&top_left_x=821)

Then the composition in $\mathbf{C t x}_{C}$ of maps in context $f: C \times X \rightarrow Y$ and $g: C \times Y \rightarrow Z$ is drawn as:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-093.jpg?height=193&width=566&top_left_y=603&top_left_x=774)

and the second projection $\pi_{2}: C \times X \rightarrow X$ is drawn

This is exactly dual to the story about Kleisli composition we saw in Section 2.3! To show that $\mathbf{C t x}{ }_{C}$ is a category, we need to note that the following equations hold:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-093.jpg?height=317&width=661&top_left_y=1094&top_left_x=729)

These say that

$$
\left(\Delta_{C} \times X\right) \stackrel{\circ}{ }\left(\pi_{2} \times X\right)=\operatorname{id}_{C \times X}=\left(\Delta_{C} \times X\right) \stackrel{\circ}{ }\left(C \times \pi_{2}\right)
$$

and

$$
\left(\Delta_{C} \times X\right) \stackrel{ }{ }\left(C \times \Delta_{C} \times X\right)=\left(\Delta_{C} \times X\right) \stackrel{ }{ } \times\left(\Delta_{C} \times X \times C\right) \text {. }
$$

These hold by some simple work in the cartesian category $C$ (see Exercise 2.6.2.2). On elements, the first says that the composite $x \mapsto(x, 0) \mapsto x$ and $x \mapsto(0, x) \mapsto x$ are both the identity function $x \mapsto x$. The second says that $x \mapsto(x, x) \mapsto((x, x), x)$ equals $x \mapsto(x, x) \mapsto(x,(x, x))$, at least when we forget about the inner parentheses.

With these laws in hand, we can prove associativity and identity of composition in $\mathrm{Ct}_{C}$ by appealing to the following diagrams:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-093.jpg?height=428&width=1596&top_left_y=2033&top_left_x=335)

Exercise 2.6.2.2. Show that the following composites are equal in any cartesian category:

1.

$$
\left(\Delta_{C} \times X\right) \stackrel{\circ}{q}\left(\pi_{2} \times X\right)=\left(\Delta_{C} \times X\right) \stackrel{ }{9}\left(C \times \pi_{2}\right)
$$

These are both maps $C \times X \rightarrow C \times C \times C \times X$.

2.

$$
\left(\Delta_{C} \times X\right) \stackrel{q}{ }\left(C \times \Delta_{C} \times X\right)=\mathrm{id}_{C \times X}=\left(\Delta_{C} \times X\right) \stackrel{\circ}{ }\left(\Delta_{C} \times X \times C\right) .
$$

These are all maps $C \times X \rightarrow C \times X$.

Exercise 2.6.2.3. Show that $\mathbf{C t} \mathbf{x}_{1}$ is equivalent to the underlying cartesian category $C$. In other words, maps in the context 1 have "no context".

Together, we can arrange the categories of maps with context into an indexed category.

Definition 2.6.2.4. The indexed category of maps with context

$$
\mathbf{C t x}_{-}: e^{\mathrm{op}} \rightarrow \text { Cat }
$$

is defined by:
- For $C \in C$, we have the category $\mathbf{C t x}_{C}$ of maps with context $C$.
- For a map $r: C^{\prime} \rightarrow C$, we get a reindexing functor

$$
r^{*}: \mathbf{C t x}_{C} \rightarrow \mathbf{C t x}_{C^{\prime}}
$$

given by sending each object to itself, but each morphism $f: C \times X \rightarrow Y$ in $\mathbf{C t x}_{C}$ to the map $r^{*} f:=f \circ(r \times X)$ :

$$
C^{\prime} \times X \xrightarrow{r \times X} C \times X \xrightarrow{f} Y .
$$

On elements,

$$
r^{*} f\left(c^{\prime}, x\right):=f\left(r\left(c^{\prime}\right), x\right) .
$$

We note that this is evidently functorial.

To see that to every $r: C^{\prime} \rightarrow C$ we get a functor $r^{*}: \mathbf{C t x}_{C} \rightarrow \mathbf{C t x}_{C^{\prime}}$, we can use string diagrams. We can draw $r$ as

so that the action of $r^{*}$ is given by
![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-094.jpg?height=262&width=328&top_left_y=2270&top_left_x=888)

If we note that $r$ satisfies the following laws:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-095.jpg?height=231&width=664&top_left_y=329&top_left_x=728)

we can then prove that $r^{*}$ is a functor graphically:
![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-095.jpg?height=306&width=1264&top_left_y=622&top_left_x=428)

Those laws mean that $x \mapsto r(x) \mapsto 0$ is equal to $x \mapsto 0$ and $x \mapsto r(x) \mapsto(r(x), r(x))$ equals $x \mapsto(x, x) \mapsto(r(x), r(x))$.

Proposition 2.6.2.5. The category Lens ${ }_{C}$ of lenses in $C$ is the Grothendieck construction of the indexed category of opposites of the categories of maps with context:

$$
\text { Lens }_{C}=\int^{C \in C} \operatorname{Ctx}_{C}^{\text {op }} \text {. }
$$

Proof. We will expand the definition of the right hand side and see that it is precisely the category of lenses.

The objects of $\int^{C \in \text { Set }} \mathbf{C t x}_{C}^{\text {op }}$ are pairs $\left(\begin{array}{l}A^{-} \\ A^{+}\end{array}\right)$of objects of $C$. All good so far.

A map in $\int^{C \in C} \mathbf{C t x}_{C}^{\text {op }}$ is a pair $\left(\begin{array}{c}f^{\sharp} \\ f\end{array}\right)$ with $f: A^{+} \rightarrow B^{+}$and $f^{\sharp}: A^{-} \leadsto f^{*} B^{-}$in $\mathbf{C t x}_{A^{+}}^{\text {op }}$. Now, $f^{*} B^{-}=B^{-}$so $f^{\sharp}$ has signature $A^{-} \leadsto B^{-}$in $\mathbf{C t x}_{A^{+}}^{\text {op }}$, which means $f^{\sharp}$ has signature $B^{-} \leadsto A^{-}$in $\mathbf{C t x}_{A^{+}}$, which means that $f^{\sharp}$ is a really a function $A^{+} \times B^{-} \rightarrow A^{-}$. In other words, a map in $\int^{C \in \text { Set }} \mathbf{C t x}_{C}^{\text {op }}$ is precisely a lens. We note that the identity map is the identity lens.

Finally, we need to check that composition in $\int^{C \in C} \mathbf{C t x}_{C}^{\text {op }}$ is lens composition. Suppose that $\left(\begin{array}{c}f^{\sharp} \\ f\end{array}\right):\left(\begin{array}{l}A^{-} \\ A^{+}\end{array}\right) \leftrightarrows\left(\begin{array}{c}B^{-} \\ B^{+}\end{array}\right)$and $\left(\begin{array}{c}g^{\sharp} \\ g\end{array}\right):\left(\begin{array}{c}B^{-} \\ B^{+}\end{array}\right) \leftrightarrows\left(\begin{array}{c}C^{-} \\ C^{+}\end{array}\right)$are lenses. In $\int^{C \in \text { Set }} \mathbf{C} x_{C}^{\text {op }}$, their composite is

$$
\left(\begin{array}{c}
f^{*} g^{\sharp} \circ f^{\sharp} \\
g \circ f
\end{array}\right) \text {. }
$$

The bottom is all good, we just need to check that the top - which, remember, lives in $\mathrm{Ctx}_{A^{+}}^{\mathrm{op}}$ - is correct. Since the composite up top is in the opposite, we are really calculating $f^{\sharp} \circ f^{*} g^{\sharp}$ in $\mathbf{C t x}_{A^{+}}$. By definition, this is

$$
\left(a^{+}, c^{-}\right) \mapsto f^{\sharp}\left(a^{+}, g^{\sharp}\left(f\left(a^{+}\right), c^{-}\right)\right)
$$

which is precisely their composite as lenses!

Exercise 2.6.2.6. Make sure you really understand Proposition 2.6.2.5.

We take Proposition 2.6.2.5 as paradigmatic of the notion of lens, and use this idea to define lenses from any indexed category.

Definition 2.6.2.7. Let $\mathcal{A}: \mathcal{C o p}^{\text {op }} \rightarrow$ Cat be an indexed category. The category of $\mathcal{A}$ lenses is the Grothendieck construction of $\mathcal{A}^{\mathrm{op}}$ :

$$
\text { Lens }_{\mathcal{A}}=\int^{C \in C} \mathcal{A}(C)^{\mathrm{op}} \text {. }
$$

Example 2.6.2.8. Recall the indexed category $\mathbf{S e t}^{(-)}:$Set $^{\mathrm{op}} \rightarrow$ Cat of dependent sets from Example 2.6.1.3. A Set ${ }^{(-)}$-lens $\left(\begin{array}{c}f^{\sharp} \\ f\end{array}\right):\left(\begin{array}{c}A_{a}^{-} \\ a \in A^{+}\end{array}\right) \leftrightarrows\left(\begin{array}{c}B_{b}^{-} \\ b \in B^{+}\end{array}\right)$consists of
- A passforward function $f: A^{+} \rightarrow B^{+}$, and
- A family of passback functions $f_{a}^{\sharp}: B^{-} \rightarrow A^{-}$for every $a \in A^{+}$. We call these dependent lenses.

\subsection*{2.6.3 Monoidal indexed categories and the product of lenses}

To describe wiring diagrams, it is not enough just to have the category of lenses; we also need the monoidal product

$$
\left(\begin{array}{l}
A^{-} \\
A^{+}
\end{array}\right) \otimes\left(\begin{array}{l}
B^{-} \\
B^{+}
\end{array}\right):=\left(\begin{array}{c}
A^{-} \times B^{-} \\
A^{+} \times B^{+}
\end{array}\right)
$$

We need this product to put systems together before wiring them. In order to wire together non-deterministic systems, we will need to generalize this product of lenses to generalized lenses. For this, we will need the notion of an monoidal indexed category and the associated monoidal Grothendieck construction as defined in [MV18].

Definition 2.6.3.1. A monoidal strict indexed category $\left(\mathcal{A}: C^{\mathrm{op}} \rightarrow \mathbf{C a t}, \otimes, 1, \boxtimes, \hat{1}\right)$ consists of:
- A strict indexed category $\mathcal{A}: C^{\mathrm{op}} \rightarrow$ Cat,
- A monoidal structure $(\otimes, 1)$ on $C$,
- A natural family of functors $\boxtimes: \mathcal{A}(C) \times \mathcal{A}\left(C^{\prime}\right) \rightarrow \mathcal{A}\left(C \otimes C^{\prime}\right)$ and $\hat{1} \in \mathcal{A}(1)$ with natural isomorphisms

$$
\begin{gathered}
A_{1} \boxtimes\left(A_{2} \boxtimes A_{3}\right) \cong\left(A_{1} \boxtimes A_{2}\right) \boxtimes A_{3}, \\
\hat{1} \boxtimes A \cong A \cong A \boxtimes \hat{1} .
\end{gathered}
$$

These natural isomorphisms are required to satisfy coherences reminiscent of those of a monoidal category.

Theorem 2.6.3.2 ([MV18]). Let $\mathcal{A}: \varrho^{\mathrm{op}} \rightarrow$ Cat be a monoidal indexed category. Then the Grothendieck construction $\int^{C: C} \mathcal{A}(C)$ may be equipped with a monoidal structure

$$
\left(\begin{array}{l}
A^{-} \\
A^{+}
\end{array}\right) \otimes\left(\begin{array}{l}
B^{-} \\
B^{+}
\end{array}\right):=\left(\begin{array}{l}
A^{-} \otimes B^{-} \\
A^{+} \otimes B^{+}
\end{array}\right)
$$

If the base of indexing $C$ is cartesian, then there is a simpler way to describe a monoidal structure on an indexed category $\mathcal{A}:$ Cop $^{\text {op }} \rightarrow$ Cat.

Theorem 2.6.3.3 ([Shu08]). Let $C$ be a cartesian category. Then a monoidal structures on a strict indexed category $\mathcal{A}: C^{\mathrm{op}} \rightarrow$ Cat whose underlying monoidal structure on $C$ is given by the cartesian product may be equivalently given by the data:
- A monoidal structure $\otimes: \mathcal{A}(C) \times \mathcal{A}(C) \rightarrow \mathcal{A}(C)$ and $1 \in \mathcal{A}(C)$ for each $C \in C$,
- A lax structure on each reindexing $r^{*}: \mathcal{A}(C) \rightarrow \mathcal{A}\left(C^{\prime}\right)$ for each $r: C^{\prime} \rightarrow C$, so that the lax structure on $\left(r_{2} \circ r_{1}\right)^{*}$ is the composite of the lax structures on $r_{2}$ and $r_{1}$.

Proof Sketch. We define the product $\otimes: \mathcal{A}(C) \times \mathcal{A}(C) \rightarrow \mathcal{A}(C)$ as $\boxtimes \circ \Delta^{*}$ where $\Delta: C \rightarrow$ $C \times C$ is the diagonal. We similarly define $1 \in \mathcal{A}(C)$ as ! ${ }^{*}(\hat{1})$.

We use Theorem 2.6.3.3 and Theorem 2.6.3.2 to recover the product of lenses.

Lemma 2.6.3.4. Let $C$ be a cartesian category and let $C \in C$. The category $\mathbf{C t x}_{C}$ has a monoidal structure given by $X \otimes Y:=X \times Y, 1:=1$, and

$$
f \otimes g:=C \times X \times Y \xrightarrow{\Delta} C \times C \times X \times Y \xrightarrow{\sim} C \times X \times C \times Y \xrightarrow{f \times g} X^{\prime} \times Y^{\prime} .
$$

In terms of elements,

$$
(f \otimes g)(c, x, y):=(f(c, x), g(c, y)) .
$$

Proof. We begin by showing that $\otimes$ is functorial:

$$
\begin{aligned}
\left(\left(f^{\prime} \circ f\right) \otimes\left(g^{\prime} \circ g\right)\right)(c, x, y) & =\left(\left(f^{\prime} \circ f\right)(c, x),\left(g^{\prime} \circ g\right)(c, y)\right) \\
& =\left(f^{\prime}(c, f(c, x)), g^{\prime}\left(c, g^{\prime}(c, y)\right)\right) \\
& =\left(f^{\prime} \otimes g^{\prime}\right)(f(c, x), g(c, y)) \\
& =\left(f^{\prime} \otimes g^{\prime}\right) \circ(f \otimes g)(x, y) .
\end{aligned}
$$

Next, we need associators $X \otimes(Y \otimes Z) \cong(X \otimes Y) \otimes Z$ and unitors $1 \otimes X \cong X \cong X \otimes 1$. We may get these by applying $!^{*}: C \rightarrow \mathbf{C t x}_{C}$ (which sends $f: X \rightarrow Y$ to $f \circ \pi_{1}: C \times X \rightarrow Y$ )
to the associators and unitors of $C$. It is straightforward to see that these are natural with respect to maps in $\mathbf{C t x}_{C}$.

Proposition 2.6.3.5. Let $C$ be a cartesian category. Then $\mathbf{C t x}_{-}: C^{\circ o p} \rightarrow$ Cat may be endowed with a monoidal structure so that the induced monoidal structure on the Grothendieck construction is the product of lenses

$$
\left(\begin{array}{l}
A^{-} \\
A^{+}
\end{array}\right) \otimes\left(\begin{array}{l}
B^{-} \\
B^{+}
\end{array}\right):=\left(\begin{array}{c}
A^{-} \times B^{-} \\
A^{+} \times B^{+}
\end{array}\right)
$$

Proof. By Lemma 2.6.3.4, there is monoidal structure on each $\mathbf{C t x}_{C}$. We note that by definition, each reindexing $r^{*}: \mathbf{C t x}_{C} \rightarrow \mathbf{C t x}_{C^{\prime}}$ along $r: C^{\prime} \rightarrow C$ preserves this monoidal structure strictly.

$$
\begin{aligned}
r^{*}(f \otimes g)\left(c^{\prime},(x, y)\right) & =(f \otimes g)\left(r\left(c^{\prime}\right),(x, y)\right) \\
& =\left(f\left(r\left(c^{\prime}\right), x\right), g\left(r\left(c^{\prime}\right), y\right)\right) \\
& =\left(r^{*} f \otimes r^{*} g\right)(c,(x, y))
\end{aligned}
$$

The rest then follows by Theorem 2.6.3.3 and Theorem 2.6.3.2.

\subsection*{2.6.4 Monadic lenses as generalized lenses}

Now we are ready to define monadic lenses. We have a formula for getting lenses out of an indexed category; we just need to find the right indexed category. We will do this by modifying the definition of $\mathbf{C t x}_{C}$ so that a map is of the form $C \times X \rightarrow M Y$. If the resulting categories $\mathbf{C t x}_{C}^{M}$ remain indexed over $C$, we have a ready made notion of monadic lens and monadic lens composition given by the Grothendieck construction!

We will be able to define composition in the categories $\mathbf{C t x}_{C}^{M}$ by making use of the natural map

$$
\lambda: C \times M X \xrightarrow{\eta \times M X} M C \times M X \xrightarrow{\sigma} M(C \times X)
$$

Using string diagrams, we may draw this map as

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-098.jpg?height=134&width=230&top_left_y=1971&top_left_x=945)

Using do notation, we may describe this map as

$$
(c, m) \mapsto \begin{array}{cc}
\mathbf{d o} & \\
& c^{\prime} \leftarrow \eta(c) \\
& x \leftarrow m \\
& \eta\left(c^{\prime}, x\right)
\end{array}=\begin{array}{ll}
\text { do } & \\
& x \leftarrow m \\
& \eta(c, x) \\
\end{array}
$$

Definition 2.6.4.1. Let $C$ be a cartesian category and $M: C \rightarrow C$ a commutative monad. For an object $C \in C$, there is a category $\mathbf{C t x}_{C}^{M}$ (called the biKleisli category of $C \times-$ and M) with:
- Objects the objects of $C$.
- Map $f: X \leadsto Y$ are maps $f: C \times X \rightarrow M Y$ in $C$.
- The identity $X \leadsto X$ is $\pi_{2}: \eta$.
- The composite $f ; g$ of $f: X \rightsquigarrow Y$ and $g: Y \rightsquigarrow Z$ is given by

$$
\begin{gathered}
f \circ g:=\left(\Delta_{C} \times X\right) \circ(C \times f) \circ \lambda \circ M g \circ \mu . \\
C \times X \rightarrow C \times C \times X \rightarrow C \times M Y \rightarrow M(C \times Y) \rightarrow M^{2} Z \rightarrow M Z
\end{gathered}
$$

Here, $\lambda:=(\eta \times M X) ; \sigma$. Using do notation, we may describe the composite $f \circ g$ as

$$
(c, m) \mapsto \begin{array}{cl}
\text { do } & \\
& x \leftarrow m \\
& y \leftarrow f(c, x) \\
& g(c, y)
\end{array}
$$

We can show that $\mathbf{C t x}_{C}^{M}$ is indeed a category using string diagrams. In string diagrams, a map $f: C \times X \rightarrow M Y$ in $\operatorname{Ctx}_{C}^{M}$ is drawn

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-099.jpg?height=114&width=225&top_left_y=1342&top_left_x=945)

and composition is drawn

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-099.jpg?height=152&width=477&top_left_y=1545&top_left_x=824)

The identity is drawn

In order to show that this composition is unital and associative, we will need to show that the following four laws hold relating $\lambda$ to the structure of $M$ and of $C \times(-)$ :

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-099.jpg?height=556&width=875&top_left_y=1972&top_left_x=625)

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-100.jpg?height=277&width=871&top_left_y=233&top_left_x=624)

We will prove these laws in the upcoming Lemma 2.6.4.2. ${ }^{3}$ Using them, we can see that composition in $\mathbf{C t x}_{C}^{M}$ is unital and associative.
![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-100.jpg?height=1170&width=1648&top_left_y=821&top_left_x=324)

This shows that $\mathbf{C t x}_{C}^{M}$ is a category. We now prove the crucial laws which undergird the above graphical arguments.
\footnotetext{
${ }^{3}$ And we will re-express them as commutative diagrams there.
}

Lemma 2.6.4.2. Let $M: C \rightarrow C$ be a commutative monad on a cartesian category $C$. Then the map $\lambda: C \times M X \rightarrow M(C \times X)$ defined by

$$
\lambda:=(\eta \times M X) \stackrel{ }{\circ} \sigma
$$

is natural in both $X$ and $C$. Furthermore, the following four diagrams commute:
![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-101.jpg?height=954&width=972&top_left_y=580&top_left_x=576)

Exercise 2.6.4.3. Prove Lemma 2.6.4.2 by showing that the diagrams commute. This uses the properties of the commutativity $\sigma$ and naturality. You may find the do notation helpful.

Lemma 2.6.4.4. Let $C$ be a cartesian category and let $M: C \rightarrow C$ be a commutative monad. Then for any $C \in C$, there is a symmetric monoidal structure on $\mathbf{C t x}_{C}^{M}$ given by $X \otimes Y:=X \times Y$, with unit 1 , and

$f \otimes g:=C \times X \times Y \xrightarrow{\Delta} C \times C \times X \times Y \xrightarrow{\sim} C \times X \times C \times Y \xrightarrow{f \times g} M X \times M Y \xrightarrow{\sigma} M(X \times Y)$.

With the do notation, $f \otimes g$ may be defined as

$$
(c, x, y) \mapsto \begin{array}{cc}
\text { do } & \\
& z \leftarrow f(c, x) \\
& w \leftarrow g(c, y) \\
& \eta(z, w)
\end{array}
$$

Proof. We will use the do notation to argue this. The proofs in the do notation can, with some care, be extended out into diagram chases if the reader desires to do so.

We will show that $\otimes$ is functorial. Let $f: X_{1} \rightarrow \Upsilon_{1}, g: X_{2} \rightarrow \Upsilon_{2}, f^{\prime}: Y_{1} \rightarrow Z_{1}$ and $g^{\prime}: \Upsilon_{2} \rightarrow Z_{2}$. Then

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-102.jpg?height=740&width=872&top_left_y=1180&top_left_x=621)

$$
\begin{aligned}
& =\left(f \circ f^{\prime}\right) \otimes\left(g \circ g^{\prime}\right)
\end{aligned}
$$

Note the use of commutativity.

Next, we need to give associators $\alpha:(X \otimes Y) \otimes Z \rightarrow X \otimes(Y \otimes Z)$ and unitors $\ell: 1 \otimes X \rightarrow X$ and $r: X \otimes 1 \rightarrow X$.

$$
\begin{aligned}
\alpha(c,(x, y), z) & :=\eta(x,(y, z)) . \\
\ell(c,(*, x)) & :=\eta(x) \\
r(c,(x, *)) & :=\eta(x)
\end{aligned}
$$

These can easily be seen to satisfy the required coherences, and they are just defined by shuffling the parentheses about.

From this, we may finally prove the following theorem.

Theorem 2.6.4.5. Let $M: C \rightarrow C$ be a commutative monad on a cartesian category. Then there is a monoidal strict indexed category

$$
\mathbf{C t x}_{-}^{M}: e^{\mathrm{op}} \rightarrow \mathbf{C a t}
$$

which sends an object $C \in C$ to the category $\mathbf{C t x}_{C}^{M}$ and which sends a map $r: C^{\prime} \rightarrow C$ to the functor

$$
r^{*}: \mathbf{C t x}_{C}^{M} \rightarrow \mathbf{C t x}_{C^{\prime}}^{M}
$$

which acts as the identity on objects and which sends a morphism $f: C \times X \rightarrow M Y$ to the composite $C^{\prime} \times X \xrightarrow{r \times X} C \times X \xrightarrow{f} M Y$.

Proof. All that remains to be proven is functoriality in $C$. Letting $r: C^{\prime} \rightarrow C$, we get a functor $r^{*}: \mathbf{C t x}_{C}^{M} \rightarrow \mathbf{C t x}_{C^{\prime}}^{M}$ given by sending $f: C \times X \rightarrow M Y$ to $f \circ(r \times X): C^{\prime} \times X \rightarrow$ $M Y$. In terms of elements, this means

$$
r^{*} f\left(c^{\prime}, x\right):=f\left(r\left(c^{\prime}\right), x\right)
$$

Using the do notation, we can quickly show that this is functorial:

$$
\begin{aligned}
r^{*}(g \circ f)\left(c^{\prime}, x\right) & =(g \circ f)\left(r\left(c^{\prime}\right), x\right) \\
& =\begin{array}{ll}
\text { do } & y \leftarrow f\left(r\left(c^{\prime}\right), x\right) \\
& g\left(r\left(c^{\prime}\right), y\right)
\end{array} \\
& =\begin{array}{ll}
\text { do } & y \leftarrow r^{*} f\left(c^{\prime}, x\right) \\
& r^{*} g\left(c^{\prime}, y\right)
\end{array} \\
& =\left(r^{*} g \circ r^{*} f\right)\left(c^{\prime}, x\right)
\end{aligned}
$$

To show that it is monoidal, we may also use the do notation:

$$
\begin{aligned}
& r^{*}(f \otimes g)\left(c^{\prime}, x, y\right)=(f \otimes g)\left(r\left(c^{\prime}\right), x, y\right)
\end{aligned}
$$

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-103.jpg?height=238&width=412&top_left_y=1971&top_left_x=995)

$$
\begin{aligned}
& =\begin{array}{ll}
\text { do } & \\
& z \leftarrow r^{*} f\left(c^{\prime}, x\right) \\
& w \leftarrow r^{*} g\left(c^{\prime}, y\right) \\
& \eta(z, w)
\end{array} \\
& =\left(r^{*} f\right) \otimes\left(r^{*} g\right)\left(c^{\prime}, x, y\right)
\end{aligned}
$$

With Theorem 2.6.4.5 in hand, we may now define the category of monadic lenses.

Definition 2.6.4.6. For a commutative monad $M: C \rightarrow C$ on a cartesian category, we define the symmetric monoidal category of $M$-lenses to be the symmetric monoidal category of $\mathbf{C t x}_{-}^{M}$-lenses:

$$
\operatorname{Lens}_{C}^{M}:=\int^{C: C} \mathbf{C t x}_{C}^{M^{\mathrm{op}}}
$$

Exercise 2.6.4.7. Show that the category of $M$-lenses may be described as follows:
- Its objects are pairs $\left(\begin{array}{c}A^{-} \\ A^{+}\end{array}\right)$of objects of $C$.
- Its maps are $M$-lenses $\left(\begin{array}{c}f^{\sharp} \\ f\end{array}\right):\left(\begin{array}{l}A^{-} \\ A^{+}\end{array}\right) \leftrightarrows\left(\begin{array}{c}B^{-} \\ B^{+}\end{array}\right)$where $f: A^{+} \rightarrow B^{+}$and $f^{\sharp}$ : $A^{+} \times B^{-} \rightarrow M A^{-}$.
- The identity is $\left(\begin{array}{c}\eta \circ \pi_{2} \\ \mathrm{id}\end{array}\right)$.
- Composition is defined by

$$
\left(\begin{array}{c}
f^{\sharp} \\
f
\end{array}\right) \circ\left(\begin{array}{c}
g^{\sharp} \\
g
\end{array}\right):=\left(\begin{array}{c}
h \\
f \circ g
\end{array}\right)
$$

where $h$ is defined in the do notation as

$$
h\left(a^{+}, c^{-}\right):=\begin{array}{ll}
\text { do } & \\
& b^{-} \leftarrow g^{\sharp}\left(f\left(a^{+}\right), c^{-}\right) \\
& f^{\sharp}\left(a^{+}, b^{-}\right)
\end{array}
$$

\subsection*{2.7 Changing the Flavor of Non-determinism}

In Section 2.5, we saw how commutative monad maps $\phi: M \rightarrow N$ let us change the flavor of non-determinism. In particular, since the unit $\eta: \mathrm{id} \rightarrow M$ is always a commutative monad map, we can always interpret a deterministic system as a nondeterministic system.

In this section, we'll show that any commutative monad morphism $\phi: M \rightarrow N$ induces a symmetric monoidal functor $\operatorname{Lens}_{C}^{M} \rightarrow \operatorname{Lens}_{C}^{N}$. We will do this using the functoriality of the Grothendieck construction: any indexed functor induces a functor on the Grothendieck constructions.

Definition 2.7.0.1. Let $\mathcal{A}: \mathrm{C}^{\mathrm{op}} \rightarrow$ Cat and $\mathscr{B}: \mathscr{D}^{\mathrm{op}} \rightarrow$ Cat be strict indexed categories. A strict indexed functor $(F, \bar{F}): \mathcal{A} \rightarrow \mathbb{B}$ is a pair consisting of
- A functor $F: C \rightarrow \mathscr{D}$, and
- A natural transformation $\bar{F}: \mathcal{A} \rightarrow \mathscr{B} \circ F^{\mathrm{op}}$. Explicitly, this is a family of functors $\bar{F}_{C}: \mathcal{A}(C) \rightarrow \mathcal{B}(F C)$ so that for any $r: C^{\prime} \rightarrow C$, we have that $\bar{F} \circ r^{*}=(F r)^{*} \circ \bar{F}$.

If $\mathcal{A}$ and $\mathscr{B}$ are monoidal strict indexed categories, then an indexed functor $(F, \bar{F})$ : $\mathcal{A} \rightarrow \mathcal{B}$ is strict monoidal if $F\left(C_{1} \otimes C_{2}\right)=F C_{1} \otimes F C_{2}$ and $\bar{F}\left(A_{1} \boxtimes A_{2}\right)=\bar{F}\left(A_{1}\right) \boxtimes \bar{F}\left(A_{2}\right)$, and $F$ and $\bar{F}$ send associators to associators and unitors to unitors.

Proposition 2.7.0.2. Let $(F, \bar{F}): \mathcal{A} \rightarrow \mathscr{B}$ be a strict indexed functor. Then there is a functor

$$
\left(\begin{array}{l}
\bar{F} \\
F
\end{array}\right): \int^{C: C} \mathcal{A}(C) \rightarrow \int^{D: D} \mathscr{B}(D)
$$

given by

$$
\left(\begin{array}{c}
\bar{F} \\
F
\end{array}\right)\left(\begin{array}{c}
f_{\mathrm{b}} \\
f
\end{array}\right):=\left(\begin{array}{c}
\bar{F} f_{\mathrm{b}} \\
F f
\end{array}\right)
$$

If furthermore $(F, \bar{F})$ is strictly monoidal, then so is $\left(\begin{array}{c}\bar{F} \\ F\end{array}\right)$.

Proof. We will show that this assignment is functorial. Recall that

$$
\left(\begin{array}{c}
f_{b} \\
f
\end{array}\right) \circ\left(\begin{array}{c}
g_{b} \\
g
\end{array}\right):=\left(\begin{array}{c}
f_{b}: f^{*} g_{b} \\
f \circ g
\end{array}\right)
$$

We may therefore calculate:

$$
\begin{aligned}
& \left(\begin{array}{c}
\bar{F} \\
F
\end{array}\right)\left(\left(\begin{array}{c}
f_{\mathrm{b}} \\
f
\end{array}\right) \circ\left(\begin{array}{c}
g_{\mathrm{b}} \\
g
\end{array}\right)\right)=\left(\begin{array}{c}
\bar{F}\left(f_{\mathrm{b}} \circ f^{*} g_{\mathrm{b}}\right) \\
F(f \circ g)
\end{array}\right)
\end{aligned}
$$

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-105.jpg?height=144&width=304&top_left_y=1736&top_left_x=1016)

$$
\begin{aligned}
& =\left(\begin{array}{c}
\bar{F} f_{b} \stackrel{\circ}{ }(F f)^{*}\left(\bar{F} g_{b}\right) \\
F f \circ F g
\end{array}\right) \\
& =\left(\begin{array}{c}
\bar{F} \\
F
\end{array}\right)\left(\begin{array}{c}
f_{b} \\
f
\end{array}\right) \circ\left(\begin{array}{l}
\bar{F} \\
F
\end{array}\right)\left(\begin{array}{c}
g_{b} \\
g
\end{array}\right)
\end{aligned}
$$

We end by noting that $\left(\begin{array}{l}\bar{F} \\ F\end{array}\right)\left(\begin{array}{l}\text { id } \\ \text { id }\end{array}\right)=\left(\begin{array}{l}\text { id } \\ \text { id }\end{array}\right)$ by functoriality of $F$ and $\bar{F}$.

If $(F, \bar{F})$ is strictly monoidal, then so is $\left(\begin{array}{c}\bar{F} \\ F\end{array}\right)$ because the monoidal structure of the Grothendieck constructions are defined by pairing the monoidal structures of the base.

Proposition 2.7.0.3. Let $\phi: M \rightarrow N$ be a commutative monad morphism. Then there is a strict monoidal indexed functor

$$
\left(\mathrm{id}, \phi_{*}\right): \mathbf{C t x}_{-}^{M} \rightarrow \mathbf{C t x}_{-}^{N}
$$

Proof. We need to give a family of strict monoidal functors $\phi_{*}: \mathbf{C t x}_{C}^{M} \rightarrow \mathbf{C t x}_{C}^{N}$, natural in $C$. We take $\phi_{*}$ to act as the identity on objects, and for $f: C \times X \rightarrow M Y$, we define

$$
\phi_{*} f:=f \circ \phi .
$$

We now show that this is functorial using the do notation:

$$
\begin{aligned}
& \phi_{*}(f \circ g)=f \circ g \circ \phi \\
& =(c, x) \mapsto \phi\left(\boxed{\begin{array}{ll}
\mathbf{d o} & \\
& y \leftarrow f(c, x) \\
& g(c, y)
\end{array}}\right) \\
& =(c, x) \mapsto \begin{array}{ll}
\text { do } & \\
& y \leftarrow \phi(f(c, x)) \\
& \phi(g(c, y))
\end{array} \quad \text { by Lemma 2.5.0.5 } \\
& =\phi_{*} f \circ \phi_{*} g .
\end{aligned}
$$

We also note that $\phi_{*} \mathrm{id}=\mathrm{id}$ since

$$
\begin{aligned}
\phi_{*}(\mathrm{id}) & =\pi_{2} \stackrel{\circ}{ } \eta_{M} \stackrel{\circ}{ } \\
& =\pi_{2} \stackrel{\circ}{ } \eta_{N} \\
& =\mathrm{id}
\end{aligned}
$$

We may also use the do notation to prove strict monoidal-ness. We begin by noting that the functor is strictly monoidal on objects since it is identity on objects and the monoidal structures are defined identically.

$$
\begin{aligned}
& \phi_{*}(f \otimes g)=\left(c, x_{1}, x_{2}\right) \mapsto \phi\left(\boxed{\left.\begin{array}{ll}
\text { do } & \\
& y_{1} \leftarrow f\left(c, x_{1}\right) \\
& y_{2} \leftarrow g\left(c, x_{2}\right) \\
& \eta_{M}\left(y_{1}, y_{2}\right)
\end{array}\right)}\right. \\
& =\left(c, x_{1}, x_{2}\right) \mapsto \begin{array}{ll}
\text { do } & \\
& y_{1} \leftarrow \phi\left(f\left(c, x_{1}\right)\right) \\
& y_{2} \leftarrow \phi\left(g\left(c, x_{2}\right)\right) \\
& \phi \eta_{M}\left(y_{1}, y_{2}\right)
\end{array} \\
& =\left(c, x_{1}, x_{2}\right) \mapsto \quad \begin{array}{ll}
\text { do } & \\
& y_{1} \leftarrow \phi\left(f\left(c, x_{1}\right)\right) \\
& y_{2} \leftarrow \phi\left(g\left(c, x_{2}\right)\right) \\
& \eta_{N}\left(y_{1}, y_{2}\right)
\end{array}
\end{aligned}
$$

$$
=\phi_{*} f \otimes \phi_{*} g
$$

Corollary 2.7.0.4. Let $\phi: M \rightarrow N$ be a commutative monad morphism. Then there is a strict monoidal functor

$$
\phi_{*}: \operatorname{Lens}_{C}^{M} \rightarrow \operatorname{Lens}_{C}^{N}
$$

Given by

$$
\phi_{*}\left(\begin{array}{c}
f^{\sharp} \\
f
\end{array}\right):=\left(\begin{array}{c}
f^{\sharp \circ} \phi \\
f
\end{array}\right) \text {. }
$$

Proof. We may apply Proposition 2.7.0.2 to Proposition 2.7.0.3 (or, more precisely, to the pointwise opposite (id, $\left.\phi_{*}^{\mathrm{op}}\right)$ ).

The theorem has a useful corollary: we can always wire together non-deterministic systems with wiring diagrams.

Corollary 2.7.0.5. For any commutative monad $M: C \rightarrow C$, there is a strictly monoidal functor

$$
\eta_{*}: \operatorname{Lens}_{C} \rightarrow \operatorname{Lens}_{C}^{M}
$$

Example 2.7.0.6. Suppose we have two people $\mathrm{S}_{1}$ and $\mathrm{S}_{2}$ flipping coins. $\mathrm{S}_{1}$ flips a single fair coin and exposes its value:

That is, State $_{1}=\{$ heads, tails $\}$

$$
\begin{aligned}
\text { update }_{\mathrm{S}_{1}}\left({ }^{-}\right) & =\frac{1}{2} \text { heads }+\frac{1}{2} \text { tails } \\
\text { expose }_{\mathrm{S}_{1}} & =\mathrm{id} .
\end{aligned}
$$

On the other hand, $\mathrm{S}_{2}$ will flip either a left coin or a right coin, and expose the resulting value. But these coins are biased in different ways The coin that $S_{2}$ flips is determined by whether it sees heads or tails.

$$
\mathrm{S}_{2}-\{\text { heads,tails }\}
$$

That is, State S $_{1}=\{$ heads, tails $\}$ and

$$
\text { update }_{\mathrm{S}_{2}}(\ldots \text { heads })=\frac{1}{4} \text { heads }+\frac{3}{4} \text { tails }
$$

$$
\begin{aligned}
\text { update }_{\mathrm{S}_{2}}(, \text { tails }) & =\frac{3}{4} \text { heads }+\frac{1}{4} \text { tails } \\
\text { expose }_{\mathrm{S}_{2}} & =\mathrm{id} .
\end{aligned}
$$

We can now imagine that $S_{1}$ sends the result of their coin flip over a channel to $S_{2}$. But this channel has noise given by

$$
\begin{aligned}
& \rho(\text { heads })=\frac{9}{10} \text { heads }+\frac{1}{10} \text { tails } \\
& \rho(\text { tails })=\frac{1}{10} \text { heads }+\frac{9}{10} \text { tails }
\end{aligned}
$$

Explictly, we will compose with the wiring diagram:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-108.jpg?height=217&width=832&top_left_y=862&top_left_x=641)

We can describe this as a D-lens

$$
\left(\begin{array}{c}
w^{\sharp} \\
w
\end{array}\right):\left(\begin{array}{c}
\ln _{\mathrm{S}_{1}} \times \ln _{\mathrm{S}_{2}} \\
\text { Out }_{\mathrm{S}_{1}} \times \text { Out }_{\mathrm{S}_{2}}
\end{array}\right) \leftrightarrows\left(\begin{array}{c}
\{*\} \\
\text { Out }_{\mathrm{S}_{2}}
\end{array}\right)
$$
- $w:$ Out $_{\mathrm{S}_{1}} \times$ Out $_{\mathrm{S}_{2}} \rightarrow$ Out $_{\mathrm{S}_{2}}$ is the projection $\pi_{2}$.
- $w^{\sharp}:$ Out $_{\mathrm{S}_{1}} \times$ Out $_{\mathrm{S}_{2}} \times\{*\} \rightarrow \mathrm{D}\left(\ln _{\mathrm{S}_{1}} \times \ln _{\mathrm{S}_{2}}\right)$ is given by

$$
w^{\sharp}(x, y, *)=(*, \rho(x)) .
$$

We may now form the composite system:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-108.jpg?height=159&width=580&top_left_y=1704&top_left_x=816)

This has states State $_{1} \times$ State $_{2}$, exposes just the state of State $_{2}$, and updates in the
following way:

$$
\begin{aligned}
& \frac{1}{2}\left(\frac{9}{10} \frac{1}{4}+\frac{1}{10} \frac{3}{4}\right)(\text { heads, heads) } \\
& +\frac{1}{2}\left(\frac{1}{10} \frac{3}{4}+\frac{9}{10} \frac{1}{4}\right)(\text { tails, heads }) \\
& +\frac{1}{2}\left(\frac{9}{10} \frac{3}{4}+\frac{1}{10} \frac{1}{4}\right)(\text { heads, tails }) \\
+ & \frac{1}{2}\left(\frac{1}{10} \frac{3}{4}+\frac{9}{10} \frac{1}{4}\right)(\text { tails, tails })
\end{aligned}
$$

\subsection*{2.8 Summary and Further Reading}

In this chapter, we extended the notion of lens to monadic lenses to accomodate nondeterministic systems. We saw how any commutative monad gave rise to a theory of non-determinism, from possibilistic to probabilistic to costs and rewards. One nice thing about monads is that you can play with them in Haskell. There are plenty of places to learn about monads in Haskell (perhaps too many), so I won't make any specific recommendations. For more about monads in category theory, check out Chapter 5 of [Per21].

We then saw how the notion of lens could be generalized to any indexed category. This notion of generalized lens is due to Spivak in [Spi19]. This generalization of lens will underly our formal notion of systems theory, which will be introduced in the next chapter.

Monads were first introduced as "standard constructions" by Huber [Hub61], and were often called "triples" in early category theory. The name "monad" was coined by BÃ©nabou in [BÃ©n67]. Kleisli defined his eponymous categories in [Kle65], and Moggi's seminal work [Mog89] [Mog91] showed the usefulness of Kleisli's categories in functional programming.

\section*{Chapter 3}

\section*{How systems behave}

\subsection*{3.1 Introduction}

So far, we have seen how to wire up dynamical systems. But we haven't seen our dynamical systems actually do anything. In this section, we will begin to study the behavior of our dynamical systems. We will see particular kinds of behaviors our systems can have, including trajectories, steady states, and periodic orbits.

Informal Definition 3.1.0.1. A behavior of a dynamical system is a particular way its states can change according to its dynamics.

There are different kinds of behavior corresponding to the different sorts of ways that the states of a system could evolve. Perhaps they eventually repeat, or they stay the same despite changing conditions.

In Section 3.3, we will give a formal definition of behavior of dynamical system. We will see that the different kinds of behaviors - trajectories, steady states, periodic orbits, etc. - can each be packaged up into a single system ${ }^{1}$ that represents that kind of behavior. This system will behave in exactly that kind of way, and do nothing else. Maps from it to a system of interest will exhibit that sort of behavior in the system of interest.

We will then investigate the definition of behaviors in terms of a double category which merges together the category of lenses with a category of charts (which are important for defining behaviors). We will see that behaviors are certain squares in this double category, and see what using this double category can tell us about how behaviors of component systems relate to the behaviors of composed systems.
\footnotetext{
${ }^{1}$ Or family of systems.
}

\subsection*{3.2 Kinds of behavior}

\subsection*{3.2.1 Trajectories}

A trajectory is the simplest and freest sort of behavior a system can have. A trajectory is just "what a state does". In this section, we will see what trajectories look like in the deterministic and differential systems theories.

\section*{Trajectories in the deterministic systems theory}

In the introduction, we saw that the Clock system Eq. (1.2) has behaves in this way if it starts at 11 o'clock:

$$
11 \stackrel{\text { tick }}{\longmapsto} 12 \stackrel{\text { tick }}{\longmapsto} 1 \stackrel{\text { tick }}{\longmapsto} 2 \stackrel{\text { tick }}{\longmapsto} \ldots
$$

This sequence of states of the clock system, each following from the last by the dynamics of the system, is called a trajectory. When our systems have input parameters, we will need to choose a sequence of input parameters to feed the system in order for the states to change.

Definition 3.2.1.1. Let

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-112.jpg?height=148&width=620&top_left_y=1216&top_left_x=747)

be a deterministic system. Suppose that $p: \mathbb{N} \rightarrow \operatorname{In} S$ is a sequence of parameters for $\mathrm{S}$. Then a $p$-trajectory of $S$ is a sequence $s: \mathbb{N} \rightarrow$ States of states so that

$$
\operatorname{update}_{S}\left(s_{i}, p_{i}\right)=s_{i+1}
$$

for all $i \in \mathbb{N}$.

If additionally $v: \mathbb{N} \rightarrow$ Out $\mathrm{S}$ is a sequence of output values for $\mathrm{S}$, then a $\left(\begin{array}{l}p \\ v\end{array}\right)$-trajectory is a sequence of states $s: \mathbb{N} \rightarrow$ States so that

$$
\begin{aligned}
\operatorname{update}_{\mathrm{S}}\left(s_{i}, p_{i}\right) & =s_{i+1} \\
\operatorname{expose}_{\mathrm{S}}\left(s_{i}\right) & =v_{i}
\end{aligned}
$$

for all $i \in \mathbb{N}$. We call the pair $\left(\begin{array}{l}p \\ v\end{array}\right)$ the chart of the trajectory $s$.

Its worth noting that a trajectory $s: \mathbb{N} \rightarrow$ States in a deterministic system is determined entirely by its start state $s_{0}$. This is what makes deterministic systems deterministic: if you know the dynamics and you know what state the system is in, you know how it will continue to behave.

Example 3.2.1.2. Consider the SIR model of Example 1.2.1.7. Suppose that we let our parameters $(a, b): \mathbb{N} \rightarrow \operatorname{In}$ SIR be constant at .2 and .3 respectively: that is, $a_{t}=.2$ and $b_{t}=.3$ for all $t$. Then a trajectory for SIR with parameters $(a, b)$ is a sequence of populations $(s, i, r): \mathbb{N} \rightarrow$ StatesIR $^{\text {such that }}$

$$
\left[\begin{array}{c}
s_{t+1} \\
i_{t+1} \\
r_{t+1}
\end{array}\right]=\left[\begin{array}{c}
s_{t}-.2 s_{t} i_{t} \\
i_{t}+.2 s_{t} i_{t}-.3 i_{t} \\
r_{t}+.3 i_{t}
\end{array}\right]
$$

Here is an example of such a trajectory with a 1000 total people and one infected person to start, that is $\left(s_{0}, i_{0}, r_{0}\right)=(999,1,0)$.

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-113.jpg?height=1174&width=1266&top_left_y=866&top_left_x=424)

Jaz: I don't know how to actually plot this...

Example 3.2.1.3. If a deterministic system is written as a transition diagram, then the trajectories in the system are paths through the diagram. Recall this system from

Example 1.2.1.8:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-114.jpg?height=271&width=350&top_left_y=287&top_left_x=882)

Suppose that $p: \mathbb{N} \rightarrow$ \{green, orange $\}$ alternates between green and orange. Then starting at the top right state, a trajectory quickly settles into alternating between the top two states:

$$
\stackrel{b}{\circ} \rightarrow \stackrel{b}{\circ} \rightarrow \stackrel{a}{\circ} \rightarrow \stackrel{b}{\circ} \rightarrow \stackrel{a}{\circ} \rightarrow \cdots
$$

Knowing about trajectories can show us another important role that deterministic systems play: they are stream transformers. From a stream $p: \mathbb{N} \rightarrow \ln _{\mathrm{S}}$ of inputs and a start state $s_{0} \in$ States, we get a trajectory $s: \mathbb{N} \rightarrow$ States given recursively by

$$
s_{t+1}:=\operatorname{update}_{S}\left(s_{t}, p_{t}\right)
$$

We then get a stream $v: \mathbb{N} \rightarrow$ Outs of output values by defining

$$
v_{t}:=\operatorname{expose}_{\mathrm{S}}\left(s_{t}\right) .
$$

The system $S$ is a way of transforming streams of input parameters into streams of output values.

Proposition 3.2.1.4 (Deterministic systems as stream transformers). Let

$$
\mathrm{S}=\left(\begin{array}{c}
\text { update }_{\mathrm{S}} \\
\text { expose }_{\mathrm{S}}
\end{array}\right):\left(\begin{array}{l}
\text { State }_{\mathrm{S}} \\
\text { State }_{\mathrm{S}}
\end{array}\right) \leftrightarrows\left(\begin{array}{c}
\operatorname{In}_{\mathrm{S}} \\
\text { Out }_{\mathrm{S}}
\end{array}\right)
$$

be a deterministic system. Then for every $s_{0} \in$ States, we get a stream transformation function

$$
\text { transform }_{\mathrm{S}}: \ln _{\mathrm{S}}^{\mathbb{N}} \rightarrow \mathrm{Out}_{\mathrm{S}}^{\mathbb{N}}
$$

\section*{Given by}

$$
\begin{aligned}
\operatorname{transform}_{\mathrm{S}}(p)_{0} & =\operatorname{expose}_{\mathrm{S}}\left(s_{0}\right) \\
\operatorname{transform}_{\mathrm{S}}(p)_{t+1} & =\operatorname{expose}_{\mathrm{S}}\left(\operatorname{update}_{\mathrm{S}}\left(s_{t}, p_{t}\right)\right)
\end{aligned}
$$

where $s_{t+1}=$ update ${ }_{S}\left(s_{t}, p_{t}\right)$ is the trajectory given by $s_{0}$.

Exercise 3.2.1.5. Say how the system of Example 3.2.1.3 acts as a stream transformer on the following streams:
1. $p_{2 t}=$ green and $p_{2 t+1}=$ orange.
2. $p_{t}=$ green.
3. $p_{0}=$ green and $p_{t}=$ orange for all $t>0$.

Later, in Section 5.3, we will see that given trajectories of component systems, we get a trajectory of a whole wired system. Even better, every trajectory of the whole wired system can be calculated this way.

\section*{Trajectories in the differential systems theory}

In a differential system, there is no "next" state after a given state. All we know is how each state is tending to change. So to define a trajectory in the differential systems theory, we can't just pick a state and see how it updates; instead, we are going to pick a state $s_{t}$ for every time $t \in \mathbb{R}$ which are changing in the way described by the system.

Definition 3.2.1.6. Let

$$
\mathrm{S}=\left(\begin{array}{c}
\text { update }_{\mathrm{S}} \\
\text { expose }_{\mathrm{S}}
\end{array}\right):\left(\begin{array}{c}
\text { State }_{\mathrm{S}} \\
\text { State }_{\mathrm{S}}
\end{array}\right) \leftrightarrows\left(\begin{array}{c}
\ln _{\mathrm{S}} \\
\text { Out }_{\mathrm{S}}
\end{array}\right)
$$

be a differential system. Suppose that $p: \mathbb{R} \rightarrow \ln$ is a differentiable choice of parameters for all times $t \in \mathbb{R}$. Then a $p$-trajectory is a differentiable function $s: \mathbb{R} \rightarrow$ States so that

$$
\operatorname{update}_{\mathrm{S}}\left(s_{t}, p_{t}\right)=\frac{d s}{d t}(t)
$$

for all $t \in \mathbb{R}$. Here, $\frac{d s}{d t}$ is the vector of derivatives $\frac{d s_{i}}{d t}$ for $i \in\{1, \ldots, n\}$ where $n$ is the number of state variables.

If, additionally, $v: \mathbb{R} \rightarrow$ Outs is a differentiable choice of outputs, then a $\left(\begin{array}{l}p \\ v\end{array}\right)$ trajectory is a differentiable function $s: \mathbb{R} \rightarrow$ States so that

$$
\begin{aligned}
\operatorname{update}_{S}\left(s_{t}, p_{t}\right) & =\frac{d s}{d t}(t) \\
\operatorname{expose}_{S}\left(s_{t}\right) & =v_{t}
\end{aligned}
$$

for all $t \in \mathbb{R}$. We call the pair $\left(\begin{array}{l}p \\ v\end{array}\right)$ the chart of the trajectory $s$.

Remark 3.2.1.7. A p-trajectory of a differential system is also referred to as a solution of the differential equation it represents which choice of parameters $p$.

The definition of trajectory is what makes our differential systems actually describe differential equations. Consider the Lotka-Volterra predator prey model from Section 1.2.2:

$$
\left\{\begin{array}{l}
\frac{d r}{d t}=\mathrm{b}_{\text {Rabbits }} \cdot r-c_{1} f r  \tag{3.1}\\
\frac{d f}{d t}=c_{2} r f-\mathrm{d}_{\text {Foxes }} \cdot f
\end{array}\right.
$$

Strictly speaking, this is not how we represent the system of differential equations as a differential system. Instead, we would describe its update function update ${ }_{\mathrm{LK}}$ :
$\mathbb{R}^{3} \times \mathbb{R}^{2} \rightarrow \mathbb{R}^{3}$ as

$$
\text { update }_{\mathrm{LK}}\left(\left[\begin{array}{l}
S \\
I \\
R
\end{array}\right],\left[\begin{array}{c}
\mathrm{b}_{\text {Rabbits }} \\
\mathrm{d}_{\text {Foxes }}
\end{array}\right]\right):=\left[\begin{array}{c}
\mathrm{b}_{\text {Rabbits }} \cdot r-c_{1} f r \\
c_{2} r f-\mathrm{d}_{\text {Foxes }} \cdot f
\end{array}\right]
$$

The differential equations Eq. (3.1) are the defining equations which make the function

$$
t \mapsto\left[\begin{array}{c}
S(t) \\
I(t) \\
R(t)
\end{array}\right]: \mathbb{R} \rightarrow \mathbb{R}^{3}
$$

a $\left[\begin{array}{c}b_{\text {Rabbits }} \\ d_{\text {Foxes }}\end{array}\right]$-trajectory. That is, we interpret a differential system $\left(\begin{array}{c}\text { update }_{s} \\ \text { expose }_{S}\end{array}\right)$ as a system of differential equations by considering the equations which define what it means for a $s: \mathbb{R} \rightarrow$ States to be a trajectory.

Unlike deterministic systems, it is not necessarily the case that a state uniquely determines a trajectory through it for differentiable systems. This is the case, however, if the differential equations are linear.

Example 3.2.1.8. Consider the following variant of an SIR model proposed by Norman Bailey in [Bai75]:

$$
\begin{cases}\frac{d S}{d t} & =\frac{-b S I}{S+I} \\ \frac{d I}{d t} & =\frac{b S I}{S+I}-b I \\ \frac{d R}{d t} & =b I\end{cases}
$$

That is,

$$
\text { update }_{\mathrm{SIR}}\left(\left[\begin{array}{l}
S \\
I \\
R
\end{array}\right]\right)=\left[\begin{array}{c}
\frac{-b S I}{S+I} \\
\frac{b S I}{S+I}-b I \\
b I
\end{array}\right]
$$

We note that the total population $N=S+I+R$ will always be constant. Suppose, for simplicity, that $b$ is a constant. Suppose that $S_{0}$ and $I_{0}$ are initial values for susceptible and infected populations respectively, and let $\kappa:=\frac{I_{0}}{S_{0}}$. Then the function

$$
\left[\begin{array}{c}
S(t) \\
I(i) \\
R(t)
\end{array}\right]:=\left[\begin{array}{c}
S_{0} e^{-\frac{b \kappa t}{1+\kappa}} \\
I_{0} e^{-\frac{b \kappa t}{1+\kappa}} \\
N-\left(S_{0}+I_{0}\right) e^{-\frac{b \kappa t}{1+\kappa}}
\end{array}\right]
$$

will be a $b$-trajectory for SIR. This can be solved in greater generality, for variable parameter $b$ and for two separate parameters governing the transition from susceptible to infected and infected to removed; see [BST19].

Example 3.2.1.9. In this example, we will consider a simple RL-circuit:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-117.jpg?height=271&width=377&top_left_y=347&top_left_x=863)

The voltage across the resistor is $V_{R}=I R$ while the voltage across the inductor is $V_{L}=L \frac{d I}{d t}$. By Kirchoff's voltage law, the total voltage differences, summed in an oriented manner, must be 0 . Therefore, $-V+V_{R}+V_{L}=0$, or, in terms of $\frac{d I}{d t}$ :

$$
\frac{d I}{d t}=\frac{V-R I}{L}
$$

We can express this RL-circuit as a differential system

$$
\left(\begin{array}{c}
\text { update }_{\mathrm{RL}} \\
\text { id }
\end{array}\right):\left(\begin{array}{l}
\mathbb{R} \\
\mathbb{R}
\end{array}\right) \leftrightarrows\left(\begin{array}{c}
\mathbb{R}^{2} \times \mathbb{R}^{*} \\
\mathbb{R}
\end{array}\right)
$$

where

$$
\text { update }_{\mathrm{RL}}\left(I,\left[\begin{array}{l}
V \\
R \\
L
\end{array}\right]\right):=\frac{V-R I}{L}
$$

We can then see that $I: \mathbb{R} \rightarrow \mathbb{R}$ defined by

$$
I(t)=\frac{V}{R}\left(1-e^{-\frac{R}{L} t}\right)
$$

gives a $\left[\begin{array}{l}V \\ R \\ L\end{array}\right]$-trajectory for the $\mathrm{RL}$ system.

\subsection*{3.2.2 Steady states}

A steady state of a system is a state which does not change. Steady states are important because they are guarantees of stability: you know what they are going to keep doing once you know what they are doing. A vase in a steady state is doing great, a heart in a steady state is in need of attention.

\section*{Steady states in the deterministic systems theory}

A steady state in the deterministic systems theory is a state which transitions to itself.

Definition 3.2.2.1. Let

$$
\mathrm{S}=\left(\begin{array}{c}
\text { update }_{\mathrm{S}} \\
\text { expose }_{\mathrm{S}}
\end{array}\right):\left(\begin{array}{l}
\text { State }_{\mathrm{S}} \\
\text { State }_{\mathrm{S}}
\end{array}\right) \leftrightarrows\left(\begin{array}{c}
\ln _{\mathrm{S}} \\
\text { Out }_{\mathrm{S}}
\end{array}\right)
$$

be a deterministic system. For input parameter $i \in \operatorname{Ins}_{\mathrm{S}}$ and output value $o \in$ Outs, an $\left(\begin{array}{l}i \\ o\end{array}\right)$-steady state is a state $s \in$ States such that

$$
\begin{aligned}
\operatorname{update}_{S}(s, i) & =s, \\
\operatorname{expose}_{S}(s) & =0 .
\end{aligned}
$$

We call the pair $\left(\begin{array}{l}i \\ o\end{array}\right)$ the chart of the steady state.

Remark 3.2.2.2. Its important to note that a steady state is relative to the input parameter chosen. For example, in Example 1.2.1.8, the top left state is steady for the input parameter orange but not for the input parameter green.

Unlike with trajectories, a system might not have any steady states. For example, the Clock has no steady states; it always keeps ticking to the next hour.

In the transition diagram of a finite deterministic system, steady states will be loops that begin and end at the same node. Since such a system is finite, we can arrange the steady states by their chart into a $\ln _{S} \times$ Out matrix. $^{\text {For }}$ example, in Example 1.2.1.8, we get the following $\{$ green, orange $\} \times\{a, b\}$ matrix:

$$
a\left[\begin{array}{cc}
\text { green }  \tag{3.2}\\
\emptyset & \left\{\begin{array}{c}
\text { orange } \\
a
\end{array}\right\} \\
\left.d \begin{array}{l}
b \\
0
\end{array}\right\} & \emptyset
\end{array}\right]
$$

This is a "matrix of sets", in that the entries are the actual sets of steady states. If we just counted how many steady states there were for each input-output pair, we would get this matrix:

$$
\left.a \begin{array}{cc}
\text { green } & \text { orange }  \tag{3.3}\\
0 & 1 \\
1 & 0
\end{array}\right]
$$

In Section 5.2, we'll see that each wiring diagram gives a formula for calculating the matrix of steady states of the composite system from the matrices of steady states of the inner systems.

Exercise 3.2.2.3. What are the steady state matrices of systems $S_{1}$ and $S_{2}$ from Exercise 1.3.2.7? What about the combined system S?

Steady-looking trajectories. The reason we are interested in steady states is that they are highly predictable; if we know we are in a steady state, then we know we are always going to get the same results. But it is possible for us to always get the same outputs for the same input even though the internal state keeps changing. These are special trajectories, and we call them steady-looking trajectories.

Definition 3.2.2.4. For $i \in \operatorname{In}$ and $o \in$ Outs of a system S, a $\left(\begin{array}{l}i \\ o\end{array}\right)$-steady looking trajectory is a sequence of states $s: \mathbb{N} \rightarrow$ States such that

$$
\begin{aligned}
\operatorname{update}_{\mathrm{S}}\left(s_{t}, i\right) & =s_{t+1} \\
\operatorname{expose}_{\mathrm{S}}\left(s_{t}\right) & =o
\end{aligned}
$$

for all $t \in \mathbb{N}$. We call the pair $\left(\begin{array}{l}i \\ o\end{array}\right)$ the chart of the steady-looking trajectory $s$.

Remark 3.2.2.5. While the steady states of a wired together system can be calculated from those of its components, this is not true for steady-looking trajectories. Intuitively, this is because the internal systems can be exposing changing outputs between eachother even while the eventual external output remains unchanged.

Exercise 3.2.2.6. Consider the wiring diagram:

$$
\mathrm{S}:=\mathrm{S}_{1}-\mathrm{S}_{2}
$$

Find systems $\mathrm{S}_{1}$ and $\mathrm{S}_{2}$ and a steady-looking trajectory of the wired system $\mathrm{S}$ which is not steady-looking on the component systems.

\section*{Steady states in the differential systems theory}

A steady state in the differential systems theory is a state which has no tendency to change.

Definition 3.2.2.7. Let

$$
\mathrm{S}=\left(\begin{array}{c}
\text { update }_{\mathrm{S}} \\
\text { expose }_{\mathrm{S}}
\end{array}\right):\left(\begin{array}{l}
\text { State }_{\mathrm{S}} \\
\text { State }_{\mathrm{S}}
\end{array}\right) \leftrightarrows\left(\begin{array}{c}
\ln _{\mathrm{S}} \\
\text { Out }_{\mathrm{S}}
\end{array}\right)
$$

be a differential system. For input parameter $i \in \operatorname{Ins}_{\mathrm{S}}$ and output value $o \in$ Outs, an $\left(\begin{array}{l}i \\ o\end{array}\right)$-steady state is a state $s \in$ States such that

$$
\begin{aligned}
& \operatorname{update}_{S}(s, i)=0 \text {, } \\
& \operatorname{expose}_{S}(s)=0 \text {. }
\end{aligned}
$$

We call the pair $\left(\begin{array}{l}i \\ o\end{array}\right)$ the chart of the steady state.

Example 3.2.2.8. Let's see if there are any steady states of the Lotka-Volterra predator prey model:

$$
\text { update }_{\mathrm{LK}}\left(\left[\begin{array}{l}
r \\
f
\end{array}\right],\left[\begin{array}{c}
\mathrm{b}_{\text {Rabbits }} \\
\mathrm{d}_{\text {Foxes }}
\end{array}\right]\right):=\left[\begin{array}{c}
\mathrm{b}_{\text {Rabbits }} \cdot r-c_{1} f r \\
c_{2} r f-\mathrm{d}_{\text {Foxes }} \cdot f
\end{array}\right]
$$

We are looking for a state $\left[\begin{array}{l}r \\ f\end{array}\right]$ whose update is 0 . That is, we want to solve the system of equations

$$
\left\{\begin{array}{l}
0=\mathrm{b}_{\text {Rabbits }} \cdot r-c_{1} r f \\
0=c_{2} r f-\mathrm{d}_{\text {Foxes }} \cdot f
\end{array}\right.
$$

If the parameters $\left[\begin{array}{c}b_{\text {Rabbits }} \\ d_{\text {Foxes }}\end{array}\right]$ are both zero, then any state is a steady state. Clearly, $\left[\begin{array}{l}0 \\ 0\end{array}\right]$ is a steady state for any choice of parameters; this steady state could be called "extinction". But if the populations and parameters are non-zero, then

$$
\left[\begin{array}{l}
r \\
f
\end{array}\right]=\left[\begin{array}{c}
\frac{\mathrm{d}_{\text {Foxes }}}{c_{2}} \\
\frac{\mathrm{b}_{\text {Rabbits }}}{c_{1}}
\end{array}\right]
$$

is a steady state.

Example 3.2.2.9. Recall the RL circuit from Example 3.2.1.9:

$$
\text { update }_{\mathrm{RL}}\left(I,\left[\begin{array}{l}
V \\
R \\
L
\end{array}\right]\right):=\frac{V-R I}{L}
$$

We can see that $I:=\frac{V}{R}$ is a steady state for this system given the parameters $V$ and $R$.

\subsection*{3.2.3 Periodic orbits}

Even if the behavior of a system isn't perfectly steady, it may continually repeat. To a reasonable approximation, the position of the earth around the sun follows a cycle that repeats every year. Using this as a paradigmatic example, we call these behaviors that repeat periodic orbits.

\section*{Periodic orbits in the deterministic systems theory}

Definition 3.2.3.1 (Periodic orbit). A $\left(\begin{array}{l}p \\ v\end{array}\right)$-trajectory $s: \mathbb{N} \rightarrow$ States is periodic if there exists a time $t_{0} \in \mathbb{N}_{\geq 1}$, called the period, such that $s_{t_{0}}=s_{0}$. If the sequence of parameters $p: \mathbb{N} \rightarrow \operatorname{In}_{S}$ is also periodic with the same period (in that $p_{t_{0}}=p_{0}$ as well), then we say that $s$ has periodic parameters.

Remark 3.2.3.2. Note that when we say that a periodic orbit has periodic parameters, we assume that they are periodic with the same period. This has important but subtle consequences for our theorems concerning the composition of behaviors in Section 5.3. We explain the difference between a periodic orbit and a periodic orbit with periodic parameters in a more precise manner in Remark 3.3.0.11.

Remark 3.2.3.3. Note that a steady state is a periodic orbit (with periodic parameters) that has a period of 1 .

Exercise 3.2.3.4. Describe a periodic orbit with period 1 that does not have periodic parameters; how are they different from steady states? Are there any of these in systems $\mathrm{S}_{1}$ and $\mathrm{S}_{2}$ of Exercise 1.3.2.7?

Example 3.2.3.5. The Clock system is an exemplary periodic system with a period of 12. The ClockWithDisplay of Eq. (1.6) has period 24.

Exercise 3.2.3.6. What are the periodic orbits in the systems $\mathrm{S}_{1}$ and $\mathrm{S}_{2}$ of Exercise 1.3.2.7 with periodic parameters, and what are their periods? What about the combined system S?

Exercise 3.2.3.7. Can you think of any periodic orbits in $\mathrm{S}_{1}$ and $\mathrm{S}_{2}$ of Exercise 1.3.2.7 which don't have periodic parameters?

\section*{Periodic orbits in the differential systems theory}

Definition 3.2.3.8. A $p$-trajectory $s: \mathbb{R} \rightarrow$ States for a differential system $S$ is a periodic orbit if there is a number $k$ such that

$$
s(t)=s(t+k)
$$

for all $t \in \mathbb{R}$. We refer to $k$ as the period of the orbit $s$. If $p$ is periodic of period $k$ as well (that is, $p(t)=p(t+k)$ for all $t$ ), then we say that $s$ has periodic parameters.

Example 3.2.3.9. Recall the Lotka-Volterra predator prey model of Section 1.2.2:

$$
\left\{\begin{array}{l}
\frac{d r}{d t}=a \cdot r-b f r \\
\frac{d f}{d t}=c r f-d f
\end{array}\right.
$$

We may take the Jacobian of this system to get the "community matrix"

$$
J(r, f)=\left(\begin{array}{cc}
a-b f & -b r \\
c f & \\
c r-d &
\end{array}\right)
$$

We may investigate the stability of the steady states (from Example 3.2.2.8) by looking at the Jacobian. In particular, we find that

$$
J\left(\frac{d}{c}, \frac{a}{b}\right)=\left(\begin{array}{cc}
0 & -\frac{b d}{c} \\
\frac{a c}{b} & 0
\end{array}\right)
$$

whose eigenvalues are $\pm i \sqrt{a d}$. Since the eigenvalues are purely imaginary and conjugate, this steady state is elliptic. Therefore the trajectories around this steady state are ellipses, which is to say, periodic.

Eventually Periodic Orbits A trajectory might not get back to where it started, but may still end up being periodic. We call these trajectories eventually periodic orbits, since they eventually end up in a repeating cycle of states.

Definition 3.2.3.10 (Eventually periodic orbit). Working in a deterministic systems

theory, a $\left(\begin{array}{l}p \\ v\end{array}\right)$-trajectory $s: \mathbb{N} \rightarrow$ States is eventually periodic if there are times $t_{0}<t_{1} \in \mathbb{N}$ such that $s_{t_{0}+t}=s_{t_{1}+t}$ for all $t \in \mathbb{N}$. If the sequence of parameters $p: \mathbb{N} \rightarrow \ln$ is also eventually periodic with the same period (in that $p_{t_{0}+t}=p_{t_{1}+t}$ for all $t$ ), then we say that $s$ has eventually periodic parameters.

The period of an eventually periodic trajectory is the smallest difference $t_{1}-t_{0}$ between times such that $s_{t_{0}}=s_{t_{1}}$.

Exercise 3.2.3.11. Formulate an analogous definition of eventually periodic orbit in the differential systems theory.

\subsection*{3.3 Behaviors of systems in the deterministic theory}

In the previous Sections 3.2.1 to 3.2.3, we saw a number of different kinds of behaviors of dynamical systems. Not only were there a lot of definitions in those sections, each of those definitions had slight variants (like periodic orbits versus periodic orbits with
periodic parameters, or steady states versus steady-looking trajectories). In this section, we'll define a general notion of behavior and see that we can package each of the above sorts of behavior into a single system ${ }^{2}$ in its own right, one that represents that sort of behavior. The representative system of a certain kind of behavior behaves in exactly that way, and does nothing else.

We will begin, for concreteness, with the deterministic systems theory. We will then return in the next section to see how we may formulate a general definition which also encompasses the differential systems theory.

We begin with a general definition of chart. A behavior is defined relative to its chart, which is the choice of parameters and the values of the variables it will expose. For example, the chart of a steady state was a parameter and an output value so that the state is steady given that parameter and it exposes that output value.

Definition 3.3.0.1. A chart $\left(\begin{array}{l}f_{\mathrm{b}} \\ f\end{array}\right):\left(\begin{array}{l}A^{-} \\ A^{+}\end{array}\right) \rightrightarrows\left(\begin{array}{c}B^{-} \\ B^{+}\end{array}\right)$in a cartesian category $C$ is a pair of maps $f: A^{+} \rightarrow B^{+}$and $f_{\mathrm{b}}: A^{+} \times A^{-} \rightarrow B^{-}$. Note that this is not a lens. We refer to the category of charts by Chart $_{C}$

\section*{Exercise 3.3.0.2.}

1. How many lenses are there $\left(\begin{array}{c}f^{\sharp} \\ f\end{array}\right):\left(\begin{array}{l}3 \\ 2\end{array}\right) \leftrightarrows\left(\begin{array}{l}4 \\ 3\end{array}\right)$ ?

2. How many charts are there $\left(\begin{array}{l}f^{b} \\ f\end{array}\right):\left(\begin{array}{l}3 \\ 2\end{array}\right) \rightrightarrows\left(\begin{array}{l}4 \\ 3\end{array}\right)$ ?

Exercise 3.3.0.3.

1. Show that a chart $\left(\begin{array}{l}1 \\ 1\end{array}\right) \rightrightarrows\left(\begin{array}{l}A^{-} \\ A^{+}\end{array}\right)$is given by the data of a pair of elements $a^{-} \in A^{-}$ and $a^{+} \in A^{+}$. Compare this to the notion of chart used in the definition of steady state (Definition 3.2.2.1).

2. Show that a chart $\left(\begin{array}{c}1 \\ \mathbb{N}\end{array}\right) \rightrightarrows\left(\begin{array}{l}A^{-} \\ A^{+}\end{array}\right)$is given by the data of a sequence $a^{-}: \mathbb{N} \rightarrow A^{+}$ and a sequence $a^{+}: \mathbb{N} \rightarrow A^{+}$. Compare this to the notion of chart used in the definition of trajectory (Definition 3.2.1.1)

Definition 3.3.0.4 (Behavior of deterministic systems). Let $\mathrm{T}$ and $\mathrm{S}$ be deterministic systems. Given a chart of interfaces $\left(\begin{array}{l}f_{\mathrm{b}} \\ f\end{array}\right):\left(\begin{array}{c}\ln _{\mathrm{T}} \\ \text { Out }\end{array}\right) \rightrightarrows\left(\begin{array}{c}\ln _{\mathrm{S}} \\ \text { Outs }\end{array}\right)$, a $\left(\begin{array}{c}f_{\mathrm{b}} \\ f\end{array}\right)$-behavior of shape $\mathrm{T}$ in $\mathrm{S}$, written $\phi: \mathrm{T} \rightarrow \mathrm{S}$, is a function $\phi:$ State $\mathrm{T}_{\mathrm{T}} \rightarrow$ States sending states of $\mathrm{T}$ to states of $S$ which preserves the dynamics and exposed variables by satisfying the following equations:

$$
\begin{align*}
\operatorname{expose}_{\mathrm{S}}(\phi(t)) & =f\left(\operatorname{expose}_{\mathrm{T}}(t)\right),  \tag{3.4}\\
\operatorname{update}_{\mathrm{S}}\left(\phi(t), f_{\mathrm{b}}\left(\operatorname{expose}_{\mathrm{T}}(t), i\right)\right) & =\phi\left(\operatorname{update}_{\mathrm{T}}(t, i)\right)
\end{align*}
$$
\footnotetext{
${ }^{2}$ Or a family of systems.
}
for all $t \in \operatorname{State}_{\mathrm{T}}$ and $i \in \ln _{\mathrm{T}}$. We say that $\left(\begin{array}{c}f_{\mathrm{b}} \\ f\end{array}\right)$ is the chart of the behavior $\phi$.

Remark 3.3.0.5. If you prefer commutative diagrams to systems of equations, don't fret. We'll reinterpret Eq. (3.4) in terms of commutative diagrams in Section 3.5

Remark 3.3.0.6. Suppose that we have transition diagrams for systems $\mathrm{T}$ and $\mathrm{S}$. Then a behavior of shape $T$ in $S$ will correspond to part of the transition diagram of $S$ which is shaped like the transition diagram of $T$. See the upcoming examples for examples of how this looks in practice.

Let's make this definition feel real with a few examples.

Example 3.3.0.7. Let Time be the system $\left(\begin{array}{c}t \mapsto t+1 \\ \text { id }\end{array}\right):\left(\begin{array}{c}\mathbb{N} \\ \mathbb{N}\end{array}\right) \leftrightarrows\left(\begin{array}{c}\{\text { tick }\} \\ \mathbb{N}\end{array}\right)$, i.e. with

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-124.jpg?height=49&width=296&top_left_y=897&top_left_x=394)

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-124.jpg?height=46&width=277&top_left_y=953&top_left_x=390)
- $\operatorname{In}_{\text {Time }}:=\{$ tick $\}$,
- $\operatorname{expose}_{\text {Time }}=\mathrm{id}$,
- update $_{\text {Time }}(t, *)=t+1$.

As a transition diagram, Time looks like this:

$$
\stackrel{0}{\mathrm{O}} \xrightarrow{\text { tick }} \stackrel{1}{\circ} \xrightarrow{\text { tick }} \stackrel{2}{\circ} \xrightarrow{\text { tick }} \stackrel{3}{\circ} \xrightarrow{\text { tick }} \stackrel{4}{\circ} \xrightarrow{\text { tick }} \ldots
$$

Let's see what a behavior of shape Time in an arbitrary system $S$ will be. We will expect the shape of Time to appear in the transition diagram of $S$, like this:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-124.jpg?height=639&width=1374&top_left_y=1540&top_left_x=365)

First, we need to know what a chart $\left(\begin{array}{c}f_{\mathrm{b}} \\ f\end{array}\right):\left(\begin{array}{c}\ln _{T_{\text {ime }}} \\ \text { Out }_{\text {Time }}\end{array}\right) \rightrightarrows\left(\begin{array}{c}\ln _{\mathrm{s}} \\ \text { Outs }\end{array}\right)$ is like. Since Out $_{\text {Time }}=\mathbb{N}$ and $\operatorname{In}_{\text {Time }} \cong 1$, this means $f: \mathbb{N} \rightarrow$ Outs is a sequence of outputs, and $f_{\mathrm{b}}: \mathbb{N} \times 1 \rightarrow \ln _{\mathrm{S}}$ is a sequence of input parameters. We might as well instead call $f$ our
sequence of exposed values $v$, and $f_{\mathrm{b}}$ our sequence of input parameters $p$, so that we have a chart $\left(\begin{array}{l}p \\ v\end{array}\right):\left(\begin{array}{l}1 \\ \mathbb{N}\end{array}\right) \rightrightarrows\left(\begin{array}{c}\operatorname{lns} \\ \text { Outs }\end{array}\right)$.

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-125.jpg?height=79&width=1393&top_left_y=367&top_left_x=385)
States satsifying some properties. But State ${ }_{\text {Time }}=\mathbb{N}$, so $\gamma: \mathbb{N} \rightarrow$ States is a sequence of states in S. Now, Eq. (3.4) becomes the equations:

$$
\begin{aligned}
\operatorname{expose}_{\mathrm{S}}(\gamma(t)) & =v(t) \\
\operatorname{update}_{\mathrm{S}}(\gamma(t), p(t)) & =\gamma(t+1)
\end{aligned}
$$

which are exactly the equations defining a $\left(\begin{array}{l}p \\ v\end{array}\right)$-trajectory from Definition 3.2.1.1!

Example 3.3.0.8. Consider the simple system Fix with:
- State Fix $=\{*\}$.
- Out Fix $=\{*\}$.
- $\ln _{\text {Fix }}=\{*\}$.
- $\operatorname{expose}_{\mathrm{Fix}}=\mathrm{id}$.
- $\operatorname{update}_{\mathrm{Fix}}(*, *)=*$.

As a transition diagram, this looks like:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-125.jpg?height=103&width=146&top_left_y=1331&top_left_x=987)

A behavior $s:$ Fix $\rightarrow$ S in an arbitrary system $S$ should be a loop of this shape within the transition diagram of $\mathrm{S}$ : a steady state.

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-125.jpg?height=444&width=656&top_left_y=1621&top_left_x=729)

Let's check that this works. First, we need to know what a chart $\left(\begin{array}{c}f_{\mathrm{b}} \\ f\end{array}\right):\left(\begin{array}{c}\ln _{\mathrm{Fix}} \\ \text { Out }_{\mathrm{Fix}}\end{array}\right) \rightrightarrows$ $\left(\begin{array}{c}\operatorname{In}_{\mathrm{S}} \\ \text { Outs }\end{array}\right)$ is. Since Out $\mathrm{Fix}_{\mathrm{Fix}}=\ln _{\text {Fix }}=\{*\}$, we have that $f:\{*\} \rightarrow$ Outs is simply an output value of $S$ and $f_{b}:\{*\} \times\{*\} \rightarrow \operatorname{In}_{S}$ is simply an input parameter. Therefore, we might as well write $o$ for $f$ and $i$ for $f_{\mathrm{b}}$, to see that a chart $\left(\begin{array}{c}i \\ o\end{array}\right):\left(\begin{array}{c}\{*\} \\ \{*\}\end{array}\right) \rightrightarrows\left(\begin{array}{c}\text { Ins } \\ \text { Outs }\end{array}\right)$ is a pair of elements $i \in \operatorname{In}_{\mathrm{S}}$ and $o \in$ Outs.

Now, let's see what a $\left(\begin{array}{l}i \\ o\end{array}\right)$-behavior $s:$ Fix $\rightarrow$ S is. It is a function $s:$ State $_{\text {Fix }} \rightarrow$ States satisfying a few properties. But States $=\{*\}$ so $s:\{*\} \rightarrow$ States is a single state of S. Then, Eq. (3.4) becomes the equations

$$
\begin{array}{r}
\operatorname{expose}_{S}(s)=0 \\
\operatorname{update}_{S}(s, i)=s
\end{array}
$$

which are precisely the equations defining a $\left(\begin{array}{l}i \\ o\end{array}\right)$-steady state from Definition 3.2.2.1.

Example 3.3.0.9. Let $0<n \in \mathbb{N}$ be a positive natural number, and consider the system Clock $\mathrm{n}_{\mathrm{n}}$ having:
- State $_{\text {Clock }_{n}}=\mathrm{n}=\{1, \ldots, n\}$.
- Out $_{\text {Clock }_{n}}=\mathrm{n}$.
- $\ln _{\text {Clock }_{n}}=\{*\}$.
- $\operatorname{expose}_{\text {Clock }_{n}}=\mathrm{id}$.
- update $_{\text {Clock }_{n}}(t, *)=\left\{\begin{array}{ll}t+1 & \text { if } t<n \\ 1 & \text { if } t=n\end{array}\right.$.

This is the clock with $n$ hours. Our example system Clock from Example 1.2.1.4 is

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-126.jpg?height=63&width=1418&top_left_y=1302&top_left_x=337)

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-126.jpg?height=309&width=314&top_left_y=1385&top_left_x=900)

A behavior $\gamma:$ Clock $_{\mathrm{n}} \rightarrow \mathrm{S}$ should be a cycle like this in the transition diagram of $\mathrm{S}$ : a periodic orbit. We can see the Clock 4 -behavior inside the system shown right:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-126.jpg?height=306&width=748&top_left_y=1880&top_left_x=686)

Let's check that this works. First, we need to know what a chart $\left(\begin{array}{l}f_{\mathrm{b}} \\ f\end{array}\right):\left(\begin{array}{l}\ln _{\text {Illockn }_{n}} \\ \text { Out Clockn }^{n}\end{array}\right) \rightrightarrows$ $\left(\begin{array}{l}\text { Ins } \\ \text { Out }\end{array}\right)$ is. Since Out Clock $_{n}=\mathrm{n}$ and $\ln _{\text {Clock }_{n}}=\{*\}, f: \mathrm{n} \rightarrow$ Out is a sequence of $n$ exposed values of $S$ while $f_{b}: n \times\{*\} \rightarrow \ln _{S}$ is a sequence of $n$ parameters. Therefore, we might
as well write $v$ for $f$ and $p$ for $f_{b}$ to find that a chart $\left(\begin{array}{l}p \\ v\end{array}\right):\left(\begin{array}{c}\{*\} \\ n\end{array}\right) \rightrightarrows\left(\begin{array}{c}\text { Ins } \\ \text { Outs }\end{array}\right)$ consists of an $n$-length sequence of parameters and an $n$-length sequence of exposed values.

$\mathrm{A}\left(\begin{array}{l}p \\ v\end{array}\right)$-behavior $\gamma:$ Clock $_{\mathrm{n}} \rightarrow \mathrm{S}$, then, is a function $\gamma:$ State $_{\text {Clock }_{\mathrm{n}}} \rightarrow$ States satisfying a few properties. Since State Clock $_{n}=\mathrm{n}, \gamma: \mathrm{n} \rightarrow$ States is a $n$-length sequence of states of $S$, and Eq. (3.4) become the equations

$$
\begin{aligned}
\operatorname{expose}_{\mathrm{S}}(\gamma(t)) & =v(t) \\
\text { update }_{\mathrm{S}}(\gamma(t), p(t)) & = \begin{cases}\gamma(t+1) & \text { if } t<n \\
\gamma(1) & \text { if } t=n\end{cases}
\end{aligned}
$$

As we can see, this determines a sequence of length $n$ of states of $S$ which repeats when it gets to the end. In other words, this is a periodic orbit with periodic parameters as in Definition 3.2.3.1!

If we have a certain kind of behavior in mind, and we find a system $T$ so that behaviors of shape $\mathrm{T}$ are precisely this kind of behavior, then we say that $\mathrm{T}$ represents that behavior. For example, we have just seen that:
- The system Time $=\left(\begin{array}{c}-+1 \\ \text { id }\end{array}\right):\left(\begin{array}{c}\mathbb{N} \\ \mathbb{N}\end{array}\right) \leftrightarrows\left(\begin{array}{c}\{*\} \\ \mathbb{N}\end{array}\right)$ represents trajectories.
- The system Fix $=\left(\begin{array}{c}\pi_{2} \\ \text { id }\end{array}\right):\left(\begin{array}{c}\{*\} \\ \{*\}\end{array}\right) \leftrightarrows\left(\begin{array}{c}\{*\} \\ \{*\}\end{array}\right)$ represents steady states.
- The systems Clock $\mathrm{n}_{\mathrm{n}}=\left(\begin{array}{c}-+1 \\ \text { id }\end{array}\right):\left(\begin{array}{l}n \\ n\end{array}\right) \leftrightarrows\left(\begin{array}{c}\{*\} \\ n\end{array}\right)$ represents periodic orbits with periodic parameters whose period divides $n$.

Note that there is always a particularly simple behavior on a system: the identity behaviors id : State $\rightarrow$ StateÑ‚. This says that every system behaves as itself. In particular, Time has a trajectory behavior given by id: Time $\rightarrow$ Time (namely, the trajectory $s_{t}=t$ ), and Fix has a steady state behavior given by id : Fix $\rightarrow$ Fix (namely, the steady state *), etc. We refer to the identity behavior of $\mathrm{T}$ as the generic behavior of type $\mathrm{T}$.

Exercise 3.3.0.10. Find a representative system for the following kinds of behavior.

1. An eventually periodic orbit (see Definition 3.2.3.10) that takes $n$ steps to get to a period of size $m$.

2. A steady-looking trajectory (see Definition 3.2.2.4).

3. A periodic orbit of period at most $n$ whose parameters aren't necessarily also periodic (see Definition 3.2.3.1).

4. A trajectory which yields the same output value at every $10^{\text {th }}$ step, but can do anything else in between.

Remark 3.3.0.11. As Exercise 3.3.0.10 shows, the difference between a periodic orbit and a periodic orbit with periodic parameters can be surmised precisely by noting that they are represented by systems with different interfaces. The dynamics of the systems are the same, but the interfaces (and accordingly, the exposed variable) are different; this
explains how the difference between a periodic orbit and a periodic orbit with periodic parameters is all in the chart.

Exercise 3.3.0.12. What kind of behaviors do the following systems represent? First, figure out what kind of charts they have, and then see what a behavior with a given chart is. Describe in your own words.

1. The system Plus with:
- Stateplus $=\mathbb{N}$.
- Outplus $=\mathbb{N}$.
- $\operatorname{In}_{\text {Plus }}=\mathbb{N}$.
- $\operatorname{expose}_{\text {Plus }}=\mathrm{id}$.
- update $_{\text {Plus }}(t, j)=t+j$.

2. The system $T_{n}$ with:
- State $_{T_{n}}=\mathbb{N}$.
- Out $_{T_{n}}=\{0, \ldots, n-1\}$.
- $\ln _{\mathrm{T}_{\mathrm{n}}}=\{*\}$.
- $\operatorname{expose}_{\mathrm{T}_{\mathrm{n}}}(t)=t \bmod n$.
- $\operatorname{update}_{\mathrm{T}_{\mathrm{n}}}(t, *)=t+1$.

3. The system XOR with:
- StatexOR $=$ Bool $=\{$ true, false $\}$.
- OutXOR $=$ Bool.
- $\operatorname{In}_{X O R}=$ Bool.
- expose $_{\mathrm{XOR}}=\mathrm{id}$.

$\bullet$

$$
\begin{array}{ll}
\text { update }_{\text {XOR }}(\text { true, true }) & =\text { false }, \\
\text { update }_{\text {XOR }}(\text { false, true }) & =\text { true }, \\
\text { update }_{\text {XOR }}(\text { true, false }) & =\text { true } \\
\operatorname{update}_{\text {XOR }}(\text { false, false }) & =\text { false. }
\end{array}
$$

4. The system Listc for a set of choices $C$ with:
- State List $_{C}=$ List $_{C}$ is the set of lists of elements in C.

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-128.jpg?height=54&width=304&top_left_y=1949&top_left_x=477)
- $\operatorname{In}_{\text {List }}=C$.
- expose $_{\text {List }_{C}}=\mathrm{id}$.
- update $_{\text {List }_{C}}(\ell, c)=c:: \ell$, that is, we update a list by appending the character $c \in C$ to the start.

While every system $T$ represents some kind of behavior - just take the kind of behavior to be exactly described by behaviors $\mathrm{T} \rightarrow \mathrm{S}$ - we are most interested in those simple systems $T$ whose behavior we can fully understand.

We have written a behavior of shape $\mathrm{T}$ in $\mathrm{S}$ with an arrow $\phi: \mathrm{T} \rightarrow \mathrm{S}$. This suggests that there is a category with deterministic systems as its objects and behaviors as its morphisms; and there is!

Definition 3.3.0.13. The category Chart $_{C}$ of charts in $C$ has
- Objects the arenas $\left(\begin{array}{c}A^{-} \\ A^{+}\end{array}\right)$, pairs of objects in $C$.
- Maps the charts $\left(\begin{array}{c}f_{b} \\ f\end{array}\right):\left(\begin{array}{c}A^{-} \\ A^{+}\end{array}\right) \rightrightarrows\left(\begin{array}{c}B^{-} \\ B^{+}\end{array}\right)$.
- Composition the composite of a chart $\left(\begin{array}{l}f_{\mathrm{b}} \\ f\end{array}\right):\left(\begin{array}{l}A^{-} \\ A^{+}\end{array}\right) \rightrightarrows\left(\begin{array}{l}B^{-} \\ B^{+}\end{array}\right)$with a chart $\left(\begin{array}{c}g_{b} \\ g\end{array}\right)$ : $\left(\begin{array}{c}B^{-} \\ B^{+}\end{array}\right) \rightrightarrows\left(\begin{array}{l}C^{-} \\ C^{+}\end{array}\right)$is

$$
\left(\begin{array}{c}
f_{\mathrm{b}} \\
f
\end{array}\right) \circ\left(\begin{array}{c}
g_{\mathrm{b}} \\
g
\end{array}\right):=\left(\begin{array}{c}
\left(a^{+}, a^{-}\right) \mapsto g_{\mathrm{b}}\left(f\left(a^{+}\right), f_{\mathrm{b}}\left(a^{+}, a^{-}\right)\right) \\
f \circ g
\end{array}\right)
$$
- The identity chart is $\left(\begin{array}{c}\pi_{2} \\ \text { id }\end{array}\right):\left(\begin{array}{l}A^{-} \\ A^{+}\end{array}\right) \rightrightarrows\left(\begin{array}{c}A^{-} \\ A^{+}\end{array}\right)$.

Exercise 3.3.0.14. Check that Chart $_{C}$ is indeed a category. That is,

1. For charts $\left(\begin{array}{c}f_{b} \\ f\end{array}\right):\left(\begin{array}{c}A^{-} \\ A^{+}\end{array}\right) \rightrightarrows\left(\begin{array}{c}B^{-} \\ B^{+}\end{array}\right),\left(\begin{array}{c}g_{b} \\ g\end{array}\right):\left(\begin{array}{c}B^{-} \\ B^{+}\end{array}\right) \rightrightarrows\left(\begin{array}{c}C^{-} \\ C^{+}\end{array}\right)$, and $\left(\begin{array}{c}h_{b} \\ h\end{array}\right):\left(\begin{array}{l}C^{-} \\ C^{+}\end{array}\right) \rightrightarrows\left(\begin{array}{c}D^{-} \\ D^{+}\end{array}\right)$, show that

$$
\left(\begin{array}{c}
f_{\mathrm{b}} \\
f
\end{array}\right) \circ\left(\left(\begin{array}{c}
g_{\mathrm{b}} \\
g
\end{array}\right) \circ\left(\begin{array}{c}
h_{\mathrm{b}} \\
h
\end{array}\right)\right)=\left(\left(\begin{array}{c}
f_{\mathrm{b}} \\
f
\end{array}\right) \circ\left(\begin{array}{c}
g_{\mathrm{b}} \\
g
\end{array}\right)\right) \circ\left(\begin{array}{c}
h_{\mathrm{b}} \\
h
\end{array}\right) .
$$

2. For a chart $\left(\begin{array}{l}f_{\mathrm{b}} \\ f\end{array}\right):\left(\begin{array}{l}A^{-} \\ A^{+}\end{array}\right) \rightrightarrows\left(\begin{array}{c}B^{-} \\ B^{+}\end{array}\right)$, show that

$$
\left(\begin{array}{l}
\pi_{2} \\
\mathrm{id}
\end{array}\right) \circ\left(\begin{array}{l}
f_{\mathrm{b}} \\
f
\end{array}\right)=\left(\begin{array}{l}
f_{\mathrm{b}} \\
f
\end{array}\right)=\left(\begin{array}{l}
f_{\mathrm{b}} \\
f
\end{array}\right) \circ\left(\begin{array}{l}
\pi_{2} \\
\mathrm{id}
\end{array}\right)
$$

In Proposition 2.6.2.5, we showed that the category of lenses is the Grothendieck construction of the pointwise opposite of the indexed category of maps with context. This may lead you to wonder: what is the the Grothendieck construction of the indexed category of maps with context, without taking the pointwise opposite. It is in fact precisely the category of charts.

Proposition 3.3.0.15. The category Chart $_{C}$ of charts in $C$ is the Grothendieck construction of the indexed category of maps with context:

$$
\text { Chart }_{C}=\int^{C: C} \mathbf{C t x}_{C}
$$

Proof. This comes down to checking that the definitions line up. The two categories have the same objects and the same morphisms. It remains to check that composition in the Grothendieck construction is as defined above in Definition 3.3.0.13. To that end, note that the function

$$
\left(a^{+}, a^{-}\right) \mapsto g_{\mathrm{b}}\left(f\left(a^{+}\right), f_{\mathrm{b}}\left(a^{+}, a^{-}\right)\right)
$$

may be written as

$$
f_{\mathrm{b}} \stackrel{\circ}{ }\left(f^{*} g_{\mathrm{b}}\right)
$$

in $\mathbf{C t x}_{A^{+}}$.

Exercise 3.3.0.16. What are the charts of the following forms in simpler terms?
1. $\left(\begin{array}{l}f_{b} \\ f\end{array}\right):\left(\begin{array}{l}1 \\ 1\end{array}\right) \rightrightarrows\left(\begin{array}{l}A^{-} \\ A^{+}\end{array}\right)$.
2. $\left(\begin{array}{c}f_{\mathrm{b}} \\ f\end{array}\right):\left(\begin{array}{l}A^{-} \\ A^{+}\end{array}\right) \rightrightarrows\left(\begin{array}{l}1 \\ 1\end{array}\right)$.
3. $\left(\begin{array}{c}f_{\mathrm{b}} \\ f\end{array}\right):\left(\begin{array}{c}1 \\ A^{+}\end{array}\right) \rightrightarrows\left(\begin{array}{c}B^{-} \\ B^{+}\end{array}\right)$.

Proposition 3.3.0.17. There is a category Sys with deterministic systems as its objects and where a map $\mathrm{T} \rightarrow \mathrm{S}$ is a pair consisting of a chart $\left(\begin{array}{c}f_{\mathrm{b}} \\ f\end{array}\right):\left(\begin{array}{c}\ln _{\mathrm{T}} \\ \text { Out }\end{array}\right) \rightrightarrows\left(\begin{array}{c}\ln _{\mathrm{S}} \\ \text { Outs }\end{array}\right)$ and a $\left(\begin{array}{l}f_{b} \\ f\end{array}\right)$-behavior $\phi: \mathrm{T} \rightarrow \mathrm{S}$. Composition is given by composing both the charts and the functions on states, and identities are given by the generic behaviors: the identity chart with the identity function id : State ${ }_{\mathrm{T}} \rightarrow$ State $_{\mathrm{T}}$.

Proof. We just need to check that the composite $\psi \circ \phi$ of two behaviors $\phi: \mathrm{T} \rightarrow \mathrm{S}$ and $\psi: \mathrm{S} \rightarrow \mathrm{U}$ with charts $\left(\begin{array}{c}f_{b} \\ f\end{array}\right):\left(\begin{array}{c}\ln _{T} \\ \text { Out }\end{array}\right) \rightrightarrows\left(\begin{array}{c}\ln _{\mathrm{S}} \\ \text { Outs }\end{array}\right)$ and $\left(\begin{array}{c}g_{b} \\ g\end{array}\right):\left(\begin{array}{c}\ln _{\mathrm{S}} \\ \text { Outs }\end{array}\right) \rightrightarrows\left(\begin{array}{c}\ln _{\mathrm{U}} \\ \text { OutU }\end{array}\right)$ is a behavior with chart $\left(\begin{array}{c}g_{b} \\ g\end{array}\right) \circ\left(\begin{array}{c}f_{b} \\ f\end{array}\right)$. That is, we need to check that Eq. (3.4) is satisfied for $\psi \circ \phi$. We can do this using the fact that it is satisfied for both $\psi$ and $\phi$.

$$
\begin{aligned}
\operatorname{expose}_{\mathrm{U}}(\psi(\phi(t))) & =\psi\left(\operatorname{expose}_{\mathrm{S}}(\phi(t))\right) \\
& =\psi\left(\phi\left(\operatorname{expose}_{\mathrm{T}}(t)\right)\right) .
\end{aligned}
$$

$$
\begin{aligned}
\operatorname{update}_{\mathrm{U}}(\psi(\phi(t)), & \left.g_{b}\left(f\left(\operatorname{expose}_{\mathrm{T}}(t)\right), f_{\mathrm{b}}\left(\operatorname{expose}_{\mathrm{T}}(t), i\right)\right)\right) \\
& =\operatorname{update}_{\mathrm{U}}\left(\psi(\phi(t)), g_{\mathrm{b}}\left(\operatorname{expose}_{\mathrm{S}}(\phi(t)), f_{\mathrm{b}}\left(\operatorname{expose}_{\mathrm{T}}(t), i\right)\right)\right) \\
& =\psi\left(\operatorname{update}_{\mathrm{S}}\left(\phi(t), f_{\mathrm{b}}\left(\operatorname{expose}_{\mathrm{T}}(t), i\right)\right)\right) \\
& =\psi\left(\phi\left(\operatorname{update}_{\mathrm{T}}(t, i)\right)\right) .
\end{aligned}
$$

There are two different ways to understand what composition of behaviors means: one based on post-composition, and the other based on pre-composition.
- We see that any behavior $S \rightarrow U$ gives a way of turning T-shaped behaviors in $S$ to T-shaped behaviors in $\mathrm{U}$.
- We see that any behavior $T \rightarrow S$ gives a way of turning S-shaped behaviors in $U$ into T-shaped behaviors in $U$.

Example 3.3.0.18. Any steady state $s$ can be seen as a particularly simple trajectory: $s_{t}=s$ for all $t$. We have seen in Example 3.3.0.8 that steady states are Fix-shaped behaviors. We can use composition of behaviors to understand how steady states give rise to trajectories.

The generic steady state $*$ of Fix (that is, the identity behavior of Fix) generates a trajectory $s: \mathbb{N} \rightarrow$ State $_{\text {Fix }}$ with input parameters $p_{t}=*$ and $s_{t}=*$. This gives us a behavior $s:$ Time $\rightarrow$ Fix.

Now, for every steady state $\gamma:$ Fix $\rightarrow \mathrm{S}$, we may compose to get a trajectory $\gamma \circ s:$ Time $\rightarrow \mathrm{S}$.

Exercise 3.3.0.19. Adapt the argument of Example 3.3.0.18 to show that

1. Any eventually periodic orbit gives rise to a trajectory.

2. If $n$ divides $m$, then any orbit of period at most $n$ gives rise to an orbit of period of most $m$.

Isomorphisms of Systems Now that we have a category of systems and behaviors, category theory supplies us with a definition of isomorphism for systems.

Definition 3.3.0.20. An isomorphism of a system $T$ with a system $S$ is a a behavior $\phi: \mathrm{T} \rightarrow \mathrm{S}$ for which there is another behavior $\phi^{-1}: \mathrm{S} \rightarrow \mathrm{T}$ such that $\phi \circ \phi^{-1}=\mathrm{id}_{\mathrm{S}}$ and $\phi^{-1} \circ \phi=\mathrm{id}_{\mathrm{T}}$.

Let's see that this is indeed a good notion of sameness for systems.

Proposition 3.3.0.21. A behavior $\phi: T \rightarrow S$ is an isomorphism if and only if the following conditions hold:

1. The map $\phi:$ State $_{\mathrm{T}} \rightarrow$ States is an isomorphism of sets - a bijection.

2. The chart $\left(\begin{array}{l}f_{b} \\ f\end{array}\right):\left(\begin{array}{c}\operatorname{In}_{T} \\ \text { Out }\end{array}\right) \rightrightarrows\left(\begin{array}{c}I_{n} \\ \text { Outs }\end{array}\right)$ of $\phi$ is an isomorphism in Chartset. That is, $f:$ Out $_{T} \rightarrow$ Outs is a bijection and there is a bijection $f_{b}^{\prime}: \ln _{T} \rightarrow$ Out $T_{T}$ such that $f_{b}=f_{b}^{\prime} \circ \pi_{2}$.

Proof. Since composition in the category of systems and behaviors is given by composition of the underlying charts and maps, $\phi$ is an isomorphism of systems if and only if its action on states is a bijection and its chart is an isomorphism in the category of charts. It just remains to see that our description of isomorphism of charts is accurate, which we leave to Exercise 3.3.0.22.

Exercise 3.3.0.22. Show that a chart $\left(\begin{array}{c}f_{b} \\ f\end{array}\right):\left(\begin{array}{l}A^{-} \\ A^{+}\end{array}\right) \rightrightarrows\left(\begin{array}{c}B^{-} \\ B^{+}\end{array}\right)$is an isomorphism if and only if $f$ is an isomorphisms and there is an isomorphism $f_{b}^{\prime}: A^{-} \rightarrow B^{-}$such that $f_{\mathrm{b}}=f_{\mathrm{b}}^{\prime} \circ \pi_{2}$.

\subsection*{3.3.1 Simulations}

While we will often be interested in behaviors of systems that change the interface in the sense of having non-trivial charts, we will also be interested in behaviors of systems that do not changed the exposed variables at all. These behaviors play a very different role in the theory of dynamical systems than behaviors like trajectories and steady states. Because they don't change observable behavior (since they have identity chart), they say more about how we model the observable behavior than what that behavior is itself. For that reason, we will call behaviors with identity chart simulations.

Definition 3.3.1.1. Let $\left(\begin{array}{l}I \\ O\end{array}\right)$ be an arena. The category

$$
\text { Sys }\left(\begin{array}{l}
I \\
O
\end{array}\right)
$$

of deterministic $\left(\begin{array}{l}I \\ O\end{array}\right)$-systems has as objects the systems $\left(\begin{array}{l}\text { update }_{S} \\ \text { expose }_{S}\end{array}\right):\left(\begin{array}{l}\text { States } \\ \text { States }\end{array}\right) \leftrightarrows\left(\begin{array}{l}I \\ O\end{array}\right)$ with interface $\left(\begin{array}{l}I \\ O\end{array}\right)$ and as maps the simulations $\phi: T \rightarrow S$, those behaviors whose chart is the identity chart on $\left(\begin{array}{l}I \\ O\end{array}\right)$.

Example 3.3.1.2. Recall the $\left(\begin{array}{c}\{\text { green,orange }\} \\ \{a, b\}\end{array}\right)$-system $\mathrm{S}$ from Example 1.2.1.8:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-132.jpg?height=309&width=398&top_left_y=1775&top_left_x=858)

If we had built this system as a model of some relationships between input colors and output letters we were seeing in the wild, then we have made this system a bit redundant. If the output is $a$, and we feed it green, the output will be $b$; if we feed it orange, the output will be $a$. Similarly, if the output is $b-$ no matter which of states 2 or 3 the system is actually in - and we feed it green, the output will again be $b$, and
if we feed it orange, the output will be $a$. And there really isn't much else going on in the system.

We can package this observation into a behavior in Sys $\left(\begin{array}{c}\{\text { green,orange }\} \\ \{a, b\}\end{array}\right)$. Let $\mathrm{U}$ be the system

$$
C \stackrel{a}{1} \underset{k}{\underset{C}{C}} \stackrel{b}{2}
$$

We can give a behavior $q: S \rightarrow U$ with identity chart as follows defined by

$$
\begin{aligned}
& q(1)=1 \\
& q(2)=2 \\
& q(3)=2
\end{aligned}
$$

We can check, by cases, that this is indeed a behavior. That it is a behavior in Sys $\left(\begin{array}{c}\{\text { green,orange }\} \\ \{a, b\}\end{array}\right)$ means that it doesn't change the observable behavior.

Example 3.3.1.2 also gives us an example of an important relation between systems: bisimulation. We saw what it means for two systems to be isomorphic: it means they have isomorphic states and the same dynamics and output relative to those isomorphisms. But this is sometimes too strong a notion of sameness for systems; we want to know when two systems look the same on the outside.

Let's see what this notion looks like for deterministic systems; then we will describe it in a doctrinal way.

Definition 3.3.1.3. In the deterministic systems theory, a bisimulation $\sim$ between $\left(\begin{array}{l}I \\ O\end{array}\right)$ systems $\mathrm{S}$ and $\mathrm{U}$ is a relation $\sim$ : States $\times$ State $\mathrm{U} \rightarrow$ \{true, false $\}$ between states of these systems such that $s \sim u$ only when $s$ and $u$ have related dynamics:

$$
\begin{aligned}
& s \sim u \operatorname{implies~}_{\operatorname{expose}_{\mathrm{S}}(s)}=\operatorname{expose}_{\mathrm{U}}(u) \\
& s \sim u \operatorname{implies~update}_{\mathrm{S}}(s, i) \sim \operatorname{update}_{\mathrm{U}}(u, i) \text { for all } i \in I
\end{aligned}
$$

If $\sim$ is a bisimulation, we say that $s$ and $u$ are bisimilar when $s \sim u$.

A bisimulation $\sim$ is said to be total if every $s \in$ States is bisimilar to some $u \in$ State $_{U}$ and vice-versa.

Bisimilarity is a strong relation between states of systems. For deterministic systems, this implies that they act the same on any input.

Proposition 3.3.1.4. Let $\mathrm{S}$ and $\mathrm{U}$ be deterministic $\left(\begin{array}{l}I \\ O\end{array}\right)$-systems, and let $\sim$ be a bisimulation between them. If $s_{0} \sim u_{0}$ are bisimilar, then they induce the same transformation on streams of inputs into streams of outputs:

$$
\operatorname{transform}_{\mathrm{S}}^{s_{0}}=\text { transform }_{\mathrm{U}}^{u_{0}}
$$

Proof. Let $i: \mathbb{N} \rightarrow I$ be a stream of inputs. Let $s: \mathbb{N} \rightarrow$ States be the stream of states generated by $s_{0}$ and similarly, let $u: \mathbb{N} \rightarrow$ State Se the st $^{2}$ $u_{0}$.

We first show that $s_{n} \sim u_{n}$ for all $n$. Our base case holds by hypothesis; now suppose that $s_{n} \sim u_{n}$ seeking $s_{n+1} \sim u_{n+1}$. Well,

$$
s_{n+1}=\operatorname{update}_{\mathrm{s}}\left(s_{n}, i_{n}\right) \sim \operatorname{update}_{\mathrm{U}}\left(u_{n}, i_{n}\right)=u_{n+1}
$$

because $\sim$ is a bisimulation.

Finally,

$$
\operatorname{transform}_{\mathrm{S}}(i)_{n}=\operatorname{expose}_{\mathrm{S}}\left(s_{n}\right)=\operatorname{expose}_{\mathrm{U}}\left(u_{n}\right)=\operatorname{transform}_{\mathrm{U}}(i)_{n}
$$

because $s_{n} \sim u_{n}$.

We can talk about bisimilar states without reference to the particular bisimulation between the systems they are a part of because, as it turns out, being bisimilar is independent of the particular bisimulation. To see this, we need to introduce an interesting system: the system of trees.

Definition 3.3.1.5. Let $\left(\begin{array}{l}I \\ O\end{array}\right)$ be an arena in the deterministic systems theory. An $\left(\begin{array}{l}I \\ O\end{array}\right)$ tree $\tau$ (or a $O$-labeled, $I$-branching tree) consists of:
- A root $\operatorname{root}(\tau) \in O$.
- For each parameter $i \in I$, a child tree $\operatorname{child}(\tau, i)$.

Definition 3.3.1.6. Let $\left(\begin{array}{l}I \\ O\end{array}\right)$ be an arena in the deterministic systems theory. The $\left(\begin{array}{l}I \\ O\end{array}\right)$ -

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-134.jpg?height=116&width=561&top_left_y=1872&top_left_x=327)

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-134.jpg?height=81&width=570&top_left_y=1976&top_left_x=390)
- Each tree exposes its root: $\operatorname{expose}_{\text {Tree }}(\tau)=\operatorname{root}(\tau)$.
- The system updates by following a tree down the $i^{\text {th }}$ branch: update $_{\text {Tree }}(\tau, i)=$ $\operatorname{child}(\tau, i)$

We can think of an $\left(\begin{array}{l}I \\ O\end{array}\right)$-tree as a stream of possible outputs of an $\left(\begin{array}{l}I \\ O\end{array}\right)$-system. In the current state, we see the root of the tree. When we transition to the next state with parameter $i$, we will see the rest of the output. This observation suggests a universal characterization of the system of $\left(\begin{array}{l}I \\ O\end{array}\right)$-trees.

Proposition 3.3.1.7. The $\left(\begin{array}{l}I \\ O\end{array}\right)$-system Tree $\left(\begin{array}{l}I \\ O\end{array}\right)$ of $\left(\begin{array}{l}I \\ O\end{array}\right)$-trees is terminal in the category of $\left(\begin{array}{l}I \\ O\end{array}\right)$-systems.

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-135.jpg?height=117&width=1450&top_left_y=489&top_left_x=327)
system $S$. For any $s \in$ States, we will define a tree $!_{s}(s)$ of outputs visible from the state $s$. We define this as follows:
- The root of $!_{s}(s)$ is the variable exposed by $S$ :

$$
\operatorname{root}\left(!_{S}(s)\right)=\operatorname{expose}_{S}(s)
$$
- The $i^{\text {th }}$ child of $!_{S}(s)$ is the tree of outputs visible from the next state update $_{S}(s, i)$ :

$$
\operatorname{child}\left(!_{S}(s), i\right)=!_{s}\left(\operatorname{update}_{S}(s, i)\right)
$$

Now, we can show that this is a simulation and that it is the unique such similation by noticing that this definition is precisely what is required to satisfy the defining laws of a simulation.

Now we can express the idea that bisimilarity of states is independent of any particular bisimulation between their systems with the following theorem.

Theorem 3.3.1.8. Let $\mathrm{S}$ and $\mathrm{U}$ be $\left(\begin{array}{l}I \\ O\end{array}\right)$-systems. A state $s \in$ States is bisimilar to a state $u \in$ State $\mathrm{u}$ for some bisimulation $\sim$ between $\mathrm{S}$ and $\mathrm{U}$ if and only if $!_{\mathrm{S}}(s)=\mathrm{l}_{\mathrm{U}}(u)$.

Proof. First, let's show that if $s$ is bisimilar to $u$ via some bisimulation $\sim$, then $!_{s}(s)=$ $!_{u}(u)$. Now, to show that two trees are equal, we need to show that they have equal roots and equal children.
- The root of $!_{\mathrm{s}}(s)$ is $\operatorname{expose}_{\mathrm{S}}(s)$, and the root of $!_{\mathrm{U}}(u)$ is $\operatorname{expose}_{\mathrm{U}}(u)$. But since $s \sim u$ by hypothesis, these are equal.
- Similarly, the $i^{\text {th }}$ child of $!_{s}(s)$ is $!_{S}\left(\right.$ update $\left._{S}(s, i)\right)$, while the $i^{\text {th }}$ child of $!_{u}(u)$ is ! $_{\mathrm{u}}\left(\right.$ update $\left._{\mathrm{U}}(u, i)\right)$. But since $\sim$ is a bisimulation, we have that update $_{\mathrm{S}}(s, i) \sim$ update $_{\mathrm{U}}(u, i)$, and so by the same argument we are giving, we will find that $!_{\mathrm{S}}\left(\operatorname{update}_{\mathrm{S}}(s, i)\right)=\mathrm{u}_{\mathrm{u}}\left(\operatorname{update}_{\mathrm{U}}(u, i)\right) .^{3}$

On the other hand, suppose that $!_{\mathrm{s}}(s)=!_{\mathrm{u}}(u)$. We now need to define a bisimulation $\sim$ between $\mathrm{S}$ and $\mathrm{U}$ for which $s \sim u$. For any sequence of inputs $i: \mathrm{n} \rightarrow I$, we can evolve a system in state $s$ by the entire sequence $i$ to yield a state update ${ }_{S}^{*}(s, i)$ in the following way:
- If $n=0$, then update ${ }_{\mathrm{S}}^{*}(s, i)=s$.
\footnotetext{
${ }^{3}$ This style of proof is called proof by co-induction. Where induction assumes a base case and then breaks apart the next step into a smaller step, co-induction shows that the proof can always be continued in a manner which covers all possible options.
}
- For $n+1$, then $\operatorname{update}_{S}^{*}(s, i)=\operatorname{update}_{S}\left(\operatorname{update}_{S}^{*}\left(s,\left.i\right|_{n}\right), i_{n+1}\right)$.

We may then define $\sim$ in the following way.

$x \sim y$ if and only if there is an $n \in \mathbb{N}$ and $i: \mathrm{n} \rightarrow I$ with $x=\operatorname{update}_{\mathrm{S}}^{*}(s, i)$ and

$$
y=\operatorname{update}_{\mathrm{U}}^{*}(u, i)
$$

It remains to show that this is a bisimulation.

For any $\left(\begin{array}{l}I \\ O\end{array}\right)$-tree $\tau$ and any $n$-length sequence $i: \mathrm{n} \rightarrow I$ of parameter (for any $n \in \mathbb{N}$ ), we can follow the path $i$ through the tree $\tau$ to get a new tree subtree $(\tau, i)$ :
- If $n=0$, then subtree $(\tau, i)=\tau$.
- For $n+1$, $\operatorname{subtree}(\tau, i)=\operatorname{child}\left(\operatorname{subtree}\left(\tau,\left.i\right|_{\mathrm{n}}\right)\right)$ is the $i^{\text {th }}$ child of the tree found by following $i$ for the first $n$ steps.

Note that $!_{S}\left(\right.$ update $\left._{S}^{*}(s, i)\right)=$ subtree $\left(!_{S}(s), i\right)$ by a quick inductive argument. Now we can show that $\sim$ is a bisimulation.
- Suppose that $x \sim y$, seeking to show that $\operatorname{expose}_{S}(x)=\operatorname{expose}_{\mathrm{U}}(y)$. By hypothesis, $x=\operatorname{update}_{\mathrm{S}}^{*}(s, i)$ and $y=\operatorname{update}_{\mathrm{U}}^{*}(u, i)$. But then

$$
\begin{aligned}
\operatorname{expose}_{S}(x) & =\operatorname{root}\left(!_{\mathrm{S}}(x)\right) \\
& =\operatorname{root}\left(\operatorname{subtree}\left(!_{\mathrm{S}}(s), i\right)\right) \\
& =\operatorname{root}\left(\operatorname{subtree}\left(!_{\mathrm{U}}(u), i\right)\right) \\
& =\operatorname{root}\left(!_{\mathrm{U}}(y)\right) \\
& =\operatorname{expose}_{\mathrm{U}}(y)
\end{aligned}
$$
- Suppose that $x \sim y$, seeking to show that $\operatorname{update}_{\mathrm{S}}(x, j) \sim \operatorname{update}_{\mathrm{U}}(y, j)$. By hypothesis, $x=\operatorname{update}_{\mathrm{S}}^{*}(s, i)$ and same for $y=\operatorname{update}_{\mathrm{U}}^{*}(u, i)$. Then letting $i^{\prime}$ : $\mathrm{n}+1 \rightarrow \mathbb{N}$ be defined by $i_{n+1}^{\prime}=j$ and $i_{k}^{\prime}=i_{k}$ otherwise, we see that update ${ }_{\mathrm{S}}(x, j)=$ $\operatorname{update}_{\mathrm{S}}^{*}\left(s, i^{\prime}\right)$ and $\operatorname{update}_{\mathrm{U}}(y, j)=\operatorname{update}_{\mathrm{U}}^{*}\left(y, i^{\prime}\right)$, so that by definition they are related by $\sim$.

\subsection*{3.4 Dealing with two kinds of composition: Double categories}

In this section, we will introduce the notion of double category to help us deal with our two kinds of composition: the composition of systems, and the composition of behaviors. By revealing that Definition 3.3.0.4 can be expressed as a square in a double category of arenas, we will find a generalization of this definition of behavior which applies to the differential systems theory as well. It is at this point that we will introduce the formal definition of a theory of dynamical systems.

Definition 3.4.0.1. A double category $\mathscr{D}$ has:
- A class ob $D$ of objects.
- A horizontal category $h \mathscr{D}$ whose objects are those of $\mathscr{D}$. We call the maps in $h \mathscr{D}$ the horizontal maps of $\mathscr{D}$.
- A vertical category $v \mathscr{D}$ whose objects are those of $\mathscr{D}$. We call the maps in $v \mathscr{D}$ the vertical maps of $\mathscr{D}$.
- For vertical maps $j: A \rightarrow B$ and $k: C \rightarrow D$ and horizontal maps $f: A \rightarrow C$ and $g: B \rightarrow D$, there is a set of squares

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-137.jpg?height=257&width=274&top_left_y=728&top_left_x=969)
- Squares can be composed both horizontally and vertically:
![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-137.jpg?height=804&width=960&top_left_y=1100&top_left_x=623)
- For every vertical map $j: A \rightarrow B$, there is an identity square

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-137.jpg?height=244&width=271&top_left_y=2014&top_left_x=968)

which we will also refer to as $j$, for convenience. Similarly, for every horizontal
$\operatorname{map} f: A \rightarrow B$, there is an identity square

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-138.jpg?height=298&width=266&top_left_y=328&top_left_x=970)

which we will also refer to as $f$, for convenience.
- Vertical and horizontal composition is associative and unital, and the interchange law holds. That is:
- For horizontally composable squares $\alpha, \beta$, and $\gamma$,

$$
(\alpha \mid \beta)|\gamma=\alpha|(\beta \mid \gamma)
$$
- For vertically composable squares $\alpha, \beta$, and $\gamma^{a}$

$$
\frac{\left(\frac{\alpha}{\beta}\right)}{\gamma}=\frac{\alpha}{\left(\frac{\gamma}{\beta}\right)}
$$
- For a square $\alpha$ with left and right vertical edges $j$ and $k$ respectively,

$$
j|\alpha=\alpha=\alpha| k .
$$
- For a square $\alpha$ with top and bottom horizontal edges $f$ and $g$,

$$
\frac{f}{\alpha}=\alpha=\frac{\alpha}{g} . b
$$
- For four appropriately composable squares $\alpha, \beta, \gamma$, and $\delta$, the following interchange law holds:

$$
\left.\frac{\alpha \mid \beta}{\gamma \mid \delta}=\frac{\alpha}{\beta} \right\rvert\, \frac{\gamma}{\delta}
$$
\footnotetext{
${ }^{a}$ If you're seeing this and feeling worried about fractions, you can put your mind at ease; we promise there will be no fractions. Only squares next to squares.

${ }^{b}$ There aren't any fractions here either.
}

Phew, that was quite the definition! The reason the definition of a double category is so much more involved than the definition of a category is that there is more than twice the data: there's the vertical category and the horizontal category, but also how they interact through the squares.

Remark 3.4.0.2. Just like we notate the identity square on a vertical morphism $j$ by $j$ and the identity square on a horizontal morphism $f$ by $f$, we will often denote composition of vertical morphisms by $\frac{f}{g}$ and of horizontal morphisms by $j \mid k$. This notation agrees
with the composition of their respective identity squares, and will be much more pleasant to look at when writing equations.

Let's see a few important examples of double categories.

\subsection*{3.4.1 The double category of arenas in the deterministic systems theory}

Finally, we are ready to meet the double category of arenas in the deterministic systems theory. This is where our dynamical systems live, and where they behave.

Definition 3.4.1.1. The double category of arenas in the deterministic systems theory is a double category which has:
- Its objects are the arenas, pairs of sets $\left(\begin{array}{l}A^{-} \\ A^{+}\end{array}\right)$.
- Its horizontal category is the category of charts.
- Its vertical category is the category of lenses.
- There is a square of the following form

$$
\begin{array}{r}
\left(\begin{array}{c}
A^{-} \\
A^{+}
\end{array}\right) \xrightarrow{\left(\begin{array}{c}
f_{b} \\
f
\end{array}\right)}\left(\begin{array}{l}
B^{-} \\
B^{+}
\end{array}\right) \\
\left(\begin{array}{c}
j^{\sharp} \\
j
\end{array}\right) \downarrow \uparrow  \tag{3.5}\\
\downarrow \uparrow\left(\begin{array}{l}
k^{\sharp} \\
k
\end{array}\right) \\
\left(\begin{array}{l}
C^{-} \\
C^{+}
\end{array}\right) \xrightarrow[\left(\begin{array}{c}
g_{b} \\
g
\end{array}\right)]{\longrightarrow}\left(\begin{array}{l}
D^{-} \\
D^{+}
\end{array}\right)
\end{array}
$$

if and only if the following equations hold:

$$
\begin{align*}
g\left(j\left(a^{+}\right)\right) & =k\left(f\left(a^{+}\right)\right)  \tag{3.6}\\
k^{\sharp}\left(f\left(a^{+}\right), g_{\mathrm{b}}\left(j\left(a^{+}\right), c^{-}\right)\right) & =f_{\mathrm{b}}\left(a^{+}, j^{\sharp}\left(a^{+}, c^{-}\right)\right) \tag{3.7}
\end{align*}
$$

for all $a^{+} \in A^{+}$and $c^{-} \in C^{-}$.

It's not obvious from this definition that we actually get a double category with this definition. It's not even clear that we have defined a way to compose the squares vertically and horizontally.

It turns out we don't need to know anything else to know that we can compose these squares, at least in principle. This is because there is at most one square filling any two charts and two lenses that line up as in Eq. (3.5); to compose these squares just means that if we have two such squares lining up, the defining equations Eq. (3.6) hold also for the appropriate composites. We call double categories with this property thin.

Definition 3.4.1.2. A double category is thin if there is at most one square of any signature.

So long as composition is well defined in a thin double category, the laws of associativity and interchange for square composition come for free; there is at most one square of the appropriate signature, so any two you can write down are already equal. We do still have to show that composition is well defined in this way, which we'll do a bit more generally in Definition 3.5.0.6

Remark 3.4.1.3. While the definition of double category we gave treated both horizontal and vertical directions the same, we will often want to see a square

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-140.jpg?height=258&width=274&top_left_y=852&top_left_x=920)

as a sort of map $\alpha: j \rightarrow k$ from its left to its right side, or a map $\alpha: f \rightarrow g$ from its top to its bottom side. For example, the systems themselves are certain lenses (vertical maps), and the behaviors are squares between them. On the other hand, we can also see a square as a way of wiring together charts.

Example 3.4.1.4. A square

$$
\begin{array}{r}
\left(\begin{array}{c}
A^{-} \\
A^{+}
\end{array}\right) \xrightarrow{\left(\begin{array}{c}
f_{\mathrm{b}} \\
f
\end{array}\right)} \xrightarrow{\longrightarrow}\left(\begin{array}{l}
B^{-} \\
B^{+}
\end{array}\right) \\
\left(\begin{array}{c}
j^{\sharp} \\
j
\end{array}\right) \downarrow \uparrow  \tag{3.8}\\
\left(\uparrow\left(\begin{array}{l}
k^{\sharp} \\
k
\end{array}\right)\right. \\
\left(\begin{array}{l}
C^{-} \\
C^{+}
\end{array}\right) \xrightarrow[\left(\begin{array}{c}
g^{\sharp} \\
g
\end{array}\right)]{\longrightarrow}\left(\begin{array}{l}
D^{-} \\
D^{+}
\end{array}\right)
\end{array}
$$

can be seen as a chart between lenses, that is, two charts which are compatible according to the wiring pattern the lenses describe. For example, consider a square of the
following form where $\left(\begin{array}{c}w^{\sharp} \\ w\end{array}\right)$ is a wiring diagram:

$$
\begin{aligned}
& \left(\begin{array}{l}
1 \\
1
\end{array}\right) \xrightarrow{\left(\begin{array}{l}
b^{-} \\
b^{+}
\end{array}\right)}\left(\begin{array}{l}
B^{-} \\
B^{+}
\end{array}\right) \\
& \|\| \| \quad \downarrow \uparrow\left(\begin{array}{c}
w^{\sharp} \\
w
\end{array}\right) \\
& \left(\begin{array}{l}
1 \\
1
\end{array}\right) \underset{\left(\begin{array}{l}
d^{-} \\
d^{+}
\end{array}\right)}{\longrightarrow}\left(\begin{array}{l}
D^{-} \\
D^{+}
\end{array}\right)
\end{aligned}
$$

By Exercise 3.3.0.16, we know that the charts in this diagram are pairs of elements $\left(\begin{array}{l}b^{-} \\ b^{+}\end{array}\right)$ and $\left(\begin{array}{l}d^{-} \\ d^{+}\end{array}\right)$in the arenas $\left(\begin{array}{c}B^{-} \\ B^{+}\end{array}\right)$and $\left(\begin{array}{c}D^{-} \\ D^{+}\end{array}\right)$respectively. The square then says that $\left(\begin{array}{l}d^{-} \\ d^{+}\end{array}\right)$ are the values you would get if you passed $\left(\begin{array}{l}b^{-} \\ b^{+}\end{array}\right)$along the wires in the wiring diagram $\left(\begin{array}{c}w^{\sharp} \\ w\end{array}\right):$

$$
\begin{aligned}
& w\left(b^{+}\right)=d^{+}, \\
& w^{\sharp}\left(b^{+}, d^{-}\right)=b^{-} .
\end{aligned}
$$

Taking for granted that the double category of arenas is indeed a double category, what does this mean for systems? Well, behaviors are particular squares in the double category of arenas.

Proposition 3.4.1.5. Let $T$ and $S$ be dynamical systems. A behavior $\phi: T \rightarrow S$ is equivalently a square of the following form in the double category of arenas:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-141.jpg?height=498&width=580&top_left_y=1773&top_left_x=770)

Proof. This is a simple matter of checking the definitions against eachother. The defining equations of Definition 3.4.1.1 specialize to the defining equations of Definition 3.3.0.4.

This re-expression of the notion of behavior in terms of the double category of arenas will let us generalize from the deterministic systems theory to other systems theories.

\subsection*{3.4.2 The double category of sets, functions, and matrices}

Now we turn to our second double category of interest, the double category of sets, functions, and matrices of sets.

Jaz: Where did I first define a matrix of sets? If it's before this, I should reference it. If it's after this, I should just introduce it here instead.

Definition 3.4.2.1. The double category Matrix of sets, functions, and matrices of sets is defined by:
- Its objects are sets.
- Its horizontal category is the category of sets and functions.
- Its vertical category is the category of sets and matrices of sets, where composition is given by matrix multiplication. We write $M: A \rightarrow B$ to say that $M$ is a $B \times A$ matrix.
- For functions $f: A \rightarrow B$ and $g: C \rightarrow D$ and matrices $M: A \rightarrow C$ and $N: B \rightarrow D$, a square

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-142.jpg?height=261&width=312&top_left_y=1279&top_left_x=950)

is a family of functions $\alpha_{b a}: M_{b a} \rightarrow N_{g(b) f(a)}$ for all $a \in A$ and $b \in B$.
- Horizontal composition of squares is given by composition of the families:

$$
(\alpha \mid \beta)_{b a}=\beta_{g(b) f(a)} \circ \alpha_{b a} .
$$
- Vertical composition of squares

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-142.jpg?height=407&width=309&top_left_y=1881&top_left_x=951)
is given by

$$
\begin{aligned}
\left(\frac{\alpha}{\beta}\right)_{a c}: \sum_{b_{1} \in B_{1}} M_{c b_{1}}^{2} \times M_{b_{1} a}^{1} & \rightarrow \sum_{b_{2} \in B_{2}} N_{h(c) b_{2}}^{2} \times N_{b_{2} f(a)}^{1} \\
\left(b_{1}, m_{2}, m_{1}\right) & \mapsto\left(g\left(b_{1}\right), \beta\left(m_{2}\right), \alpha\left(m_{1}\right)\right) .
\end{aligned}
$$

Exercise 3.4.2.2. We can see that horizontal composition of squares is associative and unital since it is basically just function composition. Show that Matrix is a double category by checking that

1. Vertical composition of squares is associative and unital (up to isomorphism).

2. The interchange law holds.

There is another useful way to express the double category of matrices in terms of pullbacks: spans. A span of sets from $A$ to $B$ is a diagram

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-143.jpg?height=173&width=350&top_left_y=1125&top_left_x=882)

We can think of this as a matrix of sets by sending any $a \in A$ and $b \in B$ to the set $S_{b a}=\left\{x \in S \mid a=s_{A}(x)\right.$ and $\left.s_{B}=b\right\}$. And to any $(B \times A)$-matrix $M$ of sets, we can associate the span

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-143.jpg?height=190&width=568&top_left_y=1491&top_left_x=773)

with the disjoint union of all $M_{b a}$ at the top, with the two maps begin the projections onto $A$ and $B$ respectively.

The composition of matrices can be represented in terms of spans as well. Given the spans $S$ from $A$ to $B$ and $T$ from $B$ to $C$, we can define their composite span $\frac{S}{T}$ from $A$ to $C$ by taking the pullback:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-143.jpg?height=306&width=740&top_left_y=1994&top_left_x=687)

where

$$
S \times \times_{B} T=\left\{(x, y) \in S \times T \mid s_{B}(x)=t_{B}(y)\right\} .
$$

A bit of thinking shows that this corresponds to the composite of matrices.

Exercise 3.4.2.3. Let $M$ be an $(A \times B)$-matrix and $N$ be a $(B \times C)$-matrix. Consider the following diagram:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-144.jpg?height=349&width=1109&top_left_y=411&top_left_x=497)

Show that there are dashed maps as in the above diagram so that the square is a pullback. This shows that the composition of matrices corresponds to the composition of spans.

One nice feature that spans have over matrices is that they work for things other than sets. We can take spans in any category with pullbacks. We'll record the double category of spans here.

Definition 3.4.2.4. Let $C$ be a category with pullbacks. The double category $\operatorname{Span}(C)$ is defined by:
- Its objects are the objects of $C$.
- Its horizontal category is $C$.
- Its vertical category has as morphisms $S: A \rightarrow B$ the spans

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-144.jpg?height=144&width=263&top_left_y=1527&top_left_x=974)

and these are composed by pullback.
- A square

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-144.jpg?height=280&width=1128&top_left_y=1844&top_left_x=539)
- Horizontal composition of squares is by composing in $C$.
- Vertical composition of squares follows from the functoriality of the pullback:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-145.jpg?height=471&width=832&top_left_y=331&top_left_x=690)

\subsection*{3.4.3 The double category of categories, profunctors, and functors}

Now we come to the primordial double category: the double category of categories, profunctors, and functors. This is an important double category because it is in some sense the setting in which all category theory takes place. Before we describe this double category, let's define the notion of profunctor and their category.

Definition 3.4.3.1. A profunctor $P: \mathcal{A} \rightarrow \mathscr{B}$ is a functor $P: \mathcal{A}^{\mathrm{op}} \times \mathscr{B} \rightarrow$ Set. Given objects $A \in \mathcal{A}$ and $B \in \mathscr{B}$, we write an element $p \in P(A, B)$ as $p: A \rightarrow B$.

In terms of this, the functoriality of $P$ can be seen as letting us compose $p: A \rightarrow B$ on the left and right by $f: A^{\prime} \rightarrow A$ and $g: B \rightarrow B^{\prime}$ to get $f p g: A^{\prime} \rightarrow B^{\prime}$. In other words, we can interpret a diagram of this form

$$
A^{\prime} \xrightarrow{f} A \xrightarrow{p} B \xrightarrow{g} B^{\prime}
$$

as an element of $P\left(A^{\prime}, B^{\prime}\right)$.

If we call maps $f: A^{\prime} \rightarrow A$ in a category $\mathcal{A}$ homomorphisms because they go between objects of the same form, we could call elements $p: A \rightarrow B$ - that is, $p \in P(A, B)-$ as heteromorphisms, maps going between objects of different forms.

We can't necessarily compose these heteromorphisms, which we can see right away from their signature: for $p: A \rightarrow B$, there is always an object of $\mathcal{A}$ on the left and an object of $\mathscr{B}$ on the right, so we'll never be able to line two of them up. However, if we have another profunctor $Q: \mathscr{B} \rightarrow C-$ another notion of heteromorphism then we can "compose" heteromorphisms $A \xrightarrow{p} B$ in $P$ with $B \xrightarrow{q} C$ in $Q$ to get a heteromorphism $A \xrightarrow{p} B \xrightarrow{q} C$ in a new profunctor $P \odot Q: \mathcal{A} \rightarrow C$.

Definition 3.4.3.2. The composite $P \odot Q$ of a profunctor $P: \mathcal{A} \rightarrow \mathbb{B}$ with a profunctor
$Q: \mathscr{B} \rightarrow C$ is defined to be the following quotient:

$$
\begin{equation*}
(P \odot Q)(A, C):=\frac{\sum_{B \in \mathcal{B}} P(A, B) \times Q(B, C)}{(p f, q) \sim(p, f q)} \tag{3.10}
\end{equation*}
$$

We write an element $[(p, q)] \in(P \odot Q)(A, C)$ as $A \xrightarrow{p} B \xrightarrow{q} C$, so that the relation we quotient by says that

$$
A \xrightarrow{p} B \xrightarrow{f} B^{\prime} \xrightarrow{q} C
$$

has a unique interpretation as an element of $P \odot Q$.

The identity profunctor $\mathcal{A}: \mathcal{A} \rightarrow \mathcal{A}$ is the hom-functor sending $A$ and $A^{\prime}$ to the set $\mathcal{A}\left(A, A^{\prime}\right)$ of maps $A \rightarrow A^{\prime}$.

We can see that composition of profunctors is associative (up to isomorphism) because the objects of $P \odot(Q \odot R)$ and $(P \odot Q) \odot R$ can both be written as

$$
A \xrightarrow{p} B \xrightarrow{q} C \stackrel{r}{\rightarrow} D .
$$

The reason the hom profunctor $\mathcal{A}: \mathcal{A} \rightarrow \mathcal{A}$ is the identity profunctor is because the elements of $\mathcal{A} \odot P$ would be written as

$$
A^{\prime} \xrightarrow{f} A \xrightarrow{p} B
$$

but by the functoriality of $P$, this is already an element of $P\left(A^{\prime}, B\right)$, which is to say more precisely that every equivalence class $[(f, p)] \in(\mathcal{A} \odot P)\left(A^{\prime}, B\right)$ is equally presented as $\left[\left(\operatorname{id}_{A^{\prime}}, f p\right)\right]$.

Exercise 3.4.3.3. Let $P: \mathcal{A} \rightarrow \mathbb{B}$ be a profunctor.

1. Show that there is a natural transformation $\mathcal{A} \odot P \rightarrow P$ given by the naturality of $P$ on the left.

2. Show that there is a natural transformation $P \odot \mathbb{B} \rightarrow P$ given by the naturality of $P$ on the right.

3. Show that both of these natural transformations are isomorphisms.

Example 3.4.3.4. A profunctor $1 \rightarrow \mathcal{A}$ is the same thing as a functor $\mathcal{A} \rightarrow$ Set, and a profunctor $\mathcal{A} \rightarrow 1$ is the same thing as a functor $\mathcal{A}^{\mathrm{op}} \rightarrow$ Set. Profunctors are therefore intimately related with presheaves.

Now, we are ready to put functors and profunctors together into a double category.

Definition 3.4.3.5. The double category Cat of categories, profunctors, and functors has
- Objects the categories.
- Horizontal category the category of categories and profunctors.
- Vertical category the category of categories and functors between them.
- A square

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-147.jpg?height=252&width=301&top_left_y=584&top_left_x=955)

Is a natural transformation $\alpha: P \rightarrow Q(F, G)$, where $Q(F, G)$ is the profunctor $\mathcal{A}^{\mathrm{op}} \times \mathcal{B} \xrightarrow{F^{\mathrm{op}} \times G} C \times \mathcal{D} \xrightarrow{Q}$ Set. For $p: A \rightarrow B$, we have $\alpha(p): F A \rightarrow G B$, and naturality says that $\alpha(f p g)=(F f) \alpha(p)(G g)$.
- Vertical composition of squares is given by composing the natural transformations.
- Given squares $\alpha: P_{1} \rightarrow Q_{1}\left(F_{1}, F_{2}\right)$ and $\beta: P_{2} \rightarrow Q_{2}\left(F_{2}, F_{3}\right)$, we define their horizontal composite $\alpha \mid \beta: P_{1} \cdot P_{2} \rightarrow\left(Q_{1} \cdot Q_{2}\right)\left(F_{1}, F_{3}\right)$ by

$$
(\alpha \mid \beta)\left(A_{1} \xrightarrow{p_{1}} A_{2} \xrightarrow{p_{2}} A_{3}\right)=F_{1} A_{1} \xrightarrow{\alpha\left(p_{1}\right)} F_{2} A_{2} \xrightarrow{\beta\left(p_{2}\right)} F_{3} A_{3}
$$

and checking that this descends correctly to the quotient.

Remark 3.4.3.6. We are using "Cat" to refer to the category of categories and functors and to the double category of categories, profunctors, and functors. The one we mean will be clear from context, and the category of categories and functors is the vertical category of the double category of categories.

Remark 3.4.3.7. We omit full proofs of associativity and unitality for profunctor composition because they are best done with the coend calculus, and this would take us quite far afield. See [Gra19] and [Lor21] for more about profunctors and double categories.

However, we will note that there is always a unique coherent isomorphism between any two sequences of profunctors which would be equal if unity and associativity held on the nose. We will do an example, since the general principle is always the same.

Consider $P: \mathcal{A} \rightarrow \mathbb{B}$ and $Q: \mathscr{B} \rightarrow \mathcal{C}$. We will give the canonical isomorphism $(\mathcal{A} \odot P) \odot(Q \odot \mathcal{C}) \xrightarrow{\sim} P \odot(\mathbb{B} \odot(\mathbb{B} \odot Q))$.

First, we begin with an isomorphism $(\mathcal{A} \odot P) \odot(Q \odot C) \xrightarrow{\sim} P \odot Q$ and then an isomorphism $P \odot Q \xrightarrow{\sim} P \odot(\mathbb{B} \odot(\mathbb{B} \odot Q))$. The first will be given by naturality, composition and re-associating; the second by inserting appropriate identities and re-associating.

An element of $(\mathcal{A} \odot P) \odot(Q \odot \mathcal{C})(A, C)$ is an equivalence class $[((f, p),(q, g))]$. We may therefore use the naturality of $P$ and $Q$ to give the class $[(f \cdot p, q \cdot g)] \in(P \odot Q)(A, C)$.

It is routine to check that this is indeed an isomorphism. It is hopefully clear how to do this in general.

Now, we go the other way. An element of $(P \odot Q)(A, C)$ is an equivalence class $[(p, q)]$. We may then insert identities to give the class $[(p,(\mathrm{id},(\mathrm{id}, q)))] \in P \odot(\mathscr{B} \odot(\mathscr{B} \odot Q))(A, C)$.

A crucial point about canonical isomorphisms constructed in this manner is that they compose: the composite of a canonical isomorphism is the canonical isomorphism of that signature.

Exercise 3.4.3.8. Describe the canonical isomorphisms between the following composites of profunctors. First, flatten them out by removing all hom profunctors using naturality; then expand them again by inserting identities. Let $P: \mathcal{A} \rightarrow \mathbb{B}, Q: \mathscr{B} \rightarrow C$, and $R: C \rightarrow \mathcal{D}$.
1. $(P \odot \mathbb{B}) \odot(\mathbb{B} \odot Q) \xrightarrow{\sim} \mathcal{A} \odot((P \odot \mathcal{B}) \odot C)$.
2. $P \odot((Q \odot C) \odot(C \odot R)) \xrightarrow{\sim}((P \odot Q) \odot C) \odot(R \odot \mathcal{D})$.

Remark 3.4.3.9. We will often need equalities between squares in the double category Cat whose boundaries are not precisely equal, but which are canonically isomorphic. The coming Lemma 3.4.3.11 is an example of this common scenario.

It would clutter already intricate proofs to keep track of the canonical isomorphisms which are being introduced and cancelled at each step. For this reason, we'll introduce notation for "equal up to canonical isomorphism on the boundary". We will write

$$
\alpha \doteq \beta
$$

to mean that although $\alpha$ and $\beta$ have different boundaries, these boundaries are canonically isomorphic and whenever they are made to be the same by any canonical isomorphism (pre- or post-composing $\alpha$ and $\beta$ as necessary), the resulting squares will be honestly equal. We will see our first example in Lemma 3.4.3.11.

Before we move on from the double category Cat, let's record an important relationship between its squares (natural transformations between profunctors) and natural transformations between functors. We will show that natural transformations are the same thing as squares in Cat whose top and bottom sides are hom profunctors.

Proposition 3.4.3.10. Let $F$ and $G: \mathcal{A} \rightarrow \mathbb{B}$ be functors. Then there is a (natural) bijection

$$
\{\text { Natural transformations } F \Rightarrow G\} \cong\left\{\begin{array}{rlc}
\mathcal{A} & \ldots & \mathcal{A} \\
\text { Squares }_{F} \downarrow & \alpha & \downarrow G \\
& & \\
\mathcal{B} & \rightleftharpoons & \mathscr{B}
\end{array}\right\}
$$

given by sending the natural transformation $\alpha: F \Rightarrow G$ to the transformation $\bar{\alpha}$ : $\mathcal{A}(X, Y) \Rightarrow \mathcal{B}(F X, G Y)$ that sends any $f: X \rightarrow Y$ the diagonal $\alpha_{f}$ of the naturality square:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-149.jpg?height=190&width=304&top_left_y=805&top_left_x=908)

Proof. First, let's check that the transformation $\bar{\alpha}(f)=\alpha_{f}$ is natural. If $x: X^{\prime} \rightarrow X$ and $y: Y \rightarrow Y^{\prime}$, then we can form the following commutative diagram of naturality squares:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-149.jpg?height=447&width=309&top_left_y=1213&top_left_x=903)

The diagonal of the outer square is by definition $\bar{\alpha}(x ; f ; y)$, but we can see from the commutativity of the diagram that it equals $F x ; \alpha_{f} ; G y$.

It remains to show that any natural transformation $\beta: \mathcal{A}(X, Y) \Rightarrow \mathcal{B}(F X, G Y)$ arises uniquely as $\bar{\alpha}$ for a natural transformation $\alpha: F \Rightarrow G$. Given such a $\beta$, define $\alpha_{X}:=\beta\left(\operatorname{id}_{X}\right)$. We need to prove the naturality of $\alpha$ by showing that any solid square

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-149.jpg?height=184&width=293&top_left_y=1965&top_left_x=908)

commutes. But note that if we put in the dashed $\beta(f)$, we can see that both triangles commute by the naturality of $\beta(f)$ :

$$
\begin{aligned}
& \beta(f)=\beta\left(\mathrm{id}_{X} \circ f\right)=\beta\left(\mathrm{id}_{X}\right) \circ G f=\alpha_{X} \circ G f . \\
& \beta(f)=\beta\left(f \circ \mathrm{id}_{Y}\right)=F f \circ \beta\left(\mathrm{id}_{Y}\right)=F f \circ \alpha_{Y} .
\end{aligned}
$$

This also shows that $\beta(f)=\bar{\alpha}(f)$, which completes the proof.

There are two ways to compose natural transformations: vertically, and horizontally. The above bijection respects both of these compositions. In the following lemmas we take the notation from Proposition 3.4.3.10.

Lemma 3.4.3.11. Let $\alpha: F \Rightarrow G$ and $\beta: G \Rightarrow H$. Then

$$
\overline{\alpha_{9}^{\circ} \beta} \doteq \bar{\alpha} \mid \bar{\beta} .
$$

Proof. Here we are using the symbol " $=$ " from Remark 3.4.3.9 for the first time; this is because the two sides do not have equal signature, only isomorphic signature. To correctly compare them, we must conjugate by the appropriate isomorphisms. Here, with signature included, is the actual equality we will prove:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-150.jpg?height=363&width=870&top_left_y=962&top_left_x=622)

We leave the canonical isomorphisms without names. They can be described by the process outlined in Remark 3.4.3.7. We note that both of these canonical isomorphisms are given by composing two arrows, so in order to prove the equality above we will show that given $f: X \rightarrow Y$ and $g: Y \rightarrow Z$,

$$
\left(\alpha_{9}^{\circ} \beta\right)_{f \circ g}=\alpha_{f} \stackrel{\circ}{9} \beta_{g} .
$$

We will do this by contemplating the following diagram:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-150.jpg?height=320&width=482&top_left_y=1851&top_left_x=819)

The naturality square for the composite $f ; g$ under the composite $\alpha ; \beta$ is the outer square, and therefore its diagonal $(\alpha ; \beta)_{f \circ g}$ is the composite of the diagonals in the diagram, which is $\alpha_{f} ; \beta_{g}$.

Lemma 3.4.3.12. Let $F_{1}, G_{1}: \mathcal{A} \rightarrow \mathbb{B}$ and $F_{2}, G_{2}: \mathscr{B} \rightarrow \mathcal{C}$ be functors, and let $\alpha: F_{1} \Rightarrow$ $G_{1}$ and $\beta: F_{2} \Rightarrow G_{2}$ be natural transformations. We may define their composite $\alpha * \beta$ by

$$
(\alpha * \beta)_{X}:=F_{2} \alpha_{X} \stackrel{\circ}{ } \beta_{G_{1} X}
$$

With this definition, we have

$$
\overline{\alpha * \beta}=\frac{\bar{\alpha}}{\bar{\beta}}
$$

Remark 3.4.3.13. Note that the equality claimed here is a bona-fide equality, and not an "equality up to canonical isomorphism" $(=)$. This is because the two squares involved have the exact same boundary, not merely a canonically isomorphic boundaries.

Proof. This time, we may prove the equality as stated. It comes down to showing that

$$
(\alpha * \beta)_{f}=\beta_{\alpha_{f}}
$$

for any $f: X \rightarrow Y$. Consider the following diagram:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-151.jpg?height=459&width=1048&top_left_y=1150&top_left_x=533)

The back and front faces are the $\alpha$ naturality square of $f$ pushed through $F_{2}$ and $G_{2}$ respectively. The $\beta$ naturality square of $\alpha_{f}$ is in the middle, colored in red. The $\alpha * \beta$ naturality square of $f$ is in the middle, colored in blue. We note that the diagonal of both these squares is the diagonal of the whole cube $F_{2} F_{1} X \rightarrow G_{2} G_{1} Y$, which means that they are equal. But this is what we were trying to show.

\subsection*{3.5 Theories of Dynamical Systems}

In Section 2.6, we saw how from the data of an indexed category $\mathcal{A}: \mathcal{C o p}^{\mathrm{op}} \rightarrow$ Cat we could define a category of $\mathcal{A}$-lenses via the Grothendieck construction:

$$
\text { Lens }_{\mathcal{A}}:=\int^{C: C} \mathcal{A}(C)^{\mathrm{op}}
$$

From this, we learned we could wire non-deterministic systems together because a system could be expressed as a monadic lens of the form $\left(\begin{array}{c}\text { update } \\ \text { expose }_{S}\end{array}\right):\left(\begin{array}{l}\text { States } \\ \text { States }_{S}\end{array}\right) \leftrightarrows\left(\begin{array}{c}\ln _{s} \\ \text { Outs }_{s}\end{array}\right)$.

Now, the form $\left(\begin{array}{l}S \\ S\end{array}\right) \leftrightarrows\left(\begin{array}{l}I \\ O\end{array}\right)$ is not something that be expressed for a general $\mathcal{A}$-lens because in an $\mathcal{A}$-lens $\left(\begin{array}{c}A^{-} \\ A^{+}\end{array}\right), A^{+} \in C$ while $A^{-} \in \mathcal{A}(C)$. In general, $C$ and $\mathcal{A}(C)$ might have different objects. This suggests that we need a way to assign an object $T C \in \mathcal{A}(C)$ to each object of $C$, so that we can define a system, in general, to be an $\mathcal{A}$-lens of the form

$$
\left(\begin{array}{c}
\text { update }_{\mathrm{S}} \\
\text { expose }_{\mathrm{S}}
\end{array}\right):\left(\begin{array}{c}
T \text { States }^{2} \\
\text { State }_{\mathrm{S}}
\end{array}\right) \leftrightarrows\left(\begin{array}{c}
\ln _{\mathrm{S}} \\
\text { Out }_{\mathrm{S}}
\end{array}\right)
$$

At this point, your categorical nose should be twitching. We've given a assignment on objects; how is this assignment functorial? We can discover what sort of functoriality we need from considering the expression in Proposition 3.4.1.5 of behaviors as squares of arenas in the deterministic systems theory:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-152.jpg?height=504&width=597&top_left_y=957&top_left_x=756)

To express this square, we did not just use the fact that we could find States in both the base $C$ and in the category $\mathcal{A}$ (States) (recall that here, $\mathcal{A}=\mathbf{C t x}_{-}: C^{\text {op }} \rightarrow$ Cat). We also used the fact that to any map $\phi:$ State $_{\mathrm{T}} \rightarrow$ States we can build a chart $\left(\begin{array}{c}\phi \circ \pi_{2} \\ \phi\end{array}\right):\left(\begin{array}{l}\text { StateT } \\ \text { StateT }\end{array}\right) \rightarrow\left(\begin{array}{c}\text { States } \\ \text { States }\end{array}\right)$. This is the sort of functoriality we need to define the notion of behavior in general.

Definition 3.5.0.1. Let $\mathcal{A}: e^{\mathrm{op}} \rightarrow$ Cat be a strict indexed category. A section $T$ of $\mathcal{A}$ consists of the following assignments:
- To every object $C \in C$, an object $T C \in \mathcal{A}(C)$.
- To every $\phi: C^{\prime} \rightarrow C$, a map $T \phi: T C^{\prime} \rightarrow \phi^{*} T C$.

These are required to satisfy the following laws:
- For any $C \in C, T \mathrm{id}_{C}=\mathrm{id}_{T C}$.
- For $\phi: C^{\prime} \rightarrow C$ and $\psi: C^{\prime \prime} \rightarrow C^{\prime}$,

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-152.jpg?height=67&width=411&top_left_y=2232&top_left_x=898)

We can express a section of an indexed category in terms of a functor into its Grothendieck construction.

Proposition 3.5.0.2. Let $\mathcal{A}: \mathcal{C}^{\mathrm{op}} \rightarrow$ Cat be a strict indexed category. A section $T$ of $\mathcal{A}$ is equivalently given by the data of a functor $\hat{T}: C \rightarrow \int^{C: C} \mathcal{A}(C)$ for which the composite $C \xrightarrow{\hat{T}} \int^{C: C} \mathcal{A}(C) \xrightarrow{\text { proj }} C$ with the projection is the identity on $C$.

Given a section $T$, we may more suggestively refer to $\hat{T}$ by $\left(\begin{array}{c}T(-) \\ (-)\end{array}\right)$.

Proof. Given a section $T$, we can form the functor

$$
C \mapsto\left(\begin{array}{c}
T C \\
C
\end{array}\right): C \rightarrow \int^{C: C} \mathcal{A}(C)
$$

sending $\phi$ to $\left(\begin{array}{c}T \phi \\ \phi\end{array}\right)$. The laws of the section show that this is a functor.

On the other hand, given a $\hat{T}: C \rightarrow \int^{C: C} \mathcal{A}(C)$ whose composite with the projection is the identity, we see that $\hat{T}(C)$ must be of the form $\left(\begin{array}{c}T C \\ C\end{array}\right)$ and that $\hat{T}(\phi)$ must be of the form $\left(\begin{array}{c}T \phi \\ \phi\end{array}\right)$, where $T C$ and $T \phi$ are defined to be the components of $\hat{T}$ which live in the categories $\mathcal{A}(C)$. It is straightforward to check that functoriality implies the laws of a section.

We can see that the assignmen $\phi \mapsto \phi \circ \pi_{2}$ is a section of $\mathbf{C t x}_{-}:$Cop $^{\rightarrow}$ Cat.

Proposition 3.5.0.3. Let $C$ be a cartesian category. Then the assigment $C \mapsto C$ and $\phi \mapsto \phi \circ \pi_{2}$ gives a section of Ctx C $_{-}$op $\rightarrow$ Cat.

Proof. We check that the two laws are satisfied.

1. id $\circ \pi_{2}=\pi_{2}$, which is the identity in $\mathbf{C t x}{ }_{C}$.

2. We may calculate:

$$
\begin{aligned}
\left(\psi \circ \pi_{2}\right) \circ \psi^{*}\left(\phi \circ \pi_{2}\right)(c, x) & =\psi^{*}\left(\phi \circ \pi_{2}\right)(c, \psi(x)) \\
& =\phi \circ \pi_{2}(\psi(c), \psi(x)) \\
& =\phi(\psi(x)) \\
& =(\psi \circ \phi) \circ \pi_{2}(c, x)
\end{aligned}
$$

In order to define lenses, we need the data of an indexed category $\mathcal{A}:$ Cop $^{\text {op }} \rightarrow$ Cat. In order to define dynamical systems as $\mathcal{A}$-lenses, and to define the behaviors between them, we need the data of a section $T$ of $\mathcal{A}$. Putting these two bits of data together, we get the notion of dynamical system systems theory.

Definition 3.5.0.4. A theory of dynamical systems consists of an indexed category $\mathcal{A}$ :

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-153.jpg?height=59&width=645&top_left_y=2429&top_left_x=328)

Having this definition of systems theory in mind, we can now define the notion of dynamical system and behavior in complete generality.

Definition 3.5.0.5. A dynamical system in a theory of dynamical systems $\mathbb{D}=(\mathcal{A}, T)$ is an $\mathcal{A}$-lens of the form

$$
\left(\begin{array}{c}
\text { update }_{\mathrm{S}} \\
\text { expose }_{\mathrm{S}}
\end{array}\right):\left(\begin{array}{c}
T \text { State }_{\mathrm{S}} \\
\text { State }_{\mathrm{S}}
\end{array}\right) \leftrightarrows\left(\begin{array}{c}
\ln _{\mathrm{S}} \\
\text { Out }_{\mathrm{S}}
\end{array}\right)
$$

Explcitly, this consists of:
- An object States $\in C$ of states.
- An object Outs $\in C$ of possible outputs.
- An object $\ln _{\mathrm{S}} \in \mathcal{A}$ (Outs) of possible inputs or parameters. What parameters are sensible may therefore depend on the output (in the sense of being an object of a category which depends for its definition on Outs).

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-154.jpg?height=60&width=1276&top_left_y=1000&top_left_x=392)
- A map update ${ }_{S}:$ expose $_{S}^{*} \ln _{S} \rightarrow T$ States which assigns to any parameter valid for the output of a given state to a possible change in state.

In order to define the notion of behavior, we will need to generalize the double category of arenas from the deterministic systems theory to an arbitrary systems theory. To do this, we will define the Grothendieck double construction, which produces a double category of arenas from an indexed category $\mathcal{A}$.

Definition 3.5.0.6. Let $\mathcal{A}: \mathrm{C}^{\mathrm{op}} \rightarrow$ Cat be an indexed category. The Grothendieck double construction

$$
\oiint^{C \in C} \mathcal{A}(C)
$$

is the double category defined by:
- Its objects are the pairs $\left(\begin{array}{l}A \\ C\end{array}\right)$ of an object $C \in C$ and an object $A \in \mathcal{A}(C)$.
- Its horizontal category is the Grothendieck construction $\int^{C \in C} \mathcal{A}(C)$ of $\mathcal{A}$.
- Its vertical category is the Grothendieck construction $\int^{C \in \mathcal{C}} \mathcal{A}(C)^{\mathrm{op}}$ of the opposite of $\mathcal{A}$.
- There is a square of the following form:

$$
\begin{aligned}
& \left(\begin{array}{l}
A_{1} \\
C_{1}
\end{array}\right) \xrightarrow{\left(\begin{array}{c}
g_{1 b} \\
g_{1}
\end{array}\right)}\left(\begin{array}{l}
A_{2} \\
C_{2}
\end{array}\right) \\
& \left(\begin{array}{c}
f_{1}^{\sharp} \\
f_{1}
\end{array}\right) \downarrow \uparrow \quad \downarrow\left(\begin{array}{c}
f_{2}^{\sharp} \\
f_{2}
\end{array}\right) \\
& \left(\begin{array}{l}
A_{3} \\
C_{3}
\end{array}\right) \underset{\left(\begin{array}{c}
g_{2 b} \\
g_{2}
\end{array}\right)}{\longrightarrow}\left(\begin{array}{l}
A_{4} \\
C_{4}
\end{array}\right)
\end{aligned}
$$

if and only if the following diagrams commute:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-155.jpg?height=263&width=1025&top_left_y=969&top_left_x=542)

We will call the squares in the Grothendieck double construction commuting squares, since they represent the proposition that the "lower" and "upper" squares appearing in their boundary commute.
- Composition of arrows in both directions is given as in the appropriate Grothendieck constructions.

It just remains to show that commuting squares compose. The lower squares compose because they are ordinary squares. It just remains to show that the upper squares commute.
- For vertical composition we appeal to the following diagram:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-155.jpg?height=418&width=940&top_left_y=1924&top_left_x=625)

The outer diagram is the "upper" square of the composite, while the "upper" squares of each factor appear in the top left and right respectively.
- For horizontal composition we appeal to the following diagram:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-156.jpg?height=545&width=708&top_left_y=343&top_left_x=749)

We can now check that this does indeed abstract the double category of arenas.

Proposition 3.5.0.7. The double category of arenas in the deterministic systems theory is the Grothendieck double construction of the indexed category of sets and functions in context $\mathbf{C t x}_{-}:$Set $^{\mathrm{op}} \rightarrow$ Cat:

$$
\text { Arena }=\oiint^{\mathrm{C} \in \mathrm{Set}} \mathrm{Ctx}_{C}
$$

Proof. By Propositions 2.6.2.5 and 3.3.0.15, the horizontal and vertical categories are the same. It remains to show that the diagrams of Eq. (3.11) mean the same things as Eq. (3.6).

Consider a square of the form

$$
\begin{aligned}
\left(\begin{array}{c}
A^{-} \\
A^{+}
\end{array}\right) & \xrightarrow{\left(\begin{array}{c}
f_{b} \\
f
\end{array}\right)}\left(\begin{array}{l}
B^{-} \\
B^{+}
\end{array}\right) \\
\left(\begin{array}{c}
j^{\sharp} \\
j
\end{array}\right) \downarrow \uparrow & \downarrow \uparrow\left(\begin{array}{l}
k^{\sharp} \\
k
\end{array}\right) \\
\left(\begin{array}{l}
C^{-} \\
C^{+}
\end{array}\right) & \underset{\left(\begin{array}{c}
g^{\sharp} \\
g
\end{array}\right)}{\longrightarrow}\left(\begin{array}{l}
D^{-} \\
D^{+}
\end{array}\right)
\end{aligned}
$$

The first diagram and first equation say:

$$
\begin{array}{llll}
A^{+} & f & B^{+} \\
j \downarrow & & \\
\downarrow & & \downarrow^{k} \\
C^{+} & & g\left(j\left(a^{+}\right)\right)=k\left(f\left(a^{+}\right)\right) \text {for all } a^{+} \in A^{+},
\end{array}
$$
which mean the same thing. The second diagram, which takes place in $\mathbf{C t x}_{A^{+}}$, is more interesting. Here's that diagram with the names we're currently using:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-157.jpg?height=239&width=588&top_left_y=393&top_left_x=758)

Let's compute the two paths from the top left to the bottom right. First is $f_{b} \circ j^{\sharp}: j^{*} C^{-} \rightarrow$ $f^{*} B^{-}$, which sends $\left(a^{+}, c^{-}\right)$to $f_{\mathrm{b}}\left(a^{+}, j^{\sharp}\left(a^{+}, c^{-}\right)\right)$. This is the right hand side of the second equation, so we're on the right track. The other path is $f^{*} k^{\sharp} \circ j^{*} g_{b}$. Recall that $j^{*} g_{b}$ sends $\left(a^{+}, c^{-}\right)$to $g_{\mathrm{b}}\left(j\left(a^{+}\right), c^{-}\right)$, and similarly $f^{*} k^{\sharp}$ sends $\left(a^{+}, d^{-}\right)$to $k^{\sharp}\left(f\left(a^{+}\right), d^{-}\right)$. Putting them together, we send $\left(a^{+}, c^{-}\right)$to $k^{\sharp}\left(f\left(a^{+}\right), g_{\mathrm{b}}\left(j\left(a^{+}\right), c^{-}\right)\right)$. Therefore the commutation of this diagram means the same thing as the second equation in the definition of a square of arenas.

Building off of this proposition, we can think of the Grothendieck double construction as giving us a double category of arenas out of any indexed category.

Definition 3.5.0.8. Let $\mathcal{A}: \mathrm{C}^{\mathrm{op}} \rightarrow$ Cat be an indexed category. Then the category of $\mathcal{A}$-arenas is defined to be the Grothendieck double construction of $\mathcal{A}$ :

$$
\text { Arena }_{\mathcal{A}}:=\oiint^{C \in C} \mathcal{A}(C)
$$

The horizontal category of Arena $\mathcal{A}_{\mathcal{A}}$ is the category Chart $_{\mathcal{A}}$ of $\mathcal{A}$-charts, and the vertical category of Arena $_{\mathcal{A}}$ is the category Lens Lef $_{\mathcal{A}}$ of $\mathcal{A}$-lenses.

With this definition of the double category of arenas in hand, we can define a behavior in a general systems theory.

Definition 3.5.0.9. Let $\mathbb{D}=(\mathcal{A}, T)$ be a systems theory, and $\mathrm{T}$ and $\mathrm{S}$ two systems in this systems theory. Given an $\mathcal{A}$-chart

$$
\left(\begin{array}{l}
f_{\mathrm{b}} \\
f
\end{array}\right):\left(\begin{array}{c}
\ln _{\mathrm{T}} \\
\text { Out }_{\mathrm{T}}
\end{array}\right) \rightrightarrows\left(\begin{array}{c}
\ln _{\mathrm{S}} \\
\text { Out }
\end{array}\right)
$$

$\mathrm{A}\left(\begin{array}{c}f_{\mathrm{b}} \\ f\end{array}\right)$-behavior $\phi: \mathrm{T} \rightarrow \mathrm{S}$ is a map $\phi:$ State $\mathrm{T} \rightarrow$ States so that the following is a square
in the double category Arena $\mathbf{A}_{\mathcal{A}}$ of $\mathcal{A}$-arenas:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-158.jpg?height=482&width=616&top_left_y=339&top_left_x=749)

We will often refer to this square by $\phi$ as well.

In Section 4.2, we will see what composition in the double category Arena $\boldsymbol{A}_{\mathcal{A}}$ of $\mathcal{A}$-arenas let's us conclude about composition of systems and behaviors. For now, in the rest of this section, we will formally introduce the theories of dynamical systems we have been working with throughout the book, with some precise variations we can now make clear.

But before we do that, let's see how the above rather terse formal definition captures the intuitive and informal definition given in Informal Definition 1.1.0.2:

Informal Definition 3.5.0.10. A theory of dynamical systems is a particular way to answer the following questions about what it means to be a dynamical system:

1. What does it mean to be a state?

2. How should the output vary with the state - discretely, continuously, linearly?

3. Can the kinds of input a system takes in depend on what it's putting out, and how do they depend on it?

4. What sorts of changes are possible in a given state?

5. What does it mean for states to change.

6. How should the way the state changes vary with the input?

Let's see how choosing an indexed category $\mathcal{A}:$ Cop $^{\mathrm{op}} \rightarrow$ Cat and a section $T$ constitutes a series of answers to each of these questions.

1. We had to choose the base category $C$. Our space of states will be an object of this category, and so choosing the objects of this category means choosing what it means to be a state.

2. Our exposed variable expose $_{S}:$ State $_{S} \rightarrow$ Out $_{S}$ will be a morphism of $C$, so choosing the morphisms of $C$ will mean choosing how the output will vary with the state.

3. The input Ins will be an object of $\mathcal{A}$ (Outs), and therefore defining the objects of $\mathcal{A}$ (Outs) - in particular, how they depend on Outs - will determine how a system's space of inputs may depend on its outputs.

4. The our update map update ${ }_{S}$ : expose $_{\mathrm{S}}^{*} I \rightarrow$ TStates has codomain TStates. Therefore, choosing object assignment of the section $T$ tells us space of possible changes which the system may make (as depending on the state it is in, in the sense that $T$ States lives in a category $\mathcal{A}$ (States) which depends for its definition on States).

5. Since a behavior will involve the chart $\left(\begin{array}{c}T \phi \\ \phi\end{array}\right):\left(\begin{array}{c}T \text { State } \\ \text { State }\end{array}\right) \rightrightarrows\left(\begin{array}{c}T \text { States } \\ \text { States }\end{array}\right)$, choosing the action of $T$ on maps $\phi$ will tell us what it means to interpret changes of state that arise from the dynamics of the system into whole behaviors of the system. We will see an elaboriation of this idea when we discuss behaviors in systems theories other than the deterministic systems theory.

6. By choosing the maps of $\mathcal{A}$, we will determine what sort of map update ${ }_{\mathrm{S}}$ is. This will determine in what sort of way the changes in state vary with parameters.

\subsection*{3.5.1 The deterministic systems theories}

We have been speaking of the deterministic systems theory throughout this book to mean the theory of machines with discrete time whose next state is entirely determined by its current state and choice of parameters. But really, there have been many deterministic systems theories, one for each cartesian category $C$.

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-159.jpg?height=57&width=1450&top_left_y=1294&top_left_x=327)
in $C$ is defined to be the indexed category $\mathbf{C t x}_{-}: C^{\text {op }} \rightarrow$ Cat together with the section $C \mapsto C$ and $\phi \mapsto \phi \circ \pi_{2}$ defined in Proposition 3.5.0.3.

Remark 3.5.1.2. Proposition 3.4.1.5 Shows that behaviors in a deterministic systems theory are precisely what we studied (and saw examples of) in Section 3.3.

There are many different deterministic systems theory, one for each choice of cartesian category $C$. For example:
- If $C=$ Set is the category of sets, we have discontinuous, discrete-time, determinisitic systems. These are often called "Moore machines".
- If $C=$ Top is the category of topological spaces, we have continuous, discretetime, deterministic systems.
- If $C=$ Man is the category of smooth manifolds, then we have smooth, discretetime, deterministic systems.
- If $C=$ Meas is the category of measurable spaces and measurable maps, then we have discrete-time, deterministic systems whose update is measurable.
- And so on...

Let's see how to interpret the determistic systems theory in the case that $C=$ Set answers the questions of Informal Definition 1.1.0.2.

1. A state is an element of a set.

2. The output varies as a function of the state, with constraints on what sort of function.

3. No, the kinds of inputs do not depend on the state - they live in a set which does not depend on the current exposed variable.

4. From a state, one may transition to any other state (since TStates $=$ States).

5. We treat the changes and the states in the same way, interpreting a change as the next state.

6. The change in state is a function of the previous state and the input.

Exercise 3.5.1.3. Answer the questions of Informal Definition 1.1.0.2 in for the following systems theories:

1. DetÑ‚op.

2. DETMan.
3. $\mathbb{D e t}_{\text {Arity. }}$

\subsection*{3.5.2 The differential systems theories}

We can now define the differential systems theories, which will finally let us see the definitions of differential behavior given in Section 3.2 as different incarnations of a single, general definition.

Unlike the case with deterministic systems theories, we will not be giving a single, general definition of "differential" systems theory. We will be defining our different differential systems theory ad-hoc. ${ }^{4}$

We begin with the differential systems theory used to define the notion of differential system in Definition 1.2.2.1.

Definition 3.5.2.1. The Euclidean differential systems theory $\mathbb{E} \cup \mathrm{Uc}$ is defined by the indexed category $\mathbf{C t x}_{-}:$Euc $^{\mathrm{OP}} \rightarrow$ Cat together with the section $T$ given by
- $T \mathbb{R}^{n}:=\mathbb{R}^{n}$, thinking of $\mathbb{R}^{n}$ as tangent space of a point in $\mathbb{R}^{n}$.
- For a differentiable map $f: \mathbb{R}^{n} \rightarrow \mathbb{R}^{m}$, we define $T f: \mathbb{R}^{n} \times \mathbb{R}^{n} \rightarrow \mathbb{R}^{m}$ to be

$$
T f(p, v):=D f_{p} v
$$

where $D f_{p}$ is the matrix of partial derivatives $\left(\left.\frac{\partial f_{i}}{\partial x_{j}}\right|_{x=p}\right)$. In other words, $T f(p, v)$ is the directional derivative in direction $v$ of $f$ at $p$. The functoriality law for the section is precisely the multivariable chain law.

Exercise 3.5.2.2. Check that $T$ as defined is indeed a section by referring to the multidimensional chain law.
\footnotetext{
${ }^{4}$ One can give a general definition of differential systems theory that specializes to these various notions with the notion of tangent category with display maps (see e.g. [CC17]). But we prefer to just describe the various categories as they come.
}

Remark 3.5.2.3. Note that if $f: \mathbb{R} \rightarrow \mathbb{R}^{n}$ is a function, then

$$
T f(t, v)=\frac{d f}{d t}(t) \cdot v
$$

The Euclidean differential systems theory Euc answers the questions of Informal Definition 1.1.0.2 in the following way:

1. A state is a $n$-tuple of real numbers, which is to say a point in $\mathbb{R}^{n}$.

2. The output is a differentiable function of the state.

3. The kind of input does not depend on the output.

4. A possible change in a state is given by a displacement vector, also in $\mathbb{R}^{n}$.

5. For a state to change means that it is tending in this direction. That is, it has a given derivative.

6. The changes in state vary differentiably with the input.

Let's see what behaviors look like in the Euclidean differential systems theory. Note that since the indexed category of $\mathbb{E} U C$ is $\mathbf{C t x}_{-}:$Euc $^{\mathrm{op}} \rightarrow$ Cat, its double category of arenas is the same as for the deterministic systems theory $\mathbb{D E T E u c}_{\text {Et }}$. However, the definition of behavior will be different because the section is different. Let's work out what a general behavior is in $\mathbb{E} U c$ explicitly.

Proposition 3.5.2.4. Let $\mathrm{T}$ and $\mathrm{S}$ be systems in the Euclidean differential systems theory. A chart $\left(\begin{array}{l}f_{\mathrm{b}} \\ f\end{array}\right):\left(\begin{array}{c}\ln _{\mathrm{T}} \\ \text { Out }_{\mathrm{T}}\end{array}\right) \rightrightarrows\left(\begin{array}{c}\ln _{\mathrm{S}} \\ \text { Outs }\end{array}\right)$ consists of a pair of smooth functions $f:$ Out $_{\mathrm{T}} \rightarrow$ Out ${ }_{\mathrm{S}}$ and $f_{\mathrm{b}}:$ Out $\mathrm{T}_{\mathrm{T}} \times \ln _{\mathrm{T}} \rightarrow \ln _{\mathrm{S}}$.

A $\left(\begin{array}{l}f_{b} \\ f\end{array}\right)$-behavior is a smooth function $\phi:$ State ${ }_{\mathrm{T}} \rightarrow$ States such that

$$
\begin{aligned}
\operatorname{expose}_{\mathrm{S}}(\phi(t)) & =f\left(\operatorname{expose}_{\mathrm{T}}(t)\right) \\
\operatorname{update}_{\mathrm{S}}\left(\phi(t), f_{\mathrm{b}}\left(\operatorname{expose}_{\mathrm{T}}(t), j\right)\right) & =D \phi_{t} \operatorname{update}_{\mathrm{T}}(t, j)
\end{aligned}
$$

Proof. This is a matter of interpreting the square

$$
\begin{aligned}
&\left(\begin{array}{c}
T \text { State }_{\mathrm{T}} \\
\text { State }_{\mathrm{T}}
\end{array}\right) \xrightarrow{\left(\begin{array}{c}
T \phi \\
\phi
\end{array}\right)}\left(\begin{array}{c}
T \text { State }_{\mathrm{S}} \\
\text { State }_{\mathrm{S}}
\end{array}\right) \\
&\left(\begin{array}{c}
\text { update }_{\mathrm{T}} \\
\text { expose }_{\mathrm{T}}
\end{array}\right) \downarrow \uparrow \\
&\left(\begin{array}{c}
\ln _{\mathrm{T}} \\
\text { Out }_{\mathrm{T}}
\end{array}\right) \underset{\left(\begin{array}{c}
f_{\mathrm{b}} \\
\text { expose }_{\mathrm{S}}
\end{array}\right)}{ }
\end{aligned}
$$

using by specializing Eq. (3.6) to the above case, using the definition of $T \phi$ in EUc.

Example 3.5.2.5. Consider the following system Time in EUC:
- State $_{\text {Time }}=\mathbb{R}=$ Out $_{\text {Time, }}$, and expose $_{\text {Time }}=\mathrm{id}$.
- In $_{\text {Time }}=\mathbb{R}^{0}$, and

$$
\text { update }_{\text {Time }}(s, *):=1 \text {. }
$$

This system represents the simple differential equation

$$
\frac{d s}{d t}=1
$$

Let $\mathrm{S}$ be another system in $\mathbb{E} U$ c. A chart $\left(\begin{array}{l}p \\ v\end{array}\right):\left(\begin{array}{l}\mathbb{R}^{0} \\ \mathbb{R}\end{array}\right) \rightrightarrows\left(\begin{array}{c}\text { Ins } \\ \text { Outs }\end{array}\right)$ consists of a function $v: \mathbb{R} \rightarrow$ Out and a function $p: \mathbb{R} \times \mathbb{R}^{0} \rightarrow \ln _{\mathrm{S}}$, which is to say $p: \mathbb{R} \rightarrow \ln _{\mathrm{S}}$. This is precisely the sort of chart we need for a trajectory.

A $\left(\begin{array}{l}p \\ v\end{array}\right)$-behavior $\phi:$ Time $\rightarrow \mathrm{S}$ consists of a differentiable function $\phi: \mathbb{R} \rightarrow$ States such that the following is a square in the double category of arenas:

$$
\begin{aligned}
\left(\begin{array}{c}
\mathbb{R} \\
\mathbb{R}
\end{array}\right) & \xrightarrow{\left(\begin{array}{c}
T \phi \\
\phi
\end{array}\right)}\left(\begin{array}{c}
\text { TStates } \\
\text { States }
\end{array}\right) \\
\left(\begin{array}{c}
1 \\
\mathrm{id}
\end{array}\right) \downarrow \uparrow & \downarrow \uparrow\left(\begin{array}{c}
\text { update }_{\mathrm{S}} \\
\text { expose }_{\mathrm{S}}
\end{array}\right) \\
\left(\begin{array}{c}
* \\
\mathbb{R}
\end{array}\right) & \underset{\left(\begin{array}{c}
p \\
v
\end{array}\right)}{\longrightarrow}\left(\begin{array}{c}
\text { Ins } \\
\text { Out }
\end{array}\right)
\end{aligned}
$$

For this to be a square means that the following two equations hold:

$$
\begin{aligned}
\operatorname{expose}_{\mathrm{S}}(\phi(t)) & =v(t) \\
\operatorname{update}_{\mathrm{S}}(\phi(t), p(t)) & =\frac{d \phi}{d t}(t)
\end{aligned}
$$

That is, a behavior of this sort is precisely a trajectory as defined in Definition 3.2.1.6

Example 3.5.2.6. Consider the following simple system Fix:
- State $_{\mathrm{Fix}}=\mathbb{R}^{0}=$ Out $_{\mathrm{Fix}}$ and expose $_{\mathrm{Fix}}=\mathrm{id}$.
- $\ln _{\text {Fix }}=\mathbb{R}^{0}$ and update Fix $(*, *)=*$.

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-162.jpg?height=90&width=1455&top_left_y=2145&top_left_x=324)
some other system $\mathrm{S}$ is not trivial; it is a pair of elements $i \in \operatorname{In}_{\mathrm{S}}$ and $o \in$ Outs.

A $\left(\begin{array}{l}i \\ o\end{array}\right)$-behavior $s:$ Fix $\rightarrow$ S consists of a differentiable function $s: \mathbb{R}^{0} \rightarrow$ States which is to say a state $s \in$ States - such that the following is a square in the double
category of arenas:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-163.jpg?height=483&width=490&top_left_y=295&top_left_x=812)

Now, $s: \mathbb{R}^{0} \rightarrow$ States is a constant function, so $T(s, *)=0$. Therefore, for this to be a square means that the following two equations hold:

$$
\begin{aligned}
\operatorname{expose}_{S}(s) & =0 . \\
\operatorname{update}_{S}(s, i) & =0 .
\end{aligned}
$$

This says that $\mathrm{S}$ is not changing in state $s$ on input $i$, or that $s$ is a steady state of $\mathrm{S}$ for input $i$ as in Definition 3.2.2.7.

The Euclidean differential systems theory $\mathbb{E} U$ is a special case of a cartesian differential systems theory. The category Euc is a cartesian differential category, and for any cartesian differential category we can make a cartesian differential systems theory. We won't define the notion of cartesian differential category here, as the definition is a bit involved. See [BCS09] for a comprehensive introduction.

Definition 3.5.2.7. For any cartesian differential category $C$ with differential operator $T$, we have a systems theory $\operatorname{CARTDIFF}_{\mathcal{C}, T}$ defined by the indexed category $\mathrm{Ctx}_{-}: \mathrm{C}^{\mathrm{op}} \rightarrow$ Cat together with the section given by $T$.

Now, we would like to also show that periodic orbits are behaviors in a differential systems theory, but we're a bit stuck. In the Euclidean systems theory, there's no way to ensure that a trajectory $\phi: \mathbb{R} \rightarrow \mathbb{R}^{n}$ is periodic. Recall that $\phi$ being periodic means that

$$
\phi(t)=\phi(t+k)
$$

for some $k \in \mathbb{R}$ called the period. If $\phi$ is periodic, then it descends to the quotient $\mathbb{R} / k \mathbb{Z}$, which is a circle of radius $\frac{k}{2 \pi}$. If we could define State Orbit $_{k}$ to be $\mathbb{R} / k \mathbb{Z}$, then a trajectory $\hat{\phi}:$ StateOrbit $_{k} \rightarrow$ States would be precisely a periodic trajectory $\phi: \mathbb{R} \rightarrow$ States. To make this expansion of representable behaviors, we will need to move beyond Euclidean spaces.

Our first guess might be to simply change out the category Euc of Euclidean spaces for the category Man of smooth manifolds in the definition of Euc. Certainly, Man is a cartesian category and so $\mathbf{C t x}: \mathbf{M a n}^{\mathrm{op}} \rightarrow$ Cat is a perfectly good indexed category.

But the tangent bundle of a general smooth manifold is not necessarily a product like it is for $\mathbb{R}^{n}$. So we would need to change our indexed category as well!

Now, strictly speaking we don't have to do this if we only want to add circles, because circles have a trivial tangent bundle. But it will turn out that defining the section $T$ will involve choosing, once and for all, a particular trivialization of the tangent bundle of the circle and expressing all derivatives in terms of this. It will end up much easier to simply jump over to general manifolds in a single leap.

We recall that to any manifold $M$ there is an associated tangent bundle $\pi: T M \rightarrow M$. A vector field on a manifold $M$ is a section $v: M \rightarrow T M$ of the tangent bundle. We recall a bit about tangent bundles now.

Proposition 3.5.2.8. The assignment of a manifold $M$ to its tangent space $T M$ is functorial in that it extends to an assignment

$$
f: M \rightarrow N \mapsto T f: T M \rightarrow T N
$$

which, on Euclidean spaces gives $T f(p, v)=D f_{p} v$. Furthermore, the tangent bundle $\pi: T M \rightarrow M$ is natural in the the diagram

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-164.jpg?height=224&width=272&top_left_y=1208&top_left_x=921)

commutes.

There is something special about the tangent bundle which allows it to be re-indexed to a different manifold: it is a submersion. Not all pullbacks of manifolds exist, but all pullbacks of submersions exist and are submersions.

Definition 3.5.2.9. A submsersion $\phi: M \rightarrow N$ is a map of manifolds for which $T_{p} \phi$ : $T_{p} M \rightarrow T_{\phi(p)} N$ is surjective for each $p \in M$.

We note that every diffeomorphism is a submersion, and that the composite of submersions is a submersion.

Lemma 3.5.2.10. Let $\phi: A \rightarrow B$ be a submersion. Then for any $f: C \rightarrow B$, the set theoretic pullback $A \times{ }_{B} C=\{(a, c) \in A \times C \mid \phi(a)=f(c)\}$ may be given the structure of a smooth manifold so that the two projections $A \times_{B} C \rightarrow A$ and $A \times_{B} C \rightarrow C$ are smooth, and so that the resulting square is a pullback square in the category of manifolds. Furthermore, the projection $f^{*} \phi: A \times_{B} C \rightarrow C$ is also a submersion.

In short, we say that pullbacks of submersions exist and are themselves submersions.

This situation arises enough that we can give an abstract definition of it.

Definition 3.5.2.11. Let $C$ be a category. A class of display maps in $C$ is a class of maps $\mathcal{D}$ which satisfies the following:
- Every isomorphism is in $\mathscr{D}$.
- $\mathscr{D}$ is closed under composition. If $f$ and $g$ are composable arrows in $\mathscr{D}$, then $f \circ g$ is in $\mathscr{D}$.
- $\mathscr{D}$ is closed under pullback. If $f: A \rightarrow B$ is in $\mathscr{D}$ and $g: C \rightarrow B$ is any map, then the pullback $g^{*} f: A \times{ }_{B} C \rightarrow C$ exists and is in $\mathscr{D}$.

A category with display maps $(C, \mathscr{D})$ is a category $C$ equipped with a class $\mathscr{D}$ of display maps.

We have seen that (Man, Subm) is a category with display maps by Lemma 3.5.2.10. There are two other common classes of display map categories.
- If $C$ has all pullbacks, then we may take all maps to be display maps.
- If $C$ is cartesian, then we may take the product projections to be the display maps. The first of these obviously works, but the second requires a bit of proof (and to be a bit more carefully defined).

Proposition 3.5.2.12. Let $C$ be a cartesian category. Let $\mathscr{D}$ denote the class of maps $f: A \rightarrow B$ for which there exists a $C \in C$ and an isomorphism $i: A \rightarrow C \times B$ for which $f=i \circ \pi_{2}$. That is, $\mathscr{D}$ is the class of maps which are product projections up to an isomorphism. Then $(C, \mathscr{D})$ is a display map category.

Proof. We verify the conditions
- If $i: A \rightarrow B$ is an isomorphism, then $f \circ \pi_{2}^{-1}: A \rightarrow 1 \times B$ is also an isomorphism. By construction, $f=f: \pi_{2}^{-1} \circ \pi_{2}$, so every isomorphism is a product projection up to isomorphism.
- Suppose that $f: A \rightarrow B$ is isomorphic to a product projection $\pi_{2}: C \times B \rightarrow B$ in that $f=i{ }_{9}^{\circ} \pi_{2}$, and $g: B \rightarrow X$ is isomorphic to a product projection $\pi_{2}: Y \times X \rightarrow X$ in that $g=j \circ \pi_{2}$. We may then see that $f \circ g$ is a product projection up to isomorphism by contemplating the following commutative diagram:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-165.jpg?height=353&width=936&top_left_y=1981&top_left_x=627)
- Let $f: A \rightarrow B$ be a equal to $i \circ \pi_{2}$ with $i: A \rightarrow C \times B$ an isomorphism. Let $k: X \rightarrow B$ be any other map. We will show that $\pi_{2}: C \times X \rightarrow X$ fits in a pullback
diagram as follows:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-166.jpg?height=363&width=662&top_left_y=347&top_left_x=775)

The square commutes since $i^{-1} \stackrel{\circ}{ } f=\pi_{2}: C \times B \rightarrow B$. We see that it satisfies the universal property by making the definition of the dashed arrow given in the diagram. The lower triangle commutes by definition, so consider the upper triangle, seeking to show that $a=\left(a \circ i \circ \pi_{1}, x\right) \risingdotseq(C \times k) \circ i^{-1}$. We calculate:

$$
\begin{aligned}
\left(a \circ i \circ \pi_{1}, x\right) \div(C \times k) \circ i^{-1} & =\left(a \circ i \circ \pi_{1}, x \circ k\right) \circ i^{-1} \\
& =\left(a \circ i \circ \pi_{1}, a \circ f\right) \circ i^{-1} \\
& =\left(a \circ i \circ \pi_{1}, a \circ i \circ \pi_{2}\right) \circ i^{-1} \\
& =a \circ i \circ i^{-1} \\
& =a .
\end{aligned}
$$

Now, if $z: Z \rightarrow C$ were any other map so that $(z, x) \stackrel{ }{ } C \times k: i^{-1}=a$, we would have $\left(z, a \circ i \circ \pi_{2}\right) \circ i^{-1}=a$, or $\left(z, a \circ i \circ p i_{2}\right)=a \circ i$, from which we may deduce that $z=a \circ i \circ \pi_{1}$. This proves uniqueness of the dashed map.

We can now construct the indexed category that will form the basis of our new differential systems theory. We will do so at the general level of display map categories, since the construction relies only on this structure.

Definition 3.5.2.13. Let $(C, \mathscr{D})$ be a category with display maps. The indexed category $\mathscr{D}: C^{\text {op }} \rightarrow$ Cat is defined as follows:
- To each object $C \in C, \mathscr{D}(M)$ is the category with objects the display maps $\phi: E \rightarrow$ $C$ and maps $f: E \rightarrow E^{\prime}$ such that $f ; \phi^{\prime}=\phi$. That is, it is the full subcategory of the slice category over $C$ spanned by the display maps.
- To each map $f: C^{\prime} \rightarrow C$, we associate the functor $f^{*}: \mathscr{D}(N) \rightarrow \mathscr{D}(M)$ given by taking the pullback along $f$.

We note that this is functorial up to coherent isomorphism by the uniqueness (up to unique isomorphism) of the pullback.

Exercise 3.5.2.14. Let $C$ be a cartesian category, and equip it with the class $\mathscr{D}$ of maps which are isomorphic to product projections, as in Proposition 3.5.2.12. Prove that
$\mathscr{D}: C^{\mathrm{op}} \rightarrow$ Cat is equivalent, as an indexed category, to $\mathbf{C t x}_{-}: C^{\mathrm{op}} \rightarrow$ Cat.

Exercise 3.5.2.15. Let $C$ be a category with all pullbacks and let $\mathscr{D}=C$ be the class of all maps. Show Show that $\mathscr{D}: C^{\text {op }} \rightarrow$ Cat is the self-indexing of $C$. It sends an object $C \in C$ to the slice category $C \downarrow C$ whose objects are maps $x: X \rightarrow C$ and whose maps $f: x \rightarrow y$ are maps $f: X \rightarrow Y$ with $f: y=x$.

We can specialize this to the category of smooth manifolds with submersions the display maps

Definition 3.5.2.16. The indexed category Subm : $\mathbf{M a n}^{\mathrm{op}} \rightarrow$ Cat is defined as follows:
- To each manifold $M, \operatorname{Subm}(M)$ is the category of submersions $\phi: E \rightarrow M$ and maps $f: E \rightarrow E^{\prime}$ such that $f ; \phi^{\prime}=\phi$.
- To each $\operatorname{map} f: M \rightarrow N$, we associate the functor $f^{*}: \operatorname{Subm}(N) \rightarrow \operatorname{Subm}(M)$ given by taking the pullback along $f$.

If $(C, \mathscr{D})$ is a category with display maps, then the category of charts of $\mathscr{D}: C$ op $\rightarrow$ Cat is easy to understand in terms of $\mathscr{D}$.

Proposition 3.5.2.17. Let $(C, \mathscr{D})$ be a category with display maps. Then the category Chart $_{\mathscr{D}}=\int^{C \in C} \mathscr{D}(C)$ of charts for $\mathscr{D}: C^{\mathrm{op}} \rightarrow$ Cat is equivalent to the category whose objects are display maps and whose morphisms are commutative squares between them.

Proof. An object $\left(\begin{array}{c}a^{-} \\ A^{+}\end{array}\right)$of the category of charts is a pair consisting of an object $A^{+} \in C$ and a display map $a^{-}: A^{-} \rightarrow A^{+}$in $\mathscr{D}\left(A^{+}\right)$. But $A^{+}$is determined, as the codomain, by $a^{-}$; so the objects of the category of charts are in bijection with the display maps. We then show that the charts are similarly in bijection with the squares between display maps.

A chart $\left(\begin{array}{c}f_{\mathrm{b}} \\ f\end{array}\right):\left(\begin{array}{c}a^{-} \\ A^{+}\end{array}\right) \rightrightarrows\left(\begin{array}{c}b^{-} \\ B^{+}\end{array}\right)$for this indexed category is a pair consisting of a map $f: A^{+} \rightarrow B^{+}$in $C$ and a triangle

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-167.jpg?height=217&width=458&top_left_y=1946&top_left_x=823)

By the universal property of the pullback, this data is equivalently given by the data of a square

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-167.jpg?height=233&width=285&top_left_y=2285&top_left_x=909)

Now, consider a composite square

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-168.jpg?height=230&width=455&top_left_y=335&top_left_x=824)

We can see that the arrow $f_{b} \stackrel{\circ}{ } f^{*} g_{b}: A^{-} \rightarrow f^{*} g^{*} B^{-}$composes with the projections from the pullbacks to give the top half of the outer square, and therefore it is the unique map into the pullback induced by the outer square.

Corollary 3.5.2.18. Let $(C, D)$ be a category with display maps. To give a section of $\mathscr{D}: C^{\text {op }} \rightarrow$ Cat, it suffices to give an endofunctor $T: C \rightarrow C$ together with a natural transformation $\pi: T \rightarrow \mathrm{id}_{e}$ whose components are all display maps.

Proof. Such an endofunctor $T$ with natural transformation $\pi$ gives us a functor $C \mapsto$ $\pi: T C \rightarrow C$ going from $C$ to the category of display maps in $C$ and squares between them. This functor will assign to each $f: C^{\prime} \rightarrow C$ the naturality square

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-168.jpg?height=225&width=266&top_left_y=1243&top_left_x=924)

We note that the evident projection of the codomain composes with this functor to give $\mathrm{id}_{\mathcal{C}}$. By Proposition 3.5.2.17, this is equivalent to giving such a functor into Chart ${ }_{D}$, which, by Proposition 3.5.0.2 is equivalent to giving a section of $\mathscr{D}$.

We may therefore define a systems theory associated to any category with display maps $(C, D)$ with such an endofunctor $T: C \rightarrow C$ and natural transformation $\pi: T \rightarrow$ $\mathrm{id}_{C}$ whose components are all display maps.

Definition 3.5.2.19. Let $(C, D)$ be a category with display maps and let $T: C \rightarrow C$ be an endofunctor and $\pi: T \rightarrow \mathrm{id}_{C}$ a natural transformation whose components are all display maps. Then this data forms a systems theory $\mathbb{D}_{\mathrm{ISP}_{\mathfrak{D}, T}}$ given by $\mathscr{D}:$ eop $\rightarrow$ Cat and the section induced by sending $C$ to $\pi: T C \rightarrow C$ in $\mathscr{D}(C)$.

Example 3.5.2.20. Let $C$ be a cartesian category and $\mathscr{D}$ be the class of product projections up to isomorphism. We can define $T: C \rightarrow C$ by $T C:=C \times C$ and define $\pi: T \rightarrow \mathrm{id}$ by $\pi_{C}:=\pi_{1}: C \times C \rightarrow C$. The systems theory $\mathbb{D}_{\mathbb{I S P}_{\mathscr{D}, T} \text { so defined is precisely the }}$ deterministic systems theory of Definition 3.5.1.1.

Example 3.5.2.21. Let $C$ be a category with pullbacks and $\mathscr{D}=C$ be all maps. If we define $T C:=C \times C$ and $\pi=\pi_{1}: C \times C \rightarrow C$ in the same way as in Example 3.5.2.20, then the

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-169.jpg?height=60&width=1453&top_left_y=379&top_left_x=325)
main difference between this systems theory and the ordinary determinstic systems theory is that what sort of input a system may take in can depend on the current exposed variable. In particular, an interface for a dependent deterministic system $S$ will consist of a map $v: \ln _{S} \rightarrow$ Out $\mathrm{S}$ which we can think of sending each input to the output it is valid for. The update is then of the form

$$
\text { State }_{S} \times_{\text {Outs }_{S}} \ln _{S} \xrightarrow{\text { update }_{S}} \text { State }_{S}
$$

In other words, update ${ }_{S}(s, i)$ is defined when $v(i)=\operatorname{expose}_{S}(s)$, or when $i$ is an input valid for the exposed variable $\operatorname{expose}_{S}(s)$ of state $s$. We can think of each $v^{-1}(o)$ for $o \in O$ as a menu of available inputs given output $o$. We will talk a bit more about dependent systems in Section 3.5.3.

With one last lemma, we will finally be able to define our general differential systems theory.

Lemma 3.5.2.22. The tangent bundle $\pi: T M \rightarrow M$ of a manifold $M$ is a submersion.

Definition 3.5.2.23. The general differential systems theory $\mathbb{D}$ IFF is defined to be the display category systems theory $\mathbb{D}_{\text {ISPSubm }, T}$ associated to the Subm : Man ${ }^{\text {op }} \rightarrow$ Cat and the tangent bundle functor $T$.

A dynamical system in the general differential systems theory DIFF consists of a state space States, and output space Outs, but then a submersion of inputs $\pi \operatorname{In}_{S}: \operatorname{In}_{S} \rightarrow$ Outs. We can think of $\pi$ as assigning to each input the output that it is valid for. The update then has signature

$$
\text { update }_{S}: \operatorname{expose}_{\mathrm{S}}^{*} \pi_{\ln \mathrm{S}} \rightarrow \pi_{T \text { States }}
$$

which is to say that it is a triangle of the form

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-169.jpg?height=219&width=765&top_left_y=1994&top_left_x=669)

which assigns to each state-input pair $(s, i)$ where $i$ is valid given the state $s$ in the sense that $\operatorname{expose}_{S}(s)=\pi_{\ln s}(i)$ to a tangent vector at $s$.

The general differential systems theory $\mathbb{D}_{\text {IFF answers the questions of Informal }}$ Definition 1.1.0.2 in the following way:

1. A state is a point in a smooth manifold.

2. The output is a smooth function of the state.

3. The kind of input can dependend on the current output, but it does so smoothly (in the sense that the assignment sending an input to the output it is valid for is a submersion).

4. A possible change in a state is given by a tangent vector.

5. For a state to change means that it is tending in this direction. That is, it has derivative equal to the given tangent vector.

6. The changes in state vary smoothly with the input.

Example 3.5.2.24. Let's see an example of a situation where the inputs may differ over different outputs. Suppose we have a robot on a distant planet, and we are directing it. When we tell it to move in a direction, the robot will move in the given direction at a given speed $k$. We want to keep track of the position of the robot as it moves around the planet.

We can model this situation as follows: since the surface of the planet is a sphere and we want to keep track of where the robot is, we will let States $=S^{2}$ be a sphere. We will also have the robot reveal its position to us, so that Out $=S^{2}$ and $\operatorname{expose}_{S}=\mathrm{id}$.

Now, in any given position $p \in$ Outs, we want the space of inputs $\ln _{S_{p}}$ valid for $p$ to be the directions we can give to the robot: that is to say, $\ln _{\mathrm{S}_{\mathrm{p}}} \cong S^{1}$ should form a circle. However, we want these directions to be directions that the robot could actually travel, so we will let $\ln _{\mathrm{S}_{\mathrm{p}}}=\left\{v \in T_{p}\right.$ Out ||$\left.v \mid=1\right\}$ be the unit circle in the tangent space at $p$. Then we may describe the fact that the robot moves in the direction we tell it by defining

$$
\operatorname{update}_{\mathrm{S}}(s, i)=k i \text {. }
$$

We note that any system $S$ in the Euclidean differential systems theory can be considered as a system in the general differential systems theory by defining the bundle of inputs to the $\pi_{1}:$ Out $\times \ln _{S} \rightarrow$ Outs and noting that the pullback of a product projection is a product projection, so that we may take the domain of the new update $u$ : $\operatorname{expose}_{\mathrm{S}}^{*} \pi_{1} \rightarrow \pi_{\text {TStates }}$ to be States $\times \ln _{\mathrm{S}}$, just as it was. We may then define $u(s, i)=\left(s\right.$, update $\left._{S}(s, i)\right)$, equating $T$ States $=T \mathbb{R}^{n}$ with $\mathbb{R}^{n} \times \mathbb{R}^{n}$. Later, when we discuss change of systems theory, we will see that this follows from a morphism of systems theories $\mathbb{E} \mathrm{Uc} \rightarrow \mathbb{D}$ IFF.

We can now describe periodic orbits as behaviors in the general differential systems theory.

Example 3.5.2.25. Let Clock $_{\mathrm{k}}$ be the system in DIFF with:
- State space State Clock $_{k}=\mathbb{R} / k \mathbb{Z}$,
- Output space Out Clock $_{k}=\mathbb{R} / k \mathbb{Z}$ with expose Clock $=\mathrm{id}$.
- Input bundle the identity $\ln _{\text {Clock }_{k}}=$ id $_{\text {Out }_{\text {Clock }_{k}}}$.
- Update update Clock : State Clock $_{k} \rightarrow$ TState $_{\text {Clock }_{k}}$ the assigning each state $s$ to
the vector $T q(1)$, the pushforward of the constant vector 1 on $\mathbb{R}$ by the quotient $q: \mathbb{R} \rightarrow \mathbb{R} / k \mathbb{Z}$.

The universal property of $\mathbb{R} / k \mathbb{Z}$ says that a smooth function $\gamma: \mathbb{R} \rightarrow M$ factors through $q: \mathbb{R} \rightarrow \mathbb{R} / k \mathbb{Z}$ if and only if $\gamma(t+k)=\gamma(t)$ for all $t \in \mathbb{R}$.

A chart for Clock $k$ into a system $S$ is a square

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-171.jpg?height=215&width=331&top_left_y=543&top_left_x=886)

By the various universal properties involved, this is the same data as a pair of maps $\hat{p}: \mathbb{R} \rightarrow \operatorname{In} \mathrm{S}$ and $\hat{v}: \mathbb{R} \rightarrow$ Outs for which $\hat{p}(t+k)=\hat{p}(t)$ and $\hat{v}(t+k)=\hat{v}(t)$ for all $t \in \mathbb{R}$, and for which $\pi \circ \hat{i}=\hat{v}$.

Now, a behavior $\phi:$ Clock $_{\mathrm{k}} \rightarrow \mathrm{S}$ is a square

$$
\begin{aligned}
& \left(\begin{array}{c}
T(\mathbb{R} / k \mathbb{Z}) \\
\mathbb{R} / k \mathbb{Z}
\end{array}\right) \xrightarrow{\left(\begin{array}{c}
T \phi \\
\phi
\end{array}\right)}\left(\begin{array}{c}
T \text { States } \\
\text { States }
\end{array}\right) \\
& \left(\begin{array}{c}
1 \\
\text { id }
\end{array}\right) \left\lvert\, \uparrow \quad \downarrow\left(\uparrow\left(\begin{array}{c}
\text { update }_{S} \\
\text { expose }_{S}
\end{array}\right)\right.\right. \\
& \left(\begin{array}{l}
* \\
\mathbb{R}
\end{array}\right) \Longrightarrow\left(\begin{array}{l}
p \\
v
\end{array}\right)=\left(\begin{array}{c}
\operatorname{Ins}_{\mathrm{S}} \\
\text { Outs }
\end{array}\right)
\end{aligned}
$$

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-171.jpg?height=63&width=1453&top_left_y=1584&top_left_x=328)
that $\operatorname{expose}_{\mathrm{S}}(\hat{\phi}(t))=v(t)$. Second, we have that

$$
\phi^{*} \text { update }_{\mathrm{S}} \circ p=T \phi \circ \text { update }_{\text {Clock }_{\mathrm{k}}}
$$

which is to say

$$
\operatorname{update}_{S}(\phi(t), p(t))=T \phi(T q(1)) \text {. }
$$

Re-expressing this in terms of $\hat{\phi}=\phi \circ q$, we see that this means that

$$
\operatorname{update}_{S}(\hat{\phi}(t), \hat{p}(t))=\frac{d \hat{\phi}}{d t}
$$

This says that $\hat{\phi}$ is a trajectory for the system. Since by definition $\hat{\phi}$ is periodic, and any such periodic map would factor through $q$, we may conclude that behaviors Clock $_{k} \rightarrow S$ are periodic orbits (with period dividing $k$ ) of $S$.

\subsection*{3.5.3 Dependent deterministic systems theory}

The systems theory $\mathbb{D}^{\operatorname{ISP}}(C, T)$ of display maps can also help us describe deterministic systems theories in which the sorts of input a system can accept depend on the output that system is currently exposing. These are called dependent deterministic systems.

Definition 3.5.3.1. Let $(C, \mathscr{D})$ be a category with display maps (Definition 3.5.2.11) and finite products, and suppose that product projections $\pi_{1}: C \times D \rightarrow C$ are display maps (although there may be other display maps). Then we have a section $T C=C \times C \xrightarrow{\pi_{1}} C$, and so we may define the depdendent deterministic systems theory $\mathbb{D P D E T}_{C}$ to be the display map systems theory $\mathbb{D}_{\operatorname{ISP}}^{(}(C, D), T$.

Let's understand dependent deterministic systems in the category of sets with every map taken as a display map.

Definition 3.5.3.2. A dependent deterministic system $S$ in the category of sets consists of:
- A set States of states,
- A set Outs of outputs, and for each output $o \in$ Out $_{S}$, a set $\ln _{S}(o)$ of inputs valid in output $o$. If we define $\ln _{\mathrm{S}}:=\sum_{o \in \text { Out }} \ln _{\mathrm{S}}(o)$ to be the disjoint union of all of these ouput sets, then we can package this assignment $o \mapsto \ln _{S}(o)$ into a function In $_{S} \rightarrow$ Outs which sends an input to the output it is valid in. That is, the interface of a dependent system is a dependent set.
- An exposed variable expose extates $_{S} \rightarrow$ Outs. .

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-172.jpg?height=59&width=1385&top_left_y=1453&top_left_x=392)
States sending an input which is valid in the output exposed by $s$ to the next state.

Example 3.5.3.3. Consider the following simple example of a dependent system. A Diner accepts orders when it is open, but when it is closed it doesn't. The set of possible outputs, or orientations of the diner in it's environment, is Out $S=\{$ open, closed $\}$. When the diner is closed it accepts no input, so $\ln _{S}($ closed $)=\{$ tick $\}$ will simply represent the ticking of time; other the other hand, when the diner is open then $\ln _{S}$ (open) will be the set of possible orders.

Even in the case $C=$ Set, the dependent deterministic systems theory is remarkably rich. So rich, in fact, that David Spivak and Nelson Niu have written a whole book just on this systems theory [NS]. So we'll leave the details of this systems theory to them.

\subsection*{3.5.4 Non-deterministic systems theories}

In this section, we will define the non-deterministic systems theories. We've already done most of the work for this back in Chapter 2. In particular, in Theorem 2.6.4.5, we
showed that to every commutative monad $M: C \rightarrow C$ on a cartesian category, there is a monoidal strict indexed category

$$
\mathbf{C t x}^{M}: e^{\mathrm{op}} \rightarrow \text { Cat }
$$

sending each object $C \in C$ to the category $\mathbf{C t x}_{C}^{M}$ of Klesli maps $C \times X \rightarrow M Y$ in the context of $C$. We will take this to be the indexed category underlying the systems theory of non-deterministic systems associated to $M$; it remains to construct a section $T$.

Proposition 3.5.4.1. The assigment defined by
- $C \in C$ is assigned to $C \in \mathbf{C t x}_{C}^{M}$, and
- $f: C^{\prime} \rightarrow C$ is assigned to

$$
C^{\prime} \times C^{\prime} \xrightarrow{\pi_{2}^{\circ} f_{9 \eta}^{\circ}} M C
$$

yields a section $T: C \rightarrow \mathbf{C t x}_{-}^{M}$.

Proof. We see immediately that $\operatorname{Tid}_{C}=\operatorname{id}_{T C}$. It remains to show that for $f: C^{\prime} \rightarrow C$ and $g: C^{\prime \prime} \rightarrow C^{\prime}$, we have

$$
T g \circ g^{*} T f=T(g \circ f) .
$$

In the do notation, the composite on the left is given by

$$
\left(c_{1}^{\prime \prime}, c_{2}^{\prime \prime}\right) \mapsto \begin{array}{cc}
\mathbf{d o} & \\
& c^{\prime} \leftarrow \eta\left(g\left(c_{2}^{\prime \prime}\right)\right) \\
& \eta\left(f\left(c^{\prime}\right)\right)
\end{array}=\eta\left(f\left(g\left(c_{2}^{\prime \prime}\right)\right)\right)
$$

which is the right hand side.

Definition 3.5.4.2. Let $M: C \rightarrow C$ be a commutative monad on a cartesian category $C$. The M-flavored non-deterministic systems theory $\mathbb{N O N D e t}_{M}$ is defined to be the indexed category $\mathbf{C t x}_{-}^{M}: \mathrm{C}^{\mathrm{op}} \rightarrow$ Cat together with section defined in Proposition 3.5.4.1.

The non-deterministic systems theory $\mathbb{N O N D E T}$ Definition 1.1.0.2 in the following way (taking $C=$ Set for concreteness):

1. A state is an element of a set.

2. The output is a deterministic function of the state.

3. The kind of input does not depend on the output.

4. A possible change in a state is given by an $M$-distribution over states (element of MStates).

5. A state changes by transitioning into another state.

6. The changes in state vary arbitrarily with input.

Exercise 3.5.4.3. Answer these questions more specifically for the following systems theories:

1. The non-deterministic Moore machine theory $\mathbb{N}$ ONDeTp.

2. The probabilistic systems theory $\mathbb{N O N D E T}_{\mathrm{D}}$.

3. The worst-case cost systems theory $\mathbb{N}$ ONDET Cost .

Behaviors in non-deterministic systems theory tend to be a little strict. This is because the notion of trajectory is a bit more subtle in the non-deterministic case. When does a sequence $s: \mathbb{N} \rightarrow$ States constitute a trajectory of the system $S$ ? Is it when $s_{t}$ will transition to $s_{t+1}$ (in that update $\left.{ }_{\mathrm{S}}\left(s_{t}, i_{t}\right)=\eta\left(s_{t+1}\right)\right)$ ? Or perhaps when it can transition that way - but how do we express this notion in general?

While the notion of behavior we have given in this chapter works well for deterministic and differential systems theories, it does not work as well for non-deterministic systems theories. Instead of asking whether or not a sequence of states is a trajectory, we might instead want to ask how possible or likely it is for such a sequence of states to occur through an evolution of the system. Figuring out how to express this idea nicely and generally remains future work.

On the other hand, simulations of non-deterministic systems theories remain interesting, because they tell us when we might be able to use a simpler model for our system without changing the exposed behavior.

Jaz: I should include an example of non-deterministic simulation here.

\subsection*{3.6 Restriction of systems theories}

Now that we have a concise, formal definition of dynamical system systems theories, we can begin to treat systems theories as mathematical objects. In this section, we will look at a simple way to construct a new systems theories from an old one: restriction along a functor.

We will use restrictions of systems theories in order to more precisely control some of the upcoming functoriality results. Often, we will only be able to prove a theorem by restricting the systems theories beforehand.

Since a systems theories $\mathbb{T}$ consists of an indexed category $\mathcal{A}: \mathcal{C}^{\mathrm{op}} \rightarrow$ Cat together with a section $T$, if we have a functor $F: \mathscr{D} \rightarrow C$ then we should be able to produce a new systems theories by composing $\mathcal{A}$ and $T$ with $F$. We call this new systems theories $\left.\mathbb{T}\right|_{F}$ the restriction of $\mathbb{T}$ along $F$.

Definition 3.6.0.1. Let $\mathbb{T}=\left(\mathcal{A}: C^{\mathrm{op}} \rightarrow\right.$ Cat, $\left.T\right)$ be a systems theories of dynamical systems. For any functor $F: \mathscr{D} \rightarrow \mathcal{C}$, we have a new systems theories

$$
\left.\mathbb{T}\right|_{F}:=\left(\mathcal{A} \circ F^{\mathrm{op}}, T \circ F\right)
$$
where $T \circ F$ is the section given by
- $(T \circ F)(D):=T(F D)$, and
- $(T \circ F)(f):=T(F f)$, which may see has the correct codomain since $\left(\mathcal{A} \circ F^{\circ}\right)(f)(T \circ$ $F)(D)=\mathcal{A}(F f) T(F D)$.

Since an indexed category is no more than a functor into Cat, $\mathcal{A} \circ F^{\mathrm{op}}$ is an indexed category. It only remains to check that $T \circ F$ as defined is indeed a section of the Grothendieck construction of $\mathcal{A} \circ F^{\mathrm{op}}$; this calculation is a straightforward unfolding of definitions.

Example 3.6.0.2. In the next chapter we will see a few approximation methods as ways of changing systems thories.

However, these approximations do not preserve all features of the systems theories; in general, they are only exact for a restricted class of functions. For example, the Euler method which approximates a differential equation

$$
\frac{d s}{d t}=F(s, p)
$$

on a Euclidean sace by the discrete-time update function

$$
u(s, p)=s+\varepsilon F(s, p)
$$

(for $\varepsilon>0$ ) only exactly reproduces affine behaviors of systems. Being affine is a rather severe restriction on the behavior of a dynamical system, but it does allow the important case of steady states.

In order to capture Euler approximation as a change of systems theories in the exact manner to be explored in the next chapter, we therefore need to restrict the Euclidean differential systems theories $\mathbb{E} U c$ to affine functions. Recall that the indexing base of $\mathbb{E} U C$ is the category Euc of Euclidean spaces and differentiable functions. We may therefore take our restriction functor to be the inclusion Aff $\hookrightarrow$ Euc of affine functions between Euclidean spaces.

Now that we have the formalities out of the way, let's understand what restricting a systems theory means for the theory of systems in it. Because we have changed the indexing base for the systems theory, we have changed the objects of states and exposed variables, and the bottom part of both the lenses and charts.

In particular, the object of states of a $\left.\mathbb{T}\right|_{F}$-system is now an object of $\mathscr{D}$ and not of $C$. The exposed variable expose $_{S}$ : States $\rightarrow$ Out $_{S}$ is now a map in $\mathscr{D}$. Furthermore, and rather drastically, the underlying map $\phi:$ State $_{\mathrm{T}} \rightarrow$ State $_{S}$ of a behavior is also a map in $\mathscr{D}$.

Example 3.6.0.3. Continuing from Example 3.6.0.2, we may consider what a behavior
represented by the system $T=\left(\begin{array}{c}t \mapsto 1 \\ \text { id }\end{array}\right):\left(\begin{array}{c}\mathbb{R} \\ \mathbb{R}\end{array}\right) \rightrightarrows\left(\begin{array}{c}1 \\ \mathbb{R}\end{array}\right)$ is in the restricted systems theory Euc| $\left.\right|_{\text {Aff. }}$.

Since $T$ represents trajectories in $\mathbb{E}_{U}$, it will represent trajectories in $\left.\mathbb{E}_{\mathrm{UC}}\right|_{\text {Aff. }}$. However, we have restricted the underlying map $s: \mathbb{R} \rightarrow$ States to lie in Aff - that is, to be affine. There are not often affine solutions to general differential equations, so for the most part we will simply find that a system $S$ has no trajectories (in this restricted systems theory), or very few.

However, any constant function is affine; for this reason, all steady states are affine functions, and so remain behaviors in this restricted systems theory.

\subsection*{3.7 Summary and Futher Reading}

In this chapter, we looked at a variety of behaviors of systems in different systems theory and saw that they could all be represented by the same equations relating charts with lenses. We saw how behaviors can be represented by dynamical systems of particular shapes - trajectories are represented by timelines, steady states by a single fixed point, periodic orbits by clocks, etc. We introduced the double category of arenas to organize charts and lenses, and finally gave a formal definition of theory of dynamical systems.

The notion of systems theory, the double category of arenas in a given systems theory, and the definition of behavior of system that these enable are novel contributions of this book. For a summary account, see [Jaz21].

For more on the theory on the systems theory of dependent lenses, see Spivak and Niu's book on polynomial functors (which remarkably form the same category) [NS].

\section*{Chapter 4}

\section*{Change of Systems Theory}

\subsection*{4.1 Introduction}

In the last chapter, we saw a general formulation of the notion of behavior of system and precise definition of the notion of systems theory. Let's recall the definition of a theory of dynamical systems.

Definition 4.1.0.1. A theory of dynamical systems consists of an indexed category $\mathcal{A}$ : eop $\rightarrow$ Cat together with a section $T$.

This concise definition packs a big punch. Describing a theory of dynamical systems amounts to answering the informal questions about what it means to be a system:

Informal Definition 4.1.0.2. A theory of dynamical systems is a particular way to answer the following questions about what it means to be a dynamical system:

1. What does it mean to be a state?

2. How should the output vary with the state - discretely, continuously, linearly?

3. Can the kinds of input a system takes in depend on what it's putting out, and how do they depend on it?

4. What sorts of changes are possible in a given state?

5. What does it mean for states to change.

6. How should the way the state changes vary with the input?

Constructing a systems theory is no small thing. But once we have a systems theory, we have may work in its double category of arenas to quickly derive a few compositionality results about systems.

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-177.jpg?height=59&width=1442&top_left_y=2342&top_left_x=339)
$\mathcal{A}$-arenas is defined to be the Grothendieck double construction of $\mathcal{A}$ :

$$
\text { Arena }_{\mathcal{A}}:=\oiint^{C \in C} \mathcal{A}(C)
$$

Note that the horizontal category of Arena $_{\mathcal{A}}$ is the category Chart $_{\mathcal{A}}$ of $\mathcal{A}$-charts (generalizing Proposition 3.3.0.15), and the vertical category of Arena $_{\mathcal{A}}$ is the category Lens $_{\mathcal{A}}$ of $\mathcal{A}$-lenses (Definition 2.6.2.7).

We are now in peak category theory territory: the statements of our propositions are far longer than their proofs, which amount to trivial calculations in the double category of arenas. As in much of categorical work, the difficulty is in understanding what to propose; once that work is done, the proof flows smoothly from the definitions.

Let's see what composition of squares in the double category of arenas means for systems. Horizontal composition is familiar because it's what lets us compose behaviors:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-178.jpg?height=204&width=1491&top_left_y=1546&top_left_x=358)

$$
\begin{aligned}
& \left(\begin{array}{l}
\text { update }_{\mathrm{T}} \\
\text { expose }_{\mathrm{T}}
\end{array}\right) \downarrow \uparrow \quad \downarrow \uparrow\left(\begin{array}{l}
\text { update }_{\mathrm{S}} \\
\text { expose }_{\mathrm{S}}
\end{array}\right) \quad \downarrow \left\lvert\,\left(\begin{array}{l}
\text { update }_{\mathrm{U}} \\
\text { expose }_{\mathrm{U}}
\end{array}\right)=\left(\begin{array}{l}
\text { update }_{\mathrm{T}} \\
\text { expose }_{\mathrm{T}}
\end{array}\right) \downarrow \uparrow \quad \downarrow \uparrow\left(\begin{array}{l}
\text { update }_{\mathrm{U}} \\
\text { expose }_{\mathrm{U}}
\end{array}\right)\right.
\end{aligned}
$$

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-178.jpg?height=203&width=1415&top_left_y=1839&top_left_x=428)

So, we have a category of systems and behaviors in any systems theory, just as we defined in the deterministic systems theory.

On the other hand, vertical composition tells us something else interesting: if you get a chart $\left(\begin{array}{c}g_{b} \\ g\end{array}\right)$ by wiring together a chart $\left(\begin{array}{c}f_{b} \\ f\end{array}\right)$, then a behavior $\phi$ with chart $\left(\begin{array}{c}f_{b} \\ f\end{array}\right)$
induces a behavior with chart $\left(\begin{array}{c}g_{b} \\ g\end{array}\right)$ on the wired together systems.

$$
\begin{aligned}
& \left(\begin{array}{c}
T \text { State }_{\mathrm{T}} \\
\text { State }_{\mathrm{T}}
\end{array}\right) \xrightarrow{\left(\begin{array}{c}
T \phi \\
\phi
\end{array}\right)}\left(\begin{array}{c}
T \text { State }_{\mathrm{S}} \\
\text { State }_{\mathrm{S}}
\end{array}\right) \\
& \left(\begin{array}{c}
\text { update }_{\mathrm{T}} \\
\text { expose }_{\mathrm{T}}
\end{array}\right) \downarrow \uparrow \quad \downarrow \uparrow\left(\begin{array}{c}
\text { update }_{S} \\
\text { expose }_{S}
\end{array}\right) \\
& \left(\begin{array}{c}
\ln _{\mathrm{T}} \\
\text { Out }_{\mathrm{T}}
\end{array}\right) \longrightarrow \underset{\left(f_{\mathrm{b}}\right)}{\longrightarrow}\left(\begin{array}{c}
\ln _{\mathrm{S}} \\
\text { Out }
\end{array}\right) \\
& \left(\begin{array}{c}
j \\
j
\end{array}\right) \downarrow \uparrow \quad\left(\begin{array}{c}
f_{b} \\
f
\end{array}\right) \quad \downarrow \uparrow\left(\begin{array}{c}
k^{\sharp} \\
k
\end{array}\right) \\
& \left(\begin{array}{l}
I \\
O
\end{array}\right) \underset{\left(\begin{array}{c}
g_{b} \\
g
\end{array}\right)}{\rightrightarrows}\left(\begin{array}{c}
I^{\prime} \\
O^{\prime}
\end{array}\right) \\
& \begin{aligned}
&\left(\begin{array}{c}
T \text { State }_{\mathrm{T}} \\
\text { State }_{\mathrm{T}}
\end{array}\right) \xrightarrow{\left(\begin{array}{c}
T \phi \\
\psi \phi
\end{array}\right)}\left(\begin{array}{c}
T \text { States } \\
\text { States }^{S}
\end{array}\right) \\
&=\left(\begin{array}{c}
f^{\sharp} \\
f
\end{array}\right) \cdot\left(\begin{array}{c}
\text { update }_{\mathrm{T}} \\
\text { expose }_{\mathrm{T}}
\end{array}\right) \uparrow \\
&\left(\begin{array}{c}
I \\
k^{\sharp} \\
k
\end{array}\right) \cdot\left(\begin{array}{c}
\text { update }_{\mathrm{S}} \\
\text { expose }_{\mathrm{S}}
\end{array}\right) \longrightarrow\left(\begin{array}{c}
I \\
O^{\prime}
\end{array}\right)
\end{aligned}
\end{aligned}
$$

The interchange law of the double category of arenas tells us precisely that these two sorts of composition of behaviors - composition as maps and wiring - commute. That is, we can compose two behaviors and then wire them together, or we can wire each together and then compose them; the end result is the same.

Example 4.1.0.4. Continuing from Example 3.4.1.4, suppose that we have a $\left(\begin{array}{l}b^{-} \\ b^{+}\end{array}\right)$-steady state $s$ in a system $\mathrm{S}$ :

$$
\begin{aligned}
& \left(\begin{array}{l}
1 \\
1
\end{array}\right) \xrightarrow{\left(\begin{array}{l}
s \\
s
\end{array}\right)}\left(\begin{array}{l}
\text { States } \\
\text { States }
\end{array}\right) \\
& \|\| \quad \downarrow \uparrow\left(\begin{array}{l}
\text { update }_{\mathrm{s}} \\
\text { expose }_{\mathrm{S}}
\end{array}\right) \\
& \left(\begin{array}{l}
1 \\
1
\end{array}\right) \underset{\left(\begin{array}{l}
b^{+} \\
b^{-}
\end{array}\right)}{\longrightarrow}\left(\begin{array}{l}
B^{-} \\
B^{+}
\end{array}\right)
\end{aligned}
$$

We can see that $s$ is a $\left(\begin{array}{l}d^{-} \\ d^{+}\end{array}\right)$-steady state of the wired system by vertically composing the square in Eq. (4.1) with the square in Eq. (3.8). This basic fact underlies our arguments in the upcoming Section 5.2.

While our results are most smoothly proven in the double category of arenas, this double category does not capture the way we think of systems and their behaviors. To think of a behavior, we must first think of its chart; we solve a differential equation in terms of its parameters, and to get a specific solution we must first choose specific
parameters. Working in the double category of arenas means treating the chart $\left(\begin{array}{l}f_{b} \\ f\end{array}\right)$ and the underlying map $\phi$ of a behavior on equal footing, but we would instead like to say that $\phi$ is a behavior for the chart $\left(\begin{array}{l}f_{b} \\ f\end{array}\right)$.

We would also like to think of the wiring together of systems along a lens $\left(\begin{array}{c}w^{\sharp} \\ w\end{array}\right)$ as an operation performed on systems, and then inquire into the relationship of this wiring operation with the (horizontal) composition of behaviors.

What we need is to separate the interface of a system from the system itself. Charts and lenses are best understood as ways of relating interfaces. It just so happens that systems and their behaviors can also be expressed as certain sorts of lenses and charts, which drastically facilitates our working with them. But there is some sense in which this is not essential; the main point is that for each interface $\left(\begin{array}{l}I \\ O\end{array}\right)$ we have a notion of system with interface $\left(\begin{array}{l}I \\ O\end{array}\right)$, for each lens $\left(\begin{array}{c}w^{\sharp} \\ w\end{array}\right):\left(\begin{array}{l}I \\ O\end{array}\right) \leftrightarrows\left(\begin{array}{l}I^{\prime} \\ O^{\prime}\end{array}\right)$ a way of wiring $\left(\begin{array}{l}I \\ O\end{array}\right)$ systems into $\left(\begin{array}{l}I^{\prime} \\ O^{\prime}\end{array}\right)$ systems, and for each chart $\left(\begin{array}{l}f_{b} \\ f\end{array}\right):\left(\begin{array}{l}I \\ O\end{array}\right) \rightrightarrows\left(\begin{array}{l}I^{\prime} \\ O^{\prime}\end{array}\right)$ a notion of behavior for this chart. It is very convenient that we can describe wiring and composition of behaviors in the same terms as charts and lenses, but we shouldn't think that they are the same thing.

In this chapter, we will define the appropriate abstract algebra of systems and their two sorts of composition keeping in mind the separation between interfaces and systems. We call this abstract algebra a doubly indexed category, since it is a sort of double categorical generalization of an indexed category. We'll see the definition of this notion in Section 4.3. Later, in Chapter 6, we'll see how this abstraction of the algebra of composition of systems can be used to work in other doctrines of dynamical systems - ways of thinking about what it means to be a systems theory at all.

Once we have organized our systems into doubly indexed categories, we can discuss what it means to change our systems theory. A change of systems theory will be a way of turning one sort of dynamical system into another. This could mean simply re-interpreting the underlying structure (for example, a deterministic system where all maps are differentiable is in particular a discrete deterministic system, just by forgetting the differentiability) or by restricting the use of certain maps (as in Definition 3.6.0.1). But it could also mean approximating one sort of system by another sort of system.

As an example, let's consider the Euler method for approximating a differential system. Suppose that

$$
\left(\begin{array}{l}
\text { update }_{\mathrm{S}} \\
\text { expose }_{\mathrm{S}}
\end{array}\right):\left(\begin{array}{l}
\mathbb{R}^{n} \\
\mathbb{R}^{n}
\end{array}\right) \leftrightarrows\left(\begin{array}{l}
\mathbb{R}^{k} \\
\mathbb{R}^{m}
\end{array}\right)
$$

is a Euclidean differential system S. This represents the differential equation

$$
\frac{d s}{d t}=\text { update }_{S}(s, p)
$$

That is, a trajectory is a map $s: \mathbb{R} \rightarrow \mathbb{R}^{n}$ satisfying this differential equation (for a choice of parameters $p: \mathbb{R} \rightarrow \mathbb{R}^{k}$ ). This means that the direction that the state $s_{0}$ is
tending is given by update ${ }_{S}\left(s_{0}, p_{0}\right)$. We could then approximate the solution, given such a starting point, by moving a small distance in this direction. We could get a whole sequence of states this way; moving in the direction our dynamics tells us we should go, and then checking where to go from there.

The result is a deterministic system $\mathscr{E}_{\varepsilon}(\mathrm{S})$ whose dynamics is given by

$$
\operatorname{update}_{\mathscr{E}_{\varepsilon}(\mathrm{S})}(s, p)=s+\varepsilon \cdot \operatorname{update}_{\mathrm{S}}(s, p) .
$$

Here, $\varepsilon>0$ is some small increment. We can take $\mathscr{E}_{\varepsilon}(\mathrm{S})$ to expose the same variable that $\mathrm{S}$ does: $\operatorname{expose}_{\mathcal{E}_{\varepsilon}(\mathrm{S})}=$ expose $_{\mathrm{S}}$.

The change of systems theory $\mathscr{E}_{\varepsilon}$ is the formula for changing from the Euclidean differential systems theory to the deterministic systems theory on the cartesian category of Euclidean spaces. We might wonder: how does changing the systems theory by using the Euler method affect the wiring together of systems? How does it affect the behaviors of the systems?

We can answer the question about behaviors here. It is not true that every behavior of a Euclidean differential system is faithfully represented by its Euler method approximation. Consider, for example, the simply system

$$
\operatorname{update}_{s}(s)=s
$$

having one state variable, and no parameters. The trajectories of this system are of the form $s(t)=C e^{t}$ for some constant $C$. However, if we let $\varepsilon=.1$ and consider the Euler approximation

$$
\text { update }_{\varepsilon_{.1}(\mathrm{~S})}(s(0))=s(0)+.1 \cdot s(0)=1.1 \cdot C
$$

This is not the same thing as $s(.1)=C e^{.1} \approx 1.105 \cdot C$ (though, as expected, they are rather close). So we see that general behaviors are not preserved!

However, suppose we have a steady state of the system. For example, taking $C=0$ we get a steady state of the system update $_{\mathrm{S}}(s)=s$ above. Then we have that

$$
\text { update }_{E_{.1}(\mathrm{~S})}(0)=0+.1 \cdot 0=0 .
$$

In other words, the steady state remains a steady state!

The goal of this chapter will be to introduce the formalism which enables us to inquire into and prove various compositionality results concerning changes of systems theory. In the above situation, we will see that the Euler method $\mathcal{E}_{\varepsilon}$ gives a change of systems theory on a restriction of the Euclidean differential systems theory to affine maps. As a result, it will preserve any behavior whose underlying map is affine (of the form $\phi(v)=A v+b$ for a matrix $A$ and vector $b$ ), which includes all steady states (since constant maps are affine) but almost no trajectories in general.

We will introduce the notion of a doubly indexed functor to organize the compositionality results concerning change of systems theory. We will also be using these doubly indexed functors in the next chapter to organize the compositionality of behaviors in general.

We will define the notion of change of systems theory formally (Definition 4.5.1.2) and show that every change of systems theory gives rise to a doubly indexed functor between the doubly indexed categories of systems in the respective systems theories. In particular, we will show that there is a functor

$$
\text { Sys : Theory } \rightarrow \text { DblIx }
$$

sending a systems theory to the doubly indexed category of systems in it.

\subsection*{4.2 Composing behaviors in general}

Before we get to this abstract defintion, we will take our time exploring the sorts of compositionality results one may prove quickly by working in the double category of arenas.

Recall the categories Sys $\left(\begin{array}{l}I \\ O\end{array}\right)$ of systems with the interface $\left(\begin{array}{l}I \\ O\end{array}\right)$ from Definition 3.3.1.1. One thing that vertical composition in the double category of arenas shows us is that wiring together systems is functorial with respect to simulations - that is, behaviors that don't change the interface.

We repeat the definition of Sys $\left(\begin{array}{l}I \\ O\end{array}\right)$ for an arbitrary systems theory.

Definition 4.2.0.1. Let $\mathbb{D}=(\mathcal{A}, T)$ be a theory of dynamical systems. For a $\mathcal{A}$-arena $\left(\begin{array}{l}I \\ O\end{array}\right)$, the category Sys $\left(\begin{array}{l}I \\ O\end{array}\right)$ of $\mathbb{D}$-systems with interface $\left(\begin{array}{l}I \\ O\end{array}\right)$ is defined by:
- Its objects are $\mathcal{A}$-lenses $\left(\begin{array}{c}\text { update }_{S} \\ \text { expose }_{S}\end{array}\right):\left(\begin{array}{c}T \text { States } \\ \text { States }\end{array}\right) \leftrightarrows\left(\begin{array}{l}I \\ O\end{array}\right)$, which are systems in this systems theory (Definition 3.5.0.5).
- Its maps are simulations, the behaviors which have identity chart. That is, the maps are the squares

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-182.jpg?height=431&width=612&top_left_y=1796&top_left_x=797)
- Composition is given by horizontal composition in the double category Arena $\boldsymbol{A}_{\mathcal{A}}$ of $\mathcal{A}$-arenas.

Now, thanks to the double category of arenas, we can show that every lens $\left(\begin{array}{c}f^{\sharp} \\ f\end{array}\right)$ :
$\left(\begin{array}{l}I \\ O\end{array}\right) \leftrightarrows\left(\begin{array}{c}I^{\prime} \\ O^{\prime}\end{array}\right)$ gives a functor

$$
\text { Sys }\left(\begin{array}{c}
f^{\sharp} \\
f
\end{array}\right): \text { Sys }\left(\begin{array}{l}
I \\
O
\end{array}\right) \rightarrow \text { Sys }\left(\begin{array}{l}
I \\
O
\end{array}\right) \text {. }
$$

We can see this functor as the operation of wiring together our $\left(\begin{array}{l}I \\ O\end{array}\right)$-systems along the lens $\left(\begin{array}{c}f^{\sharp} \\ f\end{array}\right)$ to get $\left(\begin{array}{c}I^{\prime} \\ O^{\prime}\end{array}\right)$-systems. The functoriality of this operation say that wiring preserves simulations - if systems $S_{i}$ simulate $T_{i}$ by $\phi_{i}$, then the wired together systems S simulate $\mathrm{T}$ by $\phi=\prod_{i} \phi_{i}$.

Proposition 4.2.0.2. For a lens $\left(\begin{array}{c}f^{\sharp} \\ f\end{array}\right):\left(\begin{array}{l}I \\ O\end{array}\right) \leftrightarrows\left(\begin{array}{c}I^{\prime} \\ O^{\prime}\end{array}\right)$, we get a functor

$$
\text { Sys }\left(\begin{array}{c}
f^{\sharp} \\
f
\end{array}\right): \text { Sys }\left(\begin{array}{l}
I \\
O
\end{array}\right) \rightarrow \text { Sys }\left(\begin{array}{l}
I \\
O
\end{array}\right)
$$

Given by composing with $\left(\begin{array}{c}f^{\sharp} \\ f\end{array}\right)$ :
- For a system $S=\left(\begin{array}{c}\text { update }_{S} \\ \text { expose }_{S}\end{array}\right):\left(\begin{array}{c}T \text { Trtas }^{2} \\ \text { States }\end{array}\right) \leftrightarrows\left(\begin{array}{l}I \\ O\end{array}\right)$,

$$
\text { Sys }\left(\begin{array}{c}
f^{\sharp} \\
f
\end{array}\right)(\mathrm{S})=\left(\begin{array}{c}
\text { update }_{\mathrm{S}} \\
\text { expose }_{\mathrm{S}}
\end{array}\right) \circ\left(\begin{array}{c}
f^{\sharp} \\
f
\end{array}\right) \text {. }
$$
- For a behavior, Sys $\left(\begin{array}{c}f^{\sharp} \\ f\end{array}\right)$ acts in the following way:
![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-183.jpg?height=668&width=1356&top_left_y=1628&top_left_x=427)

Proof. The functoriality of this construction can be seen immediately from the inter-
change law of the double category:

$$
\begin{aligned}
\frac{\left.\left(\begin{array}{c}
T \phi \\
\phi
\end{array}\right) \right\rvert\,\left(\begin{array}{c}
T \psi \\
\psi
\end{array}\right)}{\left(\begin{array}{c}
f^{\sharp} \\
f
\end{array}\right)} & =\frac{\left.\left(\begin{array}{c}
T \phi \\
\phi
\end{array}\right) \right\rvert\,\left(\begin{array}{c}
T \psi \\
\psi
\end{array}\right)}{\left.\left(\begin{array}{c}
f^{\sharp} \\
f
\end{array}\right) \right\rvert\,\left(\begin{array}{c}
f^{\sharp} \\
f
\end{array}\right)} \quad \text { by the horizontal identity law, } \\
& =\frac{\left(\begin{array}{c}
T \phi \\
\phi
\end{array}\right)}{\left(\begin{array}{c}
f^{\sharp} \\
f
\end{array}\right)} \left\lvert\, \frac{\left(\begin{array}{c}
T \psi \\
\psi
\end{array}\right)}{\left(\begin{array}{c}
f^{\sharp} \\
f
\end{array}\right)} \quad\right. \text { by the interchange law. }
\end{aligned}
$$

Identities are clearly preserved, since the underlying morphism $\phi:$ State $\rightarrow$ States is not changed.

The notion of profunctor gives us a nice way to understand the relationship between a behavior $\phi: \mathrm{T} \rightarrow \mathrm{S}$ and its chart $\left(\begin{array}{c}f_{\mathrm{b}} \\ f\end{array}\right):\left(\begin{array}{c}I \\ O\end{array}\right) \rightrightarrows\left(\begin{array}{c}I^{\prime} \\ O^{\prime}\end{array}\right)$. When we are using behaviors, we usually have the chart $\left(\begin{array}{c}f_{b} \\ f\end{array}\right)$ in mind first, and then look for behaviors with this chart. For example, when finding trajectories, we first set the parameters for our system and then solve it. We can use profunctors to formalize this relationship.

Proposition 4.2.0.3. Given a chart $\left(\begin{array}{c}f_{b} \\ f\end{array}\right):\left(\begin{array}{l}I \\ O\end{array}\right) \rightrightarrows\left(\begin{array}{c}I^{\prime} \\ O^{\prime}\end{array}\right)$, we get a profunctor

$$
\text { Sys }\left(\begin{array}{l}
f_{b} \\
f
\end{array}\right): \text { Sys }\left(\begin{array}{l}
I \\
O
\end{array}\right) \rightarrow \text { Sys }\left(\begin{array}{c}
I^{\prime} \\
O^{\prime}
\end{array}\right)
$$

Defined by:

$$
\begin{aligned}
& \text { Sys }\left(\begin{array}{c}
f_{\mathrm{b}} \\
f
\end{array}\right)(\mathrm{T}, \mathrm{S})=\left\{\phi: \text { State }_{\mathrm{T}} \rightarrow \text { States } \mid \phi \text { is a behavior with chart }\left(\begin{array}{l}
f_{\mathrm{b}} \\
f
\end{array}\right)\right\}
\end{aligned}
$$

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-185.jpg?height=553&width=700&top_left_y=762&top_left_x=688)

The action of the profunctor $\operatorname{Sys}\left(\begin{array}{l}f_{b} \\ f\end{array}\right)$ on simulations in the categories $\operatorname{Sys}\left(\begin{array}{l}I \\ O\end{array}\right)$ and Sys $\left(\begin{array}{l}I^{\prime} \\ O^{\prime}\end{array}\right)$ is given by composition on the left and right. That is, for simulations $\phi: \mathrm{T}^{\prime} \rightarrow$ $\mathrm{T}$ and $\psi: \mathrm{S} \rightarrow \mathrm{S}^{\prime}$ and $\left(\begin{array}{l}f_{\mathrm{b}} \\ f\end{array}\right)$-behavior $\beta \in \mathbf{S y s}\left(\begin{array}{l}f_{\mathrm{b}} \\ f\end{array}\right)(\mathrm{T}, \mathrm{S})$, we define

$$
\begin{equation*}
\phi \cdot \beta \cdot \psi:=\phi|\beta| \psi \tag{4.2}
\end{equation*}
$$

Exercise 4.2.0.4. Prove Proposition 4.2.0.3. That is, show that the action defined in Eq. (4.2) is functorial, giving a functor

$$
\text { Sys }\left(\begin{array}{c}
I \\
O
\end{array}\right)^{\mathrm{op}} \times \operatorname{Sys}\left(\begin{array}{c}
I^{\prime} \\
O^{\prime}
\end{array}\right) \rightarrow \text { Set. }
$$

(Hint: use the double categorical notation. It will be much more concise.)

With a little work in the double category of arenas, we can give a very useful example of a square in the double category of profunctors. Consider this square in the double
category of arenas:

$$
\begin{aligned}
& \left(\begin{array}{l}
I_{1} \\
O_{1}
\end{array}\right) \xrightarrow{\left(\begin{array}{c}
f^{\sharp} \\
f
\end{array}\right)}\left(\begin{array}{l}
I_{2} \\
O_{2}
\end{array}\right) \\
& \alpha=\left(\begin{array}{c}
j_{b} \\
j
\end{array}\right) \downarrow \uparrow \quad \downarrow \uparrow\left(\begin{array}{c}
k^{\sharp} \\
k
\end{array}\right) \\
& \left(\begin{array}{c}
I_{3} \\
O_{3}
\end{array}\right) \underset{\left(\begin{array}{c}
g_{b} \\
g
\end{array}\right)}{\longrightarrow}\left(\begin{array}{c}
I_{4} \\
O_{4}
\end{array}\right)
\end{aligned}
$$

As we saw in Proposition 4.2.0.2, we get functors $\operatorname{Sys}\left(\begin{array}{c}j^{\sharp} \\ j\end{array}\right): \operatorname{Sys}\left(\begin{array}{c}I_{1} \\ O_{1}\end{array}\right) \rightarrow \operatorname{Sys}\left(\begin{array}{c}I_{3} \\ O_{3}\end{array}\right)$ and Sys $\left(\begin{array}{c}k^{\sharp} \\ k\end{array}\right)$ : Sys $\left(\begin{array}{c}I_{2} \\ O_{2}\end{array}\right) \rightarrow \operatorname{Sys}\left(\begin{array}{c}I_{4} \\ O_{4}\end{array}\right)$ given by composing with these lenses. We also saw in Proposition 4.2.0.3 that we get profunctors Sys $\left(\begin{array}{l}f_{\mathrm{b}} \\ f\end{array}\right): \operatorname{Sys}\left(\begin{array}{c}I_{1} \\ O_{1}\end{array}\right) \rightarrow \operatorname{Sys}\left(\begin{array}{c}I_{2} \\ O_{2}\end{array}\right)$ and Sys $\left(\begin{array}{c}g_{b} \\ g\end{array}\right): \operatorname{Sys}\left(\begin{array}{c}I_{3} \\ O_{3}\end{array}\right) \rightarrow$ Sys $\left(\begin{array}{c}I_{4} \\ O_{4}\end{array}\right)$ from these charts. Now let's see how to get a square of profunctors from the square $\alpha$ in the double category of arenas:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-186.jpg?height=563&width=767&top_left_y=1380&top_left_x=668)

That is, a natural transformation of the following signature:

$$
\operatorname{Sys}(\alpha) \text { : Sys }\left(\begin{array}{c}
f_{b} \\
f
\end{array}\right) \rightarrow \operatorname{Sys}\left(\begin{array}{c}
g_{b} \\
g
\end{array}\right)\left(\operatorname{Sys}\left(\begin{array}{c}
j^{\sharp} \\
j
\end{array}\right), \operatorname{Sys}\left(\begin{array}{c}
k^{\sharp} \\
k
\end{array}\right)\right) \text {. }
$$

To define the natural transformation $\operatorname{Sys}(\alpha)$, we need to say what it does to an element $\phi \in \operatorname{Sys}\left(\begin{array}{c}f_{b} \\ f\end{array}\right)(T, S)$. Recall that the elements of this profunctor are behaviors
with chart $\left(\begin{array}{l}f_{b} \\ f\end{array}\right)$, so really $\phi$ is a square

$$
\begin{aligned}
& \left(\begin{array}{c}
T \text { State }_{\mathrm{T}} \\
\text { State }_{\mathrm{T}}
\end{array}\right) \xrightarrow{\left(\begin{array}{c}
T \phi \\
\phi
\end{array}\right)}\left(\begin{array}{c}
T \text { States }^{\longrightarrow} \\
\text { State }_{\mathrm{S}}
\end{array}\right) \\
& \phi=\left(\begin{array}{c}
\text { update }_{\mathrm{T}} \\
\text { expose }_{\mathrm{T}}
\end{array}\right) \downarrow \uparrow \quad \downarrow \uparrow\left(\begin{array}{c}
\text { update }_{\mathrm{S}} \\
\text { expose }_{\mathrm{S}}
\end{array}\right) \\
& \left.\left(\begin{array}{c}
I_{1} \\
O_{1}
\end{array}\right) \Longrightarrow\left(\begin{array}{c}
f_{b} \\
f
\end{array}\right)\right]\left(\begin{array}{c}
I_{2} \\
O_{2}
\end{array}\right)
\end{aligned}
$$

in the double category of arenas. Therefore, we can define $\operatorname{Sys}(\alpha)(\phi)$ to be the vertical composite:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-187.jpg?height=730&width=618&top_left_y=1180&top_left_x=748)

Or, a little more concisely in double category notation:

$$
\operatorname{Sys}(\alpha)(\phi)=\frac{\phi}{\alpha}
$$

We record this observation in a proposition.

Proposition 4.2.0.5. Given a square

$$
\begin{aligned}
& \left(\begin{array}{l}
I_{1} \\
O_{1}
\end{array}\right) \xrightarrow{\left(\begin{array}{l}
f^{\sharp} \\
f
\end{array}\right)}\left(\begin{array}{l}
I_{2} \\
O_{2}
\end{array}\right) \\
& \alpha=\left(\begin{array}{c}
j_{b} \\
j
\end{array}\right) \downarrow \uparrow \quad \downarrow \uparrow\left(\begin{array}{c}
k^{\sharp} \\
k
\end{array}\right) \\
& \left(\begin{array}{c}
I_{3} \\
O_{3}
\end{array}\right) \underset{\left(\begin{array}{c}
g_{b} \\
g
\end{array}\right)}{\longrightarrow}\left(\begin{array}{c}
I_{4} \\
O_{4}
\end{array}\right)
\end{aligned}
$$

in the double category of arenas, we get a square

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-188.jpg?height=572&width=767&top_left_y=988&top_left_x=668)

in the double category of categories, functors, and profunctors given by

$$
\operatorname{Sys}(\alpha)(\phi)=\frac{\phi}{\alpha}
$$

The naturality of this transformation follows from the double category laws. We leave the particulars as an exercise.

Exercise 4.2.0.6. Prove that the family of functions

$$
\operatorname{Sys}(\alpha) \text { : Sys }\left(\begin{array}{c}
f_{b} \\
f
\end{array}\right) \rightarrow \operatorname{Sys}\left(\begin{array}{c}
g_{b} \\
g
\end{array}\right)\left(\operatorname{Sys}\left(\begin{array}{c}
j^{\sharp} \\
j
\end{array}\right), \operatorname{Sys}\left(\begin{array}{c}
k^{\sharp} \\
k
\end{array}\right)\right)
$$

defined in Proposition 4.2.0.5 is a natural transformation. (Hint: use the double category notation, it will be much more concise.)

\subsection*{4.3 Arranging categories along two kinds of composition: Doubly indexed categories}

While we described a category of systems and behaviors in Proposition 3.3.0.17, we haven't been thinking of systems in quite this way. We have been organizing our systems a bit more particularly than just throwing them into one large category. We've made the following observations:
- Each system has an interface, and many different systems can have the same interface. From this observation, we defined the categories Sys $\left(\begin{array}{l}I \\ O\end{array}\right)$ of systems with the interface $\left(\begin{array}{l}I \\ O\end{array}\right)$ in Definition 3.3.1.1.
- Every wiring diagram, or more generally lens, gives us an operation that changes the interface of a system by wiring things together. We formalized this observation into a functor Sys $\left(\begin{array}{c}w^{\sharp} \\ w\end{array}\right): \operatorname{Sys}\left(\begin{array}{l}I \\ O\end{array}\right) \rightarrow \operatorname{Sys}\left(\begin{array}{c}I^{\prime} \\ O^{\prime}\end{array}\right)$ in Proposition 4.2.0.2.
- To describe the behavior of a system, first we have to chart out how it will look on its interface. We formalized this observation by giving a profunctor Sys $\left(\begin{array}{c}f_{b} \\ f\end{array}\right)$ : Sys $\left(\begin{array}{l}I \\ O\end{array}\right) \rightarrow$ Sys $\left(\begin{array}{c}I^{\prime} \\ O^{\prime}\end{array}\right)$ for each chart in Proposition 4.2.0.3.
- If we wire together a chart for one interface into a chart for the wired interface, then every behavior for that chart gives rise to a behavior for the wired together chart. We formalized this observation as a morphism of profunctors

$$
\operatorname{Sys}(\alpha) \text { : Sys }\left(\begin{array}{c}
f_{b} \\
f
\end{array}\right) \rightarrow \operatorname{Sys}\left(\begin{array}{c}
g_{b} \\
g
\end{array}\right)\left(\operatorname{Sys}\left(\begin{array}{c}
j^{\sharp} \\
j
\end{array}\right), \operatorname{Sys}\left(\begin{array}{c}
k^{\sharp} \\
k
\end{array}\right)\right)
$$

in Proposition 4.2.0.5.

Now comes the time to organize all these observations. In this section, we will see that collectively, these observations are telling us that there is an doubly indexed category of dynamical systems. We will also see that matrices of sets give rise to a doubly indexed category which we will call the doubly indexed category of vectors of sets.

Definition 4.3.0.1. A doubly indexed category $\mathcal{A}: \mathscr{D} \rightarrow$ Cat consists of the following: ${ }^{a}$
- A double category $\mathscr{D}$ called the indexing base.
- For every object $D \in \mathcal{D}$, we have a category $\mathcal{A}(D)$.
- For every vertical arrow $j: D \rightarrow D^{\prime}$, we have a functor $\mathcal{A}(j): \mathcal{A}(D) \rightarrow \mathcal{A}\left(D^{\prime}\right)$.
- For every horizontal arrow $f: D \rightarrow D^{\prime}$, we have a profunctor $\mathcal{A}(f): \mathcal{A}(D) \rightarrow \mathcal{A}\left(D^{\prime}\right)$.
- For every square

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-189.jpg?height=225&width=230&top_left_y=2143&top_left_x=991)
in $\mathscr{D}$, a square

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-190.jpg?height=271&width=474&top_left_y=293&top_left_x=869)

in Cat.
- For any two horizontal maps $f: A \rightarrow B$ and $g: B \rightarrow C$ in $\mathscr{D}$, we have a square $\mu_{f, g}: \mathcal{A}(f) \odot \mathcal{A}(g) \rightarrow \mathcal{A}(f \mid g)$ called the compositor:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-190.jpg?height=269&width=409&top_left_y=768&top_left_x=899)

This data is required to satisfy the following laws:
- (Vertical Functoriality) For vertical maps $j: D \rightarrow D^{\prime}$ and $k: D^{\prime} \rightarrow D^{\prime \prime}$, we have that

$$
\mathcal{A}\left(\frac{j}{k}\right)=\frac{\mathcal{A}(j)}{\mathcal{A}(k)}
$$

and that $\mathcal{A}\left(\operatorname{id}_{D}\right)=\operatorname{id}_{\mathcal{A}(D)}{ }^{b}$
- (Horizontal Lax Functoriality) For horizontal maps $f: D_{1} \rightarrow D_{2}, g: D_{2} \rightarrow D_{3}$ and $h: D_{3} \rightarrow D_{4}$, the compositors $\mu$ satisfy the following associativity and unitality conditions:
- (Associativity)

$$
\frac{\mu_{f, g} \mid \mathcal{A}(h)}{\mu_{(f \mid g), h}} \doteq \frac{\mathcal{A}(f) \mid \mu_{g, h}}{\mu_{f,(g \mid h)}}
$$
- (Unitality) The profunctor $\mathcal{A}\left(\mathrm{id}_{D_{1}}\right): \mathcal{A}\left(D_{1}\right) \rightarrow \mathcal{A}\left(D_{1}\right)$ is the identity profunctor, $\mathcal{A}\left(\mathrm{id}_{D_{1}}\right)=\mathcal{A}\left(D_{1}\right)$. Furthermore, $\mu_{\mathrm{id}_{D_{1}}, f}$ and $\mu_{f, \mathrm{id}_{D_{2}}}$ are equal to the isomorphisms of Exercise 3.4.3.3 given by the naturality of $\mathcal{A}(f)$ on the left and right respectively. We may sumarize this may saying that

$$
\mu_{\mathrm{id}, f}=\mathrm{id}_{\mathcal{A}(f)}=\mu_{f, \mathrm{id}} .
$$
- (Naturality of Compositors) For any horizontally composable squares

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-190.jpg?height=239&width=615&top_left_y=2149&top_left_x=796)

$$
\frac{\mathcal{A}(\alpha) \mid \mathcal{A}(\beta)}{\mu_{g_{1}, g_{2}}}=\frac{\mu_{f_{1}, f_{2}}}{\mathcal{A}(\alpha \mid \beta)}
$$
\footnotetext{
${ }^{a}$ This is what an expert would call a unital (or normal) lax double functor, but we won't need this concept in any other setting.

${ }^{b}$ Here, we are hiding some coherence issues. While our doubly indexed category of deterministic systems will satisfy this functoriality condition on the nose, we will soon see a doubly indexed category of matrices of sets for which this law only holds up to a coherence isomorphism. Again, the issue invovles shuffling parentheses around, and we will sweep it under the rug.
}

That's another big definition! It seems like it will be a slog to actually ever prove that something is a doubly indexed category. Luckily, in our cases, these proofs will go quite smoothly. This is because each of the three laws of a doubly indexed category has a sort of sister law from the definition of a double category which will help us prove it.
- The Vertical Functoriality law will often involve the vertical associativity and unitality of squares in the indexing base.
- The Horizontal Lax Functoriality law will often involve the horizontal associativity and unitality of squares in the indexing base.
- The Naturality of Compositors law will often involve the interchange law in the indexing base.

We'll see how these sisterhoods play out in practice as we define the doubly indexed categories of deterministic systems and vectors of sets.

The doubly indexed category of systems Let's show that systems in a systems theory $\mathbb{D}$ do indeed form a doubly indexed category

$$
\text { Sys }_{\mathbb{D}}: \text { Arena } \mathbb{D} \rightarrow \text { Cat. }
$$

Definition 4.3.0.2. The doubly indexed category Sys $\mathbb{D}_{\mathbb{D}}:$ Arena $_{\mathbb{D}} \rightarrow$ Cat of systems in the systems theory $\mathbb{D}=(\mathcal{A}, T)$ is defined as follows:
- Our indexing base is the double category Arena of arenas, since we will arrange our systems according to their interface.
- To every arena $\left(\begin{array}{l}I \\ O\end{array}\right)$, we associate the category Sys $\left(\begin{array}{l}I \\ O\end{array}\right)$ of systems with interface $\left(\begin{array}{l}I \\ O\end{array}\right)$ and behaviors whose chart is the identity chart on $\left(\begin{array}{l}I \\ O\end{array}\right)$ (Definition 4.2.0.1).
- To every lens $\left(\begin{array}{c}w^{\sharp} \\ w\end{array}\right):\left(\begin{array}{c}I \\ O\end{array}\right) \leftrightarrows\left(\begin{array}{c}I^{\prime} \\ O^{\prime}\end{array}\right)$, we associate the functor Sys $\left(\begin{array}{c}w^{\sharp} \\ w\end{array}\right): \operatorname{Sys}\left(\begin{array}{l}I \\ O\end{array}\right) \rightarrow$ Sys $\left(\begin{array}{l}I \\ O\end{array}\right)$ given by wiring according to $\left(\begin{array}{c}w^{\sharp} \\ w\end{array}\right)$ :

$$
\text { Sys }\left(\begin{array}{c}
w^{\sharp} \\
w
\end{array}\right)(\mathrm{S})=\frac{\mathbf{S}}{\left(\begin{array}{c}
w^{\sharp} \\
w
\end{array}\right)} \text {. }
$$

This is defined in Proposition 4.2.0.2.
- To every chart $\left(\begin{array}{l}f_{\mathrm{b}} \\ f\end{array}\right):\left(\begin{array}{l}I \\ O\end{array}\right) \rightrightarrows\left(\begin{array}{c}I^{\prime} \\ O^{\prime}\end{array}\right)$, we associate the profunctor Sys $\left(\begin{array}{l}f_{\mathrm{b}} \\ f\end{array}\right)$ : Sys $\left(\begin{array}{l}I \\ O\end{array}\right) \rightarrow \operatorname{Sys}\left(\begin{array}{l}I^{\prime} \\ O^{\prime}\end{array}\right)$ which sends the $\left(\begin{array}{l}I \\ O\end{array}\right)$-system $\mathrm{T}$ and the $\left(\begin{array}{c}I^{\prime} \\ O^{\prime}\end{array}\right)$-system $\mathrm{S}$ to the set of behaviors $T \rightarrow S$ with chart $\left(\begin{array}{c}f_{b} \\ f\end{array}\right)$ :

$$
\begin{aligned}
& \text { Sys }\left(\begin{array}{l}
f_{\mathrm{b}} \\
f
\end{array}\right)(\mathrm{T}, \mathrm{S})=\left\{\phi: \text { State }_{\mathrm{T}} \rightarrow \text { State }_{\mathrm{S}} \mid \phi \text { is a behavior with chart }\left(\begin{array}{l}
f_{\mathrm{b}} \\
f
\end{array}\right)\right\}
\end{aligned}
$$

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-192.jpg?height=545&width=718&top_left_y=649&top_left_x=736)

We saw this profunctor in Proposition 4.2.0.3.
- To every square $\alpha$, we assign the morphism of profunctors given by composing vertically with $\alpha$ in Arena:

$$
\operatorname{Sys}(\alpha)(\phi)=\frac{\phi}{\alpha} \text {. }
$$

We saw in Exercise 4.2.0.6 that this was a natural transformation.
- The compositor is given by horizontal composition in the double category of arenas:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-192.jpg?height=127&width=865&top_left_y=1647&top_left_x=673)

$$
\begin{aligned}
& (\phi, \psi) \mapsto \phi \mid \psi
\end{aligned}
$$

Let's check now that this does indeed satisfy the laws of a doubly indexed category. The task may appear to loom over us; there are quite a few laws, and there is a lot of data involved. But nicely, they all follow quickly from a bit of fiddling in the double category of arenas.
- (Vertical Functoriality) We show that Sys $\left(\left(\begin{array}{c}k^{\sharp} \\ k\end{array}\right) \circ\left(\begin{array}{c}j^{\sharp} \\ j\end{array}\right)\right)=\operatorname{Sys}\left(\begin{array}{c}k^{\sharp} \\ k\end{array}\right) \circ \operatorname{Sys}\left(\begin{array}{c}j^{\sharp} \\ j\end{array}\right)$ by vertical associativity:

$$
\text { Sys }\left(\left(\begin{array}{c}
k^{\sharp} \\
k
\end{array}\right) \circ\left(\begin{array}{c}
j^{\sharp} \\
j
\end{array}\right)\right)(\phi)=\frac{\phi}{\left(\frac{\left(\begin{array}{c}
j^{\sharp} \\
j
\end{array}\right)}{\left(\begin{array}{c}
k^{\sharp} \\
k
\end{array}\right)}\right)}=\frac{\left(\begin{array}{c}
\phi \\
\left(\begin{array}{c}
j^{\sharp} \\
j
\end{array}\right)
\end{array}\right)}{\left(\begin{array}{c}
k^{\sharp} \\
k
\end{array}\right)}
$$

$$
=\text { Sys }\left(\begin{array}{c}
k^{\sharp} \\
k
\end{array}\right) \circ \text { Sys }\left(\begin{array}{c}
j \\
j
\end{array}\right)(\phi) \text {. }
$$
- (Horizontal Lax Functoriality) This law follows from horizontal associativity in Arena.

$$
\begin{equation*}
\mu(\mu(\phi, \psi), \xi)=(\phi \mid \psi)|\xi=\phi|(\psi \mid \xi)=\mu(\phi, \mu(\psi, \xi)) . \tag{4.4}
\end{equation*}
$$
- (Naturality of Compositor) This law follows from interchange in Arena.

$$
\begin{aligned}
\left(\frac{\operatorname{Sys}(\alpha) \mid \operatorname{Sys}(\beta)}{\mu}\right)(\phi, \psi) & =\frac{\phi}{\alpha} \left\lvert\, \frac{\psi}{\beta}=\frac{\phi \mid \psi}{\alpha \mid \beta}\right. \\
& =\left(\frac{\mu}{\operatorname{Sys}(\alpha \mid \beta)}\right)(\phi, \psi) .
\end{aligned}
$$

The doubly indexed category of vectors of sets In addition to our doubly indexed category of systems, we have a doubly indexed category of "vectors of sets".

Classically, an $m \times n$ matrix $M$ can act on a vector $v$ of length $n$ by multiplication to get another vector $M v$ of length $m$. We can generalize this to matrices of sets if we define a vector of sets of length $A$ to be a dependent set $V: A \rightarrow$ Set.

Definition 4.3.0.3. For a set $A$, we define the category of vectors of sets of length $A$ to be

$$
\operatorname{Vec}(A):=\operatorname{Set}^{A}
$$

the category of sets depending on $A$.

Given a ( $B \times A$ )-matrix $M: B \times A \rightarrow$ Set (as in Definition 3.4.2.1), we can treat a $A$-vector $V$ as a $A \times 1$ matrix and form the $B \times 1$ matrix $M V$. This gives us a functor

$$
\begin{aligned}
\operatorname{Vec}(M): \operatorname{Vec}(A) & \rightarrow \operatorname{Vec}(B) \\
V & \mapsto(M V)_{b}=\sum_{a \in A} M_{b a} \times V_{a} \\
f: V \rightarrow W & \mapsto((a, m, v) \mapsto(a, m, f(v)))
\end{aligned}
$$

which we refer to as the linear functor given by $M$.

Definition 4.3.0.4. The doubly indexed category Vec : Matrix $\rightarrow$ Cat of vectors of sets is defined by:
- Its indexing base is the double category of matrices of sets.
- To every set $A$, we assign the category $\operatorname{Vec}(A)=\operatorname{Set}^{A}$ of vectors of length $A$.
- To every $(B \times A)$-matrix $M: A \rightarrow B$, we assign the linear functor $\operatorname{Vec}(M)$ : $\operatorname{Vec}(A) \rightarrow \operatorname{Vec}(B)$ given by $M$ (Definition 4.3.0.3).
- To every function $f: A \rightarrow B$, we associate the profunctor
$\operatorname{Vec}(f): \operatorname{Vec}(A) \rightarrow \operatorname{Vec}(B)$ defined by

$$
\operatorname{Vec}(f)(V, W)=\left\{F:(a \in A) \rightarrow V_{a} \rightarrow W_{f(a)}\right\}
$$

That is, $F \in \operatorname{Vec}(f)(V, W)$ is a family of functions $F(a,-): V_{a} \rightarrow W_{f(a)}$ indexed by $a \in A$. This is natural by index-wise composition.
- To every square

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-194.jpg?height=230&width=260&top_left_y=584&top_left_x=973)

that is, family of functions $\alpha_{c a}: M_{c a} \rightarrow N_{g(c) f(a)}$, we associate the square

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-194.jpg?height=271&width=569&top_left_y=922&top_left_x=821)

defined by sending a family of functions $F:(a \in A) \rightarrow V_{a} \rightarrow W_{f(a)}$ in $\operatorname{Vec}(f)(V, W)$ to the family

$$
\begin{aligned}
& \operatorname{Vec}(\alpha)(F):(c \in C) \rightarrow M V_{c} \rightarrow M W_{g(c)} \\
& \operatorname{Vec}(\alpha)(F)(c,(a, m, v))=(f(a), \alpha(m), F(a, v))
\end{aligned}
$$

That is, $\operatorname{Vec}(\alpha)(F)(c,-)$ takes an element $(a, m, v) \in M V_{c}=\sum_{a \in A} M_{c a} \times V_{a}$ and gives the elements $(f(a), \alpha(m), F(a, v))$ of $M W_{g(c)}=\sum_{b \in B} N_{g(c) b} \times W_{b}$.
- The compositor is given by componentwise composition: If $f: A \rightarrow B$ and $g: B \rightarrow C$ and $F \in \operatorname{Vec}(f)(V, W)$ and $G \in \operatorname{Vec}(g)(W, U)$, then

$$
\begin{aligned}
\mu_{f, g}(F, G):(a \in A) \rightarrow V_{a} & \rightarrow U_{g f(a)} \\
\mu_{f, g}(F, G)(a, v) & :=G(f(a), F(a, v))
\end{aligned}
$$

It might seem like it will turn out to be a big hassle to show that this definition satisfies all the laws of a doubly indexed category. Like with the doubly indexed category of arenas, we will find that all the laws follow for matrices by fiddling around in the double category of matrices.

Let's first rephrase the above definition in terms of the category of matrices. We note that a vector of sets $V \in \operatorname{Vec}(A)$ is equivalently a matrix $V: 1 \rightarrow A$. Then the linear functor $\operatorname{Vec}(M): \operatorname{Vec}(A) \rightarrow \operatorname{Vec}(B)$ is given by matrix multiplication, or in double category notation:

$$
\operatorname{Vec}(M)(V)=\frac{V}{M}
$$

This means that the Vertical Functoriality law follows by vertical associativity in the double category of matrices, which is to say associativity of matrix multiplication.

Similarly, we can interpret the profunctor $\operatorname{Vec}(f)$ for $f: A \rightarrow B$ in terms of the double category Matrix. An element $F \in \operatorname{Vec}(f)(V, W)$ is equivalently a square of the following form in Matrix:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-195.jpg?height=225&width=263&top_left_y=549&top_left_x=926)

Therefore, we can describe $\operatorname{Vec}(f)(V, W)$ as the following set of squares in Matrix:

$$
\operatorname{Vec}(f)(V, W)=\left\{\begin{array}{ccc}
1 & & \\
F & F & \downarrow \\
V & & \downarrow \\
A & f & B
\end{array}\right\}
$$

Then the Horizontal Lax Functoriality laws follow from associativity and unitality of horizontal composition of squares in Matrix!

Finally, we need to interpret the rather fiddly transformation $\operatorname{Vec}(\alpha)$ in terms of the double category of matrices. Its a matter of unfolding the definitions to see that $\operatorname{Vec}(\alpha)(F)=\frac{F}{\alpha}$ in Matrix, and therefore that the Naturality of Compositors law follows by the interchange law.

If this argument seemed wholly too similar to the one we gave for the doubly indexed category of systems, your suspicions are not misplaced. These are both are instances of a very general vertical slice construction, which we turn our attention to now.

\subsection*{4.4 Vertical Slice Construction}

In the previous section, we constructed the doubly indexed categories $\mathbf{S y s}_{\mathbb{D}}$ of systems in a systems theory $\mathbb{D}$ and Vec of vectors of sets "by hand". However, both constructions felt very familiar. In this section, we will show that they are both instances of a general construction: the vertical slice construction.

The main reason for recasting the above constructions in more general terms is that it will facilitate our main theorem of this chapter: change of systems theory.

The vertical slice construction will take a double functor $F: \mathscr{D}_{0} \rightarrow \mathscr{D}_{1}$ and produce a doubly indexed category $\sigma F: \mathscr{D}_{1} \rightarrow$ Cat indexed by its codomain. So, in order to describe the vertical slice construction, we will need the notion of double functor. We will need the notion of double functor for much of the coming theory as well

\subsection*{4.4.1 Double Functors}

A double functor is the correct sort of functor between double categories. Just as a double category has a bit more than twice the information involved in a category, a double functor has a bit more than twice the information involved in a functor.

Definition 4.4.1.1. Let $\mathscr{D}_{0}$ and $\mathscr{D}_{1}$ be double categories. A double functor $\mathrm{F}: \mathscr{D}_{0} \rightarrow \mathscr{D}_{1}$ consists of:
- An object assignment $F: \mathrm{Ob} \mathscr{D}_{0} \rightarrow \mathrm{Ob} \mathscr{D}_{1}$ which assigns an object $F D$ in $\mathscr{D}_{1}$ to each object $D$ in $\mathscr{D}_{0}$.
- A vertical functor $F: v \mathscr{D}_{0} \rightarrow v \mathscr{D}_{1}$ on the vertical categories, which acts the same as the object assignment on objects.
- A horizontal functor $F: h D_{0} \rightarrow h D_{1}$ on the horizontal categories, which acts the same as the object assignment on objects.
- For every square

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-196.jpg?height=222&width=228&top_left_y=995&top_left_x=992)

in $\mathscr{D}_{0}$, a square

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-196.jpg?height=249&width=312&top_left_y=1282&top_left_x=950)

such that the following laws hold:
- $F$ commutes with horizontal compostition: $F(\alpha \mid \beta)=F \alpha \mid F \beta$.
- $F$ commutes with vertical comopsition: $F\left(\frac{\alpha}{\beta}\right)=\frac{F \alpha}{F \beta}$.
- $F$ sends horizontal identities to horizontal identities, and vertical identities to vertical identities.

Remark 4.4.1.2. There is, in fact, a double category of double functors $F: \mathscr{D}_{0} \rightarrow \mathscr{D}_{1}$, but we won't need to worry about this until we consider the functoriality of the vertical slice construction in Section 4.4.4.

We will, in time, see many interesting examples of double functors. However, we will begin with the two simple examples we need to construct the doubly indexed categories Sys and Vec.

Example 4.4.1.3. Let $\mathbb{D}=\left(\mathcal{A}:\right.$ eop $^{\text {op }} \rightarrow$ Cat, $\left.T\right)$ be a systems theory. We recall that the section $T: C \rightarrow \int^{C: C} \mathcal{A}(C)$ is a functor to the Grothendieck construction of $\mathcal{A}$. We may promote this into a double functor into the double category of arenas Arena $a_{\mathbb{D}}$ in a rather simple way.

Since the horizontal category of Arena $\mathbb{D}_{\mathbb{D}}$ is $\int^{C: C} \mathcal{A}(C)$, the category of charts, we may consider $T$ as a double functor

$$
h T: h C \rightarrow \text { Arena }_{\mathbb{D}}
$$

from the double category $h C$ given by defining its horizontal category to be $C$ and taking its vertical category and its squares to consist only of identities. Its worth taking a minute to check this trivial observation against the definition of a double functor.

Example 4.4.1.4. There is a double category 1 with just one object $*$ and only identity maps and squares. A double functor $F: 1 \rightarrow \mathscr{D}$ simply picks out the object $F(*)$; there is no other data involved, since everything else must get sent to the appropriate identities.

In particular, the one element set 1 is an object of the double category Matrix of sets, functions, and matrices. Therefore, there is a double functor $1: 1 \rightarrow$ Matrix picking out this special element.

Now that we have a notion of double functor, we can define a category $\mathrm{Dbl}$ of double categories.

Definition 4.4.1.5. The category $\mathrm{Dbl}$ of double categories has as its objects the double categories and as its maps the double functors.

From any indexed category $\mathcal{A}$, we can form the double categories of arenas in $\mathcal{A}$ (Definition 4.1.0.3). In category theory, it is a good habit to inquire into the functoriality of any construction. Now that we have an appropriate category of double categories, we can ask if the construction $\mathcal{A} \mapsto$ Arena $_{\mathcal{A}}$ is functorial.

Proposition 4.4.1.6. The assignment $\mathcal{A} \mapsto$ Arena $_{\mathcal{A}}$ sending an indexed category to its Grothendieck double construction (Definition 3.5.0.6) is functorial.

Proof. Let $\mathcal{A}: C^{\mathrm{op}} \rightarrow$ Cat and $\mathscr{B}: \mathscr{D}^{\mathrm{op}} \rightarrow$ Cat be indexed categories, and let $(F, \bar{F})$ : $\mathcal{A} \rightarrow \mathscr{B}$ be an indexed functor. We will produce a double functor

$$
\left(\begin{array}{l}
\bar{F} \\
F
\end{array}\right): \text { Arena }_{\mathcal{A}} \rightarrow \text { Arena }_{\mathcal{B}}
$$

Recall that the Grothendieck construction is functorial (Proposition 2.7.0.2). From an indexed functor $(F, \bar{F}): \mathcal{A} \rightarrow \mathscr{B}$, we get a functor

$$
\left(\begin{array}{l}
\bar{F} \\
F
\end{array}\right): \int^{C: C} \mathcal{A}(C) \rightarrow \int^{D: \mathscr{D}} \mathscr{B}(D)
$$

Since the horizontal category of Arena is precisely the Grothendieck construction, we can take this to be the horizontal component of $\left(\begin{array}{l}\bar{F} \\ F\end{array}\right):$ Arena $_{\mathcal{A}} \rightarrow$ Arena $_{\mathcal{B}}$. Similarly, since the vertical category of Arena is the Grothendieck construction of the opposite, we can take the vertical component of $\left(\begin{array}{l}\bar{F} \\ F\end{array}\right):$ Arena $_{\mathcal{A}} \rightarrow \operatorname{Arena}_{\mathcal{B}}$ to be $\left(\begin{array}{c}\bar{F}^{\mathrm{op}} \\ F\end{array}\right): \int^{C: C} \mathcal{A}(C)^{\mathrm{op}} \rightarrow \int^{D: \mathscr{D}} \mathcal{B}(D)^{\mathrm{op}}$.

All that remains to check then is that $\left(\begin{array}{l}\bar{F} \\ F\end{array}\right)::$ Arena $_{\mathcal{A}} \rightarrow$ Arena $_{\mathscr{B}}$ preserves squares. Let

$$
\begin{aligned}
\left(\begin{array}{c}
A_{1} \\
C_{1}
\end{array}\right) & \xrightarrow{\left(\begin{array}{c}
g_{1 b} \\
g_{1}
\end{array}\right)}\left(\begin{array}{c}
A_{2} \\
C_{2}
\end{array}\right) \\
\left(\begin{array}{c}
f_{1}^{\sharp} \\
f_{1}
\end{array}\right) \downarrow \uparrow & \downarrow \uparrow\left(\begin{array}{l}
f_{2}^{\sharp} \\
f_{2}
\end{array}\right) \\
\left(\begin{array}{c}
A_{3} \\
C_{3}
\end{array}\right) & \left.\longrightarrow \begin{array}{c}
g_{2 b} \\
g_{2}
\end{array}\right)
\end{aligned}
$$

be a square in Arena $\mathcal{A}$. We need to show that

$$
\begin{aligned}
& \left(\begin{array}{c}
\bar{F} A_{1} \\
F C_{1}
\end{array}\right) \stackrel{\left(\begin{array}{c}
\bar{F} g_{1 b} \\
F g_{1}
\end{array}\right)}{\longrightarrow}\left(\begin{array}{l}
\bar{F} A_{2} \\
F C_{2}
\end{array}\right) \\
& \left(\begin{array}{c}
\bar{F} f_{1}^{\sharp} \\
F f_{1}
\end{array}\right) \downarrow \uparrow \quad \downarrow \uparrow\left(\begin{array}{c}
\bar{F} f_{2}^{\sharp} \\
F f_{2}
\end{array}\right) \\
& \left(\begin{array}{l}
\bar{F} A_{3} \\
F C_{3}
\end{array}\right) \underset{\left(\begin{array}{c}
\bar{F} g_{2 b} \\
F g_{2}
\end{array}\right)}{\longrightarrow}\left(\begin{array}{l}
\bar{F} A_{4} \\
F C_{4}
\end{array}\right)
\end{aligned}
$$

is a square in Arena $_{B}$. But this being a square means that the two following diagrams commute:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-198.jpg?height=277&width=1458&top_left_y=1916&top_left_x=339)

The left square commutes because $F$ is a functor, and the right square commutes because $(F, \bar{F})$ is an indexed functor.

\subsection*{4.4.2 The Vertical Slice Construction: Definition}

We are now ready to define the vertical slice construction.

Definition 4.4.2.1 (The Vertical Slice Construction). Let $F: \mathscr{D}_{0} \rightarrow \mathscr{D}_{1}$ be a double functor. The vertical slice construction of $F$ is the doubly indexed category

$$
\sigma F: \mathscr{D}_{1} \rightarrow \text { Cat }
$$

defined as follows:
- For $D \in \mathscr{D}_{1}, \sigma F(D)$ is the category whose objects are pairs $(A, j)$ of an object $A \in \mathscr{D}_{0}$ and a vertical map $f: F A \rightarrow D$. A map $\left(A_{1}, j_{1}\right) \rightarrow\left(A_{2}, j_{2}\right)$ is a pair $(f, \alpha)$ of a horizontal $f: A_{1} \rightarrow A_{2}$ and a square

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-199.jpg?height=222&width=304&top_left_y=743&top_left_x=951)

in $\mathscr{D}_{1}$.
- For every vertical $j: D \rightarrow D^{\prime}$ in $\mathscr{D}_{1}$, we associate the functor $\sigma F(j): \sigma F(D) \rightarrow$ $\sigma F\left(D^{\prime}\right)$ given by vertical composition with $j$ :

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-199.jpg?height=365&width=746&top_left_y=1189&top_left_x=730)

More concisely, this is

$$
\sigma F(j)(f, \alpha)=\left(f, \frac{\alpha}{j}\right)
$$
- For every horizontal $g: D \rightarrow D^{\prime}$ in $\mathscr{D}_{1}$, we associate the profunctor $\sigma F(g)$ : $\sigma F(D) \mapsto \sigma F\left(D^{\prime}\right)$ given by

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-199.jpg?height=265&width=834&top_left_y=1881&top_left_x=689)

We note that if $g=\mathrm{id}_{D}$ is an identity, then this reproduces the hom profunctor of $\sigma F(D)$.
- The compositor $\mu$ is given by horizontal composition:

$$
\mu_{g_{1}, g_{2}}\left(\left(f_{1}, \alpha_{1}\right),\left(f_{2}, \alpha_{2}\right)\right)=\left(f_{1}\left|f_{2}, \alpha_{1}\right| \alpha_{2}\right)
$$

Let's check now that this does indeed satisfy the laws of a doubly indexed category. The proof is exactly as it was for Sys.
- (Vertical Functoriality) We show that $\sigma F\left(\frac{k_{1}}{k_{2}}\right)=\sigma F\left(k_{2}\right) \circ \sigma F\left(k_{1}\right)$ by vertical associativity:

$$
\begin{aligned}
\sigma F\left(\frac{k_{1}}{k_{2}}\right)(f, \alpha) & =\left(f, \frac{\alpha}{\left(\frac{k_{1}}{k_{2}}\right)}\right) \\
& =\left(f, \frac{\left(\frac{\alpha}{k_{1}}\right)}{k_{2}}\right) \\
& =\sigma F\left(k_{2}\right) \circ \sigma F\left(k_{1}\right)((f, \alpha)) .
\end{aligned}
$$
- (Horizontal Lax Functoriality) This law follows from horizontal associativity in $\mathscr{D}_{1}$.

$$
\begin{aligned}
\mu\left(\mu\left(\left(f_{1}, \alpha_{1}\right),\left(f_{2}, \alpha_{2}\right)\right),\left(f_{3}, \alpha_{3}\right)\right) & =\left(\left(f_{1} \mid f_{2}\right)\left|f_{3},\left(\alpha_{1} \mid \alpha_{2}\right)\right| \alpha_{3}\right) \\
& =\left(f_{1}\left|\left(f_{2} \mid f_{3}\right), \alpha_{1}\right|\left(\alpha_{2} \mid \alpha_{3}\right)\right) \\
& =\mu\left(\left(f_{1}, \alpha_{1}\right), \mu\left(\left(f_{2}, \alpha_{2}\right),\left(f_{3}, \alpha_{3}\right)\right)\right) .
\end{aligned}
$$
- (Naturality of Compositor) This law follows from interchange in $\mathscr{D}_{1}$.

$$
\begin{aligned}
\left(\sigma F\left(\beta_{1}\right) \mid \sigma F\left(\beta_{2}\right) \mu\right)\left(\left(f_{1}, \alpha_{1}\right),\left(f_{2}, \alpha_{2}\right)\right) & =\left(f_{1}\left|f_{2}, \frac{\phi}{\alpha}\right| \frac{\psi}{\beta}\right) \\
& =\left(f_{1} \mid f_{2}, \frac{\phi \mid \psi}{\alpha \mid \beta}\right) \\
& =\left(\frac{\mu}{\sigma F\left(\beta_{1} \mid \beta_{2}\right)}\right)\left(\left(f_{1}, \alpha_{1}\right),\left(f_{2}, \alpha_{2}\right)\right)
\end{aligned}
$$

We can now see that the vertical slice construction generalizes both the constructions of Sys $\mathbb{D}_{\mathbb{D}}$ and Vec.

Proposition 4.4.2.2. The doubly indexed category Sys $_{\mathbb{D}}$ of systems in a systems theory $\mathbb{D}=\left(\mathcal{A}: C^{\mathrm{op}} \rightarrow\right.$ Cat,$\left.T\right)$ is the vertical slice construction of the double functor $h T:$ $h C \rightarrow$ Arena $_{\mathbb{D}}$ given by considering the section $T$ as a double functor.

$$
\mathbf{S y s}_{\mathbb{D}}=\sigma\left(h T: h C \rightarrow \text { Arena }_{\mathbb{D}}\right) .
$$

Proof. This is a matter of checking definitions and seeing that they are precisely the same.

Proposition 4.4.2.3. The doubly indexed category Vec of vectors of sets is the vertical slice construction of the inclusion $1: 1 \rightarrow$ Matrix of the one element set into the double category of matrices of sets.

$$
\text { Vec }=\sigma(1: 1 \rightarrow \text { Matrix }) .
$$

Proof. This is also a matter of checking that the definitions coincide.

\subsection*{4.4.3 Natural Transformations of Double Functors}

We now turn towards proving the functoriality of the vertical slice construction as a first step in proving the change of systems theory functoriality theorem. In order to express the functoriality of the vertical slice construction, we will first need learn about natural transformations between double functors.

Since double categories have two sorts of maps - vertical and horizontal â€” there are also two sorts of natural transformations between double functors. The two definitions are symmetric; we may arrive at one by replacing the words "vertical" by "horizontal" and vice-versa. We will have occasion to use both of them in this and the coming chapters.

Definition 4.4.3.1. Let $F$ and $G: \mathscr{D} \rightarrow \mathcal{E}$ be double functors. A vertical natural transformation $v: F \Rightarrow G$ consists of the following data:
- For every object $D \in \mathscr{D}$, a vertical $v_{D}: F D \rightarrow G D$ in $\mathcal{E}$.
- For every horizontal arrow $f: D \rightarrow D^{\prime}$ in $\mathscr{D}$, a square

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-201.jpg?height=255&width=347&top_left_y=1198&top_left_x=930)

This data must satisfy the following laws:
- (Vertical Naturality) For any vertical $j: D_{1} \rightarrow D_{2}$, we have

$$
\frac{F j}{v_{D_{2}}}=\frac{v_{D_{1}}}{G j}
$$
- (Horizontal Naturality) For any horizontal $f_{1}: D_{1} \rightarrow D_{2}$ and $f_{2}: D_{2} \rightarrow D_{3}$, we have

$$
v_{f_{1} \mid f_{2}}=v_{f_{1}} \mid v_{f_{2}} .
$$
- (Horizontal Unity) $v_{\mathrm{id}_{D}}=\mathrm{id}_{v_{D}}$.
- (Square naturality) For any square

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-201.jpg?height=249&width=276&top_left_y=2074&top_left_x=965)

we have

$$
\frac{F \alpha}{v_{f_{2}}}=\frac{v_{f_{1}}}{G \alpha}
$$

Dually, a horizontal transformation $h: F \Rightarrow G$ consists of the following data:
- For every object $D \in \mathscr{D}$ a horizontal morphism $h_{D}: F D \rightarrow G D$.
- For every vertical $j: D \rightarrow D^{\prime}$ in $\mathscr{D}$, a square

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-202.jpg?height=258&width=317&top_left_y=440&top_left_x=947)

This data is required to satisfy the following laws:
- (Horizontal Naturality) For horizontal $f: D_{1} \rightarrow D_{2}$, we have

$$
F f\left|v_{D_{2}}=v_{D_{1}}\right| G f .
$$
- (Vertical Naturality) For vertical $j_{1}: D_{1} \rightarrow D_{2}$ and $j_{2}: D_{2} \rightarrow D_{3}$, we have

$$
h_{\frac{j_{1}}{j_{2}}}=\frac{h_{j_{1}}}{h_{j_{2}}}
$$
- (Vertical Unity) $h_{\mathrm{id}_{D}}=\mathrm{id}_{h_{D}}$.
- (Square Naturality) For any square

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-202.jpg?height=242&width=268&top_left_y=1321&top_left_x=972)

we have

$$
F \alpha\left|h_{j_{2}}=h_{j_{1}}\right| G \alpha .
$$

Remark 4.4.3.2. Note that vertical (resp. horizontal) natural transformations are named for the direction of arrow they assign to objects. However, a vertical transformation is defined by its action $v_{f}$ on horizontal maps $f$, and dually a horizontal transformation $h_{j}$ by its action on vertical maps $j$. Taking $f$ (resp. $j$ ) to be an identity $\operatorname{id}_{D}$ yields the vertical (resp. horizontal) arrow associated to the object $D$.

Natural transformations between double functors can be composed in the appropriate directions.

Lemma 4.4.3.3. Suppose that $v_{1}: F_{1} \Rightarrow F_{2}$ and $v_{2}: F_{2} \Rightarrow F_{2}$ are vertical transformations. We have a vertical composite $\frac{v_{1}}{v_{2}}$ defined by

$$
\left(\frac{v_{1}}{v_{2}}\right)_{f}:=\frac{\left(v_{1}\right)_{f}}{\left(v_{2}\right)_{f}}
$$

for horizontal maps $f$. Dually, for horizontal transformations $h_{1}: F_{1} \Rightarrow F_{2}$ and $h_{2}: F_{2} \Rightarrow F_{3}$, there is a horizontal composite $h_{1} \mid h_{2}$ defined by

$$
\left(h_{1} \mid h_{2}\right)_{j}:=\left(h_{1}\right)_{j} \mid\left(h_{2}\right)_{j}
$$

for every vertical map $j$.

Proof. We will prove that $\frac{v_{1}}{v_{2}}$ is a vertical transformation; the proof that $h_{1} \mid h_{2}$ is a horizontal transformation is precisely dual.
- (Vertical Naturality) This follows by the same argument as for Square Naturality below, taking $\alpha=j$ for a vertical $j: D_{1} \rightarrow D_{2}$.
- (Horizontal naturality) For horizontal maps $f_{1}: D_{1} \rightarrow D_{2}$ and $f_{2}: D_{2} \rightarrow D_{3}$, we have

$$
\begin{aligned}
\frac{v_{1}}{v_{2}} f_{f_{1} \mid f_{2}} & =\frac{\left(v_{1}\right)_{f_{1} \mid f_{2}}}{\left(v_{2}\right)_{f_{1} \mid f_{2}}} \\
& =\frac{\left(v_{1}\right)_{f_{1}} \mid\left(v_{1}\right)_{f_{2}}}{\left(v_{2}\right)_{f_{1}} \mid\left(v_{2}\right)_{f_{2}}} \\
& \left.=\frac{\left(v_{1}\right)_{f_{1}}}{\left(v_{2}\right)_{f_{1}}} \right\rvert\, \frac{\left(v_{1}\right)_{f_{2}}}{\left(v_{2}\right)_{f_{2}}} \\
& \left.=\left(\frac{v_{1}}{v_{2}}\right)_{f_{1}} \right\rvert\,\left(\frac{v_{1}}{v_{2}}\right)_{f_{2}} .
\end{aligned}
$$
- (Horizontal Unity) This holds by definition.
- (Square Naturality) Consider a square $\alpha$ of the following signature:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-203.jpg?height=239&width=271&top_left_y=1843&top_left_x=970)

Then

$$
\begin{aligned}
\frac{F_{1} \alpha}{\left(\frac{v_{1}}{v_{2}}\right)_{f_{2}}} & =\frac{\frac{F_{1} \alpha}{\left(v_{1}\right)_{f_{2}}}}{\left(v_{2}\right)_{f_{2}}} \\
& =\frac{\left(v_{1}\right)_{f_{1}}}{\frac{F_{2} \alpha}{\left(v_{2}\right)_{f_{2}}}}
\end{aligned}
$$

$$
\begin{aligned}
& =\frac{\left(v_{1}\right)_{f_{1}}}{\frac{\left(v_{2}\right)_{f_{1}}}{F_{3} \alpha}} \\
& =\frac{\left(\frac{v_{1}}{v_{2}}\right)_{f_{1}}}{F_{3} \alpha} .
\end{aligned}
$$

Amongst double functors we have found two sorts of maps - vertical and horizontal - each with their own sort of composition. This suggests that there should be a double category of double functors $\mathscr{D} \rightarrow \mathcal{E}$, just as there is a category of functors between two categories.

Theorem 4.4.3.4. Let $\mathscr{D}$ and $\mathcal{E}$ be double categories. There is a double category Fun $(\mathscr{D}, \mathcal{E})$ of double functors from $\mathscr{D}$ to $\mathcal{E}$ whose vertical maps are vertical transformations, horizontal maps are horizontal transformations, and whose squares

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-204.jpg?height=233&width=279&top_left_y=1114&top_left_x=912)

are modifications defined in the following way. To each object $D \in \mathscr{D}$, we have a square

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-204.jpg?height=247&width=417&top_left_y=1465&top_left_x=843)

which satisfies the following laws:
- (Horizontal Coherence) For every horizontal $f: D_{1} \rightarrow D_{2}$, we have that

$$
\left(v_{1}\right)_{f}\left|\alpha_{D_{2}}=\alpha_{D_{1}}\right|\left(v_{2}\right)_{f} .
$$

We note that this law requires us to use the vertical naturality law of $v_{1}$ and $v_{2}$ so that these composites have the same signature.
- (Vertical Coherence) For every vertical $j: D_{1} \rightarrow D_{2}$, we have that

$$
\frac{\alpha_{D_{1}}}{\left(h_{2}\right)_{j}}=\frac{\left(h_{1}\right)_{j}}{\alpha_{D_{2}}}
$$

We note that this law requires us to use the horizontal naturality law of $h_{1}$ and $h_{2}$ so that these composites have the same signature.

The compositions $\alpha \mid \beta$ and $\frac{\alpha}{\beta}$ are given componentwise by $\alpha_{D} \mid \beta_{D}$ and $\frac{\alpha_{D}}{\beta_{D}}$.

Proof. Since the compositions of modifications are given componentwise, they will satisfy associativity and interchange. We just need to show that they are well defined, which is to say that they satisfy the laws of a modification. This is a straightforward calculation; we'll prove Vertical Coherence for horizontal composition since the other cases are similar.

Let $\alpha$ and $\beta$ be modifications with the following signatures:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-205.jpg?height=263&width=677&top_left_y=733&top_left_x=716)

Let $j: D_{1} \rightarrow D_{2}$ be a vertical map in $\mathscr{D}$. We calculate:

$$
\begin{aligned}
\frac{(\alpha \mid \beta)_{D_{1}}}{\left(h_{2} \mid h_{4}\right)_{j}} & =\frac{\alpha_{D_{1}} \mid \beta_{D_{1}}}{\left(h_{2}\right)_{j} \mid\left(h_{4}\right)_{j}} \\
& \left.=\frac{\alpha_{D_{1}}}{\left(h_{2}\right)_{j}} \right\rvert\, \frac{\beta_{D_{1}}}{\left(h_{4}\right)_{j}} \\
& \left.=\frac{\left(h_{1}\right)_{j}}{\alpha_{D_{2}}} \right\rvert\, \frac{\left(h_{3}\right)_{j}}{\beta_{D_{2}}} \\
& =\frac{\left(h_{1} \mid h_{3}\right)_{j}}{(\alpha \mid \beta)_{D_{2}}}
\end{aligned}
$$

Before we move on, let's record an important lemma relating modifications to squares.

Lemma 4.4.3.5. Let

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-206.jpg?height=238&width=282&top_left_y=293&top_left_x=911)

be a modification, and

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-206.jpg?height=241&width=268&top_left_y=584&top_left_x=923)

be a square in $\mathscr{D}$. We then have the following four-fold equality in $\varepsilon$ :

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-206.jpg?height=341&width=677&top_left_y=941&top_left_x=716)

We may refer to the single square given by any of these composites by $\alpha_{s}$.

Proof. These all follow by cycling through the square naturality laws of the transformations and the coherence laws of the modification.

\subsection*{4.4.4 Vertical Slice Construction: Functoriality}

In this section, we will describe the functoriality of the vertical slice construction. Since the vertical slice construction takes a double functor $F: \mathscr{D}_{0} \rightarrow \mathscr{D}_{1}$ and produces a doubly indexed category $\sigma F: \mathscr{D}_{1} \rightarrow$ Cat, we will need to show that from a certain sort of map between double functors we get a doubly indexed functor between the resulting vertical slices.

First, we will describe the appropriate notion of map between double functors. This gives us a category which we will call the category of double functors DblFun ${ }^{1}$

Definition 4.4.4.1. The category DblFun has objects the double functor $F: \mathscr{D}_{0} \rightarrow \mathscr{D}_{1}$. A map $F_{1} \rightarrow F_{2}$ is a triple $\left(v_{0}, v_{1}, v\right)$ where $v_{0}: \mathscr{D}_{00} \rightarrow \mathscr{D}_{10}$ and $v_{1}: \mathscr{D}_{01} \rightarrow \mathscr{D}_{11}$ are
\footnotetext{
${ }^{1}$ Though one could define other categories whose objects are double functors, this is the only such category we will use in this book.
}
double functors and $v: F_{2} \circ v_{0} \Rightarrow v_{1} \circ F_{1}$ is a vertical transformation.

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-207.jpg?height=279&width=377&top_left_y=327&top_left_x=863)

Composition of $\left(v_{0}, v_{1}, v\right): F_{1} \rightarrow F_{2}$ with $\left(w_{0}, w_{1}, w\right): F_{2} \rightarrow F_{3}$ is given by $\left(w_{0} \circ v_{0}, w_{1} \circ\right.$ $v_{1}, v * w$ ) where $v * w$ is the vertical transformation with horizontal components given by

$$
(v * w)_{f}:=\frac{w_{v_{0} f}}{w_{1} v_{f}}
$$

It remains to check that this does indeed yield a category. We leave this as an exercise, since it gives some good practice in using all the various laws for double functors and double transformations.

Exercise 4.4.4.2. Prove that the definition of DblFun does indeed yield a category. That is:

1. Prove that $\left(\mathrm{id}_{\mathscr{D}_{0}}, \mathrm{id}_{\mathscr{D}_{1}}, \mathrm{id}_{F}\right)$ provides an identity map $F \rightarrow F$.

2. Prove that composition is associative. The key part will be showing that

$$
(v * w) * u=v *(w * u) .
$$

Next, we need to describe the appropriate category of doubly indexed categories. There are two sorts of maps of doubly indexed categories which we will need in this book: lax doubly indexed functors, and (taut) doubly indexed functors. In this chapter, we will be using taut doubly indexed functors - which we may just call doubly indexed functors - which are a special case of the more general lax variety.

Definition 4.4.4.3. Let $\mathcal{A}: \mathscr{D}_{1} \rightarrow$ Cat and $\mathscr{B}: \mathscr{D}_{2} \rightarrow$ Cat be doubly indexed categories. A lax doubly indexed functor $\left(F^{0}, F\right): \mathcal{A} \rightharpoonup \mathscr{B}$ consists of:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-207.jpg?height=320&width=412&top_left_y=2022&top_left_x=843)

1. A double functor

$$
F^{0}: \mathscr{D}_{1} \rightarrow \mathscr{D}_{2} .
$$

2. For each object $D \in \mathscr{D}_{1}$, a functor

$$
F^{D}: \mathcal{A}(D) \rightarrow \mathcal{B}\left(F^{0} D\right) .
$$

3. For every vertical map $j: D_{1} \rightarrow D_{2}$ in $\mathscr{D}_{1}$, a natural transformation

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-208.jpg?height=241&width=434&top_left_y=514&top_left_x=886)

We ask that $F^{\mathrm{id}}{ }^{D}=\mathrm{id}$. We recall (from Proposition 3.4.3.10) that we may think of such a natural transformation as a square

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-208.jpg?height=241&width=477&top_left_y=926&top_left_x=865)

4. For every horizontal map $f: D_{1} \rightarrow D_{2}$, a square

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-208.jpg?height=276&width=461&top_left_y=1285&top_left_x=867)

in Cat. We ask that $F^{\mathrm{id}}{ }_{D}=\mathrm{id}$.

This data is required to satisfy the following laws:
- (Vertical Lax Functoriality) For composable vertical maps $j: D_{1} \rightarrow D_{2}$ and $k: D_{2} \rightarrow D_{3}$,

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-208.jpg?height=377&width=548&top_left_y=1801&top_left_x=821)

This is, in terms of squares in Cat:

$$
\left.F^{\frac{j}{k}} \doteq \frac{F^{j}}{\mathcal{B}\left(F^{0} k\right)} \right\rvert\, \frac{\mathcal{A}(j)}{F^{k}}
$$
- (Horizontal functoriality) For composable horizontal arrows $f: D_{1} \rightarrow D_{2}$ and $g: D_{2} \rightarrow D_{3}$,

$$
\frac{\mu_{f, g}^{\mathcal{A}}}{F^{f \mid g}}=\frac{F^{f} \mid F^{g}}{\mu_{F^{0} f, F^{0} g}^{B}}
$$
- (Functorial Interchange) For any square

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-209.jpg?height=236&width=263&top_left_y=587&top_left_x=974)

in $\mathscr{D}_{1}$, we have that

$$
F^{j}\left|\frac{\mathcal{A}(\alpha)}{F^{g}} \doteq \frac{F^{f}}{\mathscr{B}\left(F^{0} \alpha\right)}\right| F^{k}
$$

Note the use of " $=$ " here; the two sides of this equation have different, but canonically isomorphic boundary. What we are asking is that when these boundaries are made the same by composing with canonical isomorphisms in any way, they will become equal

A lax doubly indexed functor is taut â€” which we will in refer to just as a doubly indexed functor - if the natural transformations $F^{j}$ associated to vertical maps $j$ : $D_{1} \rightarrow D_{2}$ in $\mathscr{D}_{1}$ are natural isomorphisms.

The definition of doubly indexed functor involves a lot of data, but this is because it is a big collection of functoriality results.

Before getting to our functoriality theorem, we need to compose lax doubly indexed functors.

Definition 4.4.4.4. If $\left(F^{0}, F\right): \mathcal{A} \rightarrow \mathbb{B}$ and $\left(G^{0}, G\right): \mathscr{B} \rightarrow \mathcal{C}$ are two doubly indexed functors, we define their composite

$$
\left(F^{0}, F\right) \stackrel{ }{\circ}\left(G^{0}, G\right):=\left(F^{0} \circ G^{0}, F ; G\right)
$$

where $F ; G$ is defined by:
- We define $(F \circ G)^{D}:=F^{D} \stackrel{\circ}{ } G^{F_{0} D}$. We note that in Cat, where functors are the vertical maps, this can be written

$$
(F ; G)^{D}=\frac{F^{D}}{G^{F^{0} D}}
$$
- For a vertical $j: D_{1} \rightarrow D_{2}$, we define

$$
\begin{aligned}
& \mathcal{A}\left(D_{1}\right) \xrightarrow{\left(F_{9}^{\circ}\right)^{D_{1}}} \mathcal{C}\left(G^{0} F^{0} D_{1}\right) \quad \mathcal{A}\left(D_{1}\right) \xrightarrow{F^{D_{1}}} \mathcal{B}\left(F^{0} D_{1}\right) \xrightarrow{G^{F^{0} D_{1}}} \mathcal{C}\left(G^{0} F^{0} D_{1}\right)
\end{aligned}
$$

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-210.jpg?height=95&width=1276&top_left_y=408&top_left_x=468)

$$
\begin{aligned}
& \mathcal{A}\left(D_{2}\right) \underset{\left(F_{9}^{\circ} G\right)^{D_{2}^{2}}}{ } \mathcal{C}\left(G^{0} F^{0} D_{2}\right) \quad \mathcal{A}\left(D_{2}\right) \underset{F^{D_{2}}}{\longrightarrow} \mathcal{B}\left(F^{0} D_{2}\right) \underset{G^{F^{0} D_{2}}}{ } C\left(G^{0} F^{0} D_{2}\right)
\end{aligned}
$$

We note that by Lemma 3.4.3.12, this corresponds to the composite of squares:

$$
\left.(F \circ G)^{j} \doteq \frac{F^{D_{1}}}{G^{F^{0} j}} \right\rvert\, \frac{F^{j}}{G^{F^{0} D_{2}}}
$$
- For a horizontal $f: D_{1} \rightarrow D_{2}$, we define

$$
(F ; G)^{f}:=\frac{F^{f}}{G^{F^{0} f}}
$$

We refer to the category of doubly indexed categories and lax doubly indexed functors by LaxDblIx and the category of doubly indexed categories and (taut) doubly indexed functors by DblIx.

Let's show that this composition operation does indeed produce a lax doubly indexed functor.
- (Vertical Lax Functoriality) For composable vertical maps $j: D_{1} \rightarrow D_{2}$ and $k: D_{2} \rightarrow D_{3}$, consider the following diagram:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-210.jpg?height=380&width=751&top_left_y=1512&top_left_x=730)

There is a single natural transformation given as the composite of this "pasting diagram". But, if we read it by composing vertically first, and then composing horizontally second, we arive at $(F \circ G)^{\frac{j}{k}}$, while if we read it by composing horizontally first and then vertically second, we get the composite of $(F ; G)^{j}$ and $(F \circ G)^{k}$ as desired.
- (Horizontal Functoriality) Let $f: D_{1} \rightarrow D_{2}$ and $g: D_{2} \rightarrow D_{3}$ be horizontal maps. We then calculate:

$$
\frac{\mu_{f, g}^{\mathcal{A}}}{\left(F_{9}^{\circ}\right)^{f \mid g}}=\frac{\mu_{f, g}^{\mathcal{A}}}{\frac{F^{f \mid g}}{G^{F^{0}(f \mid g)}}}
$$

$$
\begin{aligned}
& =\frac{F^{f} \mid F^{g}}{\frac{\mu_{F^{0} f, F^{0} g}^{G^{0}}}{G^{F^{0} f \mid F^{0} g}}} \\
& =\frac{\frac{F^{f} \mid F^{g}}{G^{F^{0} f} \mid G^{F^{0} g}}}{\mu_{G^{0} F^{0} f, G^{0} F^{0} g}} \\
& =\frac{\left.\frac{F^{f}}{G^{F^{0} f}} \right\rvert\, \frac{F^{g}}{G^{F^{0} g}}}{\mu_{G^{0} F^{0} f, G^{0} F^{0} g}^{e}} \\
& =\frac{(F \circ G)^{f} \mid\left(F_{9}^{\circ} G\right)^{g}}{\mu_{G^{0} F^{0} f, G^{0} F^{0} g}^{e}} .
\end{aligned}
$$
- (Functorial Interchange) Consider a square

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-211.jpg?height=233&width=263&top_left_y=1006&top_left_x=974)

We may then calculate:
![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-211.jpg?height=834&width=1050&top_left_y=1354&top_left_x=580)

Now that we have a category of doubly indexed categories, we can state the functoriality result:

Theorem 4.4.4.5. The vertical slice construction (Definition 4.4.2.1) gives a functor

We will spend the rest of this section proving this theorem.

Proposition 4.4.4.6. Let $\left(v_{0}, v_{1}, v\right): F_{1} \rightarrow F_{2}$ be a map in DblFun. Then we have a doubly indexed functor

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-212.jpg?height=328&width=398&top_left_y=454&top_left_x=858)

Proof. We define $\sigma v$ as follows:
- We have $\sigma v^{D}: \sigma F_{1}(D) \rightarrow \sigma F_{2}\left(v_{1} D\right)$ given by the following action on maps:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-212.jpg?height=393&width=916&top_left_y=996&top_left_x=648)

In short:

$$
\sigma v^{D}(f, \alpha):=\left(v_{0} f, \frac{v_{f}}{v_{1} \alpha}\right)
$$
- For any vertical map $j: D_{1} \rightarrow D_{2}$ in $\mathscr{D}_{01}$, we will show that

$$
\sigma F_{2}\left(v_{1} j\right) \circ \sigma v^{D_{1}}=\sigma v^{D_{2}} \circ \sigma F_{1}(j)
$$

so that we may take $\sigma v^{j}$ to be the identity natural transformation.

$$
\begin{aligned}
\sigma F_{2}\left(v_{1} j\right) \circ \sigma v^{D_{1}}(f, \alpha) & =\sigma F_{2}\left(v_{1} j\right)\left(v_{0} f, \frac{v_{f}}{v_{1} \alpha}\right) \\
& =\left(v_{0} f, \frac{v_{f}}{v_{1} \alpha}\right) \\
& =\left(v_{0} f, \frac{v_{f}}{v_{1}\left(\frac{\alpha}{j}\right)}\right) \\
& =\sigma v^{D_{2}}\left(f, \frac{\alpha}{j}\right) \\
& =\sigma v^{D_{2}} \circ \sigma F_{1}(j)(f, \alpha)
\end{aligned}
$$
- For a horizontal map $\varphi: D_{1} \rightarrow D_{2}$, we give the square

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-213.jpg?height=257&width=534&top_left_y=343&top_left_x=839)

defined by

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-213.jpg?height=409&width=922&top_left_y=728&top_left_x=645)

In short:

$$
\sigma v^{\varphi}(f, \alpha):=\left(v_{0} f, \frac{v_{f}}{v_{1} \alpha}\right)
$$

We will show that this data satisfies the laws of a doubly indexed functor.
- (Vertical Lax Functoriality)As we've taken the natural transformations $\sigma v^{j}$ to be identities, they are functorial since composites of identities are identities.
- (Horizontal functoriality) For composable horizontal maps $\varphi_{1}: D_{1} \rightarrow D_{2}$ and $\varphi_{2}: D_{2} \rightarrow D_{3}$, we may calculate:

$$
\begin{aligned}
& \left(\frac{\mu_{\varphi_{1}, \varphi_{2}}^{\sigma F_{1}}}{\sigma v^{\varphi_{1} \mid \varphi_{2}}}\right)\left(\left(f_{1}, \alpha_{1}\right),\left(f_{2}, \alpha_{2}\right)\right)=\left(v_{1}\left(f_{1} \mid f_{2}\right), \frac{v_{f_{1} \mid f_{2}}}{v_{1}\left(\alpha_{1} \mid \alpha_{2}\right)}\right) \\
& =\left(v_{1} f_{1} \mid v_{1} f_{2}, \frac{v_{f_{1}} \mid v_{f_{2}}}{v_{1} \alpha_{1} \mid v_{1} \alpha_{2}}\right) \\
& =\left(v_{1} f_{1}\left|v_{1} f_{2}, \frac{v_{f_{1}}}{v_{1} \alpha_{1}}\right| \frac{v_{f_{2}}}{v_{1} \alpha_{2}}\right) \\
& =\left(\frac{v^{\varphi_{1}} \mid v^{\varphi_{2}}}{\mu_{v_{1} \varphi_{1}, v_{2} \varphi_{2}}^{\sigma F_{2}}}\right)\left(\left(f_{1}, \alpha_{1}\right),\left(f_{2}, \alpha_{2}\right)\right) .
\end{aligned}
$$
- (Functorial Interchange) Consider a square

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-213.jpg?height=255&width=271&top_left_y=2266&top_left_x=968)

Since $\sigma v^{j_{1}}$ and $\sigma v^{j_{2}}$ are identities, we just need to show that

$$
\frac{\sigma F_{1}(\beta)}{\sigma v^{\varphi_{2}}}=\frac{\sigma v^{\varphi_{1}}}{\sigma F_{2}\left(v_{1} \beta\right)}
$$

To that end, we calculate:

$$
\begin{aligned}
\left(\frac{\sigma F_{1}(\beta)}{\sigma v^{\varphi_{2}}}\right)(f, \alpha) & =\sigma v^{\varphi_{2}}\left(f, \frac{\alpha}{\beta}\right) \\
& =\left(v_{1} f, \frac{v_{f}}{v_{1}\left(\frac{\alpha}{\beta}\right)}\right) \\
& =\left(v_{1} f, \frac{v_{f}}{v_{1} \alpha}\right. \\
& =\sigma F_{2}\left(v_{1} \beta\right)\left(v_{1} f, \frac{v_{f}}{v_{1} \alpha}\right) \\
& =\left(\frac{\sigma v \varphi_{1}}{F_{2}\left(v_{1} \beta\right)}\right)(f, \alpha) .
\end{aligned}
$$

We now finish the proof of Theorem 4.4.4.5.

Lemma 4.4.4.7. The assignment $\left(v_{0}, v_{1}, v\right) \mapsto\left(v_{1}, \sigma v\right)$ defined in Proposition 4.4.4.6 is functorial.

Proof. Let $\left(v_{0}, v_{1}, v\right): F_{1} \rightarrow F_{2}$ and $\left(w_{0}, w_{1}, w\right): F_{2} \rightarrow F_{3}$ be maps in DblFun. We will show that

$$
\left(v_{1} \circ w_{1}, \sigma(v * w)\right)=\left(v_{1}, \sigma v\right) \circ\left(w_{1}, \sigma w\right) .
$$

The first components of these pairs are equal by definition, so we just need to show that $\sigma(v * w)=\sigma v \circ \sigma w$.
- This calculation is the same as for a general horizontal.
- For a vertical $j: D_{1} \rightarrow D_{2}$, we calculate, we note that both sides are the same identity natural transformation.
- For a horizontal $\varphi: D_{1} \rightarrow D_{2}$, we calculate:

$$
\begin{aligned}
& \sigma(v * w)^{\varphi}(f, \alpha):=\left(w_{0} v_{0} f, \frac{(v * w)_{f}}{w_{1} v_{1} \alpha}\right) \\
& =\left(w_{0} v_{0} f, \frac{w_{v_{0} f}}{\frac{w_{1} v_{f}}{w_{1} v_{1} \alpha}}\right) \\
& =\left(w_{0} v_{0} f, \frac{w_{v_{0} f}}{w_{1}\left(\frac{v_{f}}{v_{1} \alpha}\right)}\right)
\end{aligned}
$$

$$
\begin{aligned}
& =\sigma w^{v_{1} \varphi}\left(v_{0} f, \frac{v_{f}}{v_{1} \alpha}\right) \\
& =\left(\sigma v^{\varphi}\right) \stackrel{\circ}{q}\left(\sigma w^{v_{1} \varphi}\right)(f, \alpha) .
\end{aligned}
$$

\subsection*{4.5 Change of systems theory}

We have learned about a variety of systems theory in this book:
- There are the deterministic systems theory (Definition 3.5.1.1)

$$
\left(\mathbf{C t x}_{-}: e^{\mathrm{op}} \rightarrow \text { Cat, } \phi \mapsto \phi \circ \pi_{2}\right)
$$

which may be defined for any cartesian category $C$. While we have focused so far on the case $C=$ Set, many other cartesian categories are of interest in the study of deterministic dynamical systems. For example, in ergodic theory we most often use the category of measureable spaces and measurable functions. ${ }^{2}$ We often assume the dynamics of the systems are not arbitary set maps, but are furthermore continuous or differentiable; this means working in the cartesian categories of topological spaces or differentiable manifolds.
- There are also the differential systems theories (Definitions 3.5.2.1 and 3.5.2.23) where the tangent bundle plays an important role. There are also non-standard differential systems theories arising from cartesian differential categories [CC17] and tangent categories with display maps [CC14].
- There are the non-deterministic systems theories for any commutative monad $M$ on a cartesian category $C$. As we saw in Chapter 2, by varying the monad $M$ we can achieve a huge variety of flavors of non-determinism. This includes possibilistic and stochastic non-determinism, but also other variants like systems with cost-sensitive transitions and (Definition 2.3.0.7).

These are just large classes of systems theories that have been easy to describe in generality. Different particular situations will require different particular systems theories. For example, we may decide to restrict the sorts of maps appearing in our systems theories by changing the base $C$ as in Section 3.6. There may also be systems theories constructed by hand for particular purposes, such as ergodic theory.

These systems theories are not isolated from each other. We have seen already in Section 3.6 that some systems theories may be formed by restricting others. There are also some apparent inclusions of systems theories that are not explained by restriction; for example, the Euclidean differential systems theories is a special case of the general differential systems theories. We should be able to think of Euclidean differential systems and general differential systems without too much hassle, and we should
\footnotetext{
${ }^{2}$ We most often consider maps which preserve a specific measure on a space as well, but the category of such measure preserving maps is not cartesian. Often one needs to go and twiddle these general definitions of systems theory in particular cases to suit the particular needs of a subject.
}
be able to apply theorems that pertain to general differential systems to Euclidean ones. Another example of inclusion of systems theories is of deterministic systems into non-deterministic systems of any flavor.

There are also more drastic ways to change systems theories. Any map of commutative monads $\phi: M \rightarrow N$ gives us a way of changing an $M$-system into an $N$-system, changing the flavor of non-determinism. We may also approximate a differential system by a deterministic system.

These are all ways of changing our systems theories, and it is these changes of systems theories that we will attend to in this section. We will begin by defining a change of systems theory, which will give us a category of systems theories. We will then show that forming the doubly indexed category of systems Sys $(\mathbb{T})$ is functorial in the systems theories $\mathbb{T}$.

\subsection*{4.5.1 Definition}

Let's recall the informal and formal definitions of theories of dynamical systems.

The informal definition is that a systems theory is a way to answer a series of questions about what it means to be a dynamical system.

Informal Definition 4.5.1.1. A theory of dynamical systems is a particular way to answer the following questions about what it means to be a dynamical system:

1. What does it mean to be a state?

2. How should the output vary with the state - discretely, continuously, linearly?

3. Can the kinds of input a system takes in depend on what it's putting out, and how do they depend on it?

4. What sorts of changes are possible in a given state?

5. What does it mean for states to change.

6. How should the way the state changes vary with the input?

This informal definition is captured by the sparse, formal definition that a systems theory is a pair consisting of an indexed category $\mathcal{A}: C^{\mathrm{op}} \rightarrow$ Cat together with a section $T$. The various questions correspond to the choices one can make when defining such a pair.

To change a systems theory, then, means to change our answers to these questions. We want to enact this change by some formulated process. For example, if what it means to be a state is a to be a vector in Euclidean space, and we would like to change this to instead answer that to be a state means to be an element of an abstract set, then we want a way of taking Euclidean spaces and producing an abstract set.

Now, we can't just fiddle arbitrarily with the answers to our questions; they all have to hang together in a coherent way. The formal definition can guide us to what sort of changes we can make that cohere in just this way. For example, we can change what it means to be a state, how the output varies with the state, and the way the inputs vary by changing the indexed category $\mathcal{A}$.

Suppose that $\left(\mathcal{A}, T_{1}\right)$ and $\left(\mathscr{B}, T_{2}\right)$ are dynamical system systems theories. If we have an indexed functor (Definition 2.7.0.1) $(F, \bar{F}): \mathcal{A} \rightarrow \mathscr{B}$ between indexed categories,

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-217.jpg?height=87&width=1317&top_left_y=344&top_left_x=336)

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-217.jpg?height=149&width=645&top_left_y=465&top_left_x=732)

That is, we have changed what it means to be a state (FStates), how the output varies

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-217.jpg?height=56&width=1440&top_left_y=693&top_left_x=337)
a dynamical system, however, since since its domain is not $\left(\begin{array}{c}T_{2} F \text { States } \\ F S t a t e s\end{array}\right)$. In order for us to get a $\left(\mathscr{B}, T_{2}\right)$-system, we need to say how to change the what it means for a state to change.

The most direct way to produce a $\left(\mathscr{B}, T_{2}\right)$-system would be to compose with a map

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-217.jpg?height=59&width=1440&top_left_y=976&top_left_x=337)
already by $\bar{F}$ ), and get a $T_{2}$ change (for the re-interpretation of state by $F$ ). Indeed, if we considered this map $\phi$ as a lens $\left(\begin{array}{c}\phi \\ \text { id }\end{array}\right):\left(\begin{array}{c}T_{2} F \text { States } \\ F \text { States }\end{array}\right) \leftrightarrows\left(\begin{array}{c}\bar{F} T_{1} \text { States } \\ F \text { States }\end{array}\right)$, we may form the composite

$$
\left(\begin{array}{c}
\bar{F} \text { update }_{\mathrm{S}} \\
\text { Fexpose }_{\mathrm{S}}
\end{array}\right):\left(\begin{array}{c}
\phi \\
\mathrm{id}
\end{array}\right):\left(\begin{array}{c}
T_{2} \text { FStates } \\
\text { FStates }_{\mathrm{S}}
\end{array}\right) \leftrightarrows\left(\begin{array}{c}
\bar{F} \text { In }_{\mathrm{S}} \\
\text { FOut }_{\mathrm{S}}
\end{array}\right)
$$

This is a $\left(\mathscr{B}, T_{2}\right)$-system, and this process is how we may use a change of systems theories to turn $\left(\mathcal{A}, T_{1}\right)$-systems into $\left(\mathcal{B}, T_{2}\right)$-systems.

We therefore arrive at the following formal defintion of change of systems theory.

Definition 4.5.1.2. Let $\left(\mathcal{A}: C \rightarrow\right.$ Cat, $\left.T_{1}\right)$ and $\left(\mathscr{B}: \mathscr{D} \rightarrow\right.$ Cat, $\left.T_{2}\right)$ be theories of dynamical systems. A change of systems theories $((F, \bar{F}), \phi):\left(\mathcal{A}, T_{1}\right) \rightarrow\left(\mathscr{B}, T_{2}\right)$ consists of:
- An indexed functor $(F, \bar{F}): \mathcal{A} \rightarrow \mathcal{B}$.
- A transformation of sections $\phi: \bar{F} T_{1} \rightarrow T_{2} F$, which consists of a family of maps $\phi_{C}: \bar{F} T_{1} C \rightarrow T_{2} F C$ for each $C$ in $C$, satisfying the following naturality condition:
- For any $f: C \rightarrow C^{\prime}$, we have that the following square commutes in $\mathcal{B}(F C)$ :

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-217.jpg?height=241&width=464&top_left_y=2037&top_left_x=909)

We can package the transformation of sections into a natural transformation, which will make it easier to work with theoretically.

Proposition 4.5.1.3. The data of a transformation of sections as in Definition 4.5.1.2 is equivalent to the data of a natural transformation $\left(\begin{array}{c}\phi \\ \mathrm{id}\end{array}\right):\left(\begin{array}{c}\bar{F} \\ F\end{array}\right) \circ T_{1} \Rightarrow\left(\begin{array}{c}T_{2}(-) \\ (-)\end{array}\right) \circ F$ which acts as the identity on $F$ on its bottom component. We can express this condition with the following equation on diagrams of natural transformations:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-218.jpg?height=444&width=805&top_left_y=507&top_left_x=646)

Remark 4.5.1.4. We note that the components of the natural transformation $\left(\begin{array}{c}\phi \\ \text { id }\end{array}\right)$ here are charts and not lenses. We will, however, exploit the duality between lenses and charts whose lower component are identities.

Proof. That the transformation $\left(\begin{array}{c}\phi \\ \mathrm{id}\end{array}\right)$ acts as the identity on $F$ means that it is determined by its top map $\phi$. We can then see that the naturality square for $\phi$ is precisely the square given in Definition 4.5.1.2.

Every restriction (from Section 3.6) is a change of systems theory.

Proposition 4.5.1.5. Let $\mathbb{T}=\left(\mathcal{A}: C^{\mathrm{op}} \rightarrow\right.$ Cat, $\left.T\right)$ be a systems theory, and let $F: \mathscr{D} \rightarrow C$ be a functor. Then there is a change of systems theory $((F, \mathrm{id}), \mathrm{id}): \mathbb{T}_{\mid F} \rightarrow \mathbb{T}$ from the restriction $\mathbb{T}_{F}=\left(\mathcal{A} \circ F^{\mathrm{op}}, T \circ F\right)($ Definition 3.6.0.1) of $\mathbb{T}$ by $F$ to $\mathbb{T}$.

Proof. By definition, $(F, \mathrm{id}): \mathcal{A} \rightarrow\left(\mathcal{A} \circ F^{\mathrm{op}}\right)$ is an indexed functor. Since, by Proposition 4.5.1.3, the data of a transformation of sections is the same as a natural transformation of a certain sort, we may take that transformation to be the identity.

There are, however, more interesting changes of systems theory. For example, every morphism of commutative monads gives rise to a change of systems theory.

Proposition 4.5.1.6. Let $\phi: M \rightarrow N$ be a morphism of commutative monads on a cartesian category $C$. Then there is a change of systems theory given by

$$
\left(\left(\mathrm{id}, \phi_{*}\right), \mathrm{id}\right): \mathbb{N O N D E T}_{M} \rightarrow \mathbb{N O N D E T}_{N}
$$

Proof. We constructed the indexed functor (id, $\phi_{*}$ ) : $\mathbf{C t x}_{-}^{M} \rightarrow \mathbf{C t x}_{-}^{N}$ in Proposition 2.7.0.3. It remains to show that the following square of functors commutes, so that we may
take the transformation of sections to be the identity:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-219.jpg?height=263&width=490&top_left_y=340&top_left_x=812)

Let $f: C^{\prime} \rightarrow C$ be a map in $C$. Then $T^{M} f$ is $\pi_{2}: f \circ \eta^{M}: C^{\prime} \times C^{\prime} \rightarrow M C$ and $T^{N} f$ is $\pi_{2} \circ f \circ \eta^{N}: C^{\prime} \times C^{\prime} \rightarrow N C$. Now, $\phi_{*} T^{M} f$ is $\pi_{2}: f \circ \eta^{M} \circ \phi_{C}$, but by the unit law for morphisms of commutative monads, $\eta^{M} \circ \phi_{C}=\eta^{N}$. So the square commutes and we can take the transformation of sections to be the identity.

Example 4.5.1.7. For any commutative monad $M: C \rightarrow C$, there is a unique commutative monad map from the identity monad $\mathrm{id}_{C}$. Therefore, Proposition 4.5.1.6 gives us a change of systems theory $\mathbb{D E T}_{C} \rightarrow \mathbb{N O N D E T}_{M}$ which lets us interpret deterministic systems as special cases of non-deterministic systems.

Example 4.5.1.8. Proposition 2.5.0.3 constructs a commutative monad morphism $\phi$ : $\mathrm{D} \rightarrow \mathrm{P}$ sending a probability distribution to the set of elements with non-zero probability. Therefore, Proposition 4.5.1.6 gives us a change of systems theory $\mathbb{N O N D E T}_{\mathrm{D}} \rightarrow$ $\mathbb{N O N D E T P}$ which reinterprets a probabilistic system as a possibilistic one where the state $s^{\prime}$ is possibly the udpate $\phi_{*}$ update $_{S}(s, i)$ of state $s$ with input $i$ just when just when the probability update $s_{s}(s, i)\left(s^{\prime}\right)$ that $s$ will transition to $s^{\prime}$ on input $i$ is non-zero.

We may also describe changes of systems theories between various sorts of deterministic systems theory.

Proposition 4.5.1.9. Let $F: C \rightarrow \mathscr{D}$ be a cartesian functor between cartesian categories. Then there is a change of systems theory

$$
((F, \bar{F}), \mathrm{id}): \mathbb{D E T}_{C} \rightarrow \mathbb{D E T}_{\mathscr{D}}
$$

from the deterministic systems theoy in $C$ to the cartesian systems theory in $\mathscr{D}$.

Proof. We need to construct the indexed functor $(F, \bar{F})$, and then prove that the square

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-219.jpg?height=277&width=490&top_left_y=2244&top_left_x=812)
commutes, so that we may take the transformation of sections to be the identity.

We begin first by constructing $\bar{F}$. We note that since $F$ is cartesian, it extends to a functor

$$
\bar{F}_{C}: \mathbf{C t x}_{C} \rightarrow \mathbf{C t x}_{F C}
$$

by sending $f: C \times X \rightarrow Y$ to $F f: F C \times F X \rightarrow F Y$. It is routine to check that this makes $(F, \bar{F})$ into an indexed functor. In particular, for a map $r: C^{\prime} \rightarrow C$ in $C$, we see that

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-220.jpg?height=72&width=1159&top_left_y=598&top_left_x=472)

Next we check that the square commutes. Let $f: C^{\prime} \rightarrow C$ be a map in $C$. Then $T^{\mathscr{D}} \circ F(f)=\left(\begin{array}{c}\pi_{2} \circ F f \\ F f\end{array}\right)$, while $\left(\begin{array}{c}\bar{F} \\ F\end{array}\right)\left(\begin{array}{c}T f \\ f\end{array}\right)=\left(\begin{array}{c}\bar{F}\left(\pi_{2} \circ f\right) \\ F f\end{array}\right)$. But since $F$ is cartesian, $F\left(\pi_{2}\right)=\pi_{2}$, so these are equal.

Example 4.5.1.10. Proposition 4.5.1.9 gives us a number of trivial ways to change the flavor of our deterministic systems.

For example, it is obvious that any deterministic dynamical system whose update and expose maps are continuous gives rise to a deterministic dynamcial system without the constraint of continuity, simply by forgetting that the maps are continuous. We formalize this observation by applying Proposition 4.5.1.9 to the forgetful functor $U$ : Top $\rightarrow$ Set which sends a topological space to its underlying set of points.

Similarly, any deterministic dynamical system gives rise to a continuous deterministic dynamical system if we equip all sets involved with the discrete topology. This is formalized by applying Proposition 4.5.1.9 to the functor disc : Set $\rightarrow$ Top which equips a set with the discrete topology.

The most interesting examples of changes of systems theory are the ones which move between different sorts of systems theory, such as from differential to deterministic. An example of this is the Euler approximation, which takes a Eulidean differential system to a deterministic system.

Let's take a minute to recall the Euler method. If $\left(\begin{array}{l}u \\ r\end{array}\right):\left(\begin{array}{l}\mathbb{R}^{n} \\ \mathbb{R}^{n}\end{array}\right) \leftrightarrows\left(\begin{array}{l}\mathbb{R}^{k} \\ \mathbb{R}^{m}\end{array}\right)$ is a differential system representing the differential equation

$$
\frac{d s}{d t}=u(s, i)
$$

then for a sufficiently small $\varepsilon>0$, the state at time $t+\varepsilon$ will be roughly

$$
s(t+\varepsilon) \approx s(t)+\varepsilon \cdot u(s(t), i(t)) .
$$

Choosing a specific $\varepsilon$ as a time increment, we can define a discrete time, deterministic system by

$$
\begin{equation*}
\mathcal{E}_{\varepsilon} u(s, i)=s+\varepsilon \cdot u(s, i) . \tag{4.6}
\end{equation*}
$$

This simple method of approximating the solution of a differential equation is called the Euler method. We can see the Euler method as a change of systems theory from a differential systems theory to a deterministic systems theory.

Proposition 4.5.1.11. For any $\varepsilon>0$, the Euler method gives rise to a change of systems theory

$$
\mathcal{E}_{\varepsilon}: \mathbb{E}_{\mathrm{UC}}^{\text {Aff }} \rightarrow \mathbb{D}_{\text {ET Euc }}
$$

This is given by

$$
((\iota, \iota), \phi):\left(\mathbf{C t x}_{\mid \text {Aff }}: \text { Aff }^{\text {op }} \rightarrow \text { Cat }, T\right) \rightarrow\left(\text { Ctx }_{\mid \text {Euc }}: \text { Euc }^{\text {op }} \rightarrow \text { Cat, } \mathbb{R}^{n} \mapsto \mathbb{R}^{n}\right)
$$

where $\iota:$ Aff $\rightarrow$ Euc is the inclusion and $\phi: \mathbb{R}^{n} \times \mathbb{R}^{n} \rightarrow \mathbb{R}^{n}$ is defined by

$$
\phi(s, v)=s+\varepsilon \cdot v
$$

Proof. We note, first of all, that composing with $\phi$ gives us the correct formula for the Euler approximation. Explicitly,

$$
\phi \circ u(s, i)=s+\varepsilon \cdot u(s, i),
$$

which was the definition for $\mathscr{E}_{\varepsilon} u$ in Eq. (4.6).

All that we need to show is that $\phi$ is a transformation of sections. This means that the following square commutes for any affine $f: \mathbb{R}^{n} \rightarrow \mathbb{R}^{m}$ :

$$
\begin{aligned}
& \left(\begin{array}{c}
\mathbb{R}^{n} \\
\mathbb{R}^{n}
\end{array}\right) \xrightarrow{\left(\begin{array}{c}
\pi_{2} \circ f \\
f
\end{array}\right)}\left(\begin{array}{l}
\mathbb{R}^{m} \\
\mathbb{R}^{m}
\end{array}\right) \\
& \left(\begin{array}{c}
\phi \\
\mathrm{id}
\end{array}\right) \downarrow \uparrow \quad \downarrow \uparrow\left(\begin{array}{c}
\phi \\
\mathrm{id}
\end{array}\right) \\
& \left(\begin{array}{l}
\mathbb{R}^{n} \\
\mathbb{R}^{n}
\end{array}\right) \underset{\left(\begin{array}{c}
T f \\
f
\end{array}\right)}{\Longrightarrow}\left(\begin{array}{c}
\mathbb{R}^{m} \\
\mathbb{R}^{m}
\end{array}\right)
\end{aligned}
$$

The bottom component of this square commutes trivially. The top component comes down to the equation

$$
\begin{equation*}
f(s+\varepsilon \cdot v)=f(s)+\varepsilon T f(s, v) \tag{4.7}
\end{equation*}
$$

which says that incrementing $s$ by $\varepsilon$ in the $v$ direction in $f$ is the same as incrementing $f(s)$ by the $\varepsilon$ times the directional derivative of $f$ in the $v$ direction. This is true for affine functions; even more, it characterizes affine functions, so that we see that we must assume that $f$ is affine for this square to commute.

Remark 4.5.1.12. It would be very interesting to have a theory which allowed us to speak of "approximate" changes of systems theory. If we plug a function $f: \mathbb{R}^{n} \rightarrow \mathbb{R}^{m}$ into the above formulas for the Euler method, then we find that Eq. (4.7) only holds up to $O\left(\varepsilon^{2}\right)$. For affine functions, this means that it does hold, which is why we restrict to affine functions. But it would be interesting to have a theory which could account for
how these approximate equalities affected the various compositionality results all the way down.

In the upcoming Section 4.5.2, we will see what knowing that the Euler method is a change of systems theory lets us conclude about the behaviors and compositionality of Euler method approximations.

Considering systems theories together with their changes gives us a category Theory.

Definition 4.5.1.13. The category Theory has as objects the theories of dynamical systems and as morphisms the changes of theories.

If $\left(\left(F_{1}, \bar{F}_{1}\right), \phi_{1}\right):\left(\mathcal{A}_{1}, T_{1}\right) \rightarrow\left(\mathcal{A}_{2}, T_{2}\right)$ and $\left(\left(F_{2}, \bar{F}_{2}\right), \phi_{2}\right):\left(\mathcal{A}_{2}, T_{2}\right) \rightarrow\left(\mathcal{A}_{3}, T_{3}\right)$ are changes of systems theories, then their composite is defined to be

$$
\left.\left(\left(F_{1}, \bar{F}_{1}\right), \phi_{1}\right) \stackrel{(}{ }\left(\left(F_{2}, \bar{F}_{2}\right), \phi_{2}\right):=\left(\left(F_{1}, \bar{F}_{1}\right) \stackrel{\left(F_{2}\right.}{ }, \bar{F}_{2}\right), \phi_{1} * \phi_{2}\right)
$$

where $\phi_{1} * \phi_{2}$ is the transformation of sections given by

$$
\left(\phi_{1} * \phi_{2}\right)_{C}:=\bar{F}_{2} F_{1} T_{1} C \xrightarrow{\bar{F}_{2} \phi_{1}} \overline{F_{2}} T_{2} F_{1} C \xrightarrow{\left(\phi_{2}\right)_{F_{1} C}} T_{3} F_{2} F_{1} C .
$$

In terms of natural transformations (see Proposition 4.5.1.3), this is the diagram

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-222.jpg?height=399&width=878&top_left_y=1313&top_left_x=621)

\subsection*{4.5.2 Functoriality}

We use changes of systems theories to turn a system of one sort into a system of another sort. We sketched how this process goes above, but for good measure let's revisit it.

Definition 4.5.2.1. Let $\mathbb{F}=((F, \bar{F}), \phi):\left(\mathcal{A}, T_{1}\right) \rightarrow\left(\mathcal{B}, T_{2}\right)$ be a change of systems theory, and let

$$
\mathrm{S}=\left(\begin{array}{c}
\text { update }_{\mathrm{S}} \\
\text { expose }_{\mathrm{S}}
\end{array}\right):\left(\begin{array}{c}
T_{1} \text { State }_{\mathrm{S}} \\
\text { State }_{\mathrm{S}}
\end{array}\right) \leftrightarrows\left(\begin{array}{c}
\ln _{\mathrm{S}} \\
\text { Out }_{\mathrm{S}}
\end{array}\right)
$$
be a $\left(\mathcal{A}, T_{1}\right)$-system. Then we have a $\left(\mathcal{B}, T_{2}\right)$-system $\mathbb{F S}$ defined to be the composite

$$
\left(\begin{array}{c}
\phi \\
\mathrm{id}
\end{array}\right):\left(\begin{array}{l}
\bar{F} \text { update }_{\mathrm{S}} \\
\text { Fexpose }_{\mathrm{S}}
\end{array}\right):\left(\begin{array}{c}
T_{2} \text { FStates } \\
\text { FStates }
\end{array}\right) \leftrightarrows\left(\begin{array}{c}
\bar{F} \text { In }_{\mathrm{S}} \\
\text { FOuts }
\end{array}\right)
$$

Explicitly, this system has update map $\bar{F}$ update $_{S} \stackrel{ }{\circ} \phi$ and expose map $F$ expose $_{S}$.

The goal of this section will be to provide a number of compositionality results concerning how changing the theory of a system relates to wiring systems together and to behaviors. Specifically, we will prove the following theorem:

Theorem 4.5.2.2. There is a functor

$$
\text { Sys : Theory } \rightarrow \text { DblIx }
$$

sending a theory of dynamical systems $\mathbb{T}$ to the doubly indexed category Sys $_{\mathbb{T}}$ (Definition 4.3.0.2) of systems in it.

This functor sends a change of systems theory $\mathbb{F}: \mathbb{T}_{1} \rightarrow \mathbb{T}_{2}$ to the doubly indexed functor $\operatorname{Sys}\left(\mathbb{T}_{1}\right) \rightarrow \operatorname{Sys}\left(\mathbb{T}_{2}\right)$ which sends a $\mathbb{T}_{1}$-system $S$ to the $\mathbb{T}_{2}$-system $\mathbb{F S}$ from Definition 4.5.2.1.

We will prove this theorem using the vertical slice construction. Recall that the doubly indexed category $\operatorname{Sys}(\mathbb{T})$ is the vertical slice construction of the section $T$ considered as a double functor $\left(h T: h C \rightarrow\right.$ Arena $_{\mathbb{T}}$ ) (Proposition 4.4.2.2). This means that if we can show that the assignment

$$
\left(\mathcal{A}: e^{\text {op }} \rightarrow \text { Cat, } T\right) \mapsto\left(h T: h C \rightarrow \operatorname{Arena}_{(\mathcal{A}, T)}\right)
$$

gives a functor Theory $\rightarrow$ DblFun, then we can compose this with the vertical slice construction $\sigma:$ DblFun $\rightarrow$ DblIx. This is what we will focus on.

Lemma 4.5.2.3. The assignment

$$
\left(\mathcal{A}: e^{\text {op }} \rightarrow \text { Cat, } T\right) \mapsto\left(h T: h C \rightarrow \operatorname{Arena}_{(\mathcal{A}, T)}\right)
$$

gives a functor $\iota$ : Theory $\rightarrow$ DblFun. This functor sends a change of systems theories

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-224.jpg?height=383&width=490&top_left_y=524&top_left_x=812)

to the morphism double functors

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-224.jpg?height=368&width=509&top_left_y=1025&top_left_x=800)

Proof. With all that we have set up, there is not too much to prove here. We first note that the the functoriality of the assignment $\mathcal{A} \mapsto$ Arena $_{\mathcal{A}}$ was proven in Proposition 4.4.1.6. We only need to focus on the vertical transformation.

We need to show that $\left(\begin{array}{c}\phi \\ \mathrm{id}\end{array}\right)$ may be interpreted as a vertical transformation $h T_{2} \circ F \rightarrow$ $\left(\begin{array}{l}\bar{F} \\ F\end{array}\right) \circ h T_{1}$. There is some subtlety here; in Eq. (4.8), $\left(\begin{array}{c}\phi \\ \mathrm{id}\end{array}\right)$ is interpreted as a natural transformation taking place in the category of $\mathscr{B}$-charts, while in Eq. (4.9) we have a vertical transformation in the double category of arenas. But the vertical arrows in Arena $_{\left(\mathscr{B}, T_{2}\right)}$ are $\mathscr{B}$-lenses, not $\mathscr{B}$-charts. This explains the change of direction: we can consider the chart $\left(\begin{array}{c}\phi \\ \mathrm{id}\end{array}\right):\left(\begin{array}{c}\bar{F} \\ F\end{array}\right) \circ h T_{1} \rightarrow h T_{2} \circ F$ as a lens $\left(\begin{array}{c}\phi \\ \mathrm{id}\end{array}\right): h T_{2} \circ F \rightarrow\left(\begin{array}{c}\bar{F} \\ F\end{array}\right) \circ h T_{1}$ by the duality between pure charts and pure lenses. Recall that pure charts and lenses are those having an isomorphism in the bottom component (Definition 2.6.1.7).

Let's describe precisely how $\left(\begin{array}{c}\phi \\ \mathrm{id}\end{array}\right)$ becomes a vertical transformation.
- For every $C \in h C$, we have the lens $\left(\begin{array}{c}\phi \\ i d\end{array}\right):\left(\begin{array}{c}T_{2} F C \\ F X\end{array}\right) \rightarrow\left(\begin{array}{c}\bar{F} T_{1} C \\ F F\end{array}\right)$.
- For every horizontal arrow $f: C^{\prime} \rightarrow C$ in $h C$ (which is to say, any map in $C$ ), we
have the square

$$
\begin{align*}
& \left(\begin{array}{c}
T_{2} F C^{\prime} \\
F C^{\prime}
\end{array}\right) \xrightarrow{\left(\begin{array}{c}
T_{2} F f \\
F f
\end{array}\right)}\left(\begin{array}{c}
T_{2} F C \\
F C
\end{array}\right) \\
& \left(\begin{array}{c}
\phi \\
\mathrm{id}
\end{array}\right) \downarrow \uparrow  \tag{4.10}\\
& \left(\begin{array}{c}
F \\
\bar{F} T_{1} C^{\prime} \\
F C^{\prime}
\end{array}\right) \underset{\left(\begin{array}{c}
\phi \\
\mathrm{F} T_{1} f \\
F f
\end{array}\right)}{\rightrightarrows}\left(\begin{array}{c}
\bar{F} T_{1} C \\
F C
\end{array}\right)
\end{align*}
$$

in $\operatorname{Arena}_{\left(\mathscr{B}, T_{2}\right)}$. This is a square because both its top and bottom component squares commute; the bottom one trivially, and the top one by the defining Eq. (4.5) of $\phi$. We now check that this satisfies the laws of a vertical transformation. It is largely trivial, since the double categories are particularly simple (as double categories).
- (Vertical Naturality) By construction, the only vertical arrows in $h C$ are identities, so there is nothing to check.
- (Horizontal Naturality) Since Arena is thin (Definition 3.4.1.2), any two squares with the same signature are equal, so there is nothing to check.
- (Horizontal Unity) This is true since all the functors involved in defining the top and bottom of the square Eq. (4.10) preserve identities.
- (Square Naturality) This again follows trivially by the thinness of Arena.

The proof of functoriality itself follows from a straightforward comparison of the two definitions of composition. They simply give the same formula on objects, and on horizontal morphisms we get squares of the same signature in a thin double category so there is nothing more to check.

We can therefore define

$$
\text { Theory } \xrightarrow{\text { Sys }} \text { DblIx }:=\text { Theory } \xrightarrow{\iota} \text { DblFun } \xrightarrow{\sigma} \text { DblIx. }
$$

Let's take a moment to understand this definition in full. Suppose we have a change of systems theories $((F, \bar{F}), \phi):\left(\mathcal{A}, T_{1}\right) \rightarrow\left(\mathcal{B}, T_{2}\right)$. Then $\iota((F, \bar{F}), \phi)$ is a map of double functors:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-225.jpg?height=360&width=493&top_left_y=2151&top_left_x=816)

Then, by Proposition 4.4.4.6, we get a doubly indexed functor

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-226.jpg?height=350&width=477&top_left_y=324&top_left_x=821)

In this diagram, $\sigma\left(\begin{array}{c}\phi \\ \mathrm{id}\end{array}\right)$ is defined as follows:
- (Definition 4.4.4.3: Item 2) For a $\mathbb{T}_{1}$-arena $\left(\begin{array}{l}A^{-} \\ A^{+}\end{array}\right)$, we have the functor

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-226.jpg?height=127&width=666&top_left_y=869&top_left_x=770)

given by sending a simulation $\psi: T \rightarrow S$ to the composite:

$$
\begin{aligned}
& \left(\begin{array}{c}
T_{2} F \text { State }_{\mathrm{T}} \\
\text { FState }_{\mathrm{T}}
\end{array}\right) \xrightarrow{\left(\begin{array}{c}
T_{2} F \psi \\
F \psi
\end{array}\right)}\left(\begin{array}{c}
T_{2} F \text { State }_{\mathrm{S}} \\
\text { FState }_{\mathrm{S}}
\end{array}\right) \\
& \left(\begin{array}{c}
T_{1} \text { State }_{\mathrm{T}} \\
\text { State }_{\mathrm{T}}
\end{array}\right) \xrightarrow{\left(\begin{array}{c}
T_{1} \psi \\
\psi
\end{array}\right)}\left(\begin{array}{c}
T_{1} \text { States }^{\longrightarrow} \\
\text { State }_{\mathrm{S}}
\end{array}\right) \\
& \downarrow \uparrow\left(\begin{array}{c}
\text { update }_{\mathrm{S}} \\
\text { expose }_{\mathrm{S}}
\end{array}\right) \mapsto\left(\begin{array}{c}
\bar{F} T_{1} \text { State }_{\mathrm{T}} \\
\text { State }_{\mathrm{T}}
\end{array}\right) \xrightarrow{\mid F \psi)}\left(\begin{array}{c}
\bar{F} T_{1} \text { State }_{\mathrm{S}} \\
F \text { State }_{\mathrm{S}}
\end{array}\right)
\end{aligned}
$$

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-226.jpg?height=127&width=1309&top_left_y=1492&top_left_x=554)

$$
\begin{aligned}
& \left(\begin{array}{l}
\bar{F} A^{-} \\
F A^{+}
\end{array}\right) \xlongequal{\overline{\bar{k}}}\left(\begin{array}{l}
\bar{F} A^{-} \\
F A^{+}
\end{array}\right)
\end{aligned}
$$
- (Definition 4.4.4.3: Item 3) Since the doubly indexed functor is taut, for any lens $\left(\begin{array}{c}j^{\sharp} \\ j\end{array}\right):\left(\begin{array}{c}A^{-} \\ A^{+}\end{array}\right) \leftrightarrows\left(\begin{array}{c}B^{-} \\ B^{+}\end{array}\right)$have a commuting square

$$
\begin{aligned}
& \mathbf{S y s}_{\mathbb{T}_{1}}\left(\begin{array}{l}
A^{-} \\
A^{+}
\end{array}\right) \xrightarrow{\sigma\left(\begin{array}{c}
\phi \\
\mathrm{id}
\end{array}\right)\left(\begin{array}{l}
A^{-} \\
A^{+}
\end{array}\right)} \mathbf{S y s}_{\mathbb{T}_{2}}\left(\begin{array}{l}
\bar{F} A^{-} \\
F A^{+}
\end{array}\right) \\
& \mathbf{S y s}_{\mathbb{T}_{1}}\left(\begin{array}{c}
j^{\sharp} \\
j
\end{array}\right) \downarrow \quad \downarrow \mathbf{S y s}_{\mathbb{T}_{2}}\left(\begin{array}{c}
\bar{F} j^{\sharp} \\
F j
\end{array}\right) \\
& \mathbf{S y s}_{\mathbb{T}_{1}}\left(\begin{array}{c}
B^{-} \\
B^{+}
\end{array}\right) \xrightarrow[\left(\begin{array}{c}
\phi \\
\mathrm{id}
\end{array}\right)\left(\begin{array}{l}
B^{-} \\
B^{+}
\end{array}\right)]{\longrightarrow} \mathbf{S y s}_{\mathbb{T}_{2}}\left(\begin{array}{l}
\bar{F} B^{-} \\
F B^{+}
\end{array}\right)
\end{aligned}
$$

This tells us that changing systems theories and then wiring together systems gives the same result as wiring together the systems first and then changing systems theories.
- (Definition 4.4.4.3: Item 4) For a $\mathbb{T}_{1}$-chart $\left(\begin{array}{l}f_{\mathrm{b}} \\ f\end{array}\right):\left(\begin{array}{l}A^{-} \\ A^{+}\end{array}\right) \rightrightarrows\left(\begin{array}{l}B^{-} \\ B^{+}\end{array}\right)$, we have the square in Cat

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-227.jpg?height=561&width=927&top_left_y=346&top_left_x=640)

given by sending a $\left(\begin{array}{c}f_{\mathrm{b}} \\ f\end{array}\right)$-behavior to the composite:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-227.jpg?height=507&width=746&top_left_y=1156&top_left_x=424)

$$
\begin{aligned}
& \left(\begin{array}{c}
T_{2} \text { FState }_{\mathrm{T}} \\
\text { FState }_{\mathrm{T}}
\end{array}\right) \xrightarrow{\left(\begin{array}{c}
T_{2} F \psi \\
F \psi
\end{array}\right)}\left(\begin{array}{c}
T_{2} \text { FStates } \\
\text { FState }_{\mathrm{S}}
\end{array}\right) \\
& \left(\begin{array}{c}
\phi \\
\mathrm{id}
\end{array}\right) \downarrow \uparrow \quad\left(\bar{F} T_{1} \psi\right) \quad \downarrow\left(\begin{array}{c}
\phi \\
\mathrm{id}
\end{array}\right) \\
& \left(\begin{array}{c}
\bar{F} T_{1} \text { State }_{\mathrm{T}} \\
\text { State }_{\mathrm{T}}
\end{array}\right) \xrightarrow{(F \psi)}\left(\begin{array}{c}
\bar{F} T_{1} \text { State }_{\mathrm{S}} \\
\text { State }_{\mathrm{S}}
\end{array}\right) \\
& \left(\begin{array}{l}
\bar{F} \text { update }_{\mathrm{T}} \\
\text { Fexpose }_{\mathrm{T}}
\end{array}\right) \downarrow \uparrow \downarrow\left(\begin{array}{l}
\bar{F} \text { update }_{\mathrm{S}} \\
\text { Fexpose }_{\mathrm{S}}
\end{array}\right) \\
& \left(\begin{array}{l}
\bar{F} A^{-} \\
F A^{+}
\end{array}\right) \underset{\left(\begin{array}{c}
\bar{F} f_{b} \\
F f
\end{array}\right)}{\Longrightarrow}\left(\begin{array}{c}
\bar{F} B^{-} \\
F B^{+}
\end{array}\right)
\end{aligned}
$$

In other words, changes of systems theory preserve behavior in the sense that if $\psi$ is a $\left(\begin{array}{c}f_{\mathrm{b}} \\ f\end{array}\right)$-behavior then $F \psi$ is a $\left(\begin{array}{c}\bar{F} f_{b} \\ f\end{array}\right)$-behavior.

Example 4.5.2.4. For Euler approximatation

$$
\mathcal{E}_{\varepsilon}: \mathbb{E}_{\mathrm{UC}}^{\text {|Aff }} \rightarrow \mathbb{D}_{\text {ET Euc }}
$$

we get a doubly indexed functor

$$
\left(\left(\begin{array}{l}
\iota \\
\iota
\end{array}\right), \sigma\left(\begin{array}{c}
\phi \\
\mathrm{id}
\end{array}\right)\right):\left(\mathbb{E}_{\mathrm{UC}}^{\mid \text {Aff }}\right) \rightarrow \mathbb{D}_{\mathrm{ET}_{\mathrm{Euc}}}
$$

by the functoriality of Sys, where $\iota:$ Aff $\rightarrow$ Euc is the inclusion and $\phi: \mathbb{R}^{n} \times \mathbb{R}^{m} \rightarrow \mathbb{R}^{n}$ is $\phi(p, v)=p+\varepsilon \cdot v$. Let's see it means for this to be a doubly indexed functor.

First, we have a functor

$$
\sigma\left(\begin{array}{c}
\phi \\
\text { id }
\end{array}\right)\left(\begin{array}{l}
A^{-} \\
A^{+}
\end{array}\right): \mathbf{S y s}_{\mathbb{E}_{\mathrm{UC}}^{\mathrm{Aff}}}\left(\begin{array}{l}
A^{-} \\
A^{+}
\end{array}\right) \rightarrow \mathbf{S y s}_{\mathbb{D}_{\mathrm{ET}}^{\mathrm{Euc}}}\left(\begin{array}{l}
A^{-} \\
A^{+}
\end{array}\right)
$$

This says that the Euler method preserves simulations. Second, we have a square like Eq. (4.12) which says that the Euler method preserves behaviors. However, we have to be careful here; the behaviors $\left(\varphi,\left(\begin{array}{c}f_{\mathrm{b}} \\ f\end{array}\right)\right)$ which are preserved must have $\varphi$ and $\left(\begin{array}{l}f_{\mathrm{b}} \\ f\end{array}\right)$ in the appropriate double category of arenas, and here we had to restrict to those for which $\varphi$ and $f$ are affine maps so that Eq. (4.7) can hold. In other words, we see that the Euler method will preserve any affine behaviors of differential systems.

Most solutions to a system of differential equations - most trajectories - are not affine. This is to say that there aren't many behaviors of shape Time (from Example 3.5.2.5). There is, however, an important class of affine solutions: steady states. These are the behaviors of shape Fix from Example 3.5.2.6. So, in particular, we see that the Euler method preserves steady states.

That the Euler method preserves steady states is of course evident from the formula: if $u(s, i)=0$, then $\mathscr{E}_{\varepsilon} u(s, i)=s+\varepsilon \cdot u(s, i)=s$. But we deduced this fact from our general definition of change of systems theory. This sort of analysis can tell us precisely which sorts of behaviors are preserved even in situtations where it may not be so obvious from looking at a defining formula.

The fact that $\operatorname{Sys}\left(\mathscr{E}_{\varepsilon}\right)$ is a doubly indexed functor gives us a litany of compositionality checks. In particular, the commuting square (Definition 4.4.4.3: Item 3) shows that if we are to wire together a family of differential systems and then approximate the result with the Euler method, we could have approximated each one and then wired together the result with the same wiring pattern.

\subsection*{4.6 Summary and Further Reading}

In this chapter, we organized the systems in a systems theory into doubly indexed categories. While all the action takes place within the double category of arenas, the doubly indexed category of systems separates the systems from their interfaces and the behaviors from their charts. This let's us describe the various sorts of composition - of systems and of behaviors - and their relationships. We then saw how this construction varied as we changed systems theory.

There are other examples of changes of systems theories not covered here. For example, the Rutta-Kunge approximation can be seen as a change of systems theory; see [Ngo17].

\section*{Chapter 5}

\section*{Behaviors of the whole from behaviors of the parts}

\subsection*{5.1 Introduction}

Let's take stock of where we've been so far in the past couple chapters.
- In Section 1.2.1, we saw the definitions of deterministic systems and differential systems.
- In Section 1.3, we learned about lenses. We saw how systems can be interpreted as special sorts of lenses, and how we can wire together systems using lens composition.
- In Chapter 2 we learned about various sorts of non-deterministic systems.
- In Chapter 3, we learned about behaviors and charts. We saw how to define behaviors of systems using the notion of chart. Finally, we gave a formal definition of theory of dynamical systems, systematizing the various different notions discrete, differential, non-deterministic â€” of dynamical systems.

The two sorts of composition we have seen so far - lens composition and chart composition - mirror the two sorts of composition at play in systems theory:
- We can compose systems by wiring them together. This uses lens composition.
- We can compose behaviors of systems like we compose functions. This uses chart composition.

In this chapter, we will see how these two sorts of composition interact. In short, behaviors of component systems give rise to behaviors of composite systems. The way that behaviors of the whole arise from behaviors of the parts is called compositionality. In this chapter, we will prove a general compositionality theorem concerning any representable behavior in any systems theory.

But the behaviors of the component systems must be compatible with eachother: if a system $S_{1}$ has its parameters set by the exposed variables of a system $S_{2}$, then a behavior $\phi_{1}$ of $S_{1}$ will be compatible with a behavior $\phi_{2}$ of $S_{2}$ when $\phi_{2}$ is a behavior
for the parameters charted by the variables exposed by $\phi_{1}$.

We will see that, remarkably, the way behaviors of composite systems arise from behaviors of component systems (including the constraints of compatibility) are described by a "matrix arithmetic for sets". From a lens we will construct a "matrix of sets"; multiplying the "vector of behaviors" of the component systems (indexed by their charts) by this matrix yields the vector of behaviors of the composite. We begin this chapter with a section explaining this idea in detail for steady states of deterministic systems.

We have in fact already developed most of the important definitions - doubly indexed category and lax doubly indexed functor - and proven most of the crucial lemmas we need for this result in Section 4.2. In this chapter, we will then construct representable doubly indexed functors which will organize the various facts concerning the compositionality of any sort of behavior in any systems theory.

\subsection*{5.2 Steady states compose according to the laws of matrix arithmetic}

We have seen how we can compose systems, and we have seen how systems behave. We have seen a certain composition of behaviors, a form of transitivity that says that if we have a T-shaped behavior in $\mathrm{S}$ and a S-shaped behavior in U, then we get a T-shaped behavior in U. But what's the relationship between composing systems and composing their behaviors?

In this section we will give a taste by showing how steady states compose. Later, in Section 5.3, we will see a very abstract theorem that generalizes what we do here for steady states in the deterministic systems theory to something that works for any sort of behavior in any systems theory. But in order for that abstract theorem to make sense, we should first see the concrete case of steady states in detail.

Recall that the chart of a steady state $s \in$ States is the pair $\left(\begin{array}{l}i \\ o\end{array}\right)$ with $o=\operatorname{expose}_{S}(s)$ and $\operatorname{update}_{S}(s, i)=s$. The set of all possible charts for steady states is therefore In $_{\mathrm{S}} \times$ Out $_{\mathrm{S}}$, and for every chart $\left(\begin{array}{l}i \\ o\end{array}\right)$ we have the set Steady $\left(\begin{array}{l}i \\ o\end{array}\right)$ of steady states for this chart.

We can see this function Steady $\mathrm{I}_{\mathrm{S}}$ In $_{\mathrm{S}} \times$ Out $_{\mathrm{S}} \rightarrow$ Set as a matrix of sets with Steady $\left(\begin{array}{l}i \\ 0\end{array}\right)$
in the row $i$ and column $o$. For example, consider system $S_{1}$ of Exercise 1.3.2.7:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-231.jpg?height=463&width=767&top_left_y=327&top_left_x=668)

This has output value set Colors $=\{$ blue, red, green $\}$ and input parameter set Bool $=$ \{true, false\}. Here is its (Colors $\times$ Bool) steady state matrix:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-231.jpg?height=350&width=1426&top_left_y=969&top_left_x=339)

If we just want to know how many $\left(\begin{array}{l}i \\ o\end{array}\right)$-steady states there are, and not precisely which states they are, we can always take the cardinality of the sets in our matrix of sets to get a bona-fide matrix of numbers. Doing this to the above matrix gives us the matrix

$$
\left.\begin{array}{c}
\text { blue } \\
\text { frue }
\end{array} \begin{array}{ccc}
0 & 1 & 0 \\
2 & 0 & 0
\end{array}\right]
$$

Now, let's take a look at system $S_{2}$ from the same exercise:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-231.jpg?height=694&width=830&top_left_y=1835&top_left_x=642)

This has steady state matrix:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-232.jpg?height=493&width=1221&top_left_y=355&top_left_x=406)

Or, again, if we just want to know how many steady states there are for each chart:

$$
\text { Steady }_{\mathrm{S}_{2}}=\underset{\text { green }}{\text { blue }}\left[\begin{array}{cc}
\text { true } & \text { false } \\
2 & 0 \\
1 & 0 \\
0 & 1
\end{array}\right]
$$

We can wire these systems together to get a system S:

$$
S:=\boxed{S_{1}}-\mathrm{S}_{2}
$$

With just a bit of thought, we can find the steady states of this systems without fully calculating its dynamics. A state of $S$ is a pair of states $s_{1} \in$ States $_{1}$ and $s_{2} \in$ States $_{2}$, so for it to be steady both its constituent states must be steady. So let $\left(\begin{array}{l}i \\ o\end{array}\right):\left(\begin{array}{l}1 \\ 1\end{array}\right) \rightrightarrows\left(\begin{array}{l}\text { Bool } \\ \text { Bool }\end{array}\right)$ be a chart for $S-$ a pair of booleans. We need $s_{1}$ and $s_{2}$ to both be steady, so in particular $s_{1}$ must be steady at the input $i$, and $s_{2}$ must expose $o$; but, most importantly, $s_{2}$ must then be steady at the input expose $\mathrm{S}_{1}\left(s_{1}\right)$ which $s_{1}$ exposes.

So, to find the set of $\left(\begin{array}{l}\text { true } \\ \text { true }\end{array}\right)$-steady states of $S$, we must find a state $s_{1}$ of $S_{1}$ which is steady for the input true and then a steady state $s_{2}$ of $S_{2}$ whose input is what $s_{1}$ outputs and whose output is true. There are three pieces of data here: the steady state $s_{1}$ of $S_{1}$, the steady state $s_{2}$ of $S_{2}$, and the intermediate value expose by the first state and input into the second state. We can therefore describe the set of $\left(\begin{array}{l}\text { true } \\ \text { true }\end{array}\right)$-steady states of S like this:

$$
\begin{aligned}
& \text { Steady }_{\mathrm{S}}\left(\begin{array}{c}
\text { true } \\
\text { true }
\end{array}\right)=\left\{\left(m, s_{1}, s_{2}\right) \mid s_{1} \in \text { Steady }_{\mathrm{S}_{1}}\left(\begin{array}{c}
\text { true } \\
m
\end{array}\right), s_{2} \in \operatorname{Steady}_{\mathrm{S}_{2}}\left(\begin{array}{c}
m \\
\text { true }^{2}
\end{array}\right)\right\} \\
& =\sum_{m \in \text { Colors }} \text { Steady }_{\mathrm{S}_{1}}\left(\begin{array}{c}
\text { true } \\
m
\end{array}\right) \times \text { Steady }_{\mathrm{S}_{2}}\left(\begin{array}{c}
m \\
\text { true }
\end{array}\right) .
\end{aligned}
$$

This formula looks very suspiciously like matrix multiplication! Indeed, if we multiply the matrices of numbers of steady states from $S_{1}$ and $S_{2}$, we get:

$$
\left.\begin{array}{c}
\text { frue } \\
\text { false }
\end{array}\left[\begin{array}{lll}
0 & 1 & 0 \\
2 & 0 & 0
\end{array}\right]\left[\begin{array}{cc}
\text { true false } \\
1 & 0 \\
0 & 1
\end{array}\right]=\begin{array}{c}
\text { frue } \\
\text { false }
\end{array} \begin{array}{cc}
\text { false } \\
1 & 0 \\
4 & 0
\end{array}\right]
$$

which is the matrix of how many steady states $S$ has! What's even more suspicious is that our wiring diagram for $S$ looks a lot like the string diagram we would use to describe the multiplication of matrices:
![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-233.jpg?height=142&width=1064&top_left_y=828&top_left_x=518)

This can't just be a coincidence. Luckily for our sanity, it isn't. In the remainder of this section, we will show how various things one can do with matrices - multiply them, trace them, Kronecker product them - can be done for matrices of sets, and how if your wiring diagram looks like its telling you to do that thing, then you can do that thing to the steady states of your internal systems to get the steady states of the whole wired system

Matrices of sets We'll be working with matrices of sets - now and in the coming section - quite a bit, so we should really nail them down. Matrices of sets work a lot like matrices of numbers, especially when the sets are finite; then they are very nearly the same thing as matrices of whole numbers. But the matrix arithmetic of infinite sets works just the same as with finite sets, so we'll do everything in that generality. ${ }^{1}$

Definition 5.2.0.1. Let $A$ and $B$ be two sets. $B \times A$ matrix of sets is a dependent set $M: B \times A \rightarrow$ Set. For $a \in A$ and $b \in B$, we write $M_{b a}$ or $M_{(b, a)}$ for set indexed by $a$ and $b$, and call this the $(b, a)$-entry of the matrix $M$.

We draw of matrix of sets with the following string diagram:

$$
A-M-B
$$

Remark 5.2.0.2. We can see a dependent set $X_{-}: A \rightarrow$ Set through the matrix of sets point of view as a vector of sets. This is because $X_{-}$is equivalently given by $X_{-}: A \times 1 \rightarrow$ Set, which we see is a $A \times 1$ matrix of sets. A $n \times 1$ matrix is equivalently a column vector.

Now we'll go through and define the basic operations of matrix arithmetic: mutliplication, Kronecker product (also known as the tensor product), and partial trace.
\footnotetext{
${ }^{1}$ This will help us later when we deal with behaviors that have more complicated charts. For example, even finite systems can have infinitely many different trajectories, so we really need the infinite sets.
}

Definition 5.2.0.3. Given an $B \times A$ matrix of sets $M$ and a $C \times B$ matrix of sets $N$, their product $N M$ (or $M \times{ }_{B} N$ for emphasis) is the $C \times A$ matrix of sets with entries

$$
N M_{c a}=\sum_{b \in B} N_{c b} \times M_{b a}
$$

We draw the multiplication of matrices of sets with the following string diagram:

$$
A-M{ }^{B} N-C
$$

The identity matrix $I_{A}$ is an $A \times A$ matrix with entries

$$
I_{a a^{\prime}}=\left\{\begin{array}{ll}
1 & \text { if } a=a^{\prime} \\
\emptyset & \text { if } a \neq a^{\prime}
\end{array} .\right.
$$

We draw the identity matrix as a string with no beads on it.

$$
A \longrightarrow A
$$

Exercise 5.2.0.4. Multiplication of matrices of sets satisfies the usual properties of associativity and unity, but only up to isomorphism. Let $M$ be a $B \times A$ matrix, $N$ a $C \times B$ matrix, and $L$ a $D \times C$ of sets. Show that

1. For all $a \in A$ and $d \in D,((L N) M)_{d a} \cong(L(N M))_{d a}$.

2. For all $a \in A$ and $b \in B,\left(M I_{A}\right)_{b a} \cong M_{b a} \cong\left(I_{B} M\right)_{b a}$.

Remark 5.2.0.5. The isomorphisms you defined in Exercise 5.2.0.4 are coherent, much in the way the associativity and unity isomorphisms of a monoidal category are. Together, this means that there is a bicategory of sets and matrices of sets between them.

Definition 5.2.0.6. Let $M$ be a $B \times A$ matrix and $N$ a $C \times D$ matrix of sets. Their Kronecker product or tensor product $M \otimes N$ is a $(B \times C) \times(A \times D)$ matrix of sets with entries:

$$
(M \otimes N)_{(b, c)(a, d)}=M_{b a} \times N_{c d}
$$

We draw the tensor product $M \otimes M$ of matrices as:

$$
\begin{aligned}
& A-M-B \\
& C-N-D
\end{aligned}
$$

Finally, we need to define the partial trace of a matrix of sets.

Definition 5.2.0.7. Suppose that $M$ is a $(A \times C) \times(A \times B)$ matrix of sets. Its partial trace $\operatorname{tr}_{A} M$ is a $C \times B$ matrix of sets with entries:

$$
\left(\operatorname{tr}_{A}\right) M_{c b}=\sum_{a \in A} M_{(a, c)(a, b)}
$$

We draw the partial trace of a matrix of sets as:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-235.jpg?height=122&width=314&top_left_y=617&top_left_x=900)

Exercise 5.2.0.8. Here's an important sanity check we should do about our string diagrams for matrices of sets. The following two diagrams should describe the same matrix, even though they describe it in different ways:

$$
A-M-N-C
$$

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-235.jpg?height=214&width=349&top_left_y=1018&top_left_x=1121)

The diagram on the left says "multiply $M$ and $N$ ", while the diagram on the right says "tensor $M$ and $N$, and then partially trace them.". Show that these two diagrams do describe the same matrix:

$$
N M \cong \operatorname{tr}_{B}(M \otimes N)
$$

Compare this to Example 1.3.2.5, where we say that wiring an input of a system to an output of another can be seen as first taking their parallel product, and then forming a loop.

Steady states and matrix arithmetic For the remainder of this section, we will show that we can calculate the steady state matrix of a composite system in terms of its component system in a very simple way:
- First, take the steady state matrices of the component systems.
- Then consider the wiring diagram as a string diagram for multiplying, tensoring, and tracing matrices.
- Finally, finish by doing all those operations to the matrix.

In Section 5.3, we will see that this method - or something a lot like it â€” works calculating the behaviors of a composite system out of the behaviors of its components, as long as the representative of that behavior exposes its entire state. That result will be nicely packaged in a beautiful categorical way: we'll make an doubly indexed functor.

But for now, let's just show that tensoring and partially tracing steady state matrices correponds to taking the parallel product and wiring an input to an output, respectively, of systems.

Proposition 5.2.0.9. Let $S_{1}$ and $S_{2}$ be systems. Then the steady state matrix of the parallel product $S_{1} \otimes S_{2}$ is the tensor of their steady state matrices:

$$
\text { Steady }_{\mathrm{S}_{1} \otimes \mathrm{S}_{2}} \cong \text { Steady }_{\mathrm{S}_{1}} \otimes \text { Steady }_{\mathrm{S}_{2}}
$$

Proof. First, we note that these are both $\left(\right.$ Out $_{S_{1}} \times$ Out $\left._{S_{2}}\right) \times\left(\ln _{S_{1}} \times \ln _{S_{2}}\right)$-matrices of sets. Now, on a chart $\left(\begin{array}{c}\left(i_{1}, i_{2}\right) \\ \left(o_{1}, 0_{2}\right)\end{array}\right)$, a steady state in $\mathrm{S}_{1} \otimes \mathrm{S}_{2}$ will be a pair $\left(s_{1}, s_{2}\right) \in$ State $_{S_{1}} \times$ State $_{S_{2}}$ such that update $\mathrm{S}_{\mathrm{j}}\left(s_{j}, i_{j}\right)=s_{j}$ and $\operatorname{expose}_{S_{\mathrm{j}}}\left(s_{j}\right)=o_{j}$ for $j=1$, 2. In other words, its just a pair of steady states, one in $S_{1}$ and one in $S_{2}$. This is precisely the $\left(\begin{array}{c}\left(i_{1}, i_{2}\right) \\ \left(o_{1}, o_{2}\right)\end{array}\right)$-entry of the right hand side above.

Remark 5.2.0.10. Proposition 5.2.0.9 is our motiviation for using the symbol " $\otimes$ " for the parallel product of systems.

Proposition 5.2.0.11. Let $\mathrm{S}$ be a system with $\ln _{\mathrm{S}}=A \times B$ and Out $\mathrm{S} \times C$. Let $\mathrm{S}^{\prime}$ be the system formed by wiring the $A$ output into the $A$ input of $\mathrm{S}$ :

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-236.jpg?height=206&width=287&top_left_y=1130&top_left_x=908)

Then the steady state matrix of $\mathrm{S}^{\prime}$ is given by partially tracing out $A$ in the steady state matrix of S:

$$
\text { Steady }_{\mathrm{S}^{\prime}}=\frac{A}{\text { Steady }_{\mathrm{S}}}=\operatorname{tr}_{A}\left(\text { Steady }_{\mathrm{S}}\right)
$$

Proof. Let's first see what a steady state of $S^{\prime}$ would be. Since $S^{\prime}$ is just a rewiring of $S$, it has the same states; so, a steady state $s$ of $S^{\prime}$ is in particular a state of $S$. Now,

$$
\operatorname{update}_{S^{\prime}}(s, b)=\operatorname{update}_{S}\left(s,\left(\pi_{1} \operatorname{expose}_{S}(s), b\right)\right)
$$

by definition, so if $\operatorname{update}_{S^{\prime}}(s, b)=s$, then $\operatorname{update}_{S}\left(s,\left(\pi_{1} \operatorname{expose}_{S}(s), b\right)\right)=s$. If also $\operatorname{expose}_{S^{\prime}}(s)=c\left(\right.$ so that $s$ is a $\left(\begin{array}{l}b \\ c\end{array}\right)$-steady state of $\left.S^{\prime}\right)$, then $\pi_{2} \operatorname{expose}_{S}(s)=\operatorname{expose}_{S^{\prime}}(s)=$

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-236.jpg?height=94&width=1442&top_left_y=1983&top_left_x=339)
steady state of $\mathrm{S}$. That is, we have a function

$$
s \mapsto\left(\pi_{1} \operatorname{expose}_{\mathrm{s}}(s), s\right): \text { Steady }_{\mathrm{S}^{\prime}}\left(\begin{array}{l}
b \\
c
\end{array}\right) \rightarrow\left(\operatorname{tr}_{A} \text { Steady }_{\mathrm{S}}\right)\left(\begin{array}{l}
b \\
c
\end{array}\right)
$$

It remains to show that this function is a bijection. So, suppose we have a pair $(a, s) \in \operatorname{tr}_{A}$ Steady $_{S}\left(\begin{array}{l}b \\ c\end{array}\right)$ of an $a \in A$ and a $\left(\begin{array}{c}(a, b) \\ (a, c)\end{array}\right)$ steady state of $S$. Then

$$
\text { update }_{S^{\prime}}(s, b)=\operatorname{update}_{S}\left(s,\left(\pi_{1} \operatorname{expose}_{S}(s), b\right)\right)
$$

$$
\begin{array}{lr}
=\text { update }_{\mathrm{S}}(s,(a, b)) & \text { since expose } \mathrm{S}(s)=(a, c) . \\
=s & \text { since } s \text { is a }\left(\begin{array}{l}
(a, b) \\
(a, c)
\end{array}\right) \text {-steady state. }
\end{array}
$$

$\operatorname{expose}_{S^{\prime}}(s)=\pi_{2} \operatorname{expose}_{S}(s)=c$.

This shows that $s$ is also a $\left(\begin{array}{l}b \\ c\end{array}\right)$ steady state of $S^{\prime}$, giving us a function $(a, s) \mapsto s$ : $\left(\operatorname{tr}_{A}\right.$ Steady $\left._{\mathrm{S}}\right) \rightarrow$ Steady $_{\mathrm{S}^{\prime}}$. These two functions are plainly inverse.

We can summarize Proposition 5.2.0.11 in the following commutative diagram:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-237.jpg?height=740&width=1244&top_left_y=833&top_left_x=386)

The horizontal maps take the steady states of a system, while the vertical map on the left wires together the system with that wiring diagram, and the vertical map on the right applies that transformation of the matrix. In the next section, we will see how this square can be interepreted as a naturality condition in a doubly indexed functor.

One thing to notice here is that taking the partial trace (the right vertical arrow in the diagram) is itself given by multiplying by a certain matrix.

Proposition 5.2.0.12. Let $M$ be a $(A \times C) \times(A \times B)$ matrix of sets. Let $\operatorname{Tr}^{A}$ be the $(C \times B) \times((A \times C) \times(A \times B))$ matrix of sets with entries:

$$
\operatorname{Tr}^{A} A_{(c, b)\left(\left(a, c^{\prime}\right),\left(a^{\prime}, b^{\prime}\right)\right)}:= \begin{cases}1 & \text { if } a=a^{\prime}, b=b^{\prime}, \text { and } c=c^{\prime} \\ \emptyset & \text { otherwise. }\end{cases}
$$

Then, considering $M$ as a $((A \times C) \times(A \times B)) \times 1$ matrix of sets, taking its trace is given by multiplying by $\operatorname{Tr}^{A}$ :

$$
\operatorname{tr}_{A} M \cong \operatorname{Tr}^{A} M
$$

Proof. Let's calculate that matrix product on the right.

$$
\left(\operatorname{Tr}^{A} M\right)_{(c, b)}=\sum_{\left(\left(a, c^{\prime}\right),\left(a^{\prime}, b^{\prime}\right)\right) \in(A \times C) \times(A \times B)} \operatorname{Tr}_{(c, b)\left(\left(a, c^{\prime}\right),\left(a^{\prime}, b^{\prime}\right)\right)}^{A} \times M_{\left(a, c^{\prime}\right)\left(a^{\prime}, b^{\prime}\right)}
$$

Now, since $\operatorname{Tr}_{(c, b)\left(\left(a, c^{\prime}\right),\left(a^{\prime}, b^{\prime}\right)\right)}^{A}$ is a one element set (if $a=a^{\prime}, c=c^{\prime}$, and $b=b^{\prime}$ ) and is empty otherwise, the inner expression has the elements of $M_{\left(a, c^{\prime}\right)\left(a^{\prime}, b^{\prime}\right)}$ if and only if $a=a^{\prime}, b=b^{\prime}$, and $c=c^{\prime}$ and is otherwise empty. So, we conclude that

$$
\sum_{\left(\left(a, c^{\prime}\right),\left(a^{\prime}, b^{\prime}\right)\right) \in(A \times C) \times(A \times B)} \operatorname{Tr}_{(c, b)\left(\left(a, c^{\prime}\right),\left(a^{\prime}, b^{\prime}\right)\right)}^{A} \times M_{\left(a, c^{\prime}\right)\left(a^{\prime}, b^{\prime}\right)} \cong M_{(a, c)(a, b)}
$$

\subsection*{5.3 The big theorem: representable doubly indexed functors}

We have now introduced all the characters in our play: the double categories of arenas and matrices, and doubly indexed categories of systems and vectors. In this section, we will put the plot in motion.

In Section 5.2, we saw that the steady states of dynamical systems with interface $\left(\begin{array}{l}I \\ O\end{array}\right)$ compose like an $I \times O$ matrix. We proved a few propositions to this effect, namely Proposition 5.2.0.9 and Proposition 5.2.0.11, but we didn't precisely mark out the scope of these results, or describe the full range of laws that are satisfied.

In this section, we will generalize the results of that section to all behaviors of systems, not just steady states. We will precisely state all the ways that behaviors can be composed by systems, and we will give a condition on the kinds of behaviors for which we can calculate the behavior of a wired together system entirely from the behavior of its component systems. All of this will be organized into a doubly indexed functor BehaveÑ‚ : Sys $\rightarrow$ Vec which will send a system $S$ to its set of T-shaped behaviors.

In fact, our definition of Behave ${ }_{\mathrm{T}}$ will be entirely abstract; it will work for almost any doubly indexed category $\mathcal{A}: \mathscr{D} \rightarrow$ Cat (there is a small condition on the indexing double category $\mathscr{D}$ ). Behave ${ }_{\mathrm{T}}$ will be a representable doubly indexed category. Before going on to construct representable doubly indexed categories, let's take a minute to refresh ourselves on what representable functors are for categories. The essential idea is the same.

If $C$ is a category and $T$ an object of $C$, then we can see maps $f: T \rightarrow X$ as "figures of shape $T$ in $X^{\prime \prime}$. It is often the case that we have some other way of talking about figures of shape $T$ in $X$ in terms that don't mention $T$ - in this case we say that $T$ represents figures of shape $T$. This phenomenon is very widespread, so let's give a number of examples:
- Suppose that $C$ is the category of sets, and $T=1$ is a one element set. Then a map $f: T \rightarrow X$ uniquely picks out an element of $X$. We see that $T$ has the shape of a single element, and a map from $T$ to $X$ is a thing in $X$ whose shape is an element; that is, an element of $X$. We can say that 1 represents elements.
- Suppose that $C$ is the category of sets, but now that $T=2$ is a two-element set. A two-element set is an abstract pair of elements, and a map $f: T \rightarrow X$ now picks out a pair of elements in $X$. We can say that 2 represents pairs.
- Suppose that $C$ is the category of simple, undirected graphs - that is, sets $X$ equipped with an irreflexive relation $E_{X} \subseteq X \times X$ telling us which two elements are connected by an edge. The maps of this category need to preserve edges. If $T$ is the graph consisting of a single edge (formally, $T=2$ with $(0,1) \in E_{T}$ being the only edge), then a map $f: T \rightarrow X$ must pick out a pair of points in $X$ with an edge between them. In other words, maps $T \rightarrow X$ are edges in $X$. So we may say that $T$ represents edges.
- Suppose that $C$ is the category of rings, and let $T=\mathbb{Z}[x, y]$ be the ring of polynomials in two variables. A ring homomoprhism $f: T \rightarrow X$ can send $x$ to any element $f(x)$ and similarly $y$ to any element $f(y)$; once it's done that, the value of $f$ on any polynomial in $x$ and $y$ must be given by

$$
f\left(\sum a_{i j} x^{i} y^{j}\right)=\sum a_{i j} f(x)^{i} f(y)^{j}
$$

since $f$ is presumed to be a ring homomorphism. Actually, there is one constraint on $f(x)$ and $f(y)$ for this to work; since $x y=y x$ as polynomials, we must have $f(x) f(y)=f(y) f(x)$. Therefore, we see that $\mathbb{Z}[x, y]$ represents pairs of elements which commute in the category of rings.
- As we saw in Chapter 3, all sorts of behaviors of systems - trajectories, periodic orbits, steady states, etc - are represented by simple systems in the category of systems and behaviors between them.

We could continue endlessly. The idea of representability is fundamental in category theory. Let's make it a little more explicit exactly what it means for $T$ to represent something.

If $T$ is an object of $C$, then for any object $X$ of $C$ we get a set $C(T, X)$ of all maps from $T$ to $X$ in $C$. If $g: X \rightarrow Y$ is a map in $C$, then for any $f: T \rightarrow X$ we get a map $f \circ g: T \rightarrow Y$; in other words, for $g: X \rightarrow Y$ we get a map $C(T, X) \xrightarrow{-9 g} C(T, Y)$ given by post-composing with $g$. This gives us a functor $C(T,-): C \rightarrow$ Set. This is a representable functor.

The idea of this section is to use the fact that behaviors are represented by simple systems to prove a compositionality result. This compositionality result is packaged up into a doubly indexed functor, and we will construct it as a representable doubly indexed functor. Instead of going from a category to the category of sets as representable functors do, our representable doubly indexed functors will go from a doubly indexed category (satisfying a little condition) to the doubly indexed category Vec of vectors of sets.

\subsection*{5.3.1 Turning lenses into matrices: Representable double Functors}

In Section 5.2, we saw how we could re-interpret a wiring diagram as a schematic for multiplying, tensoring, and tracing matrices. At the very end, in Proposition 5.2.0.12, we saw that we can take the $\operatorname{tr}_{A} M$ of a $(A \times C) \times(A \times B)$-matrix $M$ by considering it as a $(A \times C) \times(B \times C)$ length vector and then multiplying it by a big but very sparse $(C \times B) \times((A \times C) \times(B \times C))$-matrix $\operatorname{Tr}^{A}$. Taking the trace of a matrix corresponded to the wiring diagram

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-240.jpg?height=206&width=187&top_left_y=884&top_left_x=964)

In this section, we will see a general formula for taking an arbitrary lens and turning it into a matrix. Mutliplying by the matrix will then correspond to wiring according to that lens.

This process of turning a lens into a matrix will give us a functor Lens $\rightarrow$ Matrix from the category of lenses to the category of matrices of sets. We'll start by exploring this functor in the deterministic systems theory; then we will abstract and find that the same argument works in any systems theory.

The resulting matrices will have entries that are either 1 or $\emptyset$; we can think of this as telling us whether (1) or not $(\emptyset)$ the two charts are to be wired together. As we saw in Example 3.4.1.4, we can see a square in the double category of arenas as telling us how a chart can be wired together along a lens into another chart. Therefore, we will take the entries of our matrices to be the sets of appropriate squares in arena - but there is either a single square (if the appropriate equations hold) or no square (if they don't), so we will end up with a matrix whose entries either have a single element or are empty.

Proposition 5.3.1.1. For any arena in the deterministic systems theory $\left(\begin{array}{l}I \\ O\end{array}\right)$, there is a functor

$$
\operatorname{Chart}_{\mathbb{D} \text { ET }}\left(\left(\begin{array}{c}
I \\
O
\end{array}\right),-\right): \text { Lens }_{\mathbb{D}_{\mathrm{ET}}} \rightarrow \text { Matrix }
$$

from the category of lenses to the category of matrices of sets which sends an arena $\left(\begin{array}{l}A^{-} \\ A^{+}\end{array}\right)$to the set Chart $_{\text {DET }}\left(\left(\begin{array}{l}I \\ O\end{array}\right),\left(\begin{array}{l}A^{-} \\ A^{+}\end{array}\right)\right)$of charts from $\left(\begin{array}{l}I \\ O\end{array}\right)$ to $\left(\begin{array}{l}A^{-} \\ A^{+}\end{array}\right)$, and which sends a lens $\left(\begin{array}{c}w^{\sharp} \\ w\end{array}\right):\left(\begin{array}{c}A^{-} \\ A^{+}\end{array}\right) \leftrightarrows\left(\begin{array}{c}B^{-} \\ B^{+}\end{array}\right)$to the $\operatorname{Chart}_{\text {DET }}\left(\left(\begin{array}{l}I \\ O\end{array}\right),\left(\begin{array}{c}B^{-} \\ B^{+}\end{array}\right)\right) \times \operatorname{Chart}_{\text {DET }}\left(\left(\begin{array}{l}I \\ O\end{array}\right),\left(\begin{array}{c}A^{-} \\ A\end{array}\right)\right)$ matrix of sets

$$
\operatorname{Chart}_{\mathbb{D} \mathrm{ET}}\left(\left(\begin{array}{c}
I \\
O
\end{array}\right),\left(\begin{array}{c}
w^{\sharp} \\
w
\end{array}\right)\right): \operatorname{Chart}_{\mathbb{D}_{\mathrm{ET}}}\left(\left(\begin{array}{c}
I \\
O
\end{array}\right),\left(\begin{array}{c}
B^{-} \\
B^{+}
\end{array}\right)\right) \times \operatorname{Chart}_{\mathrm{DET}_{\mathrm{ET}}}\left(\left(\begin{array}{c}
I \\
O
\end{array}\right),\left(\begin{array}{c}
A^{-} \\
A
\end{array}\right)\right) \rightarrow \text { Set }
$$

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-241.jpg?height=764&width=1289&top_left_y=800&top_left_x=491)

Proof. By vertical composition of squares,

$$
\begin{aligned}
& \left(\begin{array}{l}
I \\
O
\end{array}\right) \xrightarrow{\left(\begin{array}{c}
f_{b} \\
f
\end{array}\right)}\left(\begin{array}{c}
A^{-} \\
A^{+}
\end{array}\right) \\
& \|\| \| \quad \downarrow \uparrow\left(\begin{array}{c}
w^{\sharp} \\
w
\end{array}\right) \quad\left(\begin{array}{c}
I \\
O
\end{array}\right) \xrightarrow{(f)}\left(\begin{array}{c}
A^{-} \\
A^{+}
\end{array}\right) \\
& \left(\begin{array}{c}
I \\
O
\end{array}\right) \underset{\left(g_{b}\right)}{\longrightarrow}\left(\begin{array}{l}
B^{-} \\
B^{+}
\end{array}\right)=\|\| \quad \quad \downarrow \uparrow\left(\begin{array}{c}
w^{\sharp} \\
w
\end{array}\right) \cdot\left(\begin{array}{c}
w^{\sharp} \\
v
\end{array}\right) \\
& \|\| \quad\left(\begin{array}{c}
g_{b} \\
g
\end{array}\right) \quad \underset{1}{\mid \uparrow}\left(\begin{array}{c}
v^{\sharp} \\
v
\end{array}\right) \quad\left(\begin{array}{c}
I \\
O
\end{array}\right) \underset{\left(h_{b}\right)}{\longrightarrow}\left(\begin{array}{c}
I \\
O^{\prime}
\end{array}\right) \\
& \left(\begin{array}{l}
I \\
O
\end{array}\right) \underset{\left(\begin{array}{c}
h_{b} \\
h
\end{array}\right)}{\longrightarrow}\left(\begin{array}{c}
I^{\prime} \\
O^{\prime}
\end{array}\right)
\end{aligned}
$$

there is always a map from the composite of two of these matrices to the matrix
described by the composite. It is not, however, obvious that this map is a bijection which is what we need to prove functoriality.

Suppose we have a square as on the left hand side; let's see that we can factor it into two squares as on the right hand side. We need to construct the middle chart $\left(\begin{array}{c}g_{b} \\ g\end{array}\right):\left(\begin{array}{c}I \\ O\end{array}\right) \rightrightarrows\left(\begin{array}{c}B^{-} \\ B^{+}\end{array}\right)$from $\left(\begin{array}{c}f_{b} \\ f\end{array}\right)$ and $\left(\begin{array}{c}h_{b} \\ h\end{array}\right)$. For the bottom of top square to commute, we see that $g$ must equal $w \circ f$, so we can define $g:=w \circ f$. On the other hand, for the top of the bottom square to commute, we must have that $g_{b}(i, o)=v^{\sharp}\left(g(o), h_{b}(i, o)\right)$; again, we can take this as a definition. It remains to show that the other half of each square commutes. For the top of the top square to commute means that

$$
f_{b}(i, o)=w^{\sharp}\left(f(o), g_{b}(i, o)\right)
$$

which we can see holds by

$$
\begin{aligned}
w^{\sharp}\left(f(o), g_{\mathrm{b}}(i, o)\right) & =w^{\sharp}\left(f(o), v^{\sharp}\left(g(o), h_{\mathrm{b}}(i, o)\right)\right) \\
& =w^{\sharp}\left(f(o), v^{\sharp}\left(w f(o), h_{b}(i, o)\right)\right) \\
& =f_{\mathrm{b}}(i, o)
\end{aligned}
$$

by the commutativity of the square on the right.

On the other hand, to show that the bottom of the bottoms square commutes, we need that $h=v \circ g$. But by hypothesis, $h=v \circ w \circ f$, and we defined $g=w \circ f$.

Example 5.3.1.2. Let's see what happens when we take the functor $\operatorname{Chart}_{\mathbb{D E T}^{\text {Et }}}\left(\left(\begin{array}{l}I \\ O\end{array}\right),-\right)$ for the arena $\left(\begin{array}{l}1 \\ 1\end{array}\right)$. A chart $\left(\begin{array}{l}a^{-} \\ a^{+}\end{array}\right):\left(\begin{array}{l}1 \\ 1\end{array}\right) \rightrightarrows\left(\begin{array}{l}A^{-} \\ A^{+}\end{array}\right)$is just a pair of elements $a^{-} \in A^{-}$and $a^{+} \in A^{+}$, so

$$
\operatorname{Chart}_{\mathbb{D E T}^{\text {ET }}}\left(\left(\begin{array}{l}
1 \\
1
\end{array}\right),\left(\begin{array}{l}
A^{-} \\
A^{+}
\end{array}\right)\right)=A^{-} \times A^{+}
$$

Now, if we have a lens $\left(\begin{array}{c}w^{\sharp} \\ w\end{array}\right):\left(\begin{array}{c}A^{-} \\ A^{+}\end{array}\right) \leftrightarrows\left(\begin{array}{c}B^{-} \\ B^{+}\end{array}\right)$, we have a square

$$
\begin{aligned}
& \left(\begin{array}{l}
1 \\
1
\end{array}\right) \xrightarrow{\left(\begin{array}{l}
a^{-} \\
a^{+}
\end{array}\right)}\left(\begin{array}{c}
A^{-} \\
A^{+}
\end{array}\right) \\
& \|\| \| \quad \downarrow \uparrow\left(\begin{array}{c}
w^{\sharp} \\
w
\end{array}\right) \\
& \left(\begin{array}{l}
1 \\
1
\end{array}\right) \underset{\left(\begin{array}{l}
b^{-} \\
b^{+}
\end{array}\right)}{\longrightarrow}\left(\begin{array}{l}
B^{-} \\
B^{+}
\end{array}\right)
\end{aligned}
$$

if and only if $w\left(a^{+}\right)=b^{+}$and $w^{\sharp}\left(a^{+}, b^{-}\right)=a^{-}$. Thinking of $\left(\begin{array}{c}w^{\sharp} \\ w\end{array}\right)$ as a wiring diagram, this would mean that $b^{+}$is that part of $a^{+}$which is passed forward on the outgoing
wires, and $a^{-}$is the inner input which comes from the inner output $a^{+}$and outer input $b^{-}$.

To take a concrete example, suppose that $\left(\begin{array}{c}w \\ w\end{array}\right)$ were the following wiring diagarm:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-243.jpg?height=187&width=179&top_left_y=454&top_left_x=968)

That is, let's take $A^{+}=X \times Y$ and $A^{-}=X \times Z$, and $B^{+}=Y$ and $B^{-}=Z$, and

$$
\begin{gathered}
w(x, y)=y \\
w^{\sharp}((x, y), z)=(x, z) .
\end{gathered}
$$

Using the definition above, we can calculate the resulting matrix $\operatorname{Chart}_{\mathbb{D E T}}\left(\left(\begin{array}{c}I \\ 0\end{array}\right),\left(\begin{array}{c}w^{\sharp} \\ w\end{array}\right)\right)$ as having $\left(\left((x, y),\left(x^{\prime}, z\right)\right),\left(y^{\prime}, z^{\prime}\right)\right)$-entry

$$
\begin{cases}1 & \text { if } w(x, y)=y^{\prime} \text { and } w^{\sharp}((x, y), z)=\left(x^{\prime}, z^{\prime}\right) \\ \emptyset & \text { otherwise. }\end{cases}
$$

or, by the definition of $\left(\begin{array}{c}w^{\sharp} \\ w\end{array}\right)$,

$$
\begin{cases}1 & \text { if } x=x^{\prime}, y=y^{\prime}, \text { and } z=z^{\prime} \\ \emptyset & \text { otherwise. }\end{cases}
$$

which was the definition of $\operatorname{Tr}^{X}$ given in Proposition 5.2.0.12!

Exercise 5.3.1.3. Let $\left(\begin{array}{c}w^{\sharp} \\ w\end{array}\right):\left(\begin{array}{c}A \times B \\ B \times C\end{array}\right) \leftrightarrows\left(\begin{array}{l}A \\ C\end{array}\right)$ be the wiring diagram

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-243.jpg?height=146&width=225&top_left_y=1846&top_left_x=945)

Calculate the entries of the matrix $\operatorname{Chart}_{\mathbb{D E T}_{\text {ET }}}\left(\left(\begin{array}{l}1 \\ 1\end{array}\right),\left(\begin{array}{c}w^{\sharp} \\ w\end{array}\right)\right)$.

By the functoriality of Proposition 5.3.1.1, we can calculate the matrix of a big wiring diagram by expressing it in terms of a series of traces, and mutliplying the resulting matrices together. This means that the process of multiplying, tensoring, and tracing matrices described by a wiring diagram is well described by the matrix we constructed in Proposition 5.3.1.1, since we already know that it interprets the basic wiring diagrams correctly.

But we are also interested in charts, since we have to chart out our behaviors. So we will give a double functor Arena ${ }_{\mathbb{D E T}} \rightarrow$ Matrix that tells us not only how to turn a lens into a matrix, but also how this operation interacts with charts. This is an example of a representable double functor.

We will first define the double functor Arena $_{\mathbb{D}_{\mathrm{ET}}}\left(\left(\begin{array}{l}I \\ O\end{array}\right),-\right):$ Arena $_{\mathbb{D}_{\mathrm{ET}}} \rightarrow$ Matrix represented by an arena $\left(\begin{array}{l}I \\ O\end{array}\right)$ explicitly. Then we will see how this argument can be abstracted to a double category which satisfies a horizontal factorization property.

Proposition 5.3.1.4. There is a double functor

$$
\operatorname{Arena}_{\mathbb{D} \mathrm{ET}}\left(\left(\begin{array}{l}
I \\
O
\end{array}\right),-\right): \text { Arena } \rightarrow \text { Matrix }
$$

which acts in the following way:
- An arena $\left(\begin{array}{l}A^{-} \\ A^{+}\end{array}\right)$gets sent to the set $\operatorname{Chart}_{\text {DET }}\left(\left(\begin{array}{l}I \\ O\end{array}\right),\left(\begin{array}{l}A^{-} \\ A^{+}\end{array}\right)\right)$of charts from $\left(\begin{array}{l}I \\ O\end{array}\right)$ to $\left(\begin{array}{l}A^{-} \\ A^{+}\end{array}\right)$.
- The vertical functor is Chart $\left(\left(\begin{array}{l}I \\ O\end{array}\right),-\right):$ Lens $\rightarrow$ Matrix from Proposition 5.3.1.1.
- The horizontal functor is the representable functor Arena $\left(\left(\begin{array}{l}I \\ O\end{array}\right),-\right):$ Arena $\rightarrow$ Set which acts on a chart $\left(\begin{array}{c}f_{\mathrm{b}} \\ f\end{array}\right):\left(\begin{array}{l}A^{-} \\ A^{+}\end{array}\right) \rightrightarrows\left(\begin{array}{c}B^{-} \\ B^{+}\end{array}\right)$by post-composition.
- To a square

$$
\begin{aligned}
& \left(\begin{array}{l}
A^{-} \\
A^{+}
\end{array}\right) \xrightarrow{\left(\begin{array}{l}
f_{\mathrm{b}} \\
f
\end{array}\right)}\left(\begin{array}{l}
B^{-} \\
B^{+}
\end{array}\right) \\
& \beta=\left(\begin{array}{c}
j^{\sharp} \\
j
\end{array}\right) \downarrow \uparrow \quad \downarrow \uparrow\left(\begin{array}{c}
k^{\sharp} \\
k
\end{array}\right) \\
& \left(\begin{array}{l}
C^{-} \\
C^{+}
\end{array}\right) \underset{\left(\begin{array}{c}
g^{\sharp} \\
g
\end{array}\right)}{\longrightarrow}\left(\begin{array}{l}
D^{-} \\
D^{+}
\end{array}\right)
\end{aligned}
$$

in the double category of arenas, we give the square

$$
\begin{aligned}
& \operatorname{Arena}_{\mathbb{D}_{\mathrm{ET}}}\left(\left(\begin{array}{c}
I \\
O
\end{array}\right), \beta\right)= \\
& \text { Arena }_{\mathbb{D E T}_{\mathrm{ET}}}\left(\left(\begin{array}{c}
I \\
O
\end{array}\right),\left(\begin{array}{l}
A^{-} \\
A^{+}
\end{array}\right)\right)^{\operatorname{Arena}_{\mathrm{DE}}}\left(\left(\begin{array}{c}
I \\
O
\end{array}\right),\left(\begin{array}{l}
f_{\mathrm{b}} \\
f
\end{array}\right)\right)^{\text {Arena }} \mathbf{D}_{\mathrm{DT}}\left(\left(\begin{array}{c}
I \\
O
\end{array}\right),\left(\begin{array}{l}
B^{-} \\
B^{+}
\end{array}\right)\right) \\
& \operatorname{Arena}_{\mathbb{D E T}}\left(\left(\begin{array}{c}
I \\
O
\end{array}\right),\left(\begin{array}{c}
j^{\sharp} \\
j
\end{array}\right)\right) \downarrow \operatorname{Arena}_{\mathbb{D E T}}\left(\left(\begin{array}{c}
I \\
O
\end{array}\right),\left(\begin{array}{c}
k^{\sharp} \\
k
\end{array}\right)\right) \\
& \operatorname{Arena}_{\mathbb{D}_{\mathrm{ET}}}\left(\left(\begin{array}{c}
I \\
O
\end{array}\right),\left(\begin{array}{c}
C^{-} \\
C^{+}
\end{array}\right)\right) \xrightarrow[\text { Arena }_{\mathrm{D} \mathrm{ET}}\left(\left(\begin{array}{c}
I \\
O
\end{array}\right),\left(\begin{array}{c}
g_{\mathrm{b}} \\
g_{\mathrm{b}}
\end{array}\right)\right)]{\operatorname{Arena}_{\mathbb{D}_{\mathrm{ET}}}}\left(\left(\begin{array}{c}
I \\
O
\end{array}\right),\left(\begin{array}{c}
D^{-} \\
D^{+}
\end{array}\right)\right)
\end{aligned}
$$

in the double category of matrices defined by horizontal composition of squares in Arena $_{\mathbb{D E t}^{\text {Et }}}$ (remember that the entries of these matrices are sets of squares in Arena $_{\mathbb{D}_{\text {Ex }}}$, even though that means they either have a single element or no elements).

$$
\left.\operatorname{Arena}_{\mathbb{D}_{\mathrm{ET}}}\left(\left(\begin{array}{l}
I \\
O
\end{array}\right), \beta\right)(\alpha)=\alpha \right\rvert\, \beta
$$

Proof. We can write the double functor $\operatorname{Arena}_{\mathbb{D E T}_{\mathrm{ET}}}\left(\left(\begin{array}{l}I \\ O\end{array}\right),-\right)$ entirely in terms of the double category Arena $\mathbb{D E t}$ :
- It sends an arena $\left(\begin{array}{c}A^{-} \\ A^{+}\end{array}\right)$to the set of charts (horizontal maps) $\left(\begin{array}{l}f_{\mathrm{b}} \\ f\end{array}\right):\left(\begin{array}{l}I \\ O\end{array}\right) \rightrightarrows\left(\begin{array}{l}A^{-} \\ A^{+}\end{array}\right)$.
- It sends a chart $\left(\begin{array}{c}g_{\mathrm{b}} \\ g\end{array}\right)$ to the $\left.\operatorname{map}\left(\begin{array}{c}f_{\mathrm{b}} \\ f\end{array}\right) \mapsto\left(\begin{array}{c}f_{\mathrm{b}} \\ f\end{array}\right) \right\rvert\,\left(\begin{array}{c}g_{\mathrm{b}} \\ g\end{array}\right)$.
- It sends a lens $\left(\begin{array}{c}w^{\sharp} \\ w\end{array}\right)$ to the set of squares $\beta:\left(\begin{array}{c}I \\ O\end{array}\right) \rightarrow\left(\begin{array}{c}w^{\sharp} \\ w\end{array}\right)$, indexed by their top and bottom boundaries.
- It sends a square $\alpha$ to the map given by horizontal compostion $\beta \mapsto \beta \mid \alpha$.

We can see that this double functor (let's call it $F$, for short) takes seriously the idea that "squares are charts between lenses" from Example 3.4.1.4. From this description, and the functoriality of Proposition 5.3.1.1, we can see that the assignments above satisfy the double functor laws.
- Horizontal functoriality follows from horizontal associativity in Arena ${ }_{D \mathbb{D T}}$ :

$$
F(\alpha \mid \beta)(\gamma)=\gamma|(\alpha \mid \beta)=(\gamma \mid \alpha)| \beta=F(\alpha) \mid F(\beta)(\gamma)
$$
- Vertical functoriality follows straight from the definitions:

$$
F\left(\frac{\alpha}{\beta}\right)\left({ }_{-}, \gamma, \delta\right)=\left(_{-}, \gamma|\alpha, \delta| \beta\right)=\frac{F(\alpha)(\gamma)}{F(\beta)(\delta)}
$$
- It's pretty straightforward to check that identities get sent to identities.

This construction is an example of a more general notion of representable double functor. Using the general notion, we can construct a similar double functor

$$
\operatorname{Arena}_{\mathbb{T}}\left(\left(\begin{array}{c}
I \\
O
\end{array}\right),-\right): \text { Arena }_{\mathbb{T}} \rightarrow \text { Matrix }
$$

for any systems theory $\mathbb{T}$. Unlike for categories, not all objects in all double categories admit representable double functors ${ }^{2}$. There is a small condition on an object: the horizontal factor condition.

Definition 5.3.1.5. Let $\mathscr{D}$ be a double category. An object $D$ of $\mathscr{D}$ satisfies the horizontal factor condition when for any square

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-246.jpg?height=360&width=263&top_left_y=1831&top_left_x=931)

there is a unique triple of a horizontal $f_{2}: D \rightarrow X_{2}$ and squares $\alpha_{1}: D \Rightarrow k_{1}$ and
\footnotetext{
${ }^{2}$ Any object in a double category does admit a representable lax double functor, but we won't need any of these and so won't introduce this notion.
}
$\alpha_{2}: D \Rightarrow k_{2}$ so that

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-247.jpg?height=401&width=650&top_left_y=325&top_left_x=735)

We say that $\mathscr{D}$ is spanlike if every $D$ satisfies the horizontal factor condition.

Theorem 5.3.1.6. Let $\mathscr{D}$ be a double category and let $D$ be an object of $\mathscr{D}$ satisfying the horizontal factor condition. Then there is a representable double functor $\mathscr{D}(D,-): \mathscr{D} \rightarrow$ Matrix defined as follows:
- For an object $X, \mathscr{D}(D, X)$ is the set of horizontal arrows $D \rightarrow X$.
- For a horizontal $g: X \rightarrow Y, \mathscr{D}(D, g): \mathscr{D}(D, X) \rightarrow \mathscr{D}(D, Y)$ is given by postcomposition with $g: f \mapsto f \mid g$.
- For a vertical $k: X \rightarrow Y$, we get the matrix of sets $\mathscr{D}(D, k): \mathscr{D}(D, X) \times \mathscr{D}(D, Y) \rightarrow$ Set given by

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-247.jpg?height=298&width=788&top_left_y=1358&top_left_x=709)
- For any square

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-247.jpg?height=224&width=268&top_left_y=1704&top_left_x=972)

we define $\mathscr{D}(D, \beta)$ to be the map of matrices given by post-composing with $\beta$. That is,

$$
\mathscr{D}(D, \beta)(\alpha)=\alpha \mid \beta .
$$

Proof. We will show that this is a double functor. The horizontal component is functorial since it is the functor $h \mathscr{D} \rightarrow$ Set represented by $D$.

For vertical functoriality, we need to show that

$$
\mathscr{D}\left(D, \frac{k_{1}}{k_{2}}\right) \cong \frac{\mathscr{D}\left(D, k_{1}\right)}{\mathscr{D}\left(D, k_{2}\right)}
$$
for vertical arrows $k_{1}: X_{1} \rightarrow X_{2}$ and $k_{2}: X_{2} \rightarrow X_{3}$. There is always a map

$$
\frac{\mathscr{D}\left(D, k_{1}\right)}{\mathscr{D}\left(D, k_{2}\right)} \rightarrow \mathscr{D}\left(D, \frac{k_{1}}{k_{2}}\right)
$$

given by taking two squares and composing them. That this map is a bijection is a restatement of the horizontal factor condition which we assumed that $D$ satisfied. The right hand side is the $\mathscr{D}\left(D, X_{1}\right) \times \mathscr{D}\left(D, X_{3}\right)$-matrix of sets which between $f_{1}$ and $f_{3}$ is the set

$$
\sum_{f_{2} \in \mathscr{D}\left(D, X_{2}\right)} \mathscr{D}\left(D, k_{1}\right)_{f_{1}, f_{2}} \times \mathscr{D}\left(D, k_{2}\right)_{f_{2}, f_{3}}
$$

So to say that for any $\alpha \in \mathscr{D}\left(D, \frac{k_{1}}{k_{2}}\right)$ there exists a unique triple $\left(f_{2}, \alpha_{1}, \alpha_{2}\right)$ with $\alpha=\frac{\alpha_{1}}{\alpha_{2}}$ is precisely to say that the map which composes two squares $\alpha_{1}$ and $\alpha_{2}$ into $\frac{\alpha_{1}}{\alpha_{2}}$ is a bijection.

We then need to check vertical and horizontal functoriality for squares. Horizontal functoriality of squares comes down to associativity of horizontal composition, and vertical functoriality of squares comes down to the interchange law.

Theorem 5.3.1.6 gives us Proposition 5.3.1.4 as a special case since the double category Arena $\mathbb{T}$ of arenas in any systems theory $\mathbb{T}$ is spanlike - every arena $\left(\begin{array}{l}I \\ O\end{array}\right)$ satisfies the horizontal factor condition.

Lemma 5.3.1.7. For any systems theory $\mathbb{T}$, the double category Arena $\mathbb{T}$ of arenas in $\mathbb{T}$ is spanlike: every arena satisfies the horizontal factor condition.

Proof. Fix an arena $\left(\begin{array}{l}I \\ O\end{array}\right)$ and suppose that we have a square like so:

$$
\begin{aligned}
& \left(\begin{array}{c}
I \\
O
\end{array}\right) \xrightarrow{\left(\begin{array}{c}
f_{b} \\
f
\end{array}\right)}\left(\begin{array}{c}
A^{-} \\
A^{+}
\end{array}\right) \\
& \alpha=\|\| \begin{array}{c}
\downarrow \uparrow\left(\begin{array}{c}
w^{\sharp} \\
w
\end{array}\right) \\
\left(\begin{array}{c}
B^{-} \\
B^{+}
\end{array}\right) \\
\downarrow \uparrow \left\lvert\,\left(\begin{array}{c}
v^{\sharp} \\
v
\end{array}\right)\right.
\end{array} \\
& \left(\begin{array}{l}
I \\
O
\end{array}\right) \underset{\left(\begin{array}{c}
h_{b} \\
h
\end{array}\right)}{\longrightarrow}\left(\begin{array}{l}
C^{-} \\
C^{+}
\end{array}\right)
\end{aligned}
$$

Explicitly, this means that we have commuting squares

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-249.jpg?height=352&width=919&top_left_y=331&top_left_x=598)

We then get a chart

$$
\left(\begin{array}{c}
h_{b} \stackrel{\circ}{*} w^{*} v^{\sharp} \\
f \circ w
\end{array}\right):\left(\begin{array}{l}
I \\
O
\end{array}\right) \rightrightarrows\left(\begin{array}{l}
B^{-} \\
B^{+}
\end{array}\right) \text {. }
$$

This chart fits into two squares like so:

$$
\begin{aligned}
& \left(\begin{array}{l}
I \\
O
\end{array}\right) \xrightarrow{\left(\begin{array}{c}
f_{b} \\
f
\end{array}\right)}\left(\begin{array}{c}
A^{-} \\
A^{+}
\end{array}\right)
\end{aligned}
$$

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-249.jpg?height=238&width=374&top_left_y=1193&top_left_x=867)

$$
\begin{aligned}
& \|\| \| \quad \downarrow \uparrow\left(\begin{array}{l}
v^{\sharp} \\
v
\end{array}\right) \\
& \left(\begin{array}{l}
I \\
O
\end{array}\right) \underset{\left(\begin{array}{c}
h_{b} \\
h
\end{array}\right)}{\rightrightarrows}\left(\begin{array}{l}
C^{+} \\
C^{-}
\end{array}\right)
\end{aligned}
$$

The bottom half of the top square and the top half of the bottom square commute by definition. The bottom half of the bottom square asks that $f \circ w \circ v=h$, but this is precisely the bottom half of $\alpha$. The top half of the top square asks that the following diagram commute:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-249.jpg?height=474&width=412&top_left_y=1969&top_left_x=843)

This is a rearrangement of the second square in Diagram 5.5.

Because we have just rearranged the data of the big outer square $\alpha$, this factorization of $\alpha$ is unique.

As a corollary, Theorem 5.3.1.6 gives us a representable double functor

$$
\operatorname{Arena}_{\mathbb{T}}\left(\left(\begin{array}{l}
I \\
O
\end{array}\right),-\right): \text { Arena }_{\mathbb{T}} \rightarrow \text { Matrix }
$$

in any systems theory $\mathbb{T}$. So we can turn any lens in any systems theory into a matrix in a way that preserves the composition of lenses.

Theorem 5.3.1.8. For any systems theory $\mathbb{T}$ and any arena $\left(\begin{array}{l}I \\ O\end{array}\right)$, there is a representable double functor

$$
\text { Arena }_{\mathbb{T}}\left(\left(\begin{array}{l}
I \\
O
\end{array}\right),-\right): \text { Arena }_{\mathbb{T}} \rightarrow \text { Matrix. }
$$

\subsection*{5.3.2 How behaviors of systems wire together: representable doubly indexed functors}

We now come to the mountaintop. It's been quite a climb, and we're almost there.

We can now describe all the ways that behaviors of systems get put together when we wire systems together. There are a bunch of laws governing how behaviors get put together, and we organize them all into the notion of a lax doubly indexed functor. To any system $T$ in a systems theory $\mathbb{T}$, we will give a lax doubly indexed functor

$$
\text { Behave }_{\mathrm{T}}: \text { Sys }_{\mathbb{T}} \rightarrow \text { Vec. }
$$

Since behaviors of shape $T$ are a sort of map out of $T$, we may think of Behave ${ }_{T}$ as a representable lax doubly indexed functor.

Theorem 5.3.2.1. For any systems theory $\mathbb{T}$ and any system $T$ in $\mathbb{T}$, there is a lax doubly indexed functor Behave $\mathrm{T}_{\mathrm{T}}: \mathbf{S y s}_{\mathbb{T}} \rightarrow$ Vec which sends systems to their sets of T-shaped behaviors.

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-250.jpg?height=311&width=618&top_left_y=1910&top_left_x=748)

Let's see what this theorem is really asking for while we construct it. As with many of the constructions we have been seeing, the hard part is understanding what we are supposed to be constructing; once we do that, the answer will always be "compose in the appropriate way in the appropriate double category".
- First, we need Behave ${ }_{\mathrm{T}}^{0}:$ Arena $\rightarrow$ Matrix which send an arena to the set of charts from $\left(\begin{array}{c}\ln _{T} \\ \text { Out }_{T}\end{array}\right)$ to that arena. It will send a chart to the function given by composing with that chart, and it will send a lens to a matrix that describes the wiring pattern in that lens. We've seen how to do this in Theorem 5.3.1.8:

$$
\text { Behave }_{\mathrm{T}}^{0}=\operatorname{Arena}\left(\left(\begin{array}{c}
\ln _{\mathrm{T}} \\
\text { Out }_{\mathrm{T}}
\end{array}\right),-\right)
$$

This is the blueprint for how our systems will compose.
- Next, for any arena $\left(\begin{array}{l}I \\ O\end{array}\right)$, we need a functor

$$
\operatorname{Behave}_{\mathrm{T}}^{\left(\begin{array}{l}
I \\
O
\end{array}\right)}: \operatorname{Sys}\left(\begin{array}{l}
I \\
O
\end{array}\right) \rightarrow \operatorname{Vec}\left(\operatorname{Arena}\left(\left(\begin{array}{c}
\ln _{\mathrm{T}} \\
\text { Out }_{\mathrm{T}}
\end{array}\right),\left(\begin{array}{l}
I \\
O
\end{array}\right)\right)\right)
$$

which will send a system $S$ with interface $\left(\begin{array}{l}I \\ O\end{array}\right)$ to its set of behaviors of shape $T$, indexed by their chart. That is, we make the following definition:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-251.jpg?height=170&width=614&top_left_y=1075&top_left_x=799)

This is functorial by horizontal associativity of squares in Arena.
- For any lens $\left(\begin{array}{c}w^{\sharp} \\ w\end{array}\right):\left(\begin{array}{l}I \\ O\end{array}\right) \leftrightarrows\left(\begin{array}{c}I^{\prime} \\ O^{\prime}\end{array}\right)$, we need a natural transformation

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-251.jpg?height=526&width=1044&top_left_y=1420&top_left_x=581)

This will take any behaviors of component systems whose charts compatible according to the wiring pattern of $\left(\begin{array}{c}w^{\sharp} \\ w\end{array}\right)$ and wire them together into a behavior of the wired together systems. In other words, this will be given by vertical composition of squares in Arena. To see how that works, we need follow a $\left(\begin{array}{l}I \\ O\end{array}\right)$ system $S$ around this diagram and see how this natural transformation can be described so simply. Following $S$ around the top path of the diagram gives us the following vector of sets, we first send $S$ to the vector of sets

$$
\left(\begin{array}{c}
f_{\mathrm{b}} \\
f
\end{array}\right):\left(\begin{array}{c}
\ln _{\mathrm{T}} \\
\mathrm{Out}_{\mathrm{T}}
\end{array}\right) \rightrightarrows\left(\begin{array}{c}
I \\
O
\end{array}\right) \mapsto \operatorname{Sys}\left(\begin{array}{c}
f_{\mathrm{b}} \\
f
\end{array}\right)(\mathrm{T}, \mathrm{S})
$$

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-252.jpg?height=561&width=707&top_left_y=229&top_left_x=926)

We then multiply this by the matrix Arena $\left(\left(\begin{array}{c}\ln _{T} \\ \text { Out }_{T}\end{array}\right),\left(\begin{array}{c}w^{\sharp} \\ w\end{array}\right)\right)$ to get the vector of sets whose entries are pairs of the following form:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-252.jpg?height=550&width=1528&top_left_y=950&top_left_x=426)

On the other hand, following $S$ along the bottom path has us first composing it vertically with $\left(\begin{array}{c}w^{\sharp} \\ w\end{array}\right)$ and then finding the behaviors in it:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-252.jpg?height=557&width=1151&top_left_y=1665&top_left_x=530)

Finally, we are ready to define our natural transformation from the virst vector of sets to the second using vertical composition:

$$
\operatorname{Behave}_{\mathrm{T}}\left(\begin{array}{c}
w^{\sharp} \\
w
\end{array}\right)(\mathrm{S})_{\left(\begin{array}{c}
g_{b} \\
g
\end{array}\right)}\left(\square_{w}, \phi\right)=\frac{\phi}{\square_{w}} .
$$

That this is natural for behaviors $\psi: S \rightarrow U$ in $\operatorname{Sys}\left(\begin{array}{l}I \\ O\end{array}\right)$ follows quickly from the horizontal identity and interchange laws in Arena:

$$
\begin{aligned}
\frac{\phi \mid \psi}{\square_{w}} & =\frac{\phi \mid \psi}{\square_{w} \left\lvert\,\left(\begin{array}{c}
w^{\sharp} \\
w
\end{array}\right)\right.} \\
& =\frac{\phi}{\square_{w}} \left\lvert\, \frac{\psi}{\left(\begin{array}{c}
w^{\sharp} \\
w
\end{array}\right)} .\right.
\end{aligned}
$$
- For any chart $\left(\begin{array}{c}g_{b} \\ g\end{array}\right):\left(\begin{array}{l}I \\ O\end{array}\right) \rightrightarrows\left(\begin{array}{c}I^{\prime} \\ O^{\prime}\end{array}\right)$, we need a square

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-253.jpg?height=552&width=1376&top_left_y=838&top_left_x=445)

This will take any behavior from $\mathrm{S}$ to $\mathrm{U}$ with chart $\left(\begin{array}{c}g_{b} \\ g\end{array}\right)$ and give the function which takes behaviors of shape $\mathrm{T}$ in $\mathrm{S}$ and gives the composite behavior of shape $\mathrm{T}$ in $\mathrm{U}$. That is,

$$
\text { Behave } \left._{\mathrm{T}}^{\left(\begin{array}{c}
g_{b} \\
g
\end{array}\right)}(\mathrm{S}, \mathrm{U})(\psi)=\phi \mapsto \phi \right\rvert\, \psi \text {. }
$$

The naturality of this assignment follows from horizontal associativity in Arena. Its a bit scary to see written out with all the names and symbols, but the idea is simple enough. We are composing two sorts of things: behaviors and systems. If we have some behaviors of shape $T$ in our systems and their charts are compatible with a wiring pattern, then we get a behavior of the wired together system. If we have a chart, then behaviors with that chart give us a way of mapping forward behaviors of shape T.

The lax doubly indexed functor laws now tell us some facts about how these two sorts of composition interact.
- (Vertical Lax Functoriality) This asks us to suppose that we are wiring our systems together in two stages. The law then says that if we take a bunch of behaviors whose charts are compatible for the total wiring pattern and wire them together into a behavior of the whole system, this is the same behavior we get if we first noticed that they were compatible for the first wiring pattern, wired them together, then noticed that the result was compatible for the second wiring pattern,
and wired that together. This means that nesting of wiring diagrams commutes with finding behaviors of our systems.
- (Horizontal Functoriality) This asks us to suppose that we have two charts and a behavior of each. The law then says that composing a behavior of shape $T$ with the composite of those behaviors is the same as composing it with the first one and then with the second one.
- (Functorial Interchange) This asks us to suppose that we have a pair of wiring patterns and compatible charts between them (a square in Arena). The law then says that if we take a bunch of behaviors whose charts are compatable according to the first wiring pattern, wire them together, and then compose with a behavior of the second chart, we get the same thing as if we compose them all with behaviors of the first chart, noted that they were compatible with the second wiring pattern, and then wired them together.

Though it seems like it would be a mess of symbols to check these laws, they in fact fall right out of the laws for the double categories of arenas and matrices, and the functoriality of Proposition 5.3.1.4. That is, we've already built up all the tools we need to prove this fact, we just need to finish proving that Behave $T_{T}$ is a lax doubly indexed functor.
- (Vertical Lax Functoriality) Suppose we have composable lenses $\left(\begin{array}{c}w^{\sharp} \\ w\end{array}\right):\left(\begin{array}{c}I_{1} \\ O_{1}\end{array}\right) \leftrightarrows$ $\left(\begin{array}{c}I_{2} \\ \mathrm{O}_{2}\end{array}\right)$ and $\left(\begin{array}{c}u^{\sharp} \\ u\end{array}\right):\left(\begin{array}{c}I_{2} \\ \mathrm{O}_{2}\end{array}\right) \leftrightarrows\left(\begin{array}{c}I_{3} \\ \mathrm{O}_{3}\end{array}\right)$. We need to show that

Behave $_{\mathrm{T}}\left(\begin{array}{c}w^{\sharp} \\ w\end{array}\right):\left(\begin{array}{c}u^{\sharp} \\ u\end{array}\right)=\left(\right.$ Behave $_{\mathrm{T}}\left(\begin{array}{c}u^{\sharp} \\ u\end{array}\right)$ Sys $\left.\left(\begin{array}{c}w^{\sharp} \\ w\end{array}\right)\right) \circ\left(\operatorname{VecArena}\left(\left(\begin{array}{c}\ln _{\mathrm{T}} \\ \text { Out }_{\mathrm{T}}\end{array}\right),\left(\begin{array}{c}u^{\sharp} \\ u\end{array}\right)\right)\right.$ Behave $\left._{\mathrm{T}}^{\left(\begin{array}{c}w^{\sharp} \\ w\end{array}\right)}\right)$.

This follows immediately from vertical associativity in Arena, once both sides have been expanded out. Let $S$ be a $\left(\begin{array}{c}I_{1} \\ O_{1}\end{array}\right)$-system, then

$$
\begin{aligned}
& \text { Behave }_{\mathrm{T}}\left(\begin{array}{c}
w^{\sharp} \\
w
\end{array}\right):\left(\begin{array}{c}
u^{\sharp} \\
u
\end{array}\right)(\mathrm{S})(\alpha, \phi)=\operatorname{Behave}_{\mathrm{T}}\left(\begin{array}{c}
w^{\sharp} \\
w
\end{array}\right):\left(\begin{array}{c}
u^{\sharp} \\
u
\end{array}\right)(\mathrm{S})\left(\frac{\beta}{\gamma}, \phi\right) \\
& =\frac{\phi}{\frac{\beta}{\gamma}} \\
& =\frac{\frac{\phi}{\beta}}{\gamma}
\end{aligned}
$$

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-254.jpg?height=158&width=1192&top_left_y=2206&top_left_x=846)
- (Horizontal Functoriality) This follows directly from horizontal associativity in Arena.
- (Functorial Interchange) This law will follow directly from interchange in the double category of arenas. Let $\alpha$ be a square in Arena of the following form:

$$
\begin{array}{r}
\left(\begin{array}{c}
A^{-} \\
A^{+}
\end{array}\right) \xrightarrow{\left(\begin{array}{c}
f_{b} \\
f
\end{array}\right)}\left(\begin{array}{l}
B^{-} \\
B^{+}
\end{array}\right) \\
\alpha=\left(\begin{array}{c}
j^{\sharp} \\
j
\end{array}\right) \downarrow \uparrow \\
\downarrow \uparrow\left(\begin{array}{c}
k^{\sharp} \\
k
\end{array}\right) \\
\left(\begin{array}{l}
C^{-} \\
C^{+}
\end{array}\right) \xrightarrow[\left(\begin{array}{l}
g^{\sharp} \\
g
\end{array}\right)]{\longrightarrow}\left(\begin{array}{l}
D^{-} \\
D^{+}
\end{array}\right)
\end{array}
$$

We need to show that

$$
\text { Behave }_{\mathrm{T}}\left(\begin{array}{c}
j^{\sharp}  \tag{5.6}\\
j
\end{array}\right)\left|\frac{\operatorname{Sys}(\alpha)}{\text { Behave }_{\mathrm{T}}\left(\begin{array}{c}
g_{\mathrm{b}} \\
g
\end{array}\right)}=\frac{\text { Behave }_{\mathrm{T}}\left(\begin{array}{c}
f_{\mathrm{b}} \\
f
\end{array}\right)}{\operatorname{VecArena}\left(\left(\begin{array}{c}
\ln _{\mathrm{T}} \\
\text { Out }_{\mathrm{T}}
\end{array}\right), \alpha\right)}\right| \text { Behave }_{\mathrm{T}}\left(\begin{array}{c}
k^{\sharp} \\
k
\end{array}\right)
$$

We can see both sides as natural transformations of the signature

$$
\begin{aligned}
& \operatorname{Sys}\left(\begin{array}{l}
A^{-} \\
A^{+}
\end{array}\right) \xrightarrow{\operatorname{Sys}\left(\begin{array}{l}
f_{\mathrm{b}} \\
f
\end{array}\right)} \operatorname{Sys}\left(\begin{array}{l}
B^{-} \\
B^{+}
\end{array}\right) \\
& \text {Behave }_{\mathrm{T}}\left(\begin{array}{l}
A^{-} \\
A^{+}
\end{array}\right) \downarrow \\
& \operatorname{VecArena}\left(\left(\begin{array}{c}
\operatorname{In}_{\mathrm{T}} \\
\text { Out }_{\mathrm{T}}
\end{array}\right),\left(\begin{array}{c}
A^{-} \\
A^{+}
\end{array}\right)\right) \quad 5.6 \quad \operatorname{Sys}\left(\begin{array}{c}
D^{-} \\
D^{+}
\end{array}\right) \\
& \operatorname{VecArena}\left(\left(\begin{array}{c}
\ln _{\mathrm{T}} \\
\text { Out }_{\mathrm{T}}
\end{array}\right),\left(\begin{array}{c}
j^{\sharp} \\
j
\end{array}\right)\right) \downarrow \quad \downarrow_{\text {Behave }_{\mathrm{T}}}\left(\begin{array}{c}
D^{-} \\
D^{+}
\end{array}\right) \\
& \operatorname{VecArena}\left(\left(\begin{array}{c}
\ln _{\mathrm{T}} \\
\text { Out }_{\mathrm{T}}
\end{array}\right),\left(\begin{array}{c}
C^{-} \\
C^{+}
\end{array}\right)\right)_{\text {VecArena }\left(\left(\begin{array}{c}
\ln _{\mathrm{T}} \\
\text { Out }
\end{array}\right),\left(\begin{array}{c}
g_{\mathrm{b}} \\
g
\end{array}\right)\right.}^{\longrightarrow} \operatorname{VecArena}\left(\left(\begin{array}{c}
\ln _{\mathrm{T}} \\
\text { Out }_{\mathrm{T}}
\end{array}\right),\left(\begin{array}{c}
D^{-} \\
D^{+}
\end{array}\right)\right)
\end{aligned}
$$

So, to show this equality holds, let's start with a behavior $\psi \in \operatorname{Sys}\left(\begin{array}{c}f_{b} \\ f\end{array}\right)(S, U)$ with chart $\left(\begin{array}{c}f_{b} \\ f\end{array}\right)$. We need to show that passing this through the left side of Eq. (5.6) equals the result of passing it through the right hand side. On both sides, the result is an element of

$$
\operatorname{VecArena}\left(\left(\begin{array}{c}
\ln _{T} \\
\text { Out }_{T}
\end{array}\right),\left(\begin{array}{c}
g_{b} \\
g
\end{array}\right)\right)(\cdots, \cdots)
$$
and is for that reason a function that takes in a pair of the following form:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-256.jpg?height=547&width=1254&top_left_y=320&top_left_x=484)

The left hand side sends this pair to

$$
\operatorname{Behave}_{\mathrm{T}}^{\left(\begin{array}{c}
g_{b} \\
g
\end{array}\right)}(\operatorname{Sys}(\alpha)(\psi))\left(\operatorname{Behave}_{\mathrm{T}}\left(\begin{array}{c}
j^{\sharp} \\
j
\end{array}\right)\left(\square_{j}, \phi\right)\right)
$$

which equals, rather simply:

$$
\left.\frac{\phi}{\square_{j}} \right\rvert\, \frac{\psi}{\alpha}
$$

The right hand side sends the pair to

$$
\operatorname{Behave}_{\mathrm{T}}\left(\begin{array}{c}
k^{\sharp} \\
k
\end{array}\right)\left(\operatorname{VecArena}\left(\left(\begin{array}{c}
\ln _{\mathrm{T}} \\
\text { Out }_{\mathrm{T}}
\end{array}\right), \alpha\right)\left(\square_{j}, \operatorname{Behave}_{\mathrm{T}}^{\left(\begin{array}{c}
f_{\mathrm{b}} \\
f
\end{array}\right)}(\psi)(\phi)\right)\right)
$$

which equals, rather simply:

$$
\frac{\phi \mid \psi}{\square_{j} \mid \alpha}
$$

That these two composites are equal is precisely the interchange law of a double category.

While we have phrased this theorem in terms of systems theories, the proof uses only the structure available to the doubly indexed category $\mathbf{S y s}_{\mathbb{T}}$ : Arena $\mathbf{T}_{\mathbb{T}} \rightarrow$ Cat itself. We can therefore state this theorem entirely abstractly, which we record here.

Theorem 5.3.2.2. Let $\mathcal{A}: \mathscr{D} \rightarrow$ Cat be a doubly indexed category with $\mathscr{D}$ a spanlike double category. Then for any $T \in \mathcal{A}(D)$, there is a representable lax doubly indexed functor

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-256.jpg?height=318&width=463&top_left_y=2172&top_left_x=820)

\subsection*{5.3.3 Is the whole always more than the composite of its parts?}

Unfortunately, Behave ${ }_{\mathrm{T}}$ is lax (and not taut) for general T. This means that while behaviors of component systems will induce behaviors of composite systems, it isn't necessarily the case that all behaviors of the composite arise this way.

But there is a simple condition we can put on $T$ which will ensure that Behave ${ }_{T}$ is taut, and therefore that we can recover the behaviors of wired together systems from the behaviors of their components: we ask that $\mathrm{T}$ expose its entire state, which is to say that expose $_{\mathrm{T}}$ is an isomorphism.

Theorem 5.3.3.1. Let $T$ be a system in the systems theory $\mathbb{T}$, and suppose that expose $_{\mathrm{T}}$ is an isomorphism. Then the representable lax doubly indexed functor Behave $\mathrm{T}_{\mathrm{T}}$ is in fact taut. Explicitly, for any lens $\left(\begin{array}{c}w^{\sharp} \\ w\end{array}\right):\left(\begin{array}{l}I \\ O\end{array}\right) \leftrightarrows\left(\begin{array}{c}I^{\prime} \\ O^{\prime}\end{array}\right)$ the natural transformation

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-257.jpg?height=517&width=1033&top_left_y=1037&top_left_x=535)

is a natural isomorphism.

Many of the systems representing sorts of behavior which we saw in Chapter 3 expose their entire state. For example, the systems Time representing trajectories (Example 3.3.0.7), Fix representing steady states (Example 3.3.0.8), and Clock ${ }_{n}$ periodic orbits with periodic parameters (Example 3.3.0.9). As examples of systems which don't expose their entire state, we had the systems which represent steady looking trajectories and periodic orbits whose parameters aren't periodic from Exercise 3.3.0.10. Theorem 5.3.3.1 says that for the systems Time, Fix, and Clock ${ }_{n}$, we can recover the behaviors of component systems from the behaviors of composite systems. As we noted in Remark 3.2.2.5, the same cannot be said for steady looking trajectories.

Proof of Theorem 5.3.3.1. We recall that for a $\left(\begin{array}{l}I \\ O\end{array}\right)$-system S, the natural transformation

Behave $_{\mathrm{T}}\left(\begin{array}{c}w^{\sharp} \\ w\end{array}\right)$ goes from the vector of sets

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-258.jpg?height=560&width=1546&top_left_y=376&top_left_x=322)

to the vector of sets

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-258.jpg?height=557&width=1146&top_left_y=1017&top_left_x=484)

The transformation itself is given by vertical composition:

$$
\operatorname{Behave}_{\mathrm{T}}\left(\begin{array}{c}
w^{\sharp} \\
w
\end{array}\right)(\mathrm{S})_{\left(\begin{array}{c}
g_{b} \\
g
\end{array}\right)}\left(\square_{w}, \phi\right)=\frac{\phi}{\square_{w}} \text {. }
$$

We'll construct an inverse to this assuming that expose $_{\mathrm{T}}$ is an isomorphism. Suppose we have a square

$$
\begin{aligned}
& \left(\begin{array}{l}
\text { State }_{\mathrm{T}} \\
\text { State }_{\mathrm{T}}
\end{array}\right) \xrightarrow{\left(\begin{array}{c}
T \phi \\
\phi
\end{array}\right)}\left(\begin{array}{l}
\text { State }_{\mathrm{S}} \\
\text { States }
\end{array}\right) \\
& \alpha=\left(\begin{array}{c}
\text { update }_{\mathrm{T}} \\
\text { expose }_{\mathrm{T}}
\end{array}\right) \downarrow \uparrow \quad \downarrow\left(\left(\begin{array}{c}
\text { update }_{\mathrm{S}} \\
\text { expose }_{\mathrm{S}}
\end{array}\right),\left(\begin{array}{c}
w^{\sharp} \\
w
\end{array}\right)\right. \\
& \left(\begin{array}{c}
\ln _{\mathrm{T}} \\
\text { Out }_{\mathrm{T}}
\end{array}\right) \underset{\left(\begin{array}{c}
g_{b} \\
g
\end{array}\right)}{\longrightarrow}\left(\begin{array}{c}
I^{\prime} \\
O^{\prime}
\end{array}\right)
\end{aligned}
$$

From this data, we can define a chart

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-259.jpg?height=150&width=929&top_left_y=318&top_left_x=593)

It isn't obvious that the top composite is well defined since we have $g_{\mathrm{b}}: \ln _{\mathrm{T}} \rightarrow g^{*} I^{\prime}$ and $\operatorname{expose}_{\mathrm{T}}^{-1 *} \phi^{*} \operatorname{expose}_{\mathrm{S}}^{*} w^{\sharp}: \operatorname{expose}_{\mathrm{T}}^{-1 *} \phi^{*} \operatorname{expose}_{\mathrm{S}}^{*} w^{*} I^{\prime} \rightarrow \operatorname{expose}_{\mathrm{T}}^{-1 *} \phi^{*} \operatorname{expose}_{\mathrm{S}}^{*} I$ and the codomain of the first doesn't appear to be the domain of the second. But the square $\alpha$ tells us that $\phi \risingdotseq \operatorname{expose}_{\mathrm{S}} \risingdotseq w=\operatorname{expose}_{\mathrm{T}} \risingdotseq g$, so we have that

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-259.jpg?height=80&width=943&top_left_y=719&top_left_x=580)

So the two maps really are composable.

Next, we note that the following is a square:

$$
\begin{aligned}
& \left(\begin{array}{c}
\ln _{\mathrm{T}} \\
\text { Out }_{\mathrm{T}}
\end{array}\right) \stackrel{\left(\begin{array}{c}
h_{b} \\
h_{1}
\end{array}\right)}{=-\exists}\left(\begin{array}{c}
I \\
O
\end{array}\right) \\
& \square_{h}=\|\| \| \quad \downarrow \uparrow\left(\begin{array}{c}
w^{\sharp} \\
w
\end{array}\right) \\
& \left(\begin{array}{c}
\ln _{\mathrm{T}} \\
\text { Out }_{\mathrm{T}}
\end{array}\right) \underset{\left(\begin{array}{c}
g_{b} \\
g
\end{array}\right)}{\longrightarrow}\left(\begin{array}{c}
I^{\prime} \\
O^{\prime}
\end{array}\right)
\end{aligned}
$$

since

$$
\begin{aligned}
h \circ w & =\operatorname{expose}_{\mathrm{T}}^{-1} \circ \phi \stackrel{\circ}{ } \operatorname{expose}_{\mathrm{S}} \circ w \\
& =\operatorname{expose}_{\mathrm{T}}^{-1} \stackrel{\circ}{ } \operatorname{expose}_{\mathrm{T}} \circ g
\end{aligned}
$$

$$
\begin{array}{rlr} 
& =g & \text { and } \\
h_{b} & =g_{\mathrm{b}} \stackrel{\circ}{ } h^{*} w^{\sharp} \quad \text {, by definition. }
\end{array}
$$

We see that the definition of $\left(\begin{array}{c}h_{b} \\ h\end{array}\right)$ is basically forced on us by the commutation of this diagram. Furthermore, we note that we have a square:

$$
\begin{aligned}
& \left(\begin{array}{l}
\text { State }_{\mathrm{T}} \\
\text { State }_{\mathrm{T}}
\end{array}\right) \xrightarrow{\left(\begin{array}{c}
T \phi \\
\phi
\end{array}\right)}\left(\begin{array}{l}
\text { State }_{\mathrm{S}} \\
\text { States }
\end{array}\right) \\
& \beta=\left(\begin{array}{c}
\text { update }_{\mathrm{T}} \\
\text { expose }_{\mathrm{T}}
\end{array}\right) \left\lvert\, \uparrow \quad \downarrow \uparrow\left(\begin{array}{l}
\text { update }_{\mathrm{S}} \\
\text { expose }_{\mathrm{S}}
\end{array}\right)\right. \\
& \left(\begin{array}{c}
\ln _{\mathrm{T}} \\
\text { Out }_{\mathrm{T}}
\end{array}\right) \underset{\left(\begin{array}{c}
h_{\mathrm{b}} \\
h
\end{array}\right)}{\longrightarrow}\left(\begin{array}{l}
I \\
O
\end{array}\right)
\end{aligned}
$$
by the commutativity of $\alpha$.

Finally, it remains to show that for any $\left(\begin{array}{c}h_{b} \\ h\end{array}\right)$ fitting into these two squares $\square_{h}$ and $\beta$, we have that

$$
\left(\begin{array}{c}
h_{b} \\
h
\end{array}\right)=\left(\begin{array}{c}
g_{\mathrm{b}} \stackrel{\circ}{\operatorname{expose}} \mathrm{T}_{\mathrm{T}}^{-1 *} \phi^{*} \operatorname{expose}_{\mathrm{S}}^{*} w^{\sharp} \\
\operatorname{expose}_{\mathrm{T}}^{-1} \stackrel{ }{\circ} \phi \stackrel{\circ}{\circ} \operatorname{expose}_{\mathrm{S}}
\end{array}\right) \text {. }
$$

From the bottom of $\beta$, we see that $\operatorname{expose}_{\mathrm{T}} \stackrel{\circ}{ } h=\phi \stackrel{\circ}{ }$ expose $_{\mathrm{S}}$, which means that

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-260.jpg?height=52&width=1450&top_left_y=603&top_left_x=327)
see exactly that $h_{\mathrm{b}}=g_{\mathrm{b}} \stackrel{\circ}{ } h^{*} w^{\sharp}$.

Example 5.3.3.2. In the deterministic systems theory $\mathbb{D}_{\mathrm{ET}}$, consider the system Time of Example 3.3.0.7:

$$
\left(\begin{array}{c}
t \mapsto t+1 \\
\mathrm{id}
\end{array}\right):\left(\begin{array}{c}
\mathbb{N} \\
\mathbb{N}
\end{array}\right) \leftrightarrows\left(\begin{array}{c}
\{\text { tick }\} \\
\mathbb{N}
\end{array}\right)
$$

This system exposes its entire state since expose $_{\text {Time }}=\mathrm{id}$. A behavior of shape Time is a trajectory. So, by Theorem 5.3.3.1, we get a doubly indexed functor:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-260.jpg?height=322&width=634&top_left_y=1189&top_left_x=735)

For any $\left(\begin{array}{l}I \\ O\end{array}\right)$-system S, we get a vector of sets

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-260.jpg?height=124&width=316&top_left_y=1643&top_left_x=899)

sending each chart $\left(\begin{array}{l}i \\ o\end{array}\right):\left(\begin{array}{l}1 \\ \mathbb{N}\end{array}\right) \rightrightarrows\left(\begin{array}{l}I \\ O\end{array}\right)$ â€” which is to say sequences $o: \mathbb{N} \rightarrow O$ and $i: \mathbb{N} \rightarrow I$ of inputs - to the set of trajectories $s: \mathbb{N} \rightarrow \mathrm{S}$ for that chart. These trajectories are, explicitly, sequences which satisfy the equations

$$
\begin{aligned}
s_{t+1} & =\text { update }_{\mathrm{S}}\left(s_{t}, i_{t}\right) \\
\operatorname{expose}_{\mathrm{S}}\left(s_{t}\right) & =o_{t} .
\end{aligned}
$$

Theorem 5.3.3.1 tells us that trajectories in a composite system are families of trajectories for each system which agree on all the information passed between the wires. For example, consider the wiring diagram

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-261.jpg?height=263&width=396&top_left_y=275&top_left_x=859)

Let's suppose that all wires carry real numbers. Then this wiring diagram can be represented by the lens

$$
\left(\begin{array}{c}
w^{\sharp} \\
w
\end{array}\right):\left(\begin{array}{c}
\mathbb{R} \\
\mathbb{R} \times \mathbb{R}
\end{array}\right) \otimes\left(\begin{array}{c}
\mathbb{R} \times \mathbb{R} \\
\mathbb{R}
\end{array}\right) \leftrightarrows\left(\begin{array}{c}
\mathbb{R} \times \mathbb{R} \\
\mathbb{R}
\end{array}\right)
$$

given by

$$
\begin{aligned}
w((a, b), c) & =c \\
w^{\sharp}(((a, b), c),(x, y)) & =(x,(b, y)) .
\end{aligned}
$$

Jaz: FINISH THIS.

Example 5.3.3.3. In a differential systems theory â€” for simplicity let's say the Euclidean differential systems theory $\mathbb{E} u c$ - the system

$$
\text { Time }=\left(\begin{array}{c}
1 \\
\mathrm{id}
\end{array}\right):\left(\begin{array}{l}
\mathbb{R}^{1} \\
\mathbb{R}^{1}
\end{array}\right) \leftrightarrows\left(\begin{array}{l}
\mathbb{R}^{0} \\
\mathbb{R}^{1}
\end{array}\right)
$$

which expresses the differential equation

$$
\frac{d s}{d t}=1
$$

represents trajectories (see Example 3.5.2.5). As this system exposes its entire state, Theorem 5.3.3.1 gives us a doubly indexed functor

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-261.jpg?height=311&width=618&top_left_y=2170&top_left_x=748)

\subsection*{5.4 Summary and Further Reading}

In this chapter, we proved our main compositionality theorem relating the representable behaviors of composite systems to the behaviors of component systems. This theorem is a vast generalization of Spivak's theorem that steady states of coupled dynamical systems compose via matrix arithmetic [Spi15]. In categorical terms, we constructed representable doubly indexed functors on spanlike doubly indexed categories. On the indexing category, such representable doubly indexed functors are exactly ParÃ©'s representable double functors [Par11].

\section*{Chapter 6}

\section*{Dynamical System Doctrines}

\subsection*{6.1 Introduction}

Throughout this book so far, we seen dynamical systems modeled by state spaces exposing variables and updating according to external parameters. This sort of dynamical system is lens-based - systems are themselves lenses, and they compose by lens composition. We might describe them as parameter-setting systems, since we compose these systems by setting the parameters of some according to the exposed state variables of others.

There are many parameter-setting systems theories: deterministic (distrete, continuous, measurable), differential (Euclidean, general), non-deterministic (possibilistic, probabilistic, cost-aware, etc.). From each doctrine $\mathbb{T}$, we constructed a doubly indexed category $\mathbf{S y s}_{\mathbb{T}}:$ Arena $_{\mathbb{T}} \rightarrow$ Cat, indexed by the double category of arenas in the doctrine $\mathbb{T}$. This doubly indexed category organized the behaviors of the systems in doctrine $\mathbb{T}$ (through the charts) and the ways that systems can be composed (through the lenses).

But composing systems through lenses is not the only way to model systems. In this section we will see two more ways of understanding what it means to be a system: the behavioral approach to systems theory, which composes systems by sharing their exposed variables, and the diagrammatic approach to systems theory, which composes diagrams describing systems by gluing together their exposed parts. In the behavioral approach (see Section 6.2), systems are understood as (variable) sets of behaviors, some of which are exposed to their environment. These systems are composed by sharing these exposed behaviors - that is, by declaring the behaviors exposed by some systems to be the same. In the diagrammatic approach (see Section 6.3), systems are presented by diagrams formed by basic constituent parts, some of which are exposed to their environment. These systems are composed by gluing together their exposed parts.

In total, we will have three doctrines of dynamical systems - ways of thinking about what a theory of systems could be, including how they are to be composed.

Informal Definition 6.1.0.1. A doctrine of dynamical systems is a particular way to answer the following questions about it means to be a systems theory:
- What does it mean to be a system? Does it have a notion of states, or of behaviors? Or is it a diagram describing the way some primitive parts are organized?
- What should the interface of a system be?
- How can interfaces be connected in composition patterns?
- How are systems composed through composition patterns between their interfaces.
- What is a map between systems, and how does it affect their interfaces?
- When can maps between systems be composed along the same composition patterns as the systems.

The parameter-setting doctrine which has been the focus of the book so far answers these questions in the following way:
- A system consists of a notion of how things can be, called the states, and a notion of how things will change given how they are, called the dynamics. In total, a

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-264.jpg?height=81&width=984&top_left_y=1128&top_left_x=430)
- The dynamics of a system can invovle certain parameters, and expose some variables of its state. The admissible parameters can depend on the variables being exposed. In total, an interface for a system is an arena $\left(\begin{array}{c}\ln _{\mathrm{s}} \\ \text { Outs }\end{array}\right)$.
- A composition pattern between interfaces says which exposed variables will be passed forward, and how the internal parameters should be set according to the external parameters and the exposed variables. That is, a composition pattern is a lens.
- Systems are composed by setting the parameters of some according to the exposed variables of others. This is accomplished by lens composition.
- A map between systems is a function of state which respects observable behavior; it affects the interfaces as a chart.
- When we have a square in the double category of arenas between charts and lenses, we may compose maps of systems - behaviors - along the composition patterns represented by the lenses.

Formally, we have organized the answers to these questions in our definition of the doubly indexed category Sys $_{\mathbb{T}}:$ Arena $\mathbb{T}_{\mathbb{T}} \rightarrow$ Cat in a given doctrine $\mathbb{T}$. The doctrine further specifies these answers along the lines of Informal Definition 1.1.0.2. In general, there may be many systems theories in any doctrine, further specifying what it really means to be a system within that systems theory. At the end of the day, however, we can expect to get a doubly indexed category, indexed by a double category of interfaces and sending each interface to the category of systems with that interface.

We will not give a fully formal definition of dynamical systems doctrine in this book. Nevertheless, we can give a useful, semi-formal approximation: a doctrine is any systematic way to produce doubly indexed categories of systems.

Semi-formal Definition 6.1.0.2. A doctrine of dynamical systems is a systematic way to produce doubly indexed categories of systems. As a first pass, we might say a doctrine of composition $\mathfrak{P}$ is a functor

$$
\text { Sys }^{\mathfrak{P}}: \text { Doctrine }^{\mathfrak{P}} \rightarrow \text { DblIx }
$$

from a category of $\mathfrak{P}$-systems theories to the category of doubly indexed categories. To a $\mathfrak{P}$-doctrine $\mathbb{T}$, this associates a doubly indexed category

$$
\operatorname{Sys}_{\mathbb{T}}^{\mathfrak{P}}: \text { Interface }_{\mathbb{T}}^{\mathfrak{P}} \rightarrow \text { Cat }
$$

indexed by a double category Interface $\mathbb{T}_{\mathbb{T}}^{\mathfrak{P}}$ of interfaces in the $\mathfrak{P}$-doctrine $\mathbb{T}$. This answers the questions of Informal Definition 6.1.0.1 in the following ways:
- A system is an object of the category $\operatorname{Sys}_{\mathbb{T}}^{\mathfrak{P}}(I)$.

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-265.jpg?height=65&width=1244&top_left_y=943&top_left_x=386)
- The composition patterns between interfaces are the vertical maps of Interface $\mathbb{T}_{\mathbb{P}}^{\mathfrak{P}}$.
- The systems are composed along a composition pattern $c: I_{1} \rightarrow I_{2}$ by the functor $\mathbf{S y s}_{\mathbb{T}}^{\mathfrak{P}}(c): \mathbf{S y s}_{\mathbb{T}}^{\mathfrak{P}}\left(I_{1}\right) \rightarrow \mathbf{S y s}_{\mathbb{T}}^{\mathfrak{P}}\left(I_{2}\right)$.
- A map between systems Sys $S_{1} \in \mathbf{S y s}_{\mathbb{T}}^{\mathfrak{P}}\left(I_{1}\right)$ and $\operatorname{Sys}_{2} \in \mathbf{S y s}_{\mathbb{T}}^{\mathfrak{P}}\left(I_{2}\right)$ which acts as $f$ : $I_{1} \rightarrow I_{2}$ (a horizontal map in Interface $\mathbb{T}_{\mathbb{P}}^{\mathfrak{P}}$ ) is an element of $\mathbf{S y s}_{\mathbb{T}}^{\mathfrak{P}}(f)\left(\mathbf{S y s} S_{1}, \mathbf{S y s}_{2}\right.$ ).
- Maps can be composed along the same composition patterns as systems when there is a square $\alpha$ of the appropriate signature in Interface $\mathbb{T}_{\mathbb{T}}^{\mathfrak{P}}$; the composite morphism is $\mathbf{S y s} \mathbb{T}_{\mathbb{T}}^{\mathfrak{P}}(\alpha)(f)$.

Remark 6.1.0.3. We take the term doctrine from Lawvere. Lawvere used the term "doctrine" in categorical logic to describe the various ways to be a logical theory. For example, some theories are first order theories, expressed in first order logic. Some are algebraic theories, expressed using only equalities between function systems. The different sorts of theories - first order, higher order, algebraic, etc. - are the doctrines. We can see the following table of analogies:

\begin{tabular}{|c|c|c|c|c|c|}
\hline Level & -1 & 0 & 1 & 2 & 3 \\
\hline Logic & property & element & model & theory & doctrine \\
\hline Systems Theory & constraint & behavior & system & theory & doctrine \\
\hline
\end{tabular}

The "level" here is the categorical level, where a set is a 0-category, a category is a 1-category, and a 2-category - with maps between its objects and maps between those maps - is level 2. There is a set of behaviors in a system, a category of systems in a given theory (or, really, a doubly indexed category), and a 2-category of theories in a given doctrine (though we only described its 1-categorical structure in this book).

For nerds who like this sort of thing, I would like to emphasize that this level is not the truncation level. If instead of sets we were working with homotopy types, then level 0 would still be elements, and level -1 would be identifications between these
elements, and -2 identifications between these identifications, and so on. In general, the negative levels would have (increasingly abelian) cohomological information about the positive levels.

So far in this book, we have been working in the parameter-setting doctrine given by lens composition.

Definition 6.1.0.4. The parameter-setting doctrine $\mathfrak{P}_{\text {ARAMSETTING consists }}$ of the functor Sys $^{\mathfrak{P}_{\text {ARAMSetting }}}:$ Doctrine ${ }^{\mathfrak{P}_{\text {ARAMSEtTing }}} \rightarrow$ Dblix defined in Theorem 4.5.2.2

In this chapter, we will meet two other doctrines: the behavioral approach to systems theory which is characterized by span composition which we will call the variable sharing doctrine, and the diagrammatic approach to systems theory which is characterized by cospan composition which we will call the port-plugging doctrine. These three doctrines - parameter-setting, variable-sharing, and port-plugging - capture a wide range of categorical systems theories in use. They are, however, by no means exhaustive.

\subsection*{6.2 The Behavioral Approach to Systems Theory}

The parameter setting (lens-based) ways of thinking about systems are very useful for the design of systems; we give a minimal set of data (expose and update) which in principle determines all behaviors, though it might take some work to understand what behaviors are actually in the system once we have set it up. But for the analysis of dynamical systems, we seek to prove properties about how systems behave. It helps if we already know how a system behaves.

In the behavioral approach to systems theory, pioneered by Jan Willems, we take "behavior" as a primitive. In its most basic formulation, the behavioral approach to systems theory considers a system $\mathrm{S}$ to have a set $\mathrm{B}(S)$ of state variables or "behaviors". The system also exposes some of these state variables in a function expose ${ }_{S}: B_{S} \rightarrow V_{S}$ to a set $V_{S}$ of possible values for these exposed variables.

In other words, we can see the behavioral approach to systems theory as taking place in the doubly indexed category Vec : Matrix $\rightarrow$ Cat (or, as we'll see, some variants of it). An interface is a set $V$ of possible values, and a system is a vector $B_{v}$ of sets varying over $v \in V$ - the behaviors in which the exposed variables take that given value. This might sound a bit different from the idea of a function expose $\mathrm{S}_{\mathrm{S}}: \mathrm{B}_{\mathrm{S}} \rightarrow \mathrm{V}_{\mathrm{S}}$, but we can define the set $B_{v}$ to be $\operatorname{expose}_{\mathrm{S}}^{-1}(v)$, and pass between these two notions. That is, we are making use of the equivalence between the double categories of matrices and the double categories of spans explored in Section 3.4.2 to think of vectors of sets of length $V$ as functions into $V$, and to think of matrices of sets as spans.

Example 6.2.0.1. Consider the Lotka-Volterra predator prey model LK of Section 1.2.2
which is given by the following system of differential equations:

$$
\left\{\begin{array}{l}
\frac{d r}{d t}=\mathrm{b}_{\text {Rabbits }} \cdot r-c_{1} f r \\
\frac{d f}{d t}=c_{2} r f-\mathrm{d}_{\text {Foxes }} \cdot f
\end{array}\right.
$$

Here, $r$ is the population of rabbits and $f$ is the population of foxes. In Example 1.2.2.4, we saw how to represent this as a differential system $\left(\begin{array}{c}\mathbb{R}^{2} \\ \mathbb{R}^{2}\end{array}\right) \leftrightarrows\left(\begin{array}{c}\mathbb{R}^{2} \\ 1\end{array}\right)$ with expose ${ }_{\mathrm{LK}}=$ ! the terminal map and

$$
\operatorname{update}_{\text {Rabbits }}\left(r,\left(\mathrm{~b}_{\text {Rabbits }}, \mathrm{d}_{\text {Rabbits }}\right)\right)=\mathrm{b}_{\text {Rabbits }} \cdot r-\mathrm{d}_{\text {Rabbits }} \cdot r \text {. }
$$

For the behavioral approach, we would apply the doubly indexed functor taking trajectories (from Example 5.3.3.3) to get the behavioral point of view on the system LK:

$$
\operatorname{Behave}_{\text {Time }}^{\left(\begin{array}{l}
\mathbb{R}^{2} \\
\mathbb{R}^{2}
\end{array}\right)}(\mathrm{LK}) \in \operatorname{Vec}\left(\operatorname{Arena}_{\mathbb{E} \mathrm{Uc}}\left(\left(\begin{array}{c}
1 \\
\mathbb{R}
\end{array}\right),\left(\begin{array}{c}
\mathbb{R}^{2} \\
\mathbb{R}^{2}
\end{array}\right)\right)\right)
$$

In other words, the set $B_{L K}$ of behaviors is the set of trajectories together with their charts in LK, and $V_{L K}$ is the set of charts for those trajectories - that is, parameters varying in time. We can calculate this from the definitions in Theorem 5.3.2.1:

$$
\begin{aligned}
& \mathrm{V}_{\mathrm{LK}}=\operatorname{Arena}_{\mathbb{E} \mathrm{UC}}\left(\left(\begin{array}{c}
1 \\
\mathbb{R}
\end{array}\right),\left(\begin{array}{c}
\mathbb{R}^{2} \\
1
\end{array}\right)\right) \\
& =\left\{\left(\begin{array}{c}
\text { (babbits } \left.\text { d }_{\text {Foxes }}\right) \\
!
\end{array}\right):\left(\begin{array}{l}
1 \\
\mathbb{R}
\end{array}\right) \rightrightarrows\left(\begin{array}{c}
\mathbb{R}^{2} \\
1
\end{array}\right)\right\} \\
& =\left\{\left(\left(\text { b }_{\text {Rabbits }}, \mathrm{d}_{\text {Foxes }}\right)\right): \mathbb{R} \rightarrow \mathbb{R}^{4}\right\}
\end{aligned}
$$

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-267.jpg?height=553&width=675&top_left_y=1605&top_left_x=476)

$$
\begin{aligned}
& =\left\{\left((r, f),\left(\mathrm{b}_{\text {Rabbits }} \mathrm{d}_{\text {Foxes }}\right)\right): \mathbb{R} \rightarrow \mathbb{R}^{4} \left\lvert\,\left\{\begin{array}{l}
\frac{d r}{d t}=\mathrm{b}_{\text {Rabbits }} \cdot r-c_{1} f r \\
\frac{d f}{d t}=c_{2} r f-\mathrm{d}_{\text {Foxes }} \cdot f
\end{array}\right\}\right.\right.
\end{aligned}
$$
and the map $B_{\mathrm{LK}} \rightarrow V_{\mathrm{LK}}$ is the projection exposing the parameters:

$$
\left((r, f),\left(\mathrm{b}_{\text {Rabbits }}, \mathrm{d}_{\text {Foxees }}\right)\right) \mapsto\left(\mathrm{b}_{\text {Rabbits }}, \mathrm{d}_{\text {Foxees }}\right)
$$

Remark 6.2.0.2. Note that the parameters of the original differential system LK are considered as exposed variables of state in the behavioral approach. This is because the behavioral approach composes systems by setting exposed variables equal to eachother, so the parameters must be considered as exposed variables so that they can be set equal to other variables.

In this section, we'll a bit of how the behavioral approach to systems theory works, and why we might want to do it. We'll begin with the main idea in Section 6.2.1. Then, in Section 6.2.2, we'll see that in the behavioral approach, there is a different sort of undirected wiring diagram which is used to compose systems

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-268.jpg?height=314&width=477&top_left_y=927&top_left_x=821)

The idea of these bubble diagrams is that each wire carries an exposed variable of the behaviors of each system. An connection between wires expresses an equality of the variables carried on them. The wiring diagram as a whole shows how the systems in it share their variables. If lens-based systems are all about setting parameters, the behavioral approach to systems theory using spans is all about sharing variables.

Just as the wiring diagrams for lens based systems are the lenses in categories of arities, the wiring diagrams for the span-based behavioral approach are spans in the category of arities â€” cospans of finite sets.

\subsection*{6.2.1 The idea of the behavioral approach}

In the behavioral approach to systems theory, a system is a set (or variable set, see Section 6.2.3) of behaviors of the system. A system exposes some variables of its behaviors. We can draw a behavioral system as a blob with wires dangling out of it which we imagine are carrying the exposed variables. For example, the following system $S$ exposes three variables:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-268.jpg?height=128&width=138&top_left_y=2126&top_left_x=972)

As we have seen in Theorem 5.3.2.1, we can get behavioral systems for any type of behavior in any doctrine. One benefit of the behavioral approach is that all of these different systems theories can be composed on the same footing: they're all just behavioral systems.

Consider the following example borrowed from Willems' [Wil07]. We consider a square bucket of water with two pipes at the bottom through which the water can flow:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-269.jpg?height=295&width=588&top_left_y=384&top_left_x=758)

The variable behaviors here are the pressures $p_{11}$ and $p_{12}$, the flows $f_{11}$ and $f_{12}$, and the height of the water $h_{1}$. We suppose that these quantities are related in the following ways:

$$
\begin{aligned}
\frac{d h_{1}}{d t} & =F_{1}\left(h_{1}, p_{11}, p_{12}\right) \\
f_{11} & =H_{11}\left(h_{1}, p_{11}\right) \\
f_{12} & =H_{12}\left(h_{1}, p_{12}\right)
\end{aligned}
$$

for some functions $F_{1}, H_{11}$, and $H_{12}$. Therefore, the set of behaviors is the set real valued functions of time which satisfy these laws:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-269.jpg?height=246&width=1102&top_left_y=1297&top_left_x=506)

We will suppose that we will only pump water to and from the bucket through the pipes at the bottom. This means that we will only expose the variables concerning those pipes.

$$
\bigvee_{\text {Bucket }_{1}}=\left(\mathbb{R}^{4}\right)^{\mathbb{R}}
$$

and where

$$
\operatorname{expose}_{\text {Bucket }_{1}}\left(h_{1}, f_{11}, f_{12}, p_{11}, p_{12}\right)=\left(f_{11}, f_{12}, p_{11}, p_{12}\right) \text {. }
$$

We can bubble up the Bucket $_{1}$ system as the following bubble diagram:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-269.jpg?height=309&width=350&top_left_y=2036&top_left_x=882)

Each wire carries a variable element of $\mathbb{R} \rightarrow \mathbb{R}$, and the Bucket $_{1}$ system exposes four of such variables.

Now, suppose we had another bucket Bucket , $_{2}$, governed by a similar set of variables satisfying a similar set of laws, but with functions $F_{2}, H_{21}$, and $H_{22}$ instead.

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-270.jpg?height=325&width=1108&top_left_y=426&top_left_x=495)

Suppose we connect the two buckets up by the pipes at the bottom:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-270.jpg?height=306&width=892&top_left_y=931&top_left_x=606)

To express this combined system, we need that the pressures in the connected pipes to be equal (since they are now one pipe), and we need the flows to be opposite (since any flow out of one bucket goes into the other). That is, we need $p_{12}=p_{21}$ and $f_{12}+f_{21}=0$. All in all, the combined system has behaviors

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-270.jpg?height=370&width=1097&top_left_y=1574&top_left_x=509)

Meanwhile, the only variables which are exposed by Buckets are the two remaining open pipes, so

$$
\begin{aligned}
V_{\text {Buckets }} & =\mathbb{R} \rightarrow \mathbb{R}^{4} \\
\text { expose }_{\text {Buckets }}\left(\begin{array}{l}
\left(h_{1}, f_{11}, f_{12}, p_{11}, p_{12}\right) \\
\left(h_{2}, f_{21}, f_{22}, p_{21}, p_{22}\right)
\end{array}\right) & =\left(f_{11}, f_{22}, p_{11}, p_{22}\right)
\end{aligned}
$$

We can express the pattern of interconnection between Bucket ${ }_{1}$ and Bucket $_{2}$ as a
bubble diagram to see precisely how Buckets arises as a composition of the two systems:
![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-271.jpg?height=598&width=1068&top_left_y=346&top_left_x=534)

When the wires are connected in this diagram, they express an equality between their exposed variables. The top connection signifies that $p_{12}=p_{21}$. The bubbled up + signifies a relation (other than equality): it says that the sum of the variables on the top two wires equals the third wire. We set that third wire to be constant at 0 , so in total we get the relation that $f_{12}+f_{21}=0$.

We can analyze this composition of the systems Bucket ${ }_{1}$ and Bucket B $_{2}$ in terms of the doubly indexed category Vec : Matrix $\rightarrow$ Cat, or rather the equivalent doubly indexed category Set/(-) : Span(Set) $\rightarrow$ Cat. To see how this works, let's remember how we compose lens based systems in a given doctrine $\mathbb{T}$.

Let's compose the Clock and Meridian systems into the ClockWithDisplay system from Example 1.3.2.5.

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-271.jpg?height=628&width=1303&top_left_y=1624&top_left_x=365)

We begin with Clock, which is in the category $\mathbf{S y s}_{\mathbb{D}_{\text {ET }}}\left(\begin{array}{c}1 \\ \text { Hour }\end{array}\right)$ of deterministic systems with interface $\left(\begin{array}{c}1 \\ \text { Hour }\end{array}\right)$, and Meridian, which is in the category $\mathbf{S y s}_{\mathbb{D} \text { ET }}\left(\begin{array}{c}\text { Hour } \\ \text { a.m./p.m. }\end{array}\right)$ of deterministic systems with interface $\left(\begin{array}{c}\text { Hour } \\ \text { a.m./p.m. }\end{array}\right)$. We then form their parallel product

Meridian $\otimes$ Clock, which is in $\mathbf{S y s}_{\mathbb{D} \text { ET }}\left(\begin{array}{c}1 \times \text { Hour } \\ \text { a.m./p.m. } \times \text { Hour }\end{array}\right)$. The wiring diagram itself

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-272.jpg?height=436&width=564&top_left_y=389&top_left_x=778)

is a lens

$$
\left(\begin{array}{c}
w^{\sharp} \\
w
\end{array}\right):\left(\begin{array}{c}
1 \times \text { Hour } \\
\text { a.m./p.m. } \times \text { Hour }
\end{array}\right) \leftrightarrows\left(\begin{array}{c}
1 \\
\text { a.m./p.m. } \times \text { Hour }
\end{array}\right)
$$

That is, it is a vertical arrow in the double category Arena $\mathbb{D E t}$ of arenas in the deterministic doctrine. The doubly indexed category $\mathbf{S y s}_{\mathbb{D}_{\mathrm{ET}}}:$ Arena $_{\mathbb{D}_{\mathrm{ET}}} \rightarrow$ Cat furnishes us with a functor

$$
\mathbf{S y s}_{\mathbb{D E T}^{\text {ET }}}\left(\begin{array}{c}
w^{\sharp} \\
w
\end{array}\right): \mathbf{S y s}_{\mathbb{D E T}_{\text {ET }}}\left(\begin{array}{c}
1 \times \text { Hour } \\
\text { a.m./p.m. } \times \text { Hour }
\end{array}\right) \rightarrow \mathbf{S y s}_{\mathbb{D E T}}\left(\begin{array}{c}
1 \\
\text { a.m./p.m. } \times \text { Hour }
\end{array}\right)
$$

Despite its formidible name, this functor is just given by composing in Arena $\mathbb{D}_{\mathbb{E}}$ with the lens $\left(\begin{array}{c}w^{\sharp} \\ w\end{array}\right)$. We then apply this functor to Meridian $\otimes$ Hour to get the composite system ClockWithDisplay:

$$
\text { ClockWithDisplay }=\mathbf{S y s}_{\mathbb{D E T}^{E}}\left(\begin{array}{c}
w^{\sharp} \\
w
\end{array}\right)(\text { Meridian } \otimes \text { Hour }) \text {. }
$$

This story is mirrored in the behavioral approach to systems theory, except instead of working with the doubly indexed category $\mathbf{S y s}_{\mathbb{T}}:$ Arena $_{\mathbb{T}} \rightarrow$ Cat, we work in the doubly indexed category Vec : Matrix $\rightarrow$ Cat - or, rather, the equivalent doubly indexed category Set/(-) : Span(Set) $\rightarrow$ Cat.

We begin with the systems Bucket ${ }_{1}$ and Bucket ${ }_{2}$, both in Set $/\left(\mathbb{R}^{4}\right)^{\mathbb{R}}$; that is, Bucket ${ }_{1}$ is identified with the map expose Bucket $_{1}:$ B $_{\text {Bucket }_{1}} \rightarrow\left(\mathbb{R}^{4}\right)^{\mathbb{R}}$, and the same for Bucket . $_{2}$. We then form the parallel product of these systems, which in this case is given by their cartesian product

$$
\text { expose }_{\text {Bucket }_{1}} \times \text { expose }_{\text {Bucket }_{2}}: \text { B }_{\text {Bucket }_{1}} \times \text { B }_{\text {Bucket }_{2}} \rightarrow\left(\mathbb{R}^{4}\right)^{\mathbb{R}} \times\left(\mathbb{R}^{4}\right)^{\mathbb{R}}
$$
which is an object of Set $/\left(\left(\mathbb{R}^{4}\right)^{\mathbb{R}} \times\left(\mathbb{R}^{4}\right)^{\mathbb{R}}\right)$. Now, the wiring diagram

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-273.jpg?height=609&width=702&top_left_y=308&top_left_x=706)

may be described as a span of sets,

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-273.jpg?height=192&width=634&top_left_y=1010&top_left_x=735)

which is to say a vertical arrow in $\operatorname{Span}($ Set). We'll explain how bubble diagrams correspond to spans in a more systematic way in Section 6.2.2, but for now we can define $W$ as follows:

$$
\begin{gather*}
W=\left\{\left(a_{i}\right)_{1 \leq i \leq 8}: \mathbb{R} \rightarrow \mathbb{R}^{8} \left\lvert\, \begin{array}{c}
a_{2}+a_{3}=a_{4} \\
a_{4}=0
\end{array}\right.\right\}  \tag{6.6}\\
w_{1}\left(\left(a_{i}\right)_{1 \leq i \leq 8}\right)=\left(\left(a_{6}, a_{2}, a_{5}, a_{1}\right),\left(a_{3}, a_{8}, a_{1}, a_{7}\right)\right) \\
w_{2}\left(\left(a_{i}\right)_{1 \leq i \leq 8}\right)=\left(a_{6}, a_{7}, a_{5}, a_{8}\right)
\end{gather*}
$$

We then compose Bucket ${ }_{1}$ and Bucket ${ }_{2}$ into the composite Buckets by applying $\operatorname{Vec}(W)$ to Bucket $_{1} \times$ Bucket $_{2}$ :

$$
\text { Buckets }=\operatorname{Vec}(W)\left(\text { Bucket }_{1} \times \text { Bucket }_{2}\right) .
$$

This means composing with $W$ in the category of spans. Recall that we can see the map expose Bucket $_{1} \times$ expose $_{\text {Bucket }_{2}}:$ B $_{\text {Bucket }_{1}} \times$ B $_{\text {Bucket }_{2}} \rightarrow\left(\mathbb{R}^{4}\right)^{\mathbb{R}} \times\left(\mathbb{R}^{4}\right)^{\mathbb{R}}$ as a span from 1 to $\left(\mathbb{R}^{4}\right)^{\mathbb{R}} \times\left(\mathbb{R}^{4}\right)^{\mathbb{R}}$. Composing with $W$ therefore means we have the following pullback diagram:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-273.jpg?height=330&width=1068&top_left_y=2185&top_left_x=518)

Quite explicitly, this defines $B_{\text {Buckets }}$ to be the set

$$
\text { B }_{\text {Buckets }}=\left\{\begin{array}{r|rl}
\frac{d h_{i}}{d t} & =F_{i}\left(h_{i}, p_{i 1}, p_{i 2}\right) &  \tag{6.8}\\
\left(h_{1}, f_{11}, f_{12}, p_{11}, p_{12}\right): \mathbb{R} \rightarrow \mathbb{R}^{5} & f_{i 1}=H_{i 1}\left(h_{i}, p_{i 1}\right) & \\
\left(h_{2}, f_{21}, f_{22}, p_{21}, p_{22}\right): \mathbb{R} \rightarrow \mathbb{R}^{5} & h_{i 2}\left(h_{i}, p_{i 2}\right) & \\
\left(a_{i}\right)_{1 \leq i \leq 8}: \mathbb{R} \rightarrow \mathbb{R}^{8} & a_{3}=a_{4} & a_{4}=0 \\
p_{11}=a_{5} & f_{11}=a_{6} \\
p_{12}=a_{1} & f_{12}=a_{2} \\
p_{21}=a_{1} & f_{21}=a_{3} \\
p_{22}=a_{7} & f_{22}=a_{8}
\end{array}\right\}
$$

At first glance, this is quite a bit larger than the definition of $B_{\text {Buckets }}$ we gave in Eq. (6.2). But most of the equations here are setting the $f \mathrm{~s}$ and $p \mathrm{~s}$ from each Bucket $_{i}$ to be equal to the as coming form the wiring diagram $W$. When the dust has settled, the two definitions are equivalent â€” which is to say more precisely that they are isomorphic in the category $\operatorname{Vec}\left(\left(\mathbb{R}^{4}\right)^{\mathbb{R}}\right)$.

Exercise 6.2.1.1. Describe explicitly the isomorphism between the definitions of $\mathrm{B}_{\text {Bucket }}$ in Eq. (6.2) and Eq. (6.8). Check that this isomorphism commutes with the two definitions of expose Buckets as well.

A crucial feature of the behavioral approach to systems theory is that constraints on system behaviors are treated at the same level as the systems themselves. Suppose we want to constrain the system Buckets so that the water flows from left to right. That is, we want $f_{11}>0$ and $f_{22}<0$. These constraints give rise to a subset $C$ of the set $\left(\mathbb{R}^{2}\right)^{\mathbb{R}}$ :

$$
\begin{equation*}
C=\left\{\left(f_{11}, f_{22}\right) \mid f_{11}>0 \text { and } f_{22}<0\right\} \text {. } \tag{6.9}
\end{equation*}
$$

We can consider the subset $C$ of $\left(\mathbb{R}^{4}\right)^{\mathbb{R}}$ as an object of $\operatorname{Set} /\left(\mathbb{R}^{4}\right)^{\mathbb{R}}$ by equipping it with the inclusion $C \hookrightarrow\left(\mathbb{R}^{4}\right)^{\mathbb{R}}$. We can bubble up this constraint just like a system (though to emphasize that we are thinking of it as a constraint and not as a system, we will not fill the bubble with blue):

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-274.jpg?height=217&width=349&top_left_y=2206&top_left_x=888)

To express the system Buckets constrained so that the inequalities $f_{11}>0$ and $f_{22}<0$
hold, we can use another bubble diagram:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-275.jpg?height=458&width=697&top_left_y=324&top_left_x=711)

This is a new system with interface $\left(\mathbb{R}^{2}\right)^{\mathbb{R}}$. Suppose we want to ask if this constraint $C$ is sufficient to ensure that the pressures on the pipes (the remaining exposed variables) are within certain bounds $\left[b_{i 0}, b_{i 1}\right]$. We can express these constraints $\mathrm{P}$ on pressure as a subset of $\left(\mathbb{R}^{2}\right)^{\mathbb{R}}$ :

$$
\mathrm{P}=\left\{\left(p_{11}, p_{22}\right) \mid b_{10} \leq p_{11} \leq b_{11} \text {, and } b_{20} \leq p_{22} \leq b_{21}\right\} \hookrightarrow\left(\mathbb{R}^{4}\right)^{\mathbb{R}}
$$

The question of whether the constrained system BucketsConstrained satisfies the constraints $\mathrm{P}$ is then the question of whether there is a map $\phi:$ BucketsConstrained $\rightarrow \mathrm{P}$ in Set $/\left(\mathbb{R}^{4}\right)^{\mathbb{R}}$ :

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-275.jpg?height=209&width=696&top_left_y=1332&top_left_x=709)

The map $\phi$ is a function of all the state variables of BucketsConstrained, but the commutativity of Eq. (6.10) says that it must be given by

$$
\phi\left(\ldots, p_{11}, \ldots, p_{22}, \ldots\right)=\left(p_{11}, p_{22}\right) .
$$

Therefore, the existence of $\phi$ is the same thing as the proposition that $p_{11}$ is between $b_{10}$ and $b_{11}$ and the same for $p_{22}$ - that is, the proposition that BucketsConstrained satisfies the constraint $P$.

The question of devising such a constraint $C$ (or, even better, a system which implements this constraint) for a system Buckets so that the constrained system admits a map to another constraint $\mathrm{P}$ is known as a control problem.

All of our variables in the previous examples were variables of time. This is very common for behaviors, especially those coming from differential systems theories. Instead of having all our sets be $A^{\mathbb{R}}$ for an $A$ varying in time $t \in \mathbb{R}$, we could bake in this variation into time into our notion of set itself. That is, we can work in a category of variable sets, or sheaves. In such a category, the set $A$ would already include, implicitly, the variation in time. The sheaf theoretic setting for the behavioral appraoch to systems theory is explored in [SSV16] and [SS19]; we will summarize it in Section 6.2.3, and
show that trajectories in the general differential doctrine $\mathbb{D}_{\text {IFF }}$ (Definition 3.5.2.23) land in sheaves over time intervals.

Though we have been describing the behavioral approach to systems theory as taking place within the doubly indexed category Set/(-) : Span(Set) $\rightarrow$ Cat, we can do it in any category that allows us to compose spans - namely, and category with finite limits. Just like we had different theory of dynamical system for lens-based systems, we can see each category with finite limits as a doctrine for the behavioral approach. We will call these behavioral systems theories.

Definition 6.2.1.2. A doctrine for the behavioral approach or a behavioral doctrine is a category $C$ with finite limits.

The variable sharing doctrine of composition $\mathfrak{B}_{\text {ARIABLESHARING }}$ which encapsulates the behavioral approach to systems theory is the functor which sends each behavioral doctrine $C$ to the doubly indexed category of systems in the behavioral doctrine $C$ :

$$
\text { BSys }_{C}:=C /(-): \operatorname{Span}(C) \rightarrow \text { Cat }
$$

This is defined as the vertical slice construction applied to the inclusion $1: 1 \rightarrow \mathbf{S p a n}(C)$ of the terminal object of $C$ :

$$
\text { BSys }_{C}:=\sigma(1: 1 \rightarrow \operatorname{Span}(C))
$$

This definition of the variable sharing doctrine answers the questions of Informal Definition 6.1.0.1 in the following ways:
- A system is a notion of behavior $\mathrm{B}_{\text {Sys }}$ together with a function exposing its ${\text { variables } \text { expose }_{S}: \mathrm{B}_{\text {Sys }} \rightarrow I \text {. }}$.
- An interface is a codomain $I$ for the exposed variables.
- Interfaces are connected in composition patterns given by spans.
- Systems are composed by sharing variables - that is, by setting their exposed variables equal according to the composition pattern. This is accomplished via span composition.
- A map between systems is a function of their behaviors which respects their exposed variables. This acts on interfaces via a function that tells us how to translate exposed variables of the first system to exposed variables of the second.
- Maps between systems can be composed along composition patterns when we have a square in the double category of spans.

We will discuss these points in more detail in Section 6.2.2.

So far, we have only seen the behavioral doctrine Set of sets, but in Section 6.2.3 we will see a behavioral doctrine of sheaves over time intervals. Though we won't see as many different examples of behavioral systems theories as we have for parametersetting systems theories, the notion can help us clarify the basic ideas of the behavioral approach: it's all about spans, much in the way that parameter-setting theories are all about lenses.

\subsection*{6.2.2 Bubble diagrams as spans in categories of arities}

All the way back in Section 1.3.3, we saw that wiring diagrams are lenses in special categories: the free cartesian categories. We needed a cartesian category to describe the notion of lens given in Definition 1.3.1.1. We can make an analogy here: to describe the behavioral approach to systems theory, we use spans which require finite limits. It stands to reason that we should expect our bubble diagrams for the behavioral approach to be spans in free finitely complete categories. We'll see that this is precisely the case, although we will want to restrict to a certain class of "nice" spans.

Before we see a formal definiton of bubble diagram, let's give an informal definition.

Informal Definition 6.2.2.1. A bubble diagram is a diagram which consists of a number of inner bubbles drawn within an outer bubble, each with some ports. There are furthermore links, which are drawn as small dots. The bubbles are wired together by connecting to the links:

1. Every port on an inner bubble is wired to a unique link, and every link is wired to some inner port.

2. Every port on the outer bubble is wired to a unique link, and a link is wired to at most one outer port.

3. No two links are connected.

The category of bubble diagrams has as its objects the bubbles and as its morphisms the bubble diagrams. Bubble diagrams are composed by filling the inner bubbles with other bubble diagrams, the erasing the middle layer of bubbles, and coalescing any connected links into a single link.

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-277.jpg?height=713&width=1477&top_left_y=1655&top_left_x=324)

Composition of bubble diagrams is given by nesting and then coalescing links, so
that no two links are connected in a diagram.
![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-278.jpg?height=544&width=1472&top_left_y=332&top_left_x=368)

We can formalize these diagrams as certain spans in the free finitely complete category. Luckily, we already know what the free finitely complete category is; it turns out to be the same as the free cartesian category, the category Arity of arities (Definition 1.3.3.2)!

Proposition 6.2.2.2. For a set $\mathcal{T}$ of types, the category Arity $\mathcal{T}_{\mathfrak{T}}$ of arities typed in $\mathfrak{T}$ is the free finitely complete category on the set of objects $\mathcal{T}$. That is, for any finitely complete category $C$ and function $C_{(-)}: \mathscr{T} \rightarrow C$, there is a functor $\mathrm{ev}_{C}:$ Arity $_{\mathscr{T}} \rightarrow C$ which preserves finite limits, and this functor is unique up to a unique natural isomorphism.

Proof Sketch. Since $C$ is finitely complete, it is in particular cartesian. Therefore, we get a unique cartesian functor $\mathrm{ev}_{\mathcal{C}}:$ Arity $_{\mathcal{T}} \rightarrow C$. We can then check that this functor preserves finite limits in addition to products; this is ultimately because pullbacks of product projections are given by other product projections.

Recall that the category of arities is equivalent to the opposite of the category of finite sets (Proposition 1.3.3.3). A span in the category of arities is a diagram

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-278.jpg?height=184&width=398&top_left_y=1854&top_left_x=858)

where $I, L$, and $O$ are finite sets and $i: I \rightarrow L$ and $o: O \rightarrow L$ are functions. We interpret such a span as a bubble diagram in the following way:
- The set $I$ is the set of inner ports on any of the inner bubbles, the set $O$ is the set of outer ports on the outer bubble, and the set $L$ is the set of links.
- The function $i: I \rightarrow L$ sends each inner port to the link it is connected too, and the function $o: O \rightarrow L$ sends each outer port to the link it is connected to.

If we have multiple inner bubbles, then we take a span with domain the cartesian product $X^{I_{1}} \times \cdots \times X^{I_{n}}$, so that $I=I_{1}+\cdots+I_{n}$.

\section*{Jaz: Diagram from D.Spivak.}
![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-279.jpg?height=492&width=1386&top_left_y=328&top_left_x=369)

Note what can happen if we use just any old span in Arity: we can have "passing wires", like the wire connecting outer port 1 with outer port 2 in Eq. (6.13), and we can have dangling links like $s$ which aren't connected to anything. These are drawn in red above. This sort of loosey-goosey diagram is well known; it is an undirected wiring diagram.

Definition 6.2.2.3. An undirected wiring diagram is a span in the category Arity of arities. Equivalently, it is a cospan of finite sets. A span

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-279.jpg?height=187&width=593&top_left_y=1343&top_left_x=755)

is an undirected wiring diagram with $n$ inner bubbles, with bubble $i$ having the finite set $I_{i}$ of ports, with the finite set of links $L$, and the outer bubble having finite set of ports $O$. Informally, these satisfy the laws:

1. Every inner port $p \in I_{i}$ is wired to a unique link $i(p)$

2. Every outer port $p \in O$ is wired to a unique link $o(p)$

3. No two links are wired together.

However, these external connections and dangling wires tend to clutter up the works. A bubble diagram is an undirected wiring diagram without these cluttering bits. We enforce the extra two parts of the bubble diagram laws - that every link is connected to some inner port and that a link is connected to at most one outer port by asking that the left leg $i$ of the span is surjective while the right leg $o$ of the span is injective. That $i$ is surjective means that every link is wired to some inner port. That $o$ is injective means that each link is wired to at most one outer port.

Definition 6.2.2.4. A bubble diagram is a span in the category Arity of arities whose left leg is surjective (as a finite set map) and whose right leg is injective. Equivalently, it is
a cospan of finite sets where the left leg is surjective and the right leg is injective. A span

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-280.jpg?height=187&width=596&top_left_y=356&top_left_x=754)

Exercise 6.2.2.5. Draw the corresponding undirected wiring diagrams for the following cospans. Is it a bubble diagram?

1.

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-280.jpg?height=374&width=550&top_left_y=781&top_left_x=836)

2.

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-280.jpg?height=404&width=607&top_left_y=1218&top_left_x=802)

3.

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-280.jpg?height=157&width=531&top_left_y=1689&top_left_x=838)

Exercise 6.2.2.6. Express the following undirected diagrams as spans in the category of arities. Which are bubble diagrams?

1.

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-280.jpg?height=317&width=480&top_left_y=2162&top_left_x=866)

2.

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-281.jpg?height=185&width=393&top_left_y=276&top_left_x=909)

3.

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-281.jpg?height=219&width=222&top_left_y=530&top_left_x=992)

4.

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-281.jpg?height=113&width=390&top_left_y=800&top_left_x=911)

5.

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-281.jpg?height=309&width=144&top_left_y=951&top_left_x=1031)

Both undirected wiring diagrams and bubble diagrams are composed by pullback in the category Arity of arities, which is pushout in the category of finite sets. Let's recall the definition of pushout in the category of finite sets.

Definition 6.2.2.7. Given a solid diagram

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-281.jpg?height=309&width=442&top_left_y=1664&top_left_x=836)

The pushout $B+{ }_{A} C$ of $f$ and $g$ is defined to be the disjoint union of $A$ and $B$, quotiented by the relation which sets $f(a)$ equal to $g(a)$ :

$$
P=\frac{A+B}{f(a) \sim g(a)}
$$

The map $B \rightarrow B+_{A} C$ is the map $b \mapsto[b]$, the inclusion $B \rightarrow B+C$ followed by the quotient map, and similarly $C \rightarrow B+{ }_{A} C$ is $c \mapsto[c]$.

Note that though the notation $B+{ }_{A} C$ only mentions the sets involved, to form the pushout we need to know the functions $f$ and $g$ as well.

We can understand the composite of undirected wiring diagrams as follows:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-282.jpg?height=326&width=819&top_left_y=566&top_left_x=642)

The set $L$ is the set of links in the first diagram, and the set $L^{\prime}$ is the set of links in the second diagram. The set of links in the new diagram is their pushout $L+{ }_{M} L^{\prime}$ over the set of middle ports; this is the disjoint union of $L$ and $L^{\prime}$ with any two links set equal when they are connected to the same middle port.

Exercise 6.2.2.8. Consider the composite Eq. (6.12) reproduced here:
![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-282.jpg?height=546&width=1472&top_left_y=1347&top_left_x=367)

Using that you have already seen how to express each constituent bubble diagram as a span in Exercise 6.2.2.6, compute the composite diagram using pullbacks in the category of arities (or pushouts in the category of finite sets). Check that it gives the correct diagram.

It is not obvious that the composite of bubble diagrams is itself a bubble diagram; we need to check that the resulting legs of the span are respectively surjective and injective. Let's do that now.

Lemma 6.2.2.9. The composite of bubble diagrams is a bubble diagram. That is, in the following diagram

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-283.jpg?height=325&width=832&top_left_y=404&top_left_x=641)

If $i_{1}$ and $i_{2}$ are surjective and $o_{1}$ and $o_{2}$ are injective, then $i_{3}$ is surjective and $o_{3}$ is injective.

Proof. We will show that the inclusion $j_{1}: L \rightarrow L+{ }_{M} L^{\prime}$ is surjective, and the inclusion $j_{2}: L^{\prime} \rightarrow L+_{M} L^{\prime}$ is injective.

An element of $L+_{M} L^{\prime}$ is either of the form [ $\left.\ell\right]$ for $\ell \in L$ or $\left[\ell^{\prime}\right]$ for $\ell^{\prime} \in L^{\prime}$. If it is of the form $[\ell]$ for $\ell \in L$, the it is in the image of $j_{1}$ by definition. Suppose that it is of the form $\left[\ell^{\prime}\right]$ for $\ell^{\prime} \in L^{\prime}$. By hypothesis, $i_{2}: M \rightarrow L^{\prime}$ is surjective, so $\ell^{\prime}=i_{2} m$ for some $m \in M$. But then $\left[\ell^{\prime}\right]=\left[i_{2} m\right]=\left[o_{1} m\right]$ is in the image of $j_{1}$.

Now, suppose we have two elements $x$ and $y \in L^{\prime}$ for which $[x]=[y]$ in $L+{ }_{M} L^{\prime}$. This means that $x$ and $y$ are related by the equivalence relation generated by $i_{2}(m) \sim o_{1}(m)$ for any $m \in M$. Explicitly, this means there is a zig-zag of elements in $L$ and $L^{\prime}$, each related by a element of $M$, connecting $x$ and $y$; that is, a sequence of elements $\ell_{1}, \ldots \ell_{n} \in L$ and $m_{1}, \ldots, m_{2 n}$ with $x=i_{2} m_{1}, i_{2} m_{2 n}=y$, and that $o_{1} m_{2 k-1}=\ell_{k}=o_{1} m_{2 k}$ for $1<k \leq n$ and $i_{2} m_{2 k-2}=i_{2} m_{2 k-1}$ for $1<k \leq n$.

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-283.jpg?height=214&width=1198&top_left_y=1611&top_left_x=453)

We may prove this by induction on the length $n$ of the zig-zag. If the zig-zag has length 0 , then $x$ already equals $y$. Suppose that the zig-zag has length $n+1$; we will show that $i_{2} m_{2 n-1}=y$ so that by the inductive hypothesis, $x=i_{2} m_{2 n-1}=y$. Now, by assumption, $o_{1} m_{2 n-1}=\ell_{n}=o_{1} m_{2 n}$. Since $o_{1}$ was presumed to be injective, this means that $m_{2 n-1}=m_{2 n}$; but then $i_{2} m_{2 n-1}=i_{2} m_{2 n}=y$.

The main upside to using bubble diagrams over the more general undirected wiring diagrams is that bubble diagrams have a nicer double category for our purposes.

Definition 6.2.2.10. For a set $\mathfrak{T}$ of types, the double category Bubble $_{\mathscr{T}}$ is the sub-
double category of the double category $\operatorname{Span}\left(\mathbf{A r i t y}_{\mathcal{T}}\right)$ of undirected wiring diagrams consisting of the bubble diagram.

Let's understand this double category. A vertical map in the double category $\operatorname{Span}\left(\right.$ Arity) is an undirected wiring diagram, and a horizontal map is a map $f^{*}: \mathrm{X}^{I} \rightarrow$ $X^{J}$ in Arity $_{\mathscr{J}}$. A square is a diagram

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-284.jpg?height=360&width=285&top_left_y=579&top_left_x=909)

That is, $\alpha: L_{2} \rightarrow L_{1}$ is an assignment of links from the diagram $W_{2}$ on the right to the diagram $W_{1}$ on the left which preserves connectivity, relative to the maps $f$ and $g$ on ports. For undirected wiring diagrams, this is an extra bit of data above and beyond the data of the surrounding diagram - there may be multiple different $\alpha$ which could make the diagram commute. But if we restrict our attention to bubble diagrams, then $i_{2}: I_{3} \rightarrow L_{2}$ is surjective; therefore, there can be at most one $\alpha$ making the above diagram commute. This is because $\alpha\left(i_{2}(x)\right)$ must equal $i_{1}(f(x))$ and for every $\ell \in L_{2}$, $\alpha(\ell)=\alpha\left(i_{2}(x)\right)$ for some $x \in I_{2}$ by the surjectivity of $i_{2}$. We can record this observation in a proposition.

Proposition 6.2.2.11. For a set $\mathfrak{T}$ of types, the double category Bubble $\mathcal{T}_{\mathcal{T}}$ is thin (Definition 3.4.1.2) - there is at most one square of any given signature. Furthermore, Bubble $_{\mathcal{T}}$ is spanlike (Definition 5.3.1.5).

Proof. We have just argued that there can be at most once square of any given signature. As a double category of spans, Bubble $_{\mathcal{T}}$ is spanlike.

In order to prove Proposition 6.2.2.11, we only used the assumption in a bubble diagram that $i$ was surjective. However, we will see that bubble diagrams are also useful in the diagrammatic approach to systems theory (see Section 6.3), but this time as cospans in the category of finite sets. The double category of such cospans differs only from the double category considered here in that its horizontal arrows go the other direction. In order to prove that this double category of bubble diagrams is spanlike (Theorem 6.3.2.3), we will also need the assumption that $o$ is injective.

Recall from Proposition 1.3.3.15 that we can interpret lenses in categories of arities in any cartesian category by the universal property of Arity as the free cartesian category. Since Arity is the free finitely complete category, we can use the same trick to interpret bubble diagrams into spans in any finitely complete category. We can then use these spans to compose systems using the behavioral approach to systems theory.

Proposition 6.2.2.12. For a set $\mathfrak{T}$ of types and a function $C_{(-)}: \mathfrak{T} \rightarrow \mathfrak{T}$ interpreting each type as an object of a finitely complete category $C$, there is a unique (up to unique isomorphism) double functor

$$
\mathrm{ev}_{C}: \operatorname{Span}\left(\operatorname{Arity}_{\mathscr{J}}\right) \rightarrow \operatorname{Span}(C)
$$

interpreting each undirected wiring diagram as a span in $C$.

Explicitly, for $\mathcal{T}=1$, the functor $\mathrm{ev}_{C}$ sends $X^{I}$ to $C^{I}$.

We can use Proposition 6.2.2.12 to describe composition in any behavioral doctrine with bubble diagrams. If we have a behavioral doctrine $C$, we get the doubly indexed category BSys $_{C}: \operatorname{Span}(C) \rightarrow$ Cat. If we have some types of behaviors $\mathfrak{T}$ of interest, and interpretations $C: \mathscr{T} \rightarrow C$ of these types as objects of $C$, then we can restrict along $\mathrm{ev}_{\mathcal{C}}: \boldsymbol{S p a n}\left(\operatorname{Arity}_{\mathscr{T}}\right) \rightarrow \mathbf{S p a n}(C)$ to get the doubly indexed category

$$
\text { Bubble }_{\mathscr{T}} \hookrightarrow \operatorname{Span}\left(\text { Arity }_{\mathscr{T}}\right) \xrightarrow{\mathrm{ev}_{c}} \operatorname{Span}(C) \xrightarrow{\text { BSys }_{C}} \text { Cat. }
$$

This gives us the compositionality of behavioral systems according to bubble diagrams.

In Section 1.3.4, we saw how we can add green beads with operations from some Lawvere theory to our wiring diagrams by taking lenses in that Lawvere theory. We can do this same with undirected wiring diagrams, but in this case we need to use essentially algebraic theories, which are algebraic theories that can take advantage of all finite limits.

Definition 6.2.2.13. An essentially algebraic theory is a category $C$ with finite limits. A model of the theory $C$ in a category $\mathscr{D}$ with finite limits is a finite limit preserving functor $F: C \rightarrow D$.

Any model $F: C \rightarrow \mathscr{D}$ of an essentially algebraic theory $C$ gives rise to a double functor $F: \operatorname{Span}(C) \rightarrow \operatorname{Span}(\mathscr{D})$. We can use this to interpret undirected wiring diagrams over the theory $C$ into the category $\mathscr{D}$.

Definition 6.2.2.14. An undirected wiring diagram over an essentially algebraic theory $C$ is a span in $C$.

Example 6.2.2.15. The diagram

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-286.jpg?height=604&width=705&top_left_y=343&top_left_x=707)

is an undirected wiring diagram in the essentially algebraic theory of real vector spaces. This happens to be the category Vect of finite dimensional real vector spaces, the same as the Lawvere theory of real vector spaces. This diagram is represented by the span given in Eq. (6.6) - we note $W$ is a real vector space and that both legs of the span are linear, so that this span can be understood as living in Vect.

Remark 6.2.2.16. It is not as easy to separate the bubble diagrams from the undirected wiring diagrams when passing to a general essentially algebraic theory. This is fundamentally because the operations of the theory could be arbitrary, and so no longer guarentee that the diagrams really satisfy the properties that bubble diagrams should.

\subsection*{6.2.3 The behavioral doctrine of interval sheaves}

So far, we have only really seen the behavioral doctrine Set of sets, which gives rise to the doubly indexed category Vec : Matrix $\rightarrow$ Cat. In this section, we will see another behavioral doctrine: the topos of interval sheaves.

Many systems, especially differential systems, give rise to trajectories which vary in time. While in Section 6.2.1 we simply included the time variable into our definition in of the sets - taking $\left(\mathbb{R}^{4}\right)^{\mathbb{R}}$ instead of $\mathbb{R}^{4}$ - it would be nice if we didn't have to worry about this every time and could instead focus on the actual type of the variables. We will see that by moving from sets to variable sets, or sheaves, we can incorporate the variation of our trajectories in time without cluttering the types of our variables. A great deal can be said about the sheaf approach to modelling dynamical systems - for example, see [SSV16] and [SS19]. We will just scratch the surface here.

We will end this section by showing that the doubly indexed functor Traj defined in Example 5.3.3.3 which takes the trajectories in a differential doctrine actually lands in the behavioral doctrine of interval sheaves, and not just in the behavioral doctrine of sets.

The fundamental idea behind interval sheaves is that we would like to bake in variation in time into the definition of our objects. Instead of having a set $X$ of behaviors, we would like to have a set $X(\ell)$ of behaviors which last for a time of length $\ell$. We say that $\ell$ is the time interval during which the behavior $b \in X(\ell)$ takes place. If we have any time interval $\ell^{\prime}$ which is contained in $\ell$, then we can restrict $b$ to its part which occurs during the interval $\ell^{\prime}$; we write this as $\left.b\right|_{\ell^{\prime}} \in X\left(\ell^{\prime}\right)$.

We will begin by describing the category of intervals.

Definition 6.2.3.1. An interval is a positive real number $\ell \in(0, \infty)$. A morphism $a: \ell^{\prime} \rightarrow \ell$ is a real number $a \in[0, \ell)$ so that $a+\ell^{\prime} \leq \ell$. Morphisms are composed by addition, and the identity is $0: \ell \rightarrow \ell$.

We denote the category of intervals by II. We say a morphism $a: \ell^{\prime} \rightarrow \ell$ of intervals is strict if $a>0$ and $a+\ell^{\prime}<\ell$ (strict inequalities on both sides). We will write $a: \ell^{\prime} \leadsto \ell$ to say that $a$ is strict.

We can picture a morphism $a: \ell^{\prime} \rightarrow \ell$ as in the following diagram:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-287.jpg?height=276&width=371&top_left_y=1144&top_left_x=866)

An interval sheaf is a sheaf on the category of intervals. We won't introduce sheaves in general, just this special case. A sheaf on the category of intervals a functor $\mathbb{I}^{\text {op }} \rightarrow$ Set (a "presheaf") satisfying a certain gluing property.

Definition 6.2.3.2. An interval sheaf $X$ consists of:

1. For every interval $\ell$, a set $X(\ell)$ of behaviors which may occur during the interval $\ell$.

2. For every morphism $a: \ell^{\prime} \rightarrow \ell$ of intervals, a restriction function

$$
\left.b \mapsto b\right|_{a}: X(\ell) \rightarrow X\left(\ell^{\prime}\right)
$$

which selects out the part of $b$ occuring during the subinterval $\ell^{\prime}$ beginning at $a$ in $\ell$.

This data is required to satisfy the following conditions:

1. (Unity) For any $\ell$, we have that $\left.b\right|_{0}=b$ for $0: \ell \rightarrow \ell$.

2. (Functoriality) For any $a^{\prime}: \ell^{\prime \prime} \rightarrow \ell^{\prime}$ and $a: \ell^{\prime} \rightarrow \ell$, we have

$$
\left.\left(\left.b\right|_{a}\right)\right|_{a^{\prime}}=\left.b\right|_{a^{\prime}+a}
$$

for all $b \in X(\ell)$, with $a^{\prime}+a: \ell^{\prime \prime} \rightarrow \ell$ the composite of $a^{\prime}$ and $a$.

3. (Gluing) $X(\ell)$ is the limit of $X\left(\ell^{\prime}\right)$ taken over all strict inclusions $a: \ell^{\prime} \rightarrow \ell$ (where $a>0$ and $\left.a+\ell^{\prime}<\ell\right)$ :

$$
X(\ell) \xrightarrow{\sim} \lim _{a: \ell^{\prime} \sim \ell} X\left(\ell^{\prime}\right) .
$$

More explicitly, we ask that the canonical map from $X(\ell)$ to the limit given by restricting behaviors in $X(\ell)$ along strict inclusions $a: \ell^{\prime} \leadsto \ell$ is an isomorphism. An assignment of sets $X(\ell)$ with restriction maps satisfying Unity and Functoriality is known as an interval presheaf. This is equivalently a functor $X: \mathbb{I}^{\mathrm{op}} \rightarrow$ Set.

The unity and functoriality laws are straightforward consistency checks on behaviors. Unity says that the portion of a behavior taking place over a whole interval is that behavior. Functoriality says that if we have a behavior $b$ taking place in interval $\ell$, and we look at the portion of that behavior taking place in the subinterval $a: \ell^{\prime} \rightarrow \ell$, and then at the further subinterval $a^{\prime}: \ell^{\prime \prime} \rightarrow \ell^{\prime}$, the result is the same as simply looking at the portion of that behavior taking place in that further subinterval.

Gluing is a bit more tricky. It means that a behavior is determined by what it does on all strict subintervals. We can split the gluing condition apart into two further conditions.

Lemma 6.2.3.3. An interval presheaf $X$ is an interval sheaf - satisfies the Gluing condition - if and only if

1. (Separation) For every pair of behaviors $b_{1}, b_{2} \in X(\ell)$, if $\left.b_{1}\right|_{a}=\left.b_{2}\right|_{a}$ for all strict $a: \ell^{\prime} \leadsto \ell$, then $b_{1}=b_{2}$.

2. (Existence) For any family $b_{a} \in X\left(\ell_{a}\right)$ of behaviors indexed by strict inclusions $a: \ell_{a} \rightarrow \ell$ which are compatible in the sense that for any $a^{\prime}: \ell_{a+a^{\prime}} \rightarrow \ell_{a}$ so that $b_{a+a^{\prime}}=\left.b_{a}\right|_{a^{\prime}}$, there is a whole behavior $b \in X(\ell)$ such that $b_{a}=\left.b\right|_{a}$.

Proof. These two properties say that the canonical map

$$
X(\ell) \rightarrow \lim _{a: \ell^{\prime} m \ell} X\left(\ell^{\prime}\right)
$$

are injective and surjective respectively.

Example 6.2.3.4. For any $n \in \mathbb{N}$, there is an interval sheaf $e^{n}$ of $n$-times continuously differentiable real valued functions. Explicitly,

$$
C^{n}(\ell)=\{f:(0, \ell) \rightarrow \mathbb{R} \mid f \text { is } n \text {-times continously differentiable. }\} \text {. }
$$

The restriction maps are given by restricting: if $a: \ell^{\prime} \rightarrow \ell$, then $\left.f\right|_{a}=f \circ(x \mapsto a+x)$ where $x \mapsto a+x:\left(0, \ell^{\prime}\right) \rightarrow(0, \ell)$ is the inclusion of $\left(0, \ell^{\prime}\right)$ into $(0, \ell)$ shifted over by $a$. Unit and functoriality conditions follow directly from unit and associativity of composition; the only tricky law to check is the gluing condition. We can check both parts of the gluing condition using Lemma 6.2.3.3:

1. (Separation) Suppose that $f_{1}$ and $f_{2}:(0, \ell) \rightarrow \mathbb{R}$ are $n$-times continuously differentiable and that their restriction to any subintervals are equal. Since $(0, \ell)$ is open, for every $x \in(0, \ell)$ there is a strict subinterval in $(0, \ell)$ containing $x$; therefore, $f_{1}$ and $f_{2}$ are equal on this subinterval and therefore at $x$. So $f_{1}=f_{2}$.

2. (Existence) Suppose we have compatible functions $f_{a}:\left(0, \ell_{a}\right) \rightarrow \mathbb{R}$ for every $a: \ell_{a} \leadsto \ell$. For any $x \in(0, \ell)$, there is a strict subinterval $a_{x}: \ell_{a_{x}} \leadsto \ell$ containing $x$ in the sense that $x \in\left(a_{x}, a_{x}+\ell_{a_{x}}\right)$. We may therefore define a function $f$ : $(0, \ell) \rightarrow \mathbb{R}$ by $f(x)=f_{a_{x}}(x)$. This is well defined since if $a^{\prime}: \ell^{\prime} \leadsto \ell$ is any other strict subinterval containing $x$, then $x$ is also in their intersection which is a strict subinterval; by the compatibility of the functions $f_{a}$, it follows that $f_{a_{x}}(x)=$ $f_{a^{\prime}}(x)$ on this intersection. Since being $n$-times continuously differentiable is a local property and $f$ is defined to be a $n$-times continously differentiable in the neighborhood of any point, $f$ is also $n$-times continously differentiable.

We can think of the interval $C^{0}$ as the set of real numbers varying continuously in time.

Example 6.2.3.5. We can adapt all the sets of Section 6.2.1 to be interval sheaves by building in the variation of time. For example, we may define B $_{\text {Bucket }_{1}}$ from Eq. (6.1) as an interval sheaf by

$$
\operatorname{B~}_{\text {Bucket }_{1}}(\ell):=\left\{\begin{array}{l|l}
\left(h_{1}, f_{11}, f_{12}, p_{11}, p_{12}\right):(0, \ell) \rightarrow \mathbb{R}^{5} & \begin{array}{l}
\frac{d h_{1}}{d t}=F_{1}\left(h_{1}, p_{11}, p_{12}\right) \\
f_{11}=H_{11}\left(h_{1}, p_{11}\right) \\
f_{12}=H_{12}\left(h_{1}, p_{12}\right)
\end{array}
\end{array}\right\}
$$

with restriction given by restriction of functions.

A map of interval sheaves is a natural transformation between the functors $X$ : $\mathbb{I}^{\mathrm{op}} \rightarrow$ Set and $Y: \mathbb{I}^{\text {op }} \rightarrow$ Set.

Definition 6.2.3.6. Let $X$ and $Y$ be interval sheaves. A map $f: X \rightarrow Y$ is a family of functions $f_{\ell}: X(\ell) \rightarrow Y(\ell)$ for which the following naturality square commutes:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-289.jpg?height=239&width=309&top_left_y=1954&top_left_x=903)

for any $a: \ell^{\prime} \rightarrow \ell$. That is, for any behavior $b \in X(\ell)$, we have

$$
\left.f_{\ell}(b)\right|_{a}=f_{\ell^{\prime}}\left(\left.b\right|_{a}\right)
$$

We denote the category of interval sheaves by $\mathbf{S h}(\mathbb{I})$.

Example 6.2.3.7. Continuing on from Example 6.2.3.4, suppose that $\phi: \mathbb{R} \rightarrow \mathbb{R}$ is any $n$-times continuously differentiable function. Then we get a map of interval sheaves $\phi_{*}: C^{n} \rightarrow C^{n}$ given by post-composition with $\phi$ : we define $\left(\phi_{*}\right)_{\ell}(f):=\phi \circ f$ for $f:(0, \ell) \rightarrow \mathbb{R}$ in $C^{n}(\ell)$. Naturality then follows from associativity of composition.

If we think of $f \in C^{n}$ as a real number varying in time, then $\phi_{*}(f)$ is its image under the function $\phi$.

In order for interval sheaves to give a behavioral doctrine, we need to be able to take pullbacks of interval sheaves. Luckily, pullbacks of interval sheaves can be taken componentwise in the category of sets.

Proposition 6.2.3.8. Let $f: X \rightarrow Z$ and $g: Y \rightarrow Z$ be maps of interval sheaves. Then their pullback $X \times_{Z} Y$ may be defined at $\ell$ by taking the pullback of $f_{\ell}: X(\ell) \rightarrow Z(\ell)$ and $g_{\ell}: Y(\ell) \rightarrow Z(\ell)$ in the category of sets.

$$
\left(X \times_{Z} Y\right)(\ell):=X(\ell) \times_{Z(\ell)} Y(\ell)
$$

The terminal interval sheaf is defined by $1(\ell)=1$ or all $\ell$.

Definition 6.2.3.9. The behavioral doctrine of interval sheaves is the finitely complete category $\operatorname{Sh}(\mathbb{I})$.

Exercise 6.2.3.10. Go through Section 6.2.1 and adapt the story to work within the behavioral doctrine of interval sheaves. What has to change, and what remains the same?

For the rest of this section, we will show that trajectories in differential systems theories land in the behavioral doctrine of interval sheaves.

Theorem 6.2.3.11. There is a doubly indexed functor

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-290.jpg?height=317&width=634&top_left_y=1937&top_left_x=735)

sending a system in the general differential doctrine to its interval sheaf of trajectories.

We begin by including the interval category $\mathbb{I}$ into the category Sys $_{\mathbb{D D F F}_{\text {IFF }}}$ of systems and behaviors in the general differential doctrine. We will then show that we get a
doubly indexed functor landing in interval presheaves. We will then note that these presheaves are in fact sheaves - they satisfy the gluing condition.

Lemma 6.2.3.12. There is an inclusion $\iota: \mathbb{I} \rightarrow$ Sys $_{\mathbb{D}_{\mathrm{IFF}}}$ sending each interval $\ell$ to the system

$$
\left(\begin{array}{c}
1 \\
\mathrm{id}
\end{array}\right):\left(\begin{array}{c}
T(0, \ell) \\
(0, \ell)
\end{array}\right) \leftrightarrows\left(\begin{array}{c}
1 \\
(0, \ell)
\end{array}\right)
$$

and every morphism $a: \ell^{\prime} \rightarrow \ell$ to the square

$$
\begin{aligned}
& \left(\begin{array}{c}
T\left(0, \ell^{\prime}\right) \\
\left(0, \ell^{\prime}\right)
\end{array}\right) \xrightarrow{\left.\begin{array}{c}
T(a+) \\
(a+)
\end{array}\right)}\left(\begin{array}{c}
T(0, \ell) \\
(0, \ell)
\end{array}\right) \\
& \left(\begin{array}{c}
1 \\
\mathrm{id}
\end{array}\right) \downarrow \uparrow \\
& \left(\begin{array}{c}
1 \\
\left(0, \ell^{\prime}\right)
\end{array}\right) \xrightarrow[\left(\begin{array}{c}
1 \\
\mathrm{id}
\end{array}\right)]{\longrightarrow}\left(\begin{array}{c}
1 \\
(0, \ell)
\end{array}\right)
\end{aligned}
$$

Proof. The square commutes since the derivative of the function $x \mapsto a+x$ is 1 . The assignment is functorial by the definition of composition in the interval category.

We can use this inclusion of $\mathbb{I}$ into Sys $_{\mathbb{D I F F}_{\mathrm{IF}}}$ to show that the trajectories form interval sheaves. A trajectory $\gamma$ of length $\ell$ in a system $\mathrm{S}$ is a behavior of shape $\iota(\ell)$ in $\mathrm{S}$ :

$$
\begin{aligned}
& \left(\begin{array}{c}
T(0, \ell) \\
(0, \ell)
\end{array}\right) \xrightarrow{\left(\begin{array}{c}
T \gamma \\
\gamma
\end{array}\right)}\left(\begin{array}{c}
T \text { States } \\
\text { States }
\end{array}\right) \\
& \left.\left(\begin{array}{c}
1 \\
\mathrm{id}
\end{array}\right) \downarrow \uparrow \quad \downarrow \right\rvert\,\left(\begin{array}{l}
\text { update }_{S} \\
\text { expose }_{S}
\end{array}\right) \\
& \left(\begin{array}{c}
1 \\
(0, \ell)
\end{array}\right) \underset{\left(\begin{array}{c}
f_{b} \\
f
\end{array}\right)}{\rightrightarrows}\left(\begin{array}{c}
\ln _{\mathrm{S}} \\
\text { Outs }
\end{array}\right)
\end{aligned}
$$

We can restrict trajectories by pre-composition:

$$
\begin{aligned}
& \left(\begin{array}{c}
T\left(0, \ell^{\prime}\right) \\
\left(0, \ell^{\prime}\right)
\end{array}\right) \xrightarrow{\left(\begin{array}{c}
T(a+) \\
(a+)
\end{array}\right)}\left(\begin{array}{c}
T(0, \ell) \\
(0, \ell)
\end{array}\right) \xrightarrow{\left(\begin{array}{c}
T \gamma \\
\gamma
\end{array}\right)}\left(\begin{array}{c}
T \text { States } \\
\text { States }
\end{array}\right) \\
& \left(\begin{array}{c}
1 \\
\mathrm{id}
\end{array}\right) \downarrow \uparrow \quad \downarrow \uparrow\left(\begin{array}{c}
1 \\
\mathrm{id}
\end{array}\right) \quad \downarrow \uparrow\left(\begin{array}{c}
\text { update }_{\mathrm{s}} \\
\text { expose }_{\mathrm{S}}
\end{array}\right)
\end{aligned}
$$

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-291.jpg?height=195&width=696&top_left_y=2315&top_left_x=671)

Explicitly, we will make the following definition.

Definition 6.2.3.13. For a system $S \in \operatorname{Sys}_{\mathbb{D I F F}^{I F}}\left(\begin{array}{l}I \\ O\end{array}\right)$ in the general differential doctrine, we define the interval (pre)sheaf of trajectories Traj(S) by

$$
\begin{aligned}
& \operatorname{Traj}(S)(\ell):=\operatorname{Behave}_{t(\ell)}(\mathrm{S})
\end{aligned}
$$

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-292.jpg?height=455&width=656&top_left_y=561&top_left_x=800)

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-292.jpg?height=92&width=1453&top_left_y=1041&top_left_x=328)
by

$$
\left(\gamma,\left(\begin{array}{c}
f_{b} \\
f
\end{array}\right)\right) \mapsto\left(\begin{array}{c}
f_{b} \\
f
\end{array}\right)
$$

Restriction is given by precomposition.

Exercise 6.2.3.14. Check that the definition of $\operatorname{Traj}(\mathrm{S})$ in Definition 6.2.3.13 really gives an interval presheaf. Then check that the projection $\operatorname{Traj}(S) \rightarrow \operatorname{Arena}_{\mathbb{D} \text { IfF }}\left(l^{-,}\left(\begin{array}{l}I \\ O\end{array}\right)\right)$ is a map of interval presheaves.

We can, at this point, show that we are in fact working with interval sheaves. The rest of the proof does not depend on this, since maps and pullbacks of interval sheaves are the same as those of interval presheaves. But it will be nice to get it out of the way.

Lemma 6.2.3.15. For a system $S \in \operatorname{Sys}_{\mathbb{D I F F}}\left(\begin{array}{l}I \\ O\end{array}\right)$ in the general differential doctrine, the interval presheaf of trajectories Traj(S) is a sheaf.

Proof. We will show that Traj)(S) satisfies the two conditions of Lemma 6.2.3.3:

1. If two trajectories are equal on every strict subinterval, then they are in particular equal at every point, and are therefore equal.

2. If we have a compatible family of trajectories $\gamma_{a}$ on every strict subinterval $a: \ell^{\prime} \leadsto$ $\ell$, then we can define a trajectory $\gamma$ by $\gamma(t)=\gamma_{a_{t}}(t)$ for some strict subinterval $a_{t}: \ell_{t} \leadsto \ell$ containing $t$. This is well defined by the compatibility of the family $\gamma_{a}$.

To show that the rest of the doubly indexed functor lands correctly in interval sheaves all comes down to this: in defining the doubly indexed functors Behave ${ }_{l(\ell)}$, we only compose on the right. Since our interval sheaf structure is given by composing on the left, the various parts of the doubly indexed functor will give rise to sheaf maps by the associativity of left and right composition.

\subsection*{6.2.4 Further Reading in the Behavioral Doctrine}

I have only given the briefest sketch of what can be done in the behavioral doctrine here. In this subsection, I would like to suggest some further reading on this doctrine of dynamical systems.

The behavioral doctrine of dynamical systems is named for Jan Willems' behavioral approach to systems theory, which was put forward in the paper [Wil87] and expanded significantly in subsequent papers (see e.g. the book [WP13]). For a nice introduction, see [Wi107; WP13].

A central reference, and one that we drew on in Section 6.2.3, is the book Temporal Type Theory by David I. Spivak and Patrick Schultz [SS19]. For more detailed examples of how interval sheaves can be used to describe the behavioral approach to dynamical systems, see [SSV16].

John Baez and his students and collaborators have produced a great deal of wonderful work within the behavioral doctrine. See for example [BF18][BM20][BP17][BFP16][BE15][BC18]. Often, these papers also work within the port-plugging doctrine and describe "black boxing functors" which take the behaviors of port-plugging systems, naturally landing the in the behavioral doctrine.

For an interesting and deep examples of behavioral theories, see Baez, Weisbart, and Yassine's Open Systems in Classical Mechanics [BWY17]. In this paper, the authors construct categories of Hamiltonian and (respectively) Lagrangian spans, and express the Legendre transform as a functor from the Lagrangian to the Hamiltonian category. While this does not exactly fit into the formalism presented here for technical reasons (namely, the category of manifolds does not have all pullbacks), it is close enough for the same reasoning to apply. I expect that by expanding the class of objects considered by Baez, Weisbart, and Yassine, from manifolds to some more convientient category of differential spaces, one could see these Hamiltonian and Lagrangian systems as theories in the behavioral doctrine proper. Another approach would be to expand the behavioral doctrine to allow for categories that might not have all pullbacks, but still admit some sort of span double category.

\subsection*{6.3 Drawing Systems: The Port Plugging Doctrine}

There is another approach to systems modelling which is very common in the sciences: drawing diagrams! Diagrams help express the structure of complex systems in ways that can be appreciated visually.

Consider, for example, a circuit diagram:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-294.jpg?height=238&width=374&top_left_y=328&top_left_x=867)

L

In Example 3.2.1.9 we saw how we could use Kirchoff's laws to interpret this circuit as a differential system

$$
\left(\begin{array}{c}
\text { update }_{\mathrm{RL}} \\
\text { id }
\end{array}\right):\left(\begin{array}{l}
\mathbb{R} \\
\mathbb{R}
\end{array}\right) \leftrightarrows\left(\begin{array}{c}
\mathbb{R}^{2} \times \mathbb{R}^{*} \\
\mathbb{R}
\end{array}\right)
$$

where

$$
\operatorname{update}_{\mathrm{RL}}\left(I,\left[\begin{array}{l}
V \\
R \\
L
\end{array}\right]\right):=\frac{V-R I}{L}
$$

But why not consider the circuit itself as a system? This is a different way of thinking about systems: the circuit is a diagram, it doesn't have a set of states, exposed variables of state, and it doesn't update according to parameters. Nevertheless, we can compose circuits together to get more complex circuits. For example, we can think of the circuit (6.15) as the composite of two smaller circuits:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-294.jpg?height=287&width=965&top_left_y=1472&top_left_x=577)

We compose circuit diagrams by gluing their wires together - just like we might actually solder two physical circuits together. Another example of a system like circuit diagrams is a population flow graph (as, for example, in Definition 1.3.2.8). A simple population flow graph consists of a graph whose vertices are places and whose edges are paths between places, each labeled by its flow rate.

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-294.jpg?height=236&width=610&top_left_y=2121&top_left_x=752)

We can compose population flow graphs by gluing places together. Fore example, we can think of population flow graph (6.17) as the composite of two smaller population
flow graphs:
![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-295.jpg?height=246&width=1750&top_left_y=332&top_left_x=338)

(6.18)

We have added in the connection between New York and Tallahassee by gluing together the places in these two population flow graphs.

How can we describe this kind of composition in general? Instead of exposing variables of state, systems like circuit diagrams and population flow graphs expose certain parts of themselves (the ends of wires, some of their places) to their environment. We can refer to these parts of a circuit-diagram like system as its ports. The ports form the interface of this sort of system.

For now, let's suppose that a system $\mathrm{S}$ has a finite set of ports Portss, which acts as its interface. For example, we can see the ports of the open circuit diagram on the left of (6.16):

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-295.jpg?height=377&width=586&top_left_y=1167&top_left_x=756)

We can see the set of ports as a trivial sort of circuit diagram - one with no interesting components of any kind - which has been included into the circuit diagram $S$. That is, the way we describe an interface is by a map $\partial_{\mathrm{S}}: L$ Ports $\rightarrow S$ which picks out the ports in the system $S$, and where $L$ is some operation that takes a finite set into a particularly simple sort of system.

Suppose we want to describe the composition of Eq. (6.16). This will compose system $\mathrm{S}$ with the system $\mathrm{T}$ :

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-295.jpg?height=328&width=322&top_left_y=1953&top_left_x=888)

Just like with the parameter setting systems we've been composing the whole book, we will compose these two systems first by considering them together as a joint system, and then composing them according to a composition pattern. Here, the composition pattern should tell us which ports get glued together, and then which of the resulting
things should be re-exposed as ports. For this, we will use a cospan:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-296.jpg?height=453&width=748&top_left_y=321&top_left_x=686)

Here, the composite system exposes no ports, so we leave its set of ports empty. But with the map on the left, we show how we want to glue the ports of $S$ and $T$ together. To actually get the composite system, we actually glue these ports along this plan. Gluing objects together in a category means taking a pushout:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-296.jpg?height=325&width=941&top_left_y=1060&top_left_x=581)

The symbol + here is denoting the coproduct, or disjoint union, which lets us put our circuit diagrams side by side.

We can describe systems like circuit diagrams and population flow graphs which are composed using pushouts in the above way port-plugging systems. The idea with these systems is that they expose some ports, and we compose them by plugging the ports of one system into the ports of another - gluing them together using pushouts.

Definition 6.3.0.1. A doctrine for the port-plugging doctrine is a category $\mathscr{D}$ with finite colimits, which we can think of as a category of diagrams.

The port-plugging doctrine $\mathfrak{P}$ ortPlugGing which encapsulates the diagrammatic approach to systems theory is the functor which sends each port-plugging doctrine $\mathscr{D}$ to the vertical slice construction of the inclusion of the initial object into the double category of cospans in $\mathscr{D}$ :

$$
\operatorname{Sys}_{\mathscr{D}}^{\mathfrak{P O R T P L U G G I N G}}:=\sigma(1 \rightarrow \operatorname{Cospan}(\mathscr{D}))
$$

This definition of the port plugging doctrine answers the questions of Informal Definition 6.1.0.1 in the following ways:

1. A system is a diagram $D$ in $\mathscr{D}$ together with an map $\partial: I \rightarrow D$ picking out the interface of the diagram - the parts of it which are considered to be exposed to the environment.

2. An interface is a diagram $I$ which may be included as an exposed part of another diagram. That is, I consists of the ports of a diagram.

3. Interfaces are connected by cospans which describe which parts of the interfaces are to be glued together.

4. Systems are composed by gluing their interfaces together, that is, by plugging the ports of one system into those of another. This is accomplished by cospan composition.

5. A map between systems is a map of diagrams which acts in a specified way on their interfaces.

6. Maps between systems can be composed along the composition patterns when we have a square in the double category of cospans.

There is a close formal analogy between the diagrammatic and the behavioral approaches to systems theory: Eq. (6.22) is the same diagram as Eq. (6.7), just with all the arrows going the other way around. This means we can use bubble diagrams to describe composites of diagrams as well!

\subsection*{6.3.1 Port-plugging systems theories: Labelled graphs}

Let's work out a class of port-plugging theories to get a sense for how it feels to work within the doctrine. Most of the examples we gave above were graphs whose nodes and edges were labebelled with some sort of data. We can formalize this situation in general.

First, let's recall what a graph is, for a category theorist. There are many different flavors of graph, and what category theorists tend to prefer would be called directed multi-graphs with loops by more traditional graph theorists. We will just call them graphs.

Definition 6.3.1.1. A graph $G$ consists of a set $G_{0}$ of nodes, a set $G_{1}$ of edges, and functions $s, t: G_{1} \rightrightarrows G_{0}$ sending each edge $e$ to its source node $s(e)$ and target node $t(e)$. We write $e: a \rightarrow b$ to say that $s(e)=a$ and $t(e)=b$.

A graph map $\varphi: G \rightarrow H$ consists of two functions $\varphi_{0}: G_{0} \rightarrow H_{0}$ and $\varphi_{1}: G_{1} \rightarrow H_{1}$ sending nodes to nodes and edges to edges which commute with source and target. That is, $\varphi_{0}(s(e))=s\left(\varphi_{1}(e)\right)$ and same for $t$. In other words, if $e: a \rightarrow b$ in $G$, then $\varphi_{1}(e): \varphi_{0}(a) \rightarrow \varphi_{0}(b)$ in $H$. We'll usually refer to both $\varphi_{0}$ and $\varphi_{1}$ as $\varphi$ as long as it isn't confusing to do so.

We denote the category of graphs by Graph.

Example 6.3.1.2. Here's an example of a graph $G$ :

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-298.jpg?height=341&width=295&top_left_y=350&top_left_x=880)

We can describe this using Definition 6.3 .1 by setting $G_{0}:=\{1,2,3\}$ and $G_{1}:=$ $\{a, b, c, d\}$, together with

$$
\begin{array}{ll}
s(a)=1 & t(a)=2 \\
s(b)=2 & t(b)=3 \\
s(c)=2 & t(c)=3 \\
s(d)=2 & t(d)=2
\end{array}
$$

We should emphasize that the names we have adorned the picture $G$ with are just that: names. They are unique identifiers for each node and edge in $G$, not labels (which might be shared by different nodes and edges). We'll soon see a definition of a labelled graph which will make this disctinction more stark.

Using some category theory, we can expedite our understanding of the category of graphs.

Proposition 6.3.1.3. The category Graph of graphs is the category of presheaves on the category $0 \rightrightarrows 1$ consisting of two objects 0 and 1 and two arrows $s$ and $t$ from 0 to 1 .

Proof. This is a matter of checking definitions against eachother. A presheaf $G$ on that small category would consists of two sets $G(0)$ and $G(1)$ together with two functions $G(s), G(t): G(1) \rightrightarrows G(0)$ - precisely a graph. Furthermore, a natural transformation between these presheaves will be a graph map.

As a corollary, we note that the category of graphs has all limits and colimits, and that they may be calculated in the category of sets. That is, the (co)limit of a diagram of graphs has as nodes the (co)limit of the diagram of sets of nodes, and similarly for its edges. In particular, the category of graphs is has all finite colimits.

Corollary 6.3.1.4. The category Graph has all finite colimits. The empty graph has no nodes or edges, and the pushout of graphs is the graph with nodes the pushout of the nodes and edges the pushout of the edges.

In effect, what this means is that taking the pushout of graphs means gluing them together.

Example 6.3.1.5. Consider the graph $\bullet \rightarrow \bullet$ with two nodes and a single edge from one to the other. There are two maps from the graph $\bullet$ having just one node into $\bullet \rightarrow \bullet$ which we might call $s$ and $t$; the first picks out the source of the edge, and the second picks out the target. We can then form the following pushout square:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-299.jpg?height=203&width=486&top_left_y=522&top_left_x=814)

The two maps from $\bullet \rightarrow \bullet$ to $\bullet \rightarrow \bullet \rightarrow \bullet$ include it as the first edge and second edge respectively. The fact that the square commutes means that the target of the first edge is the source of the second edge. That this is a pushout means that to map out of $\bullet \rightarrow \bullet \rightarrow \bullet$, it suffices to give two maps out of $\bullet \rightarrow \bullet$ which send the target of the first to the same place as the source of the second.

As you can see, taking the pushout glues together the two graphs over their shared part.

We are interested in labelled graphs. We will give a general definition of labelled graphs in the upcoming Definition 6.3.1.12, but for now we make the following defintion of two important special cases.

Definition 6.3.1.6. Let $G$ be a graph and $L$ a set of labels. Then

1. An edge labelling of $G$ in $L$ is a function $\ell: G_{1} \rightarrow L$.

2. A node labelling of $G$ in $L$ is a function $\ell: G_{0} \rightarrow L$.

Example 6.3.1.7. The transition diagrams we drew as way back in Example 1.2.1.8 to

describe our deterministic systems can be seen as labelled graphs. An $\left(\begin{array}{l}I \\ O\end{array}\right)$-system $S$ will be a graph with nodes States and an edge $s \rightarrow s^{\prime}$ for each $i \in I$ with update ${ }_{S}(s, i)=$ $s^{\prime}$. This will have a node-labeling given by expose $_{S}$ and an edge labelling given by sending the edge $s \rightarrow s^{\prime}$ corresponding to input $i$ to $i$.

Example 6.3.1.8. We can view a graph with edge labels in the set $(0, \infty)$ of positive real numbers as a network of connections $c$ with capacity $\ell(c)$. Or, we can see such a labelling as telling us the flow which is currently moving through the connection $c$. There are many ways we could use such a labelling.

Example 6.3.1.9. We can see an RL-circuit such as this one from Example 3.2.1.9

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-300.jpg?height=253&width=379&top_left_y=345&top_left_x=865)

L

as a labelled graph, with edge labels in the set $\{V, R, L\} \times(0, \infty)+\{W\}$. A plain wire will be labelled $W$, while a voltage source will be labelled $(V, v)$ where $v \in(0, \infty)$ is the voltage, and similarly for resistors and inductors.

In order to better understand the categories of labelled graphs, we can re-interpret the definition of labelled graphs in terms of graph maps. First, we need to describe a few special graphs.

Definition 6.3.1.10. Let $L$ be a set. We describe two important graphs built out of $L$ :

1. First, the graph $E L$ has a single node with all the elements of $L$ as edges. That is, $E L_{0}=1$ and $E L_{1}=L$, with $s$ and $t$ both being the unique function from $L$ to 1 .

2. Second, the graph $K L$ - the complete graph on $L-$ has nodes $L$ and a single edge from each node to each other node (including from a node to itself). Formally, $K L_{0}=L$ and $K L_{1}=L \times L$, with $s$ and $t$ the first and second projection $L \times L \rightarrow L$.

Now we can re-interpret Definition 6.3.1.6

Proposition 6.3.1.11. Let $G$ be a graph and $L$ a set of labels.

1. An edge labelling of $G$ in $L$ is a graph map $\ell: G \rightarrow E L$.

2. A node labelling of $G$ in $L$ is a graph map $\ell: G \rightarrow K L$.

Proof. First, let's consider edge labellings. An edge labelling of $G$ in $L$ is a function $\ell: G_{1} \rightarrow L$; given such a function, we can make a map $\hat{\ell}: G \rightarrow E L$ by defining $\hat{\ell}_{1}=\ell$ and $\hat{\ell}_{0}$ to be the unique function $G_{0} \rightarrow 1$. Conversely, any graph map $\ell: G \rightarrow E L$ gives us $\ell_{1}: G_{1} \rightarrow L$. These two processes are inverse, because there is a unique function $G_{0} \rightarrow 1$.

The case of node labellings is very similar. Let $\ell: G_{0} \rightarrow L$ be a node labelling. We can then define a map $\hat{\ell}: G \rightarrow K L$ by $\hat{\ell}_{0}=\ell$ and $\hat{\ell}_{1}(e)=(\ell(s(e)), \ell(t(e)))$. Conversely, for any map $\ell: G \rightarrow K L$ we have $\ell_{0}: G_{0} \rightarrow L$. These two processes are inverse since any edge $e: a \rightarrow b$ must be sent to a edge $\ell(a) \rightarrow \ell(b)$, but there is exactly one such edge.

This reframing justifies us generaling the notion of labelling to allow values in any graph.

Definition 6.3.1.12. Let $\mathcal{L}$ be a graph. A graph labelled in $\mathcal{L}$ is a graph $G$ together with a labelling map $\ell: G \rightarrow \mathcal{L}$.

A map of $\mathcal{L}$-labelled graphs $G \rightarrow H$ is a map $\varphi: G \rightarrow H$ which preserves labels in the sense that $\varphi ; \ell_{H}=\ell_{G}$. We denote the category of $\mathcal{L}$-labelled graphs by $\mathbf{G r a p h}_{\mathscr{L}}$. Category theoretically, the category $\operatorname{Graph}_{\mathscr{L}}$ is the slice category of Graph over $\mathcal{L}$.

Example 6.3.1.13. Continuing Example 6.3.1.7, we can think of a transition diagram for an $\left(\begin{array}{l}I \\ O\end{array}\right)$-system as a $K O \times E L$ labelled graph. By the universal propoerty of the product, a labelling in $K O \times E I$ is a labelling in $K O$ together with a labelling in $E I$, which is to say a node labelling in $O$ together with an edge labelling in $I$.

We can think of a general graph $\mathcal{L}$ as giving us a system of labels with constraints. The nodes of $\mathcal{L}$ are the possible node-labels, and the edges of $\mathcal{L}$ are the possible edge labels. But an edge label is constrained to go between two node labels. Therefore, the way the edges are linked together constrains what sort of labels an edge might have given the labels its source and target have.

Example 6.3.1.14. Let $\mathcal{L}$ be the graph $0 \rightarrow 1$ with two nodes and a single edge between them. A $\mathcal{L}$-labelled graph is a bipartite graph. That is, a graph $G$ with a map $\ell: G \rightarrow \mathcal{L}$ divides the nodes of $G$ in two - those with $\ell(n)=0$ and those with $\ell(n)=1$ - and there can only be edges from a node labelled 0 to a node labelled 1.

As a corollary of our abstract description of labelled graphs, we can see quite quickly that the category of labelled graphs has finite colimits for any labelling graph $\mathcal{L}$.

Proposition 6.3.1.15. For any graph $\mathcal{L}$ of labels, the category Graph $_{\mathscr{L}}$ of graphs labelled in $\mathcal{L}$ has all finite colimits which can be calculated in Graph.

Proof. This is a general fact concerning slice categories, see for example Proposition 3.3.8 of [Rie17].

Going further, if we have any map $f: \mathcal{L} \mathcal{L}^{\prime}$ of label graphs, we get a functor

$$
f_{*}: \operatorname{Graph}_{\mathscr{L}} \rightarrow \operatorname{Graph}_{\mathscr{L}^{\prime}}
$$

given by sending $\ell: G \rightarrow \mathcal{L}$ to $\ell: f: G \rightarrow \mathcal{L}^{\prime}$. This functor preserves finite colimits, since by Proposition 6.3.1.15 we may calculate these on the underlying graphs without reference to the labelling. For this reason, we get a functor

$$
\text { Graph }_{(-)}: \text {Graph } \rightarrow \text { FinCoCompleteCat }
$$

This will let us define the doctrine of labelled graphs.

Definition 6.3.1.16. A theory for the doctrine of labelled graphs is a graph of labels $\mathcal{L}$.

The doctrine of labelled graphs $\mathfrak{L}_{\text {ABELLEDGRAPHS }}$ is the functor that sends a graph $\mathcal{L}$ of labels to the vertical slice construction of the inclusion of the empty graph into the double category of cospans in the category $\mathbf{G r a p h}_{\mathscr{L}}$ of graphs labelled in $\mathcal{L}$ :

$$
\operatorname{Sys}_{\mathscr{L}}^{\mathfrak{L}_{\text {ABELLEDGRAPHS }}}:=\sigma\left(1 \rightarrow \operatorname{Cospan}\left(\operatorname{Graph}_{\mathscr{L}}\right)\right) .
$$

The doctrine of labelled graphs is a restriction of the port-plugging doctrine Definition 6.3.0.1. For that reason, it answers the questions of Informal Definition 6.1.0.1 in much the same way.

1. A system is a labelled graph $\ell: G \rightarrow \mathcal{L}$ in $\operatorname{Graph}_{\mathscr{L}}$ together with an map $\partial: I \rightarrow G$ picking out the interface of the diagram - the parts of it which are considered to be exposed to the environment.

2. An interface is a labelled graph $\ell: I \rightarrow \mathcal{L}$ which may be included as an exposed part of another labelled graph. That is, $I$ consists of the ports of a diagram.

3. Interfaces are connected by cospans which describe which parts of the interfaces are to be glued together. These cospans respect the labelling.

4. Systems are composed by gluing their interfaces together, that is, by plugging the ports of one system into those of another. This is accomplished by cospan composition.

5. A map between systems is a map of labelled graphs which acts in a specified way on their interfaces.

6. Maps between systems can be composed along the composition patterns when we have a square in the double category of cospans.

The examples we saw in the introduction to this section can all be seen as labelled graphs, so we have seen how composition works in the doctrine of labelled graphs. But we still need to see how we can use bubble diagrams to describe composition patterns in the port-plugging doctrine.

\subsection*{6.3.2 Bubble diagrams for the port-plugging doctrine}

In the definition of the port-plugging paradigm, we take the double category of interfaces in a theory (that is, a finitely cocomplete category "of diagrams") $\mathscr{D}$ to be the double category of cospans $\operatorname{Cospan}(\mathscr{D})$ in $\mathscr{D}$. The thing about this double category is that it is not spanlike in the sense of Definition 5.3.1.5.1 But, somewhat remarkably, the double category of bubble diagrams Bubble $_{\mathcal{T}}$ of Definition 6.2.2.10 is spanlike. So, if we use bubble diagrams to compose our port-plugging systems, we can take advantage of Theorem 5.3.2.2 to construct representable functors in this doctrine as well. But first, let's describe how we can use bubble diagrams in the port-plugging doctrine in the first place.
\footnotetext{
${ }^{1}$ It is, as you might guess, cospanlike instead.
}

We can use bubble diagrams to describe composition in the port-plugging doctrine, just like we did for the behavioral doctrine. In Section 6.2.2, we exploited the fact that Arity was the free category with finite limits generated by a single object to interpret spans in Arity in any category with finite limits. Since we defined bubble diagrams to be spans in Arity (or its many-typed variants) with left leg surjective and right leg injective, this let us interpret bubble diagrams in any category with finite limits.

But Arity $\cong$ FinSet $^{\mathrm{op}}$ is the opposite of the category of finite sets. This means that FinSet is the free category with finite colimits generated by a single object. We can see bubble diagrams as certain cospans in FinSet, which is arguably a more direct way to understand what they are. For this reason, we can interpret bubble diagrams in any category with finite colimits, allowing us to use them to describe composition in the port-plugging doctrine.

Proposition 6.3.2.1. The category of typed finite sets FinSet $_{\mathcal{T}}$ is the free category with finite colimits on the set of objects $\mathfrak{T}$. As a corollary, for every function $P: \mathscr{T} \rightarrow \mathscr{D}$ from $\mathscr{T}$ to a finitely cocomplete category $\mathscr{D}$, we get a double functor

$$
\mathrm{ev}_{P}: \operatorname{Cospan}\left(\text { FinSet }_{\mathscr{T}}\right) \rightarrow \operatorname{Cospan}(\mathscr{D})
$$

which sends $\tau: X \rightarrow \mathscr{T}$ to the coproduct $\sum_{x \in X} P_{\tau x}$ in $\mathscr{D}$.

Given a collection $P: \mathfrak{T} \rightarrow \mathscr{D}$ of ports, Proposition 6.3.2.1 will let us restrict the double indexed category

$$
\sigma(1 \xrightarrow{0} \operatorname{Cospan}(\mathscr{D}))=(-) / \mathscr{D}: \operatorname{Cospan}(\mathscr{D}) \rightarrow \text { Cat }
$$

along the double functor Bubble $_{\mathcal{T}} \hookrightarrow \operatorname{Cospan}\left(\right.$ FinSet $\left._{\mathscr{T}}\right) \xrightarrow{\mathrm{ev}_{P}} \operatorname{Cospan}(\mathscr{D})$.

Let's now prove that Bubble $_{\mathcal{T}}$ is spanlike. We will need a crucial lemma.

Lemma 6.3.2.2. Let $I \rightarrow L \leftarrow M$ and $M \rightarrow L^{\prime} \leftarrow O$ be bubble diagrams. Then in the following diagram describing their composite:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-303.jpg?height=304&width=746&top_left_y=1889&top_left_x=684)

the middle square is a pullback in addition to being a pushout.

Proof. This follows from the fact that FinSet $_{\mathcal{T}}$ is an adhesive category which implies in particular that any pushout of a monomorphism is also a pullback. We'll prove this fact directly.

We will show that $M$ is bijective with the pullback $L \times_{L+{ }_{M} L^{\prime}} L^{\prime}$ via the map $m \mapsto$ $\left(o_{1}(m), i_{2}(m)\right)$. Suppose we have $\ell \in L$ and $\ell^{\prime} \in L^{\prime}$ with $[\ell]=\left[\ell^{\prime}\right]$ in $L+_{M} L^{\prime}$, seeking to show that there is a unique $m \in M$ for which $\ell=o_{1}(m)$ and $i_{2}(m)=\ell^{\prime}$. First, we note that uniqueness follows immediately from the assumption that $o_{1}$ is injective; if $\ell=o_{1}(m)$ and $\ell=o_{1}\left(m^{\prime}\right)$, then we may conclude that $m=m^{\prime}$. So it remains to show that there is any such $m$.

We know that $[\ell]=\left[\ell^{\prime}\right]$ in $L+{ }_{M} L^{\prime}$, so we know that there is a zig-zag of elements in $L$ and $L^{\prime}$, each related by an element of $M$, which connect $\ell$ and $\ell^{\prime}$. We can show that this zig-zag may be taken to have length 1 , so that $\ell$ and $\ell^{\prime}$ are directly connected by a single $m \in M$.

Suppose that we have a zig-zag

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-304.jpg?height=208&width=981&top_left_y=888&top_left_x=564)

Note that all the middle terms have $o_{1} m_{2 i}=o_{1} m_{2 i+1}$; by the injectivity of $o_{1}$, this implies that $m_{2 i}=m_{2 i+1}$ so that in fact there is a single $m$ directly connecting $\ell$ and $\ell^{\prime}$.

Now we can prove that the double category of bubble diagrams is spanlike.

Theorem 6.3.2.3. The double category Bubble $_{\mathcal{T}}$ of bubble diagrams (with wires typed in the set $\mathfrak{T}$ ) is spanlike.

Proof. We have to show that any square into the composite of two bubble diagrams factors uniquely as the vertical composite of two squares. Consider the following diagram:

![](https://cdn.mathpix.com/cropped/2024_02_19_47a3035bd8e46d0763f1g-304.jpg?height=561&width=683&top_left_y=1714&top_left_x=713)

The solid diagram is a square $\alpha$ from $D$ to the composite of $I \rightarrow L \leftarrow M$ and $M \rightarrow$ $L^{\prime} \leftarrow O$. The dashed arrows are uniquely determined by compsing $D \rightarrow I \rightarrow L$ and $D \rightarrow O \rightarrow L^{\prime}$ respectively, and because the two solid squares commute, they are both equalized when composed with the inclusions into $L+{ }_{M} L^{\prime}$. Then, by Lemma 6.3.2.2,
there is a unique map $D \rightarrow M$ making the diagram commute; but this is precisely the splitting of $\alpha$ into two squares that we needed.

Because of Theorem 6.3.2.3, we can use Theorem 5.3.2.2 to construct representable lax doubly indexed functors in the port-pluggin doctrine. As an example of such a functor, consider the theory of (unlabelled) graphs and the system $\mathrm{P}_{3}=\bullet \rightarrow \bullet \rightarrow$ $\bullet \rightarrow \bullet$, with interface $\bullet \bullet$ included as its endpoints, in this theory. This system $P_{3}$ represents paths of length 3, and so we get a lax doubly indexed functor sending a graph with boundary to the set of paths of length 3 from one boundary node to another in it. That is doubly indexed functor is lax and not taut reflects an important fact about the compositionality of graphs: when graphs are composed, new paths can appear which weren't possible before.

\subsection*{6.3.3 Further Reading in the port-plugging doctrine}

There has been a lot of work done in the port-plugging doctrine, and we have hardly scratched the surface.

In its categorical guise, this doctrine was innaugurated with Brendan Fong's work on decorated cospans [Fon15]. For examples of putting this theory to work, see [BF18; BFP16; BP17]. This was later expanded by Kenny Courser and John Baez to a theory of structured cospans in [BC20] (see [BCV22] for a detailed comparison between these approaches, and see [Pat23] for a unified and generalized approach using doubly indexed categories). John Baez and his students have used these theories in a variety of settings; see for example [BM20; BC18].

\section*{Bibliography}

[Bai75] Norman T J Bailey. The mathematical theory of infectious diseases. en. 2nd ed. Mathematics in Medicine series. London, England: Hodder Arnold, Sept. 1975 (cit. on p. 104).

[Bak21] Georgios Bakirtzis. "Compositional Cyber-Physical Systems Theory". PhD thesis. 2021. DOI: 10 . 18130 / XN8V - 5D89. URL: https : //libraetd. lib . virginia.edu/public_view/cr56n179w (cit. on p. 43).

[BC18] John C. Baez and Kenny Courser. â€œCoarse Graining Open Markov Processes". In: Theory and Applications of Categories 33.39 (2018), pp. 1223-1268 (cit. on pp. 281, 293).

[BC20] John C Baez and Kenny Courser. "Structured cospans". In: Theory and Applications of Categories 35.48 (2020) (cit. on p. 293).

[BCS09] RF Blute, JRB Cockett, and RAG Seely. "Cartesian differential categories". In: Theory and Applications of Categories 22.23 (2009), pp. 622-672 (cit. on p. 151).

[BCV22] John C. Baez, Kenny Courser, and Christina Vasilakopoulou. "Structured versus Decorated Cospans". In: Compositionality 4 (3 Sept. 2022). ISsN: 26314444. DOI: 10.32408/compositionality-4-3. URL: https://doi .org/10. 32408/compositionality-4-3 (cit. on p. 293).

[BE15] John C. Baez and Jason Erbele. "Categories in control". In: Theory and Applications of Categories 30 (2015), Paper No. 24, 836-881 (cit. on p. 281).

[BÃ©n67] Jean BÃ©nabou. "Introduction to bicategories". In: Reports of the Midwest Category Seminar. Berlin, Heidelberg: Springer Berlin Heidelberg, 1967, pp. 1-77. ISBN: 978-3-540-35545-8 (cit. on p. 97).

[BF18] John C. Baez and Brendan Fong. "A compositional framework for passive linear networks". In: Theory and Applications of Categories 33.38 (2018), pp. 1158-1222 (cit. on pp. 281, 293).

[BFP16] John C. Baez, Brendan Fong, and Blake S Pollard. "A compositional framework for Markov processes". In: Journal of Mathematical Physics 57.3 (2016) (cit. on pp. 281, 293).

[BM20] John C. Baez and Jade Master. "Open Petri nets". In: Mathematical Structures in Computer Science 30.3 (Mar. 2020), pp. 314-341. Issn: 1469-8072. DOI: 10.1017 /s0960129520000043. uRL: http : //dx . doi .org / 10 . 1017 / S0960129520000043 (cit. on pp. 281, 293).

[BP17] John C Baez and Blake S Pollard. "A compositional framework for reaction networks". In: Reviews in Mathematical Physics 29.09 (2017) (cit. on pp. 281, 293).

[BSF21] Georgios Bakirtzis, Eswaran Subrahmanian, and Cody H. Fleming. â€œCompositional Thinking in Cyberphysical Systems Theory". In: Computer 54.12 (2021), pp. 50-59. Dor: 10.1109/MC. 2021.3085532 (cit. on p. 43).

[BST19] Martin Bohner, Sabrina Streipert, and Delfim F.M. Torres. "Exact solution to a dynamic SIR model". In: Nonlinear Analysis: Hybrid Systems 32 (2019), pp. 228-238. Issn: 1751-570X. Dor: https : //doi . org/10.1016/j . nahs . 2018.12.005. URL: https://www. sciencedirect.com/science/article/ pii/S1751570X18301092 (cit. on p. 104).

[BVF21] Georgios Bakirtzis, Christina Vasilakopoulou, and Cody H. Fleming. "Compositional Cyber-Physical Systems Modeling". In: Electronic Proceedings in Theoretical Computer Science 333 (Feb. 2021), pp. 125-138. Dor: 10.4204/ eptcs. 333 . 9. URL: https://doi . org/10.4204\%2Feptcs . 333.9 (cit. on p. 43).

[BWY17] John C. Baez, David Weisbart, and Adam M. Yassine. "Open systems in classical mechanics". In: Journal of Mathematical Physics 62 (2017), p. 042902. URL: https : / /api . semanticscholar . org/CorpusID : 119583567 (cit. on p. 281).

[CC14] J. R. B. Cockett and G. S. H. Cruttwell. "Differential Structure, Tangent Structure, and SDG". In: Applied Categorical Structures 22 (2014), pp. 331417. Dor: $10.1007 /$ s10485-013-9312-0 (cit. on p. 203).

[CC17] J. R. B. Cockett and G. S. H. Cruttwell. Differential bundles and fibrations for tangent categories. 2017. arXiv: 1606.08379 [math. CT] (cit. on pp. 148, 203).

[Chu58] Alonzo Church. "Edward F. Moore. Gedanken-experiments on sequential machines. Automata studies, edited by C. E. Shannon and J. McCarthy, Annals of Mathematics studies no. 34, litho-printed, Princeton University Press, Princeton1956, pp. 129-153." In: The Journal of Symbolic Logic 23.1 (1958), pp. 60-60. Dor: 10.2307/2964500 (cit. on p. 43).

[Fon15] Brendan Fong. "Decorated cospans". In: Theory and Applications of Categories 30.33 (2015), pp. 1096-1120 (cit. on p. 293).

[Fos+07] J. Nathan Foster, Michael B. Greenwald, Jonathan T. Moore, Benjamin C. Pierce, and Alan Schmitt. "Combinators for Bidirectional Tree Transformations: A Linguistic Approach to the View-Update Problem". In: ACM Trans. Program. Lang. Syst. 29.3 (May 2007), 17-es. IssN: 0164-0925. Dor: 10.1145 / 1232420 . 1232424. URL: https://doi . org/10 . 1145/1232420. 1232424 (cit. on p. 43).

[FS19] Brendan Fong and David I. Spivak. An Invitation to Applied Category Theory: Seven Sketches in Compositionality. Cambridge University Press, 2019 (cit. on p. 4).

[Gra19] Marco Grandis. Higher Dimensional Categories. WORLD SCIENTIFIC, 2019. Dor: 10.1142 /11406. eprint: https : / / www . worldscientific . com/doi / pdf/10 . 1142/11406. uRL: https://www. worldscientific.com/doi/abs/ 10.1142/11406 (cit. on p. 135).

[Hed] Jules Hedges. Lenses for Philsophers. https : //julesh. com/2018/08/16/ lenses-for-philosophers/. Accessed: 2023-08-30 (cit. on p. 43).

[Hub61] Peter J Huber. "Homotopy theory in general categories". In: Mathematische Annalen 144 (1961), pp. 361-385 (cit. on p. 97).

[Jaz21] David Jaz Myers. "Double Categories of Open Dynamical Systems (Extended Abstract)". In: Electronic Proceedings in Theoretical Computer Science 333 (Feb. 2021), pp. 154-167. Issn: 2075-2180. Dor: 10.4204/eptcs. 333 . 11. URL: http://dx.doi .org/10.4204/EPTCS. 333.11 (cit. on p. 164).

[Kle65] H. Kleisli. "Every Standard Construction is Induced by a Pair of Adjoint Functors". In: Proceedings of the American Mathematical Society 16.3 (1965), pp. 544-546. ISSN: 00029939, 10886826. URL: http : / / www . jstor . org / stable/2034693 (visited on 09/02/2023) (cit. on p. 97).

[Law04] F William Lawvere. "Functorial Semantics of Algebraic Theories and Some Algebraic Problems in the context of Functorial Semantics of Algebraic Theories". In: Reprints in Theory and Applications of Categories 5 (2004), pp. 1121 (cit. on p. 41).

[Lor21] Fosco Loregian. (Co)end Calculus. London Mathematical Society Lecture NoteSeries. Cambridge University Press, 2021. Dor: 10 . 1017/9781108778657 (cit. on p. 135).

[Mog89] Eugenio Moggi. "Computational lambda-calculus and monads". In: [1989] Proceedings. Fourth Annual Symposium on Logic in Computer Science. 1989, pp. 14-23. DOI: 10.1109/LICS. 1989.39155 (cit. on p. 97).

[Mog91] Eugenio Moggi. "Notions of computation and monads". In: Information and Computation 93.1 (1991). Selections from 1989 IEEE Symposium on Logic in Computer Science, pp. 55-92. Issn: 0890-5401. Dor: https://doi . org/10. 1016/0890-5401(91)90052-4. URL: https : / /www . sciencedirect . com/ science/article/pii/0890540191900524 (cit. on p.97).

[MV18] Joe Moeller and Christina Vasilakopoulou. "Monoidal Grothendieck Construction". In: preprint (2018). arXiv: 1809.00727 (cit. on pp. 84, 85).

[Ngo17] Timothy Ngotiaoco. Compositionality of the Runge-Kutta Method. 2017. eprint: arXiv: 1707.02804 (cit. on p. 216).

[NS] Nelson Niu and David I. Spivak. Polynomial Functors: A mathematical theory of interaction. URL: \%5Curl\%7Bhttps : //topos . site/poly-book . pdf\%7D (cit. on pp. 160, 164).

[Ole83] Frank Joseph Oles. "A Category-theoretic approach to the semantics of programming languages". PhD thesis. Syracuse University, 1983 (cit. on p. 43).

[Par11] Robert Pare. "Yoneda theory for double categories". In: Theory and Applications of Categories 25.17 (2011), pp. 436-489 (cit. on p. 250).

[Pat23] Evan Patterson. Structured and decorated cospans from the viewpoint of double category theory. 2023. arXiv: 2304.00447 [math. CT] (cit. on p. 293).

[Per21] Paolo Perrone. Notes on Category Theory with examples from basic mathematics. 2021. arXiv: 1912.10642 [math.CT] (cit. on pp. 4, 97).

[Rie17] Emily Riehl. Category theory in context. Courier Dover Publications, 2017 (cit. on pp. 4, 289).

[Shu08] Michael Shulman. "Framed bicategories and monoidal fibrations". In: Theory and Applications of Categories 20 (2008), Paper No. 18, 650-738 (cit. on p. 85).

[Spi15] David I. Spivak. The steady states of coupled dynamical systems compose according to matrix arithmetic. 2015. eprint: arXiv: 1512.00802 (cit. on p. 250).

[Spi19] David I. Spivak. Generalized Lens Categories via functors e $^{\mathrm{p}} \rightarrow$ Cat. 2019. eprint: arXiv: 1908.02202 (cit. on p. 97).

[SS19] Patrick Schultz and David I. Spivak. Temporal Type Theory: A topos-theoretic approach to systems and behavior. Springer, BirkhÃ¤user, 2019 (cit. on pp. 263, 274, 281).

[SSV16] Patrick Schultz, David I Spivak, and Christina Vasilakopoulou. â€œDynamical systems and sheaves". In: Applied Categorical Structures (2016), pp. 1-57 (cit. on pp. 263, 274, 281).

[VSL15] Dmitry Vagner, David I. Spivak, and Eugene Lerman. "Algebras of open dynamical systems on the operad of wiring diagrams". In: Theory and Applications of Categories 30 (2015), Paper No. 51, 1793-1822 (cit. on p. 43).

[Wil07] Jan C Willems. "The behavioral approach to open and interconnected systems". In: IEEE Control Systems 27.6 (2007), pp. 46-99 (cit. on pp. 257, 281).

[Wil87] J.C. Willems. "From time series to linear system-Part I. Finite dimensional linear time invariant systems, Part II. Exact modelling, (Part III. Approximate modelling)". In: Automatica 22(23) (1986(1987)), 561-580, 675-694, (87-115) (cit. on p. 281).

[WP13] Jan C Willems and Jan W Polderman. Introduction to mathematical systems theory: a behavioral approach. Vol. 26. Springer Science \& Business Media, 2013 (cit. on p. 281).